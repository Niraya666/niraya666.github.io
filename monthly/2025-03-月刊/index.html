<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>2025-03 月刊 | LZY Blog</title>
<meta name="keywords" content="月刊">
<meta name="description" content="值得关注的模型和新技术
DeepSeek V3 0324
DeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。
Huggingface
QwQ-32B
由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。
Blog
Huggingface
Qwen2.5 Omni
Qwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。
Blog
Huggingface
Gemma 3
Gemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出
Blog
Huggingface
Phi-4-multimodal
具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。
Blog
Huggingface
Qwen2.5-VL-32B-Instruct
32B参数量版本的Qwen2.5-VL多模态模型；

72B too big for VLM? 7B not strong enough! Teh you should use 32B model!">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://niraya666.github.io/monthly/2025-03-%E6%9C%88%E5%88%8A/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://niraya666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://niraya666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://niraya666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://niraya666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://niraya666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://niraya666.github.io/monthly/2025-03-%E6%9C%88%E5%88%8A/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="2025-03 月刊" />
<meta property="og:description" content="值得关注的模型和新技术
DeepSeek V3 0324
DeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。
Huggingface
QwQ-32B
由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。
Blog
Huggingface
Qwen2.5 Omni
Qwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。
Blog
Huggingface
Gemma 3
Gemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出
Blog
Huggingface
Phi-4-multimodal
具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。
Blog
Huggingface
Qwen2.5-VL-32B-Instruct
32B参数量版本的Qwen2.5-VL多模态模型；

72B too big for VLM? 7B not strong enough! Teh you should use 32B model!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://niraya666.github.io/monthly/2025-03-%E6%9C%88%E5%88%8A/" />
<meta property="og:image" content="https://niraya666.github.io/img/monthly/2025-03/246C29B4-C253-4C00-BE9C-8E9650884D4E_1_105_c.jpeg" /><meta property="article:section" content="monthly" />
<meta property="article:published_time" content="2025-03-29T17:00:00+08:00" />
<meta property="article:modified_time" content="2025-03-29T17:00:00+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://niraya666.github.io/img/monthly/2025-03/246C29B4-C253-4C00-BE9C-8E9650884D4E_1_105_c.jpeg" />
<meta name="twitter:title" content="2025-03 月刊"/>
<meta name="twitter:description" content="值得关注的模型和新技术
DeepSeek V3 0324
DeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。
Huggingface
QwQ-32B
由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。
Blog
Huggingface
Qwen2.5 Omni
Qwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。
Blog
Huggingface
Gemma 3
Gemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出
Blog
Huggingface
Phi-4-multimodal
具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。
Blog
Huggingface
Qwen2.5-VL-32B-Instruct
32B参数量版本的Qwen2.5-VL多模态模型；

72B too big for VLM? 7B not strong enough! Teh you should use 32B model!"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Monthlies",
      "item": "https://niraya666.github.io/monthly/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "2025-03 月刊",
      "item": "https://niraya666.github.io/monthly/2025-03-%E6%9C%88%E5%88%8A/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "2025-03 月刊",
  "name": "2025-03 月刊",
  "description": "值得关注的模型和新技术 DeepSeek V3 0324 DeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。\nHuggingface\nQwQ-32B 由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。\nBlog\nHuggingface\nQwen2.5 Omni Qwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。\nBlog\nHuggingface\nGemma 3 Gemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出\nBlog\nHuggingface\nPhi-4-multimodal 具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。\nBlog\nHuggingface\nQwen2.5-VL-32B-Instruct 32B参数量版本的Qwen2.5-VL多模态模型；\n72B too big for VLM? 7B not strong enough! Teh you should use 32B model!\n",
  "keywords": [
    "月刊"
  ],
  "articleBody": "值得关注的模型和新技术 DeepSeek V3 0324 DeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。\nHuggingface\nQwQ-32B 由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。\nBlog\nHuggingface\nQwen2.5 Omni Qwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。\nBlog\nHuggingface\nGemma 3 Gemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出\nBlog\nHuggingface\nPhi-4-multimodal 具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。\nBlog\nHuggingface\nQwen2.5-VL-32B-Instruct 32B参数量版本的Qwen2.5-VL多模态模型；\n72B too big for VLM? 7B not strong enough! Teh you should use 32B model!\nHuggingface\nGemini 2.5 Pro Gemini 2.5 Pro 是一个reasoning模型，具有100万token的上下文窗口，支持多模态输入，包括文本、音频、图像、视频。\nGemini 2.5 Pro\nOpenAI Transcribe OpenAI 最近推出了新的语音转文本模型，包括“gpt-4o-transcribe”和“gpt-4o-mini-transcribe” 仅通过 API 提供服务；这些模型基于 GPT-4o 架构，相较于此前的 Whisper 模型，采用了更广泛且高质量的音频数据集进行训练，提升了对多样化口音、语速变化及嘈杂环境的适应能力，同时显著减少了“幻听”问题\nopenAI docs\nMistral OCR Mistral AI 推出的OCR API，专注于复杂文档的多模态理解,擅长处理科学论文等含图表混合内容的文档；支持 PNG、JPEG 图像及 PDF格式输入，输出Markdown 或 JSON 结构化数据，基础版价格为 1000 页/美元\nMistral OCR\nOpenAI 4o Image Generation OpenAI 最新更新的 4o-Image 生成技术，显著提升了图像生成能力：支持更高分辨率与逼真光影渲染，更精准的文本到图像转换，支持局部修改（如替换物体、调整风格）、多图像生成和3D/动态生成；拥有更强的内容过滤机制和水印或元数据标记；\nIntroducing 4o Image Generation\n值得关注的开源项目 OpenManus 由 MetaGPT 社区开发的Manus复刻项目，主要功能包括基于自然语言指令自主执行复杂任务，如网页浏览、文件操作、代码编写、数据分析等，适用于自动化工作流、研究支持和应用开发等场景。\ngithub\nBrowser Use 因为被Manus所带火的一个开源项目；使AI Agents能够高效访问网页并完成各种任务，\n该项目通过解析网页的 DOM 数据（而非依赖视觉模型）生成结构化数据，从而实现快速、轻量级的网页交互。其主要功能包括支持 AI 代理自动浏览网页、提取信息、执行操作，并与多种大语言模型无缝集成。\ngithub\nOpenAI Agent SDK 由 OpenAI 开发的开源Agent框架，核心功能包括：Agents，Handoffs（允许一个代理将任务委托给另一个代理），Guardrails（验证代理的输入，确保数据符合要求）和Tracing（内置追踪记录代理工作流中的所有事件）；该 SDK 兼容支持 OpenAI Chat Completions API 的多种模型提供商；\nOpenAI Agents SDK 官方文档概述\nGitHub 仓库 openai/openai-agents-python\nOlmOCR 采用多模态大模型实现的OCR项目，其核心VLM—allenai/olmOCR-7B-0225-preview， 是采用Qwen2-VL-7B 在250,000 页的多样化 PDF 数据集上进行微调而得。不仅是开源了模型， 同时还开源了一整套的解析工作管道，及其微调数据集、训练和推理代码。\ngithub\nTrendPublish 全自动 AI 内容生成与发布系统，支持多源数据采集，使用AI进行内容总结生成摘要，支持自动发布。\nGithub\nCSM：A Conversational Speech Generation Model 效果非常不错的语音生成模型。\nGithub\nBlog\nYuE Open Full-song Music Generation Foundation Model\nGithub\n值得关注的研究和论文 Qwen2.5-VL cookbook: https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks\nBlog\nTechnical Report\nMethods\n从头开始训练了一个原生动态分辨率的 ViT\n窗口注意力（Window Attention）： 在视觉编码器中引入，减少计算量，提升推理效率 动态 FPS 采样： 允许模型处理不同帧率的视频 升级的 MRoPE： 在时间域中使用多模态旋转位置嵌入（Multimodal Rotary Position Embedding），通过绝对时间对齐改善长视频处理 动态分辨率处理： 模型能原生处理任意尺寸的图像，无需传统归一化，保留空间细节，特别适用于文档和图表分析 绝对时间编码： 支持处理长达数小时的视频，并实现秒级事件定位 数据规模扩展： 预训练数据从 1.2 万亿 tokens 增加至 4.1 万亿 tokens 预训练过程分为三个阶段：\n在第一阶段，仅训练ViT，即视觉预训练：\n针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p\u003e）、表格（）、图表（）、公式（）、图像标注（）、OCR文本（）、乐谱（）、化学式（）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系 OCR 数据: 利用高质量的合成图像和现实世界的自然场景图像, 整合了一个大规模的多语言OCR数据集,支持多种语言; 针对图表类型数据，使用python可视化库合成了100万个样本； 对于表格数据，利用表格识别模型处理了600万个真实样本用于训练； 视频数据： 在训练过程中动态采样FPS，确保在理解不同帧率的视频数据时增强鲁棒性； Agent：收集移动设备、网页和桌面平台的截图。使用合成数据引擎生成截图字幕和UI元素定位注释。字幕任务帮助Qwen2.5-VL理解图形界面，而定位任务帮助它对齐元素的外观和功能；通过人类和模型注释者(AGUVIS),为每个步骤生成推理过程 在第二阶段，解冻所有模型参数，并在多样化的多模态图像数据上训练模型，以增强其处理复杂视觉信息的能力\n在第三阶段，长上下文预训练： 序列长度扩展至 32768，专注于长视频、长代理任务和长文档，训练长上下文能力\npost-training： Qwen2.5-VL 的后训练对齐框架采用了一个双阶段优化范式，包括监督微调（SFT）和直接偏好优化（DPO）（ViT参数冻结）SFT,约200万条记录，纯文本数据和多模态数据各占50%，多模态数据包括图文和视频文本组合;\n设计了一个两阶段的数据过滤pipeline，用于数据清洗和低质量数据过滤；在初始阶段即领域特定分类，采用Qwen2-VL-Instag（Qwen2-VL-72B）对问答对进行层次分类（八个主要领域，每个领域可进一步细分）；第二阶段涉及领域定制过滤，根据上一步的分类，有针对的对于不同领域提高数据质量（rule-based and model-based），如对于文档处理、OCR 和视觉定位任务相关的数据集，识别并移除重复模式\n采用拒绝采样作为策略以增强推理能力\n效果：单纯从视觉问答和 OCR benchmark的跑分结果看，72B 模型与 GPT-4o 和 Claude 3.5 Sonnet 相当\nR1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning https://arxiv.org/abs/2503.05592\n现有的大型推理模型(LRMs)主要依赖其内部知识来解决问题,这在处理知识密集型问题、私有信息查询和时效性问题时容易产生不准确和幻觉。因此,需要让模型能够在推理过程中访问外部信息来实现更深入的推理 。\nR1-Searcher,一个通过强化学习来增强大语言模型搜索能力的框架。该框架采用两阶段的基于结果的强化学习方法,使模型能够在推理过程中自主调用外部搜索系统获取相关知识\n创新点：\n提出了一个两阶段的基于结果的强化学习框架,使模型能够自主调用外部搜索系统 完全依赖强化学习进行训练 奖励机制,包括检索奖励和格式奖励,以引导模型学习正确使用外部检索系统 具体方法：\n第一阶段： 使用检索奖励和格式奖励来训练模型学习调用外部检索系统，不考虑答案的准确性,主要目标是让模型学会正确的检索调用格式\n第二阶段： 引入答案奖励，使用F1分数作为答案奖励的度量\nRL训练采用Reinforce++算法\n设计的两个算法：\nRAG-based Rollout\n目的：让模型能够基于检索到的文档继续进行推理，不会中断思维链\n使用特定标签来指示搜索工具的调用：... 当模型生成到时，会暂停生成过程 提取查询并执行检索 将检索到的文档用...标签封装 将检索结果整合到模型的推理过程中 Retrieval Mask-based Loss Calculation\n将...指定为特殊标记 在训练过程中对这些部分进行掩码处理，忽略这些被掩码的部分 这些文档不是模型应该学习生成的内容 一些有趣的观察：\nRL比SFT表现更好,在域内和域外测试集上都显示出更强的检索能力和泛化性， SFT倾向于依赖模型的内部知识,这可能导致错误答案 Detecting misbehavior in frontier reasoning models https://openai.com/index/chain-of-thought-monitoring/\n解决问题：\nreward hacking（AI可能通过开发者意图之外的方式实现高奖励，即利用系统漏洞达成指标而非真正解决问题） 创新点：\n提出CoT monitor 用AI监督AI 将思维透明性作为安全机制，而非仅作为解释工具 Methods： 使用另一个LLM（GPT-4o），来观察被监控模型（如 o3-mini）的 CoT 推理过程，这个监控 LLM 会根据特定提示来判断模型是否尝试通过reward hacking来“作弊”\n研究还探讨了将 CoT 监控集成到强化学习奖励中的效果。在低优化条件下（对模型的 CoT 不施加太大压力，允许它自然生成推理过程）此时CoT 保持透明和可监控，模型的行为更一致，监控系统可以有效识别不当行为。反之，在强优化下（通过强化学习大幅调整奖励函数，迫使模型尽可能提高任务表现），模型可能会学会“模糊化”其推理过程，使监控模型难以察觉其不当行为。\n研究建议支付“可监控性税”（monitorability tax），即不对 CoT 施加强优化压力，确保 CoT 保持可监控性，从而有效检测不一致行为。\nA-MEM: Agentic Memory for LLM Agents Paper\n解决的问题：\n当前LLM代理的记忆系统（依赖预定义结构和固定操作流程，无法动态组织知识 如何在开放域任务中实现记忆的自适应组织和持续演化，以支持复杂长期交互 创新点：\n受Zettelkasten启发的动态架构： 每条记忆包含内容、时间戳、LLM生成的关键词/标签/上下文描述、嵌入向量和关联链接\nm_i = {c_i, t_i, K_i, G_i, X_i, e_i, L_i}\n通过语义相似性和LLM推理动态建立记忆间的多层次关联（如因果关系、概念相似性）\n新记忆触发历史记忆的上下文和属性更新\nm_j^* \\leftarrow \\text{LLM}(m_n | \\mathcal{M}^{\\text{near}}n \\setminus m_j | m_j | P{s3})\nMethods\n笔记构建：用LLM解析原始交互，生成结构化属性）。 链接生成：先通过嵌入相似性筛选Top-K候选记忆，再用LLM分析深层关联。 记忆检索：将查询分解为关键词，在记忆网络中搜索。 graph LR A[新记忆m_n] --\u003e B[生成嵌入e_n] B --\u003e C[计算相似性] C --\u003e D[筛选Top-K] D --\u003e E[生成链接] E --\u003e F[触发记忆演化] G[用户查询q] --\u003e H[查询嵌入] H --\u003e I[匹配记忆] I --\u003e J[返回结果] prompt_template_for_Memory_Evolution = \"\"\"You are an AI memory evolution agent responsible for managing and evolving a knowledge base. Analyze the new memory note according to keywords and context, also with their several nearest neighbors memory. Make decisions about its evolution. The new memory context: {context} content: {content} keywords: {keywords} The nearest neighbors memories: {nearest_neighbors_memories} Based on this information, determine: 1. What specific actions should be taken (strengthen, update_neighbor)? 1.1 If choose to strengthen the connection, which memory should it be connected to? Can you give the updated tags of this memory? 1.2 If choose to update neighbor, you can update the context and tags of these memories based on the understanding of these memories. Tags should be determined by the content of these characteristic of these memories, which can be used to retrieve them later and categorize them. All the above information should be returned in a list format according to the sequence: [[new_memory], [neighbor_memory_1], ... [neighbor_memory_n]] These actions can be combined. Return your decision in JSON format with the following structure: { \"should_evolve\": true/false, \"actions\": [\"strengthen\", \"merge\", \"prune\"], \"suggested_connections\": [\"neighbor_memory_ids\"], \"tags_to_update\": [\"tag_1\", ... \"tag_n\"], \"new_context_neighborhood\": [\"new context\", ..., \"new context\"], \"new_tags_neighborhood\": [[\"tag_1\", ..., \"tag_n\"], ..., [\"tag_1\", ..., \"tag_n\"]] }\"\"\" 一些发现：\n移除链接生成（LG）和记忆演化（ME）后，多跳任务F1从45.85降至24.55，仅保留LG时性能恢复至31.24，证明链接生成是基础，演化机制是增强\nSFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training Paper\n这篇文章研究了SFT和RL在基础模型后训练中对模型泛化能力和记忆能力的影响，特别是在面对未见过的规则和视觉变体时的表现。\n发现：\nSFT倾向于记忆训练数据，难以泛化到未见过的规则或视觉变体，甚至在视觉任务中可能导致视觉识别能力下降 RL不仅在训练任务上表现更好，还能泛化到OOD任务，尤其是在视觉任务中表现出色 虽然RL表现更好，但SFT在RL训练中起到了稳定输出的关键作用 推荐内容 汇总：使用LLM生成精美网页、PPT、卡片 一键搞定、100%成功、80老太都能操作，宝藏提示词将你的任何内容生成网页\n【教程】必须收藏的 4 段 Prompt 提示词，生成PPT、生成3D动画，生成网站，生成万物…\n一键生成产品网站和小红书卡片，DeepSeek v3的代码能力和审美提升太大了\nAI 快速生成精美卡片\nMCP-Server合集 Model Context Protocol servers\nMCP.co\ncomposio\n课程 10 Lessons teaching everything you need to know to start building AI Agents ｜Microsoft\n其他 AI Blindspots：一个列出了当前 AI 开发盲点合集的网站\n影音记录 精选歌单 Live演出 03.15 Corn Wave\n03.18 Mogwai\n03.21-23 Can festival\n书\u0026阅读摘录 Career advice in 2025 Many people who first entered senior roles in 2010-2020 are finding current roles a lot less fun. There are a number of reasons for this. First, managers were generally evaluated in that period based on their ability to hire, retain and motivate teams. The current market doesn’t value those skills particularly highly, but instead prioritizes a different set of skills: working in the details, pushing pace, and navigating the technology transition to foundational models / LLMs.This means many members of the current crop of senior leaders are either worse at the skills they currently need to succeed, or are less motivated by those activities. Either way, they’re having less fun.\nValuations and funding are relatively less accessible to non-AI companies than they were three years ago.\nI also wouldn’t personally try to sit this cycle out unless you’re comfortable with a small risk that reentry is quite difficult: I think it’s more likely that the ecosystem is meaningfully different in five years than that it’s largely unchanged.\n一个人年入千万，AI创业的十个步骤 AI造就的是一个极度放大你个人能力的时代，你一个人可以干以前几十个人干的事情\n真正的门槛是洞察需求的能力。你做了一个手电筒或者天气软件，不管再精美，也很难帮你做到商业上的成功。\nMVP就是拿出去让用户骂，用户骂得越针对，你产品进步得越快\n大模型的未来，是 Agent 还是 App？ Deep Research 不是标准的 LLM，也不是普通的聊天机器人。它是一种新型的\"研究语言模型\"，专门设计用于端到端执行搜索任务。\n目前大多数 Agent 初创公司构建的不是真正的 Agent，而是工作流，即\"通过预定义的代码路径编排 LLM 和工具的系统\"。工作流在某些垂直领域仍可能创造价值\n当前的 RAG 系统由许多相互关联但脆弱的工作流组成：路由、分块、重排序、查询解释、查询扩展、源上下文化和搜索工程。随着训练技术栈的发展，有可能将所有这些流程整合到两个独立但相互关联的模型中：一个用于数据准备，另一个用于搜索/检索/报告生成。 这需要精心设计的合成流程和全新的强化学习奖励函数。真正的训练，真正的研究。这在实践中意味着：转移复杂性。\nAI Will Upend a Basic Assumption About How Companies Are Organized 经济体系长期以来建立在这样的观念之上：专业知识稀缺且昂贵。而人工智能即将让这种专业知识变得丰富且几乎免费。\n支撑我们社会制度的基本假设——“人类洞察力稀缺且昂贵”——将不复存在 摆在个人与企业面前的问题是：当智能本身随处可得且几乎不需成本时，你将如何行动？\n一个原因在于人们很难想象，“必须依靠资深管理者或顶尖专家”才能完成的工作，居然可以（或者部分可以）由机器来承担。正因为卓越人才稀缺，那些高价值任务才显得格外珍贵。我们的组织结构便是在“真正的高智商人才供给有限”这一认知下设计的。\n大多数企业领导人仍处在“尝试接受 AI”而非“真正相信 AI”的阶段\n当智慧的成本几近于零时，真正的瓶颈已不再是“如何获取大脑”，而是“我们如何善加利用”。\n为什么 AI 模型离科学革命还差得很远？ 当前 AI 模型的评估方式有误区\n现在，我们评估 AI 模型智能提升的方式，大多是通过一些“高难度考试题…这些正是我当年擅长的考试类型。这样的评估方法，只能测试模型是否能回答人类已经知道答案的问题。但真正的科学突破，并不来自于答题，而是来自于提出别人没问过的问题，挑战主流认知，质疑已有理论。\n尽管大语言模型已经掌握了几乎所有人类知识，但它们还没有真正“生成”出新的科学发现。…我们现在训练出来的是“听话的好学生”，不是“有主见的创造者”\n那么我们也许需要重新定义 AI 模型的评估方式：评估标准不再是“答对多少题”，而是它是否具备真正的科学思维，\n我们常犯的一个错误，是把牛顿或爱因斯坦看作是“超级优等生”，以为只要把一个好学生按比例放大，就能变成一个天才。… 但这种线性外推忽略了科学最本质的一点：提出正确问题的能力，以及挑战既有知识体系的勇气。\n",
  "wordCount" : "912",
  "inLanguage": "en",
  "image":"https://niraya666.github.io/img/monthly/2025-03/246C29B4-C253-4C00-BE9C-8E9650884D4E_1_105_c.jpeg","datePublished": "2025-03-29T17:00:00+08:00",
  "dateModified": "2025-03-29T17:00:00+08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://niraya666.github.io/monthly/2025-03-%E6%9C%88%E5%88%8A/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LZY Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://niraya666.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niraya666.github.io/" accesskey="h" title="LZY Blog (Alt + H)">LZY Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niraya666.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/posts/" title="AI">
                    <span>AI</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/musik/" title="Musik!">
                    <span>Musik!</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/monthly/" title="月刊">
                    <span>月刊</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/essay/" title="杂文">
                    <span>杂文</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/travel/" title="游记">
                    <span>游记</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://niraya666.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://niraya666.github.io/monthly/">Monthlies</a></div>
    <h1 class="post-title entry-hint-parent">
      2025-03 月刊
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-03-29 17:00:00 +0800 CST'>March 29, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Theme PaperMod

</div>
  </header> 
<figure class="entry-cover"><a href="https://niraya666.github.io/img/monthly/2025-03/246C29B4-C253-4C00-BE9C-8E9650884D4E_1_105_c.jpeg" target="_blank"
            rel="noopener noreferrer"><img loading="eager" src="https://niraya666.github.io/img/monthly/2025-03/246C29B4-C253-4C00-BE9C-8E9650884D4E_1_105_c.jpeg" alt="摄于 舟山 朱家尖大青山"></a>
        <p>摄于 舟山 朱家尖大青山</p>
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e6%a8%a1%e5%9e%8b%e5%92%8c%e6%96%b0%e6%8a%80%e6%9c%af" aria-label="值得关注的模型和新技术">值得关注的模型和新技术</a><ul>
                        
                <li>
                    <a href="#deepseek-v3-0324" aria-label="DeepSeek V3 0324">DeepSeek V3 0324</a></li>
                <li>
                    <a href="#qwq-32b" aria-label="QwQ-32B">QwQ-32B</a></li>
                <li>
                    <a href="#qwen25-omni" aria-label="Qwen2.5 Omni">Qwen2.5 Omni</a></li>
                <li>
                    <a href="#gemma-3" aria-label="Gemma 3">Gemma 3</a></li>
                <li>
                    <a href="#phi-4-multimodal" aria-label="Phi-4-multimodal">Phi-4-multimodal</a></li>
                <li>
                    <a href="#qwen25-vl-32b-instruct" aria-label="Qwen2.5-VL-32B-Instruct">Qwen2.5-VL-32B-Instruct</a></li>
                <li>
                    <a href="#gemini-25-pro" aria-label="Gemini 2.5 Pro">Gemini 2.5 Pro</a></li>
                <li>
                    <a href="#openai-transcribe" aria-label="OpenAI Transcribe">OpenAI Transcribe</a></li>
                <li>
                    <a href="#mistral-ocr" aria-label="Mistral OCR">Mistral OCR</a></li>
                <li>
                    <a href="#openai-4o-image-generation" aria-label="OpenAI 4o Image Generation">OpenAI 4o Image Generation</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e5%bc%80%e6%ba%90%e9%a1%b9%e7%9b%ae" aria-label="值得关注的开源项目">值得关注的开源项目</a><ul>
                        
                <li>
                    <a href="#openmanus" aria-label="OpenManus">OpenManus</a></li>
                <li>
                    <a href="#browser-use" aria-label="Browser Use">Browser Use</a></li>
                <li>
                    <a href="#openai-agent-sdk" aria-label="OpenAI Agent SDK">OpenAI Agent SDK</a></li>
                <li>
                    <a href="#olmocr" aria-label="OlmOCR">OlmOCR</a></li>
                <li>
                    <a href="#trendpublish" aria-label="TrendPublish">TrendPublish</a></li>
                <li>
                    <a href="#csma-conversational-speech-generation-model" aria-label="CSM：A Conversational Speech Generation Model">CSM：A Conversational Speech Generation Model</a></li>
                <li>
                    <a href="#yue" aria-label="YuE">YuE</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e7%a0%94%e7%a9%b6%e5%92%8c%e8%ae%ba%e6%96%87" aria-label="值得关注的研究和论文">值得关注的研究和论文</a><ul>
                        
                <li>
                    <a href="#qwen25-vl" aria-label="Qwen2.5-VL">Qwen2.5-VL</a></li>
                <li>
                    <a href="#r1-searcher-incentivizing-the-search-capability-in-llms-via-reinforcement-learning" aria-label="R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning">R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning</a></li>
                <li>
                    <a href="#detecting-misbehavior-in-frontier-reasoning-models" aria-label="Detecting misbehavior in frontier reasoning models">Detecting misbehavior in frontier reasoning models</a></li></ul>
                </li>
                <li>
                    <a href="#a-mem-agentic-memory-for-llm-agents" aria-label="A-MEM: Agentic Memory for LLM Agents">A-MEM: Agentic Memory for LLM Agents</a><ul>
                        
                <li>
                    <a href="#sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training" aria-label="SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%8e%a8%e8%8d%90%e5%86%85%e5%ae%b9" aria-label="推荐内容">推荐内容</a><ul>
                        
                <li>
                    <a href="#%e6%b1%87%e6%80%bb%e4%bd%bf%e7%94%a8llm%e7%94%9f%e6%88%90%e7%b2%be%e7%be%8e%e7%bd%91%e9%a1%b5ppt%e5%8d%a1%e7%89%87" aria-label="汇总：使用LLM生成精美网页、PPT、卡片">汇总：使用LLM生成精美网页、PPT、卡片</a></li>
                <li>
                    <a href="#mcp-server%e5%90%88%e9%9b%86" aria-label="MCP-Server合集">MCP-Server合集</a></li>
                <li>
                    <a href="#%e8%af%be%e7%a8%8b" aria-label="课程">课程</a><ul>
                        
                <li>
                    <a href="#%e5%85%b6%e4%bb%96" aria-label="其他">其他</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%e5%bd%b1%e9%9f%b3%e8%ae%b0%e5%bd%95" aria-label="影音记录">影音记录</a><ul>
                        
                <li>
                    <a href="#%e7%b2%be%e9%80%89%e6%ad%8c%e5%8d%95" aria-label="精选歌单">精选歌单</a></li>
                <li>
                    <a href="#live%e6%bc%94%e5%87%ba" aria-label="Live演出">Live演出</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%b9%a6%e9%98%85%e8%af%bb%e6%91%98%e5%bd%95" aria-label="书&amp;阅读摘录">书&amp;阅读摘录</a><ul>
                        
                <li>
                    <a href="#career-advice-in-2025httpslethaincomcareer-advice-2025" aria-label="Career advice in 2025">Career advice in 2025</a></li>
                <li>
                    <a href="#%e4%b8%80%e4%b8%aa%e4%ba%ba%e5%b9%b4%e5%85%a5%e5%8d%83%e4%b8%87ai%e5%88%9b%e4%b8%9a%e7%9a%84%e5%8d%81%e4%b8%aa%e6%ad%a5%e9%aa%a4httpswwwyoutubecomwatchv4cuqeauh3-4" aria-label="一个人年入千万，AI创业的十个步骤">一个人年入千万，AI创业的十个步骤</a></li>
                <li>
                    <a href="#%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%9c%aa%e6%9d%a5%e6%98%af-agent-%e8%bf%98%e6%98%af-apphttpsxcomoran_gestatus1898373775156199730" aria-label="大模型的未来，是 Agent 还是 App？">大模型的未来，是 Agent 还是 App？</a></li>
                <li>
                    <a href="#ai-will-upend-a-basic-assumption-about-how-companies-are-organizedhttpswwwbloombergcomnewsarticles2025-02-28how-ai-reasoning-models-will-change-companies-and-the-economy" aria-label="AI Will Upend a Basic Assumption About How Companies Are Organized">AI Will Upend a Basic Assumption About How Companies Are Organized</a></li>
                <li>
                    <a href="#%e4%b8%ba%e4%bb%80%e4%b9%88-ai-%e6%a8%a1%e5%9e%8b%e7%a6%bb%e7%a7%91%e5%ad%a6%e9%9d%a9%e5%91%bd%e8%bf%98%e5%b7%ae%e5%be%97%e5%be%88%e8%bf%9chttpsmpweixinqqcomsgkmesaitsftfvoippc_mag" aria-label="为什么 AI 模型离科学革命还差得很远？">为什么 AI 模型离科学革命还差得很远？</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="值得关注的模型和新技术">值得关注的模型和新技术<a hidden class="anchor" aria-hidden="true" href="#值得关注的模型和新技术">#</a></h1>
<h2 id="deepseek-v3-0324">DeepSeek V3 0324<a hidden class="anchor" aria-hidden="true" href="#deepseek-v3-0324">#</a></h2>
<p>DeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。</p>
<p><a href="https://github.com/deepseek-ai/DeepSeek-V3">Huggingface</a></p>
<h2 id="qwq-32b">QwQ-32B<a hidden class="anchor" aria-hidden="true" href="#qwq-32b">#</a></h2>
<p>由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。</p>
<p><a href="https://qwenlm.github.io/zh/blog/qwq-32b/">Blog</a></p>
<p><a href="https://huggingface.co/Qwen/QwQ-32B">Huggingface</a></p>
<h2 id="qwen25-omni">Qwen2.5 Omni<a hidden class="anchor" aria-hidden="true" href="#qwen25-omni">#</a></h2>
<p>Qwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。</p>
<p><a href="https://qwenlm.github.io/zh/blog/qwen2.5-omni/">Blog</a></p>
<p><a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B">Huggingface</a></p>
<h2 id="gemma-3">Gemma 3<a hidden class="anchor" aria-hidden="true" href="#gemma-3">#</a></h2>
<p>Gemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出</p>
<p><a href="https://blog.google/technology/developers/gemma-3/">Blog</a></p>
<p><a href="https://huggingface.co/google/gemma-3-27b-it">Huggingface</a></p>
<h2 id="phi-4-multimodal">Phi-4-multimodal<a hidden class="anchor" aria-hidden="true" href="#phi-4-multimodal">#</a></h2>
<p>具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。</p>
<p><a href="https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/">Blog</a></p>
<p><a href="https://huggingface.co/microsoft/Phi-4-multimodal-instruct">Huggingface</a></p>
<h2 id="qwen25-vl-32b-instruct">Qwen2.5-VL-32B-Instruct<a hidden class="anchor" aria-hidden="true" href="#qwen25-vl-32b-instruct">#</a></h2>
<p>32B参数量版本的Qwen2.5-VL多模态模型；</p>
<blockquote>
<p>72B too big for VLM? 7B not strong enough! Teh you should use 32B model!</p>
</blockquote>
<p><a href="https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct">Huggingface</a></p>
<h2 id="gemini-25-pro">Gemini 2.5 Pro<a hidden class="anchor" aria-hidden="true" href="#gemini-25-pro">#</a></h2>
<p>Gemini 2.5 Pro 是一个reasoning模型，具有100万token的上下文窗口，支持多模态输入，包括文本、音频、图像、视频。</p>
<p><a href="https://deepmind.google/technologies/gemini/pro/">Gemini 2.5 Pro</a></p>
<h2 id="openai-transcribe">OpenAI Transcribe<a hidden class="anchor" aria-hidden="true" href="#openai-transcribe">#</a></h2>
<p>OpenAI 最近推出了新的语音转文本模型，包括“gpt-4o-transcribe”和“gpt-4o-mini-transcribe” 仅通过 API 提供服务；这些模型基于 GPT-4o 架构，相较于此前的 Whisper 模型，采用了更广泛且高质量的音频数据集进行训练，提升了对多样化口音、语速变化及嘈杂环境的适应能力，同时显著减少了“幻听”问题</p>
<p><a href="https://platform.openai.com/docs/models/gpt-4o-transcribe">openAI docs</a></p>
<h2 id="mistral-ocr">Mistral OCR<a hidden class="anchor" aria-hidden="true" href="#mistral-ocr">#</a></h2>
<p>Mistral AI 推出的OCR API，专注于复杂文档的多模态理解,擅长处理科学论文等含图表混合内容的文档；支持 PNG、JPEG 图像及 PDF格式输入，输出Markdown 或 JSON 结构化数据，基础版价格为 1000 页/美元</p>
<p><a href="https://mistral.ai/news/mistral-ocr">Mistral OCR</a></p>
<h2 id="openai-4o-image-generation">OpenAI 4o Image Generation<a hidden class="anchor" aria-hidden="true" href="#openai-4o-image-generation">#</a></h2>
<p>OpenAI 最新更新的 4o-Image 生成技术，显著提升了图像生成能力：支持更高分辨率与逼真光影渲染，更精准的文本到图像转换，支持局部修改（如替换物体、调整风格）、多图像生成和3D/动态生成；拥有更强的内容过滤机制和水印或元数据标记；</p>
<p><a href="https://openai.com/index/introducing-4o-image-generation/">Introducing 4o Image Generation</a></p>
<h1 id="值得关注的开源项目">值得关注的开源项目<a hidden class="anchor" aria-hidden="true" href="#值得关注的开源项目">#</a></h1>
<h2 id="openmanus">OpenManus<a hidden class="anchor" aria-hidden="true" href="#openmanus">#</a></h2>
<p>由 MetaGPT 社区开发的Manus复刻项目，主要功能包括基于自然语言指令自主执行复杂任务，如网页浏览、文件操作、代码编写、数据分析等，适用于自动化工作流、研究支持和应用开发等场景。</p>
<p><a href="https://github.com/mannaandpoem/OpenManus">github</a></p>
<h2 id="browser-use">Browser Use<a hidden class="anchor" aria-hidden="true" href="#browser-use">#</a></h2>
<p>因为被Manus所带火的一个开源项目；使AI Agents能够高效访问网页并完成各种任务，</p>
<p>该项目通过解析网页的 DOM 数据（而非依赖视觉模型）生成结构化数据，从而实现快速、轻量级的网页交互。其主要功能包括支持 AI 代理自动浏览网页、提取信息、执行操作，并与多种大语言模型无缝集成。</p>
<p><a href="https://github.com/browser-use/browser-use">github</a></p>
<h2 id="openai-agent-sdk">OpenAI Agent SDK<a hidden class="anchor" aria-hidden="true" href="#openai-agent-sdk">#</a></h2>
<p>由 OpenAI 开发的开源Agent框架，核心功能包括：Agents，Handoffs（允许一个代理将任务委托给另一个代理），Guardrails（验证代理的输入，确保数据符合要求）和Tracing（内置追踪记录代理工作流中的所有事件）；该 SDK 兼容支持 OpenAI Chat Completions API 的多种模型提供商；</p>
<p><a href="https://openai.github.io/openai-agents-python/">OpenAI Agents SDK 官方文档概述</a></p>
<p><a href="https://github.com/openai/openai-agents-python">GitHub 仓库 openai/openai-agents-python</a></p>
<h2 id="olmocr">OlmOCR<a hidden class="anchor" aria-hidden="true" href="#olmocr">#</a></h2>
<p>采用多模态大模型实现的OCR项目，其核心VLM—<a href="https://huggingface.co/allenai/olmOCR-7B-0225-preview">allenai/olmOCR-7B-0225-preview</a>， 是采用Qwen2-VL-7B 在250,000 页的多样化 PDF 数据集上进行微调而得。不仅是开源了模型， 同时还开源了一整套的解析工作管道，及其微调数据集、训练和推理代码。</p>
<p><a href="https://github.com/allenai/olmocr">github</a></p>
<h2 id="trendpublish">TrendPublish<a hidden class="anchor" aria-hidden="true" href="#trendpublish">#</a></h2>
<p>全自动 AI 内容生成与发布系统，支持多源数据采集，使用AI进行内容总结生成摘要，支持自动发布。</p>
<p><a href="https://github.com/OpenAISpace/ai-trend-publish">Github</a></p>
<h2 id="csma-conversational-speech-generation-model">CSM：A Conversational Speech Generation Model<a hidden class="anchor" aria-hidden="true" href="#csma-conversational-speech-generation-model">#</a></h2>
<p>效果非常不错的语音生成模型。</p>
<p><a href="https://github.com/SesameAILabs/csm">Github</a></p>
<p><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice">Blog</a></p>
<h2 id="yue">YuE<a hidden class="anchor" aria-hidden="true" href="#yue">#</a></h2>
<p>Open Full-song Music Generation Foundation Model</p>
<p><a href="https://github.com/multimodal-art-projection/YuE">Github</a></p>
<h1 id="值得关注的研究和论文">值得关注的研究和论文<a hidden class="anchor" aria-hidden="true" href="#值得关注的研究和论文">#</a></h1>
<h2 id="qwen25-vl">Qwen2.5-VL<a hidden class="anchor" aria-hidden="true" href="#qwen25-vl">#</a></h2>
<p>cookbook: <a href="https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks">https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks</a></p>
<p><a href="https://qwenlm.github.io/zh/blog/qwen2.5-vl/">Blog</a></p>
<p><a href="https://arxiv.org/abs/2502.13923">Technical Report</a></p>
<p>Methods</p>
<p>从头开始训练了一个原生动态分辨率的 ViT</p>
<ul>
<li>窗口注意力（Window Attention）： 在视觉编码器中引入，减少计算量，提升推理效率</li>
<li>动态 FPS 采样： 允许模型处理不同帧率的视频</li>
<li>升级的 MRoPE： 在时间域中使用多模态旋转位置嵌入（Multimodal Rotary Position Embedding），通过绝对时间对齐改善长视频处理</li>
<li>动态分辨率处理： 模型能原生处理任意尺寸的图像，无需传统归一化，保留空间细节，特别适用于文档和图表分析</li>
<li>绝对时间编码： 支持处理长达数小时的视频，并实现秒级事件定位</li>
<li>数据规模扩展： 预训练数据从 1.2 万亿 tokens 增加至 4.1 万亿 tokens</li>
</ul>
<p>预训练过程分为三个阶段：</p>
<p>在第一阶段，仅训练ViT，即视觉预训练：</p>
<ul>
<li>针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（<code>p&gt;</code>）、表格（<code>&lt;table&gt;</code>）、图表（<code>&lt;div class=&quot;chart&quot;&gt;</code>）、公式（<code>&lt;div class=&quot;formula&quot;&gt;</code>）、图像标注（<code>&lt;div class=&quot;image caption&quot;&gt;</code>）、OCR文本（<code>&lt;div class=&quot;image ocr&quot;&gt;</code>）、乐谱（<code>&lt;div class=&quot;music sheet&quot;&gt;</code>）、化学式（<code>&lt;div class=&quot;chemical formula&quot;&gt;</code>）等模块。每个模块均通过 <code>data-bbox</code> 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系</li>
<li>OCR 数据: 利用高质量的合成图像和现实世界的自然场景图像, 整合了一个大规模的多语言OCR数据集,支持多种语言; 针对图表类型数据，使用python可视化库合成了100万个样本； 对于表格数据，利用表格识别模型处理了600万个真实样本用于训练；</li>
<li>视频数据： 在训练过程中动态采样FPS，确保在理解不同帧率的视频数据时增强鲁棒性；</li>
<li>Agent：收集移动设备、网页和桌面平台的截图。使用合成数据引擎生成截图字幕和UI元素定位注释。字幕任务帮助Qwen2.5-VL理解图形界面，而定位任务帮助它对齐元素的外观和功能；通过人类和模型注释者(AGUVIS),为每个步骤生成推理过程</li>
</ul>
<p>在第二阶段，解冻所有模型参数，并在多样化的多模态图像数据上训练模型，以增强其处理复杂视觉信息的能力</p>
<p>在第三阶段，长上下文预训练： 序列长度扩展至 32768，专注于长视频、长代理任务和长文档，训练长上下文能力</p>
<p>post-training： Qwen2.5-VL 的后训练对齐框架采用了一个双阶段优化范式，包括监督微调（SFT）和直接偏好优化（DPO）（ViT参数冻结）SFT,约200万条记录，纯文本数据和多模态数据各占50%，多模态数据包括图文和视频文本组合;</p>
<p>设计了一个两阶段的数据过滤pipeline，用于数据清洗和低质量数据过滤；在初始阶段即领域特定分类，采用Qwen2-VL-Instag（Qwen2-VL-72B）对问答对进行层次分类（八个主要领域，每个领域可进一步细分）；第二阶段涉及领域定制过滤，根据上一步的分类，有针对的对于不同领域提高数据质量（rule-based and model-based），如对于文档处理、OCR 和视觉定位任务相关的数据集，识别并移除重复模式</p>
<p>采用拒绝采样作为策略以增强推理能力</p>
<p>效果：单纯从视觉问答和 OCR benchmark的跑分结果看，72B 模型与 GPT-4o 和 Claude 3.5 Sonnet 相当</p>
<h2 id="r1-searcher-incentivizing-the-search-capability-in-llms-via-reinforcement-learning">R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#r1-searcher-incentivizing-the-search-capability-in-llms-via-reinforcement-learning">#</a></h2>
<p><a href="https://arxiv.org/abs/2503.05592">https://arxiv.org/abs/2503.05592</a></p>
<p>现有的大型推理模型(LRMs)主要依赖其内部知识来解决问题,这在处理知识密集型问题、私有信息查询和时效性问题时容易产生不准确和幻觉。因此,需要让模型能够在推理过程中访问外部信息来实现更深入的推理 。</p>
<p>R1-Searcher,一个通过强化学习来增强大语言模型搜索能力的框架。该框架采用两阶段的基于结果的强化学习方法,使模型能够在推理过程中自主调用外部搜索系统获取相关知识</p>
<p>创新点：</p>
<ul>
<li>提出了一个两阶段的基于结果的强化学习框架,使模型能够自主调用外部搜索系统</li>
<li>完全依赖强化学习进行训练</li>
<li>奖励机制,包括检索奖励和格式奖励,以引导模型学习正确使用外部检索系统</li>
</ul>
<p>具体方法：</p>
<p>第一阶段： 使用检索奖励和格式奖励来训练模型学习调用外部检索系统，不考虑答案的准确性,主要目标是让模型学会正确的检索调用格式</p>
<p>第二阶段： 引入答案奖励，使用F1分数作为答案奖励的度量</p>
<p>RL训练采用Reinforce++算法</p>
<p>设计的两个算法：</p>
<p><strong>RAG-based Rollout</strong></p>
<p>目的：让模型能够基于检索到的文档继续进行推理，不会中断思维链</p>
<ul>
<li>使用特定标签来指示搜索工具的调用：<code>&lt;begin_of_query&gt;...&lt;end_of_query&gt;</code></li>
<li>当模型生成到<code>&lt;end_of_query&gt;</code>时，会暂停生成过程</li>
<li>提取查询并执行检索</li>
<li>将检索到的文档用<code>&lt;begin_of_documents&gt;...&lt;end_of_documents&gt;</code>标签封装</li>
<li>将检索结果整合到模型的推理过程中</li>
</ul>
<p><strong>Retrieval Mask-based Loss Calculation</strong></p>
<ul>
<li>将<code>&lt;begin_of_documents&gt;...&lt;end_of_documents&gt;</code>指定为特殊标记</li>
<li>在训练过程中对这些部分进行掩码处理，忽略这些被掩码的部分</li>
<li>这些文档不是模型应该学习生成的内容</li>
</ul>
<p>一些有趣的观察：</p>
<ul>
<li>RL比SFT表现更好,在域内和域外测试集上都显示出更强的检索能力和泛化性， SFT倾向于依赖模型的内部知识,这可能导致错误答案</li>
</ul>
<h2 id="detecting-misbehavior-in-frontier-reasoning-models"><strong>Detecting misbehavior in frontier reasoning models</strong><a hidden class="anchor" aria-hidden="true" href="#detecting-misbehavior-in-frontier-reasoning-models">#</a></h2>
<p><a href="https://openai.com/index/chain-of-thought-monitoring/">https://openai.com/index/chain-of-thought-monitoring/</a></p>
<p>解决问题：</p>
<ul>
<li>reward hacking（AI可能通过开发者意图之外的方式实现高奖励，即利用系统漏洞达成指标而非真正解决问题）</li>
</ul>
<p>创新点：</p>
<ul>
<li>提出CoT monitor</li>
<li>用AI监督AI</li>
<li>将<strong>思维透明性作为安全机制，而非仅作为解释工具</strong></li>
</ul>
<p>Methods： 使用另一个LLM（GPT-4o），来观察被监控模型（如 o3-mini）的 CoT 推理过程，这个监控 LLM 会根据特定提示来判断模型是否尝试通过reward hacking来“作弊”</p>
<p>研究还探讨了将 CoT 监控集成到强化学习奖励中的效果。在低优化条件下（对模型的 CoT 不施加太大压力，允许它自然生成推理过程）此时CoT 保持透明和可监控，模型的行为更一致，监控系统可以有效识别不当行为。反之，在强优化下（通过强化学习大幅调整奖励函数，迫使模型尽可能提高任务表现），模型可能会学会“模糊化”其推理过程，使监控模型难以察觉其不当行为。</p>
<p>研究建议支付“可监控性税”（monitorability tax），即不对 CoT 施加强优化压力，确保 CoT 保持可监控性，从而有效检测不一致行为。</p>
<h1 id="a-mem-agentic-memory-for-llm-agents"><strong>A-MEM: Agentic Memory for LLM Agents</strong><a hidden class="anchor" aria-hidden="true" href="#a-mem-agentic-memory-for-llm-agents">#</a></h1>
<p><a href="https://arxiv.org/abs/2502.12110">Paper</a></p>
<p>解决的问题：</p>
<ul>
<li>当前LLM代理的记忆系统（依赖预定义结构和固定操作流程，无法动态组织知识</li>
<li>如何在开放域任务中实现记忆的<strong>自适应组织</strong>和<strong>持续演化</strong>，以支持复杂长期交互</li>
</ul>
<p>创新点：</p>
<ul>
<li><strong>受Zettelkasten启发的动态架构：</strong>
<ul>
<li>
<p>每条记忆包含内容、时间戳、LLM生成的关键词/标签/上下文描述、嵌入向量和关联链接</p>
<p>m_i = {c_i, t_i, K_i, G_i, X_i, e_i, L_i}</p>
</li>
<li>
<p>通过语义相似性和LLM推理动态建立记忆间的多层次关联（如因果关系、概念相似性）</p>
</li>
<li>
<p>新记忆触发历史记忆的上下文和属性更新</p>
<p>m_j^* \leftarrow \text{LLM}(m_n | \mathcal{M}^{\text{near}}<em>n \setminus m_j | m_j | P</em>{s3})</p>
</li>
</ul>
</li>
</ul>
<p>Methods</p>
<p><img loading="lazy" src="https://arxiv.org/html/2502.12110v3/x3.png" alt=""  />
</p>
<ol>
<li><strong>笔记构建</strong>：用LLM解析原始交互，生成结构化属性）。</li>
<li><strong>链接生成</strong>：先通过嵌入相似性筛选Top-K候选记忆，再用LLM分析深层关联。</li>
<li><strong>记忆检索</strong>：将查询分解为关键词，在记忆网络中搜索。</li>
</ol>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">graph LR
    A[新记忆m_n] --&gt; B[生成嵌入e_n]
    B --&gt; C[计算相似性]
    C --&gt; D[筛选Top-K]
    D --&gt; E[生成链接]
    E --&gt; F[触发记忆演化]
    G[用户查询q] --&gt; H[查询嵌入]
    H --&gt; I[匹配记忆]
    I --&gt; J[返回结果]
</code></pre><pre tabindex="0"><code>prompt_template_for_Memory_Evolution = &#34;&#34;&#34;You are an AI memory evolution agent responsible for managing and evolving a knowledge base.

Analyze the new memory note according to keywords and context, also with their several nearest neighbors memory.

Make decisions about its evolution.

The new memory context: {context}

content: {content}

keywords: {keywords}

The nearest neighbors memories: {nearest_neighbors_memories}

Based on this information, determine:

1. What specific actions should be taken (strengthen, update_neighbor)?
    1.1 If choose to strengthen the connection, which memory should it be connected to? Can you give the updated tags of this memory?
    1.2 If choose to update neighbor, you can update the context and tags of these memories based on the understanding of these memories.

Tags should be determined by the content of these characteristic of these memories, which can be used to retrieve them later and categorize them.

All the above information should be returned in a list format according to the sequence: [[new_memory], [neighbor_memory_1], ... [neighbor_memory_n]]

These actions can be combined.

Return your decision in JSON format with the following structure:

{
    &#34;should_evolve&#34;: true/false,
    &#34;actions&#34;: [&#34;strengthen&#34;, &#34;merge&#34;, &#34;prune&#34;],
    &#34;suggested_connections&#34;: [&#34;neighbor_memory_ids&#34;],
    &#34;tags_to_update&#34;: [&#34;tag_1&#34;, ... &#34;tag_n&#34;],
    &#34;new_context_neighborhood&#34;: [&#34;new context&#34;, ..., &#34;new context&#34;],
    &#34;new_tags_neighborhood&#34;: [[&#34;tag_1&#34;, ..., &#34;tag_n&#34;], ..., [&#34;tag_1&#34;, ..., &#34;tag_n&#34;]]
}&#34;&#34;&#34;
</code></pre><p>一些发现：</p>
<p>移除链接生成（LG）和记忆演化（ME）后，多跳任务F1从45.85降至24.55，仅保留LG时性能恢复至31.24，证明<strong>链接生成是基础，演化机制是增强</strong></p>
<h2 id="sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training<a hidden class="anchor" aria-hidden="true" href="#sft-memorizes-rl-generalizes-a-comparative-study-of-foundation-model-post-training">#</a></h2>
<p><a href="https://arxiv.org/abs/2501.17161">Paper</a></p>
<p>这篇文章研究了SFT和RL在基础模型后训练中对模型泛化能力和记忆能力的影响，特别是在面对未见过的规则和视觉变体时的表现。</p>
<p>发现：</p>
<ul>
<li>SFT倾向于记忆训练数据，难以泛化到未见过的规则或视觉变体，甚至在视觉任务中可能导致视觉识别能力下降</li>
<li>RL不仅在训练任务上表现更好，还能泛化到OOD任务，尤其是在视觉任务中表现出色</li>
<li>虽然RL表现更好，但SFT在RL训练中起到了稳定输出的关键作用</li>
</ul>
<h1 id="推荐内容">推荐内容<a hidden class="anchor" aria-hidden="true" href="#推荐内容">#</a></h1>
<h2 id="汇总使用llm生成精美网页ppt卡片">汇总：使用LLM生成精美网页、PPT、卡片<a hidden class="anchor" aria-hidden="true" href="#汇总使用llm生成精美网页ppt卡片">#</a></h2>
<p><a href="https://mp.weixin.qq.com/s/f1IozQKgIEDODfLRP5E2qg"><strong>一键搞定、100%成功、80老太都能操作，宝藏提示词将你的任何内容生成网页</strong></a></p>
<p><a href="https://mp.weixin.qq.com/s/mUThidjurJkLauQDU8ceCQ"><strong>【教程】必须收藏的 4 段 Prompt 提示词，生成PPT、生成3D动画，生成网站，生成万物&hellip;</strong></a></p>
<p><a href="https://mp.weixin.qq.com/s/UX_GaRlepP8GGxKFyh5rSA"><strong>一键生成产品网站和小红书卡片，DeepSeek v3的代码能力和审美提升太大了</strong></a></p>
<p><a href="https://card.3min.top/?v=BETA"><strong>AI 快速生成精美卡片</strong></a></p>
<h2 id="mcp-server合集">MCP-Server合集<a hidden class="anchor" aria-hidden="true" href="#mcp-server合集">#</a></h2>
<p><a href="https://github.com/modelcontextprotocol/servers"><strong>Model Context Protocol servers</strong></a></p>
<p><a href="https://mcp.so/">MCP.co</a></p>
<p><a href="https://mcp.composio.dev/">composio</a></p>
<h2 id="课程">课程<a hidden class="anchor" aria-hidden="true" href="#课程">#</a></h2>
<p><a href="https://github.com/microsoft/ai-agents-for-beginners"><strong>10 Lessons teaching everything you need to know to start building AI Agents ｜Microsoft</strong></a></p>
<h3 id="其他">其他<a hidden class="anchor" aria-hidden="true" href="#其他">#</a></h3>
<p><a href="https://ezyang.github.io/ai-blindspots/">AI Blindspots</a>：一个列出了当前 AI 开发盲点合集的网站</p>
<hr>
<h1 id="影音记录">影音记录<a hidden class="anchor" aria-hidden="true" href="#影音记录">#</a></h1>
<h2 id="精选歌单">精选歌单<a hidden class="anchor" aria-hidden="true" href="#精选歌单">#</a></h2>
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/79yCdpiyyzJYC45MxTpvGD?utm_source=generator" width="100%" height="400" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>
<h2 id="live演出">Live演出<a hidden class="anchor" aria-hidden="true" href="#live演出">#</a></h2>
<p>03.15 Corn Wave</p>
<p>03.18 Mogwai</p>
<p>03.21-23 Can festival</p>
<h1 id="书阅读摘录">书&amp;阅读摘录<a hidden class="anchor" aria-hidden="true" href="#书阅读摘录">#</a></h1>
<h2 id="career-advice-in-2025httpslethaincomcareer-advice-2025"><a href="https://lethain.com/career-advice-2025/">Career advice in 2025</a><a hidden class="anchor" aria-hidden="true" href="#career-advice-in-2025httpslethaincomcareer-advice-2025">#</a></h2>
<blockquote>
<p>Many people who first entered senior roles in 2010-2020 are finding current roles a lot less fun. There are a number of reasons for this. First, managers were generally evaluated in that period based on their ability to hire, retain and motivate teams. The current market doesn’t value those skills particularly highly, but instead prioritizes a different set of skills: working in the details, pushing pace, and navigating the technology transition to foundational models / LLMs.This means many members of the current crop of senior leaders are either worse at the skills they currently need to succeed, or are less motivated by those activities. Either way, they’re having less fun.</p>
</blockquote>
<blockquote>
<p>Valuations and funding are relatively less accessible to non-AI companies than they were three years ago.</p>
</blockquote>
<blockquote>
<p>I also wouldn’t personally try to sit this cycle out unless you’re comfortable with a small risk that reentry is quite difficult: I think it’s more likely that the ecosystem is meaningfully different in five years than that it’s largely unchanged.</p>
</blockquote>
<h2 id="一个人年入千万ai创业的十个步骤httpswwwyoutubecomwatchv4cuqeauh3-4"><a href="https://www.youtube.com/watch?v=4CUqeauH3-4"><strong>一个人年入千万，AI创业的十个步骤</strong></a><a hidden class="anchor" aria-hidden="true" href="#一个人年入千万ai创业的十个步骤httpswwwyoutubecomwatchv4cuqeauh3-4">#</a></h2>
<blockquote>
<p>AI造就的是一个极度放大你个人能力的时代，你一个人可以干以前几十个人干的事情</p>
</blockquote>
<blockquote>
<p>真正的门槛是洞察需求的能力。你做了一个手电筒或者天气软件，不管再精美，也很难帮你做到商业上的成功。</p>
</blockquote>
<blockquote>
<p>MVP就是拿出去让用户骂，用户骂得越针对，你产品进步得越快</p>
</blockquote>
<h2 id="大模型的未来是-agent-还是-apphttpsxcomoran_gestatus1898373775156199730"><a href="https://x.com/oran_ge/status/1898373775156199730"><strong>大模型的未来，是 Agent 还是 App？</strong></a><a hidden class="anchor" aria-hidden="true" href="#大模型的未来是-agent-还是-apphttpsxcomoran_gestatus1898373775156199730">#</a></h2>
<blockquote>
<p>Deep Research 不是标准的 LLM，也不是普通的聊天机器人。它是一种新型的&quot;研究语言模型&quot;，专门设计用于端到端执行搜索任务。</p>
</blockquote>
<blockquote>
<p>目前大多数 Agent 初创公司构建的不是真正的 Agent，而是工作流，即&quot;通过预定义的代码路径编排 LLM 和工具的系统&quot;。工作流在某些垂直领域仍可能创造价值</p>
</blockquote>
<blockquote>
<p>当前的 RAG 系统由许多相互关联但脆弱的工作流组成：路由、分块、重排序、查询解释、查询扩展、源上下文化和搜索工程。随着训练技术栈的发展，有可能将所有这些流程整合到两个独立但相互关联的模型中：一个用于数据准备，另一个用于搜索/检索/报告生成。
这需要精心设计的合成流程和全新的强化学习奖励函数。真正的训练，真正的研究。这在实践中意味着：转移复杂性。</p>
</blockquote>
<h2 id="ai-will-upend-a-basic-assumption-about-how-companies-are-organizedhttpswwwbloombergcomnewsarticles2025-02-28how-ai-reasoning-models-will-change-companies-and-the-economy"><a href="https://www.bloomberg.com/news/articles/2025-02-28/how-ai-reasoning-models-will-change-companies-and-the-economy"><strong>AI Will Upend a Basic Assumption About How Companies Are Organized</strong></a><a hidden class="anchor" aria-hidden="true" href="#ai-will-upend-a-basic-assumption-about-how-companies-are-organizedhttpswwwbloombergcomnewsarticles2025-02-28how-ai-reasoning-models-will-change-companies-and-the-economy">#</a></h2>
<blockquote>
<p>经济体系长期以来建立在这样的观念之上：专业知识稀缺且昂贵。而人工智能即将让这种专业知识变得丰富且几乎免费。</p>
</blockquote>
<blockquote>
<p>支撑我们社会制度的基本假设——“人类洞察力稀缺且昂贵”——将不复存在
摆在个人与企业面前的问题是：当智能本身随处可得且几乎不需成本时，你将如何行动？</p>
</blockquote>
<blockquote>
<p>一个原因在于人们很难想象，“必须依靠资深管理者或顶尖专家”才能完成的工作，居然可以（或者部分可以）由机器来承担。正因为卓越人才稀缺，那些高价值任务才显得格外珍贵。我们的组织结构便是在“真正的高智商人才供给有限”这一认知下设计的。</p>
</blockquote>
<blockquote>
<p>大多数企业领导人仍处在“尝试接受 AI”而非“真正相信 AI”的阶段</p>
</blockquote>
<blockquote>
<p>当智慧的成本几近于零时，真正的瓶颈已不再是“如何获取大脑”，而是“我们如何善加利用”。</p>
</blockquote>
<h2 id="为什么-ai-模型离科学革命还差得很远httpsmpweixinqqcomsgkmesaitsftfvoippc_mag"><a href="https://mp.weixin.qq.com/s/gkmESaiTsfTFvOIPpC_Mag"><strong>为什么 AI 模型离科学革命还差得很远？</strong></a><a hidden class="anchor" aria-hidden="true" href="#为什么-ai-模型离科学革命还差得很远httpsmpweixinqqcomsgkmesaitsftfvoippc_mag">#</a></h2>
<blockquote>
<p>当前 AI 模型的评估方式有误区</p>
</blockquote>
<blockquote>
<p>现在，我们评估 AI 模型智能提升的方式，大多是通过一些“高难度考试题…这些正是我当年擅长的考试类型。这样的评估方法，只能测试模型是否能回答人类已经知道答案的问题。但真正的科学突破，并不来自于答题，而是来自于提出别人没问过的问题，挑战主流认知，质疑已有理论。</p>
</blockquote>
<blockquote>
<p>尽管大语言模型已经掌握了几乎所有人类知识，但它们还没有真正“生成”出新的科学发现。…我们现在训练出来的是“听话的好学生”，不是“有主见的创造者”</p>
</blockquote>
<blockquote>
<p>那么我们也许需要重新定义 AI 模型的评估方式：评估标准不再是“答对多少题”，而是它是否具备真正的科学思维，</p>
</blockquote>
<blockquote>
<p>我们常犯的一个错误，是把牛顿或爱因斯坦看作是“超级优等生”，以为只要把一个好学生按比例放大，就能变成一个天才。… 但这种线性外推忽略了科学最本质的一点：提出正确问题的能力，以及挑战既有知识体系的勇气。</p>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://niraya666.github.io/tags/%E6%9C%88%E5%88%8A/">月刊</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-03 月刊 on x"
            href="https://x.com/intent/tweet/?text=2025-03%20%e6%9c%88%e5%88%8a&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-03-%25E6%259C%2588%25E5%2588%258A%2f&amp;hashtags=%e6%9c%88%e5%88%8a">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-03 月刊 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-03-%25E6%259C%2588%25E5%2588%258A%2f&amp;title=2025-03%20%e6%9c%88%e5%88%8a&amp;summary=2025-03%20%e6%9c%88%e5%88%8a&amp;source=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-03-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-03 月刊 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-03-%25E6%259C%2588%25E5%2588%258A%2f&title=2025-03%20%e6%9c%88%e5%88%8a">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-03 月刊 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-03-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-03 月刊 on whatsapp"
            href="https://api.whatsapp.com/send?text=2025-03%20%e6%9c%88%e5%88%8a%20-%20https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-03-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-03 月刊 on telegram"
            href="https://telegram.me/share/url?text=2025-03%20%e6%9c%88%e5%88%8a&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-03-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-03 月刊 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=2025-03%20%e6%9c%88%e5%88%8a&u=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-03-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="utterances">
  <script src="https://utteranc.es/client.js"
        repo="https://github.com/Niraya666/niraya666.github.io.git"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>


<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: {'[+]': ['ams']}
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://niraya666.github.io/">LZY Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
