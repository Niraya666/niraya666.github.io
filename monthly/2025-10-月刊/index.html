<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>2025-10 月刊 | LZY Blog</title>
<meta name="keywords" content="月刊">
<meta name="description" content="值得关注的模型和新技术
OpenAI-Sora
Anthropic introduced Agent Skills
DeepSeek-OCR
MiniMaxAI/MiniMax-M2
zai-org/GLM-4.6
ChatGPT Atlas
OCR-VL系列：
PaddlePaddle/PaddleOCR-VL
allenai/olmOCR-2-7B-1025
datalab-to/chandra
lightonai/LightOnOCR-1B-1025
nanonets/Nanonets-OCR2-3B
值得关注的开源项目
MineContext： MineContext is your proactive context-aware AI partner（Context-Engineering&#43;ChatGPT Pulse）
DeepAnalyze: agentic LLM for autonomous data science.
Enterprise Deep Research
PokeeResearch-7B Agent: Pokee Deep Research Model Open Source Repo
值得关注的研究和论文
ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory
arXiv:2509.25140
ReasoningBank： 从成功与失败经验中提炼可迁移推理策略的记忆框架
不再存储原始轨迹或仅成功流程，而是提取结构化、可复用的推理单元（title &#43; description &#43; content）
并提出Memory-aware Test-Time Scaling (MaTTS)，Test-Time Scaling 与记忆系统结合，更多探索 → 更丰富对比信号 → 更高质量记忆 → 更好指导未来探索， 采用两种扩展方式：Parallel Scaling（多轨迹对比，提取稳定策略）和Sequential Scaling （单轨迹内自我修正）">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://niraya666.github.io/monthly/2025-10-%E6%9C%88%E5%88%8A/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://niraya666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://niraya666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://niraya666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://niraya666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://niraya666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://niraya666.github.io/monthly/2025-10-%E6%9C%88%E5%88%8A/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="2025-10 月刊" />
<meta property="og:description" content="值得关注的模型和新技术
OpenAI-Sora
Anthropic introduced Agent Skills
DeepSeek-OCR
MiniMaxAI/MiniMax-M2
zai-org/GLM-4.6
ChatGPT Atlas
OCR-VL系列：
PaddlePaddle/PaddleOCR-VL
allenai/olmOCR-2-7B-1025
datalab-to/chandra
lightonai/LightOnOCR-1B-1025
nanonets/Nanonets-OCR2-3B
值得关注的开源项目
MineContext： MineContext is your proactive context-aware AI partner（Context-Engineering&#43;ChatGPT Pulse）
DeepAnalyze: agentic LLM for autonomous data science.
Enterprise Deep Research
PokeeResearch-7B Agent: Pokee Deep Research Model Open Source Repo
值得关注的研究和论文
ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory
arXiv:2509.25140
ReasoningBank： 从成功与失败经验中提炼可迁移推理策略的记忆框架
不再存储原始轨迹或仅成功流程，而是提取结构化、可复用的推理单元（title &#43; description &#43; content）
并提出Memory-aware Test-Time Scaling (MaTTS)，Test-Time Scaling 与记忆系统结合，更多探索 → 更丰富对比信号 → 更高质量记忆 → 更好指导未来探索， 采用两种扩展方式：Parallel Scaling（多轨迹对比，提取稳定策略）和Sequential Scaling （单轨迹内自我修正）" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://niraya666.github.io/monthly/2025-10-%E6%9C%88%E5%88%8A/" />
<meta property="og:image" content="https://niraya666.github.io/img/monthly/2025-07/339C0C07-DAA4-4FAB-A66F-C3B9FE174A4E_1_105_c.jpeg" /><meta property="article:section" content="monthly" />
<meta property="article:published_time" content="2025-10-31T20:25:00+08:00" />
<meta property="article:modified_time" content="2025-10-31T20:25:00+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://niraya666.github.io/img/monthly/2025-07/339C0C07-DAA4-4FAB-A66F-C3B9FE174A4E_1_105_c.jpeg" />
<meta name="twitter:title" content="2025-10 月刊"/>
<meta name="twitter:description" content="值得关注的模型和新技术
OpenAI-Sora
Anthropic introduced Agent Skills
DeepSeek-OCR
MiniMaxAI/MiniMax-M2
zai-org/GLM-4.6
ChatGPT Atlas
OCR-VL系列：
PaddlePaddle/PaddleOCR-VL
allenai/olmOCR-2-7B-1025
datalab-to/chandra
lightonai/LightOnOCR-1B-1025
nanonets/Nanonets-OCR2-3B
值得关注的开源项目
MineContext： MineContext is your proactive context-aware AI partner（Context-Engineering&#43;ChatGPT Pulse）
DeepAnalyze: agentic LLM for autonomous data science.
Enterprise Deep Research
PokeeResearch-7B Agent: Pokee Deep Research Model Open Source Repo
值得关注的研究和论文
ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory
arXiv:2509.25140
ReasoningBank： 从成功与失败经验中提炼可迁移推理策略的记忆框架
不再存储原始轨迹或仅成功流程，而是提取结构化、可复用的推理单元（title &#43; description &#43; content）
并提出Memory-aware Test-Time Scaling (MaTTS)，Test-Time Scaling 与记忆系统结合，更多探索 → 更丰富对比信号 → 更高质量记忆 → 更好指导未来探索， 采用两种扩展方式：Parallel Scaling（多轨迹对比，提取稳定策略）和Sequential Scaling （单轨迹内自我修正）"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Monthlies",
      "item": "https://niraya666.github.io/monthly/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "2025-10 月刊",
      "item": "https://niraya666.github.io/monthly/2025-10-%E6%9C%88%E5%88%8A/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "2025-10 月刊",
  "name": "2025-10 月刊",
  "description": "值得关注的模型和新技术 OpenAI-Sora\nAnthropic introduced Agent Skills\nDeepSeek-OCR\nMiniMaxAI/MiniMax-M2\nzai-org/GLM-4.6\nChatGPT Atlas\nOCR-VL系列：\nPaddlePaddle/PaddleOCR-VL\nallenai/olmOCR-2-7B-1025\ndatalab-to/chandra\nlightonai/LightOnOCR-1B-1025\nnanonets/Nanonets-OCR2-3B\n值得关注的开源项目 MineContext： MineContext is your proactive context-aware AI partner（Context-Engineering+ChatGPT Pulse）\nDeepAnalyze: agentic LLM for autonomous data science.\nEnterprise Deep Research\nPokeeResearch-7B Agent: Pokee Deep Research Model Open Source Repo\n值得关注的研究和论文 ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory arXiv:2509.25140\nReasoningBank： 从成功与失败经验中提炼可迁移推理策略的记忆框架\n不再存储原始轨迹或仅成功流程，而是提取结构化、可复用的推理单元（title + description + content）\n并提出Memory-aware Test-Time Scaling (MaTTS)，Test-Time Scaling 与记忆系统结合，更多探索 → 更丰富对比信号 → 更高质量记忆 → 更好指导未来探索， 采用两种扩展方式：Parallel Scaling（多轨迹对比，提取稳定策略）和Sequential Scaling （单轨迹内自我修正）\n",
  "keywords": [
    "月刊"
  ],
  "articleBody": "值得关注的模型和新技术 OpenAI-Sora\nAnthropic introduced Agent Skills\nDeepSeek-OCR\nMiniMaxAI/MiniMax-M2\nzai-org/GLM-4.6\nChatGPT Atlas\nOCR-VL系列：\nPaddlePaddle/PaddleOCR-VL\nallenai/olmOCR-2-7B-1025\ndatalab-to/chandra\nlightonai/LightOnOCR-1B-1025\nnanonets/Nanonets-OCR2-3B\n值得关注的开源项目 MineContext： MineContext is your proactive context-aware AI partner（Context-Engineering+ChatGPT Pulse）\nDeepAnalyze: agentic LLM for autonomous data science.\nEnterprise Deep Research\nPokeeResearch-7B Agent: Pokee Deep Research Model Open Source Repo\n值得关注的研究和论文 ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory arXiv:2509.25140\nReasoningBank： 从成功与失败经验中提炼可迁移推理策略的记忆框架\n不再存储原始轨迹或仅成功流程，而是提取结构化、可复用的推理单元（title + description + content）\n并提出Memory-aware Test-Time Scaling (MaTTS)，Test-Time Scaling 与记忆系统结合，更多探索 → 更丰富对比信号 → 更高质量记忆 → 更好指导未来探索， 采用两种扩展方式：Parallel Scaling（多轨迹对比，提取稳定策略）和Sequential Scaling （单轨迹内自我修正）\n具体实现\n1. ReasoningBank\n使用 LLM-as-a-Judge 自动判断轨迹是“成功”还是“失败”， 对成功轨迹：提取“成功策略”， 对失败轨迹：提取“失败教训”或“反面警示”， 输出结构化记忆项， 如：\nTitle: “使用搜索功能定位商品” Description: “当页面无直接链接时，优先使用搜索框” Content: “在购物网站中，若未找到目标商品，应使用顶部搜索栏输入关键词，而非盲目点击分类”\n而后对当前任务embedding，通过语义检索召回相关top-k\n将检索到的记忆项注入系统提示\n每完成一个任务，自动提取新记忆项，直接追加到记忆库\nMaTTS 实现 Parallel Scaling： 对同一任务生成 k 条轨迹， self-contrast识别哪些策略导致成功，哪些导致失败，提炼出更鲁棒、可迁移的记忆项\nSequential Scaling： 在单条轨迹内进行多次self-refinement， 每次修正后记录中间推理过程，作为记忆信号\nReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization arXiv:2509.13313\nReAct会将每一轮的“思考-动作-观察”全部追加到上下文中，导致随着搜索轮次增加，上下文迅速膨胀，最终超出模型的上下文长度限制\nReSum 的核心目标：通过周期性上下文压缩（摘要），让智能体在不丢失关键信息的前提下，突破上下文长度限制。\n创新点：\n在 ReAct 基础上，周期性调用摘要工具，将历史对话压缩为结构化摘要（包含已确认证据 + 信息缺口 + 下一步方向），用摘要 + 原始问题组成“压缩状态”，重启推理，从而绕过上下文长度限制 不是直接用通用大模型做摘要，而是专门训练一个30B参数的模型 专门设计 ReSum-GRPO算法：自动将长轨迹按摘要点分段，每段作为独立训练样本，将整个轨迹的最终奖励（是否答对）广播到所有分段 Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models arXiv:2510.04618\n在context adaptation中，无需微调权重、仅通过动态演化上下文即可自我改进的框架，即ACE（Agentic Context Engineering）\nGenerator + Reflector + Curator： Generator（生成推理轨迹），Reflector（从成功/失败中提炼具体经验），以及Curator（将经验以结构化“增量条目”形式合并进上下文，避免整体重写） 上下文不是整体重写，而是以bullet points形式局部更新，每个条目包含元数据和内容 Grow-and-Refine： 新条目追加，旧条目可更新（如增加计数），并定期通过语义嵌入去重，保持上下文精炼、无冗余，同时支持长期扩展 Mem-α: Learning Memory Construction via Reinforcement Learning arXiv:2509.25911\n如何让 LLM 代理自主学会高效、结构化地构建和管理复杂记忆系统\n提出基于RL的记忆构建框架 Mem-α，不再依赖预设指令，而是通过**与环境交互 + 奖励反馈，**奖励信号直接来自下游任务表现\n记忆架构：\n核心记忆（Core）：512 token 的摘要，始终在上下文中，只支持 update，强制保持摘要简洁。 语义记忆（Semantic）：存储事实性知识，支持增删改。 情节记忆（Episodic）：按时间戳存储事件，支持时序推理。 每个组件配备专用操作工具（insert/update/delete） 所有操作通过结构化函数调用完成：\nmemory_insert(memory_type='semantic', content='用户喜欢安静的环境') RL设计：\n状态：当前记忆状态 ℳₜ₋₁ + 当前输入块 cₜ 动作：在每一步 t，模型可执行多个记忆操作（如 insert/update/delete），每个操作是结构化函数调用 奖励： r₁（正确性）：用 RAG 从最终记忆中回答问题，根据准确率打分。 r₂（工具格式）：检查函数调用是否符合规范（如参数类型、格式）。 r₃（压缩率）：鼓励记忆长度远小于原始输入长度（1 - lₘ/l꜀）。 r₄（语义有效性）：用 Qwen3-32B 作为“裁判”判断操作是否语义合理 优化算法： GRPO DeepSeek-OCR: Contexts Optical Compression https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\n通过Contexts Optical Compression将将文本信息“压缩”到视觉信息中。\n用比原始数字文本少得多的vision tokens来表示，从而实现信息压缩率。\n视觉编码器DeepEncoder\n将以Window Attention为主的SAM模型和以Global Attention为主的CLIP串联起来 在两者之间加入一个2层卷积层，以实现16倍下采样 Decoder\nDeepSeek-3B-MoE\n数据集\nOCR 1.0数据：包含3000万页多语言文档（粗略/精细标注）和场景文字图像，用于训练基础的文字识别能力。 OCR 2.0数据：包含图表、化学分子式、平面几何图形等复杂结构化图像的解析数据，提升模型的深度解析能力。 通用视觉数据：用于保留模型对自然图像的理解能力，如图像描述、目标检测等。 纯文本数据：用于维持和增强模型的语言能力 采用两阶段训练：\n第一阶段：单独训练DeepEncoder，使其学会从图像中提取有效的特征。 第二阶段：将训练好的DeepEncoder与DeepSeek-3B-MoE解码器连接起来，进行端到端的联合训练。训练时，编码器的部分参数被冻结，以稳定训练过程。 推荐内容 Hello-Agents: 从零开始的智能体原理与实践教程\ndeeplearningAI: agentic-ai-course\nHuggingface: The Smol Training Playbook\n影音记录 精选歌单 Live演出 10.17 Valentina Lisitsa钢琴独奏｜东艺\n书\u0026阅读摘录 Not Another Workflow Builder An LLM agent runs tools in a loop to achieve a goal.\nThere’s two main reasons people are interested in this:\nMany companies are more resource constrained on engineering talent than others Non-technical users are the ones who know what agents to build / what they should do The issue with visual workflow builders: 1.Visual workflow builders are not “low” barrier to entry. 2.Complex tasks quickly get too complicated to manage in a visual builder.\nI think agents (prompt + tools) should be strictly easier to create in a no-code way than workflows.\n关于 AI Infra 的一切 算法人只有两年的保质期，两年后 Ta 把自己的聪明才智发挥完了，就会陷入思维定势，反而跟不上后面的新东西了。\n所以这里可以给从业者一个建议，就是不要做夹在模型和硬件中间的那个人。\n这正是我认为需要被纠正的观念。前面提到过，Infra 实际上是可以对模型效果有正向影响的，而不仅仅是只能降本。\n模型其实由算法、Infra 和数据这个铁三角决定。三者缺一不可，必须协同。\n所以实际上比较合理的组织架构是，让 Infra 人去设计模型结构，因为 Infra 人最知道该怎么提高效率、节省成本，让数据的人去负责刷模型的点数和 benchmark 分数，因为他们最懂怎么喂模型，而算法人应该主要负责训练的范式革新。\nBuilding in the AI Era: The HeyGen Way We move fast AND be the absolute best… moving fast allows us to build better quality… our success metric: the average video quality any user can achieve.\nWhy 2 months? This aligns with model upgrade cycles… 2-month roadmap… 6-12 month strategic bets… Daily shipping\nDay 1: Define hypothesis… Day 2: Build MVP… Days 3-5: Ship… Week 2: Analyze… Failure with learning = victory\nSpeed Is Everything… Embrace the Technology Wave… Disagree and Commit… When in doubt, ship an experiment\nGrowth teams are built for a different game… In the AI era, code is cheap. Impact is valuable\nAI落地为什么这么难？ 千万别，不要向老板卖降本的方案。 降本是个陷阱。 老板说要降本， 其实要的是利润。\nAI to B 落地，往往会遇到三方博弈：老板要利润，但说不清路径。 中层要业绩，不敢碰组织。 员工要饭碗，会软抵抗。\n但还有第三条路：找一个“无主之地”。不是取代现有岗位， 是做现在没人做的事，做增收，或者能帮采用部门，扩大预算或者人头的事。听起来好像跟老板原来降本的需求南辕北辙。但却是真实发生的案例。\nAI落地的关键， 不是证明 AI 比人强。是让人觉得： 我用了 AI，我更强。\n别动存量，找增量。\n别替代人， 武装人。\n对李想的第二次3小时访谈：智慧是我们和万物的关系 人类并不擅长处理特别复杂的信息——人类要做“熵减”，而不是“熵增”。人类之所以发明方法论、创造工具，本质是为了减少能量消耗——因为大脑运作需要能量。\n今天的人工智能，处理通用信息表现不错，但一旦进入专业领域，往往不如人。这是因为，这些领域需要更高质量的数据和更专业的CoT\n今天Chatbot，或者Reasoning，做更长链条思考和推理任务时，也能看到一系列严重的问题——几乎所有人，还是把它当成“信息工具”\n还是要站在用户价值的角度，如果大家拼命使用AI，为AI投资，但我的工作时长没减少，结果也没变好，问题到底在哪？ 今天通过对话（Chatbot），无论文本多么长，它都是“信息工具”，对大家是参考作用。 什么时候真正改善工作成果和减少工作时长？它必须变成“生产工具”。\n另外当我Action做完以后，我还会做一个Diffusion（扩散）预测，下边会发生什么样的时长的一个场景。这个主要根据性能，会做出4到8秒Diffusion的轨迹和环境预测\n五年内没有通用Agent， 但会有Agent OS\n先做好公司内部的Agent，因为公司内部需要有很多Agent——客服Agent、编程Agent\n搭建出一个有效的Agent OS，让每个专业的人在上面开发自己所在领域的Agent\n你不可能让智能商业团队做客服Agent，应该是客服团队借助这个Agent OS，开发出自己专业的客服Agent\n汽车会从智能终端变成人工智能终端。这是我们要做的。\n今天，工厂也面临这样的机会。很多人想的是，造一个人形机器人进工厂替代人，但我认为有两个问题： 工厂里人的成本占比并没有大家想象的那么高，替代起来并不划算； 这些就业岗位长期来看还是必要的\n只关注替代人，我认为是狭隘的。\n我们没有把握住用户的需求，我们会消失； 我们没有掌握最好的产品和技术，我们会消失； 我们的组织能力方面出现了巨大的问题，我们也会消失\n这些所有的讨论、争执、吵架，背后有一个更重要的一点——是能量。\n当人和人之间的能量始终存在的时候，这些争执、讨论、吵架就是一个更完善的大脑。 当这些能量消失的时候，这些争执、讨论、不同想法就会变成内耗。\n",
  "wordCount" : "520",
  "inLanguage": "en",
  "image":"https://niraya666.github.io/img/monthly/2025-07/339C0C07-DAA4-4FAB-A66F-C3B9FE174A4E_1_105_c.jpeg","datePublished": "2025-10-31T20:25:00+08:00",
  "dateModified": "2025-10-31T20:25:00+08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://niraya666.github.io/monthly/2025-10-%E6%9C%88%E5%88%8A/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LZY Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://niraya666.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niraya666.github.io/" accesskey="h" title="LZY Blog (Alt + H)">LZY Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niraya666.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/posts/" title="AI">
                    <span>AI</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/musik/" title="Musik!">
                    <span>Musik!</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/monthly/" title="月刊">
                    <span>月刊</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/essay/" title="杂文">
                    <span>杂文</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/travel/" title="游记">
                    <span>游记</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://niraya666.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://niraya666.github.io/monthly/">Monthlies</a></div>
    <h1 class="post-title entry-hint-parent">
      2025-10 月刊
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-10-31 20:25:00 +0800 CST'>October 31, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Theme PaperMod

</div>
  </header> 
<figure class="entry-cover"><a href="https://niraya666.github.io/img/monthly/2025-07/339C0C07-DAA4-4FAB-A66F-C3B9FE174A4E_1_105_c.jpeg" target="_blank"
            rel="noopener noreferrer"><img loading="eager" src="https://niraya666.github.io/img/monthly/2025-07/339C0C07-DAA4-4FAB-A66F-C3B9FE174A4E_1_105_c.jpeg" alt="摄于 高见岛"></a>
        <p>摄于 高见岛</p>
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e6%a8%a1%e5%9e%8b%e5%92%8c%e6%96%b0%e6%8a%80%e6%9c%af" aria-label="值得关注的模型和新技术">值得关注的模型和新技术</a></li>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e5%bc%80%e6%ba%90%e9%a1%b9%e7%9b%ae" aria-label="值得关注的开源项目">值得关注的开源项目</a></li>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e7%a0%94%e7%a9%b6%e5%92%8c%e8%ae%ba%e6%96%87" aria-label="值得关注的研究和论文">值得关注的研究和论文</a><ul>
                        
                <li>
                    <a href="#reasoningbank-scaling-agent-self-evolving-with-reasoning-memory" aria-label="ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory">ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory</a></li>
                <li>
                    <a href="#resum-unlocking-long-horizon-search-intelligence-via-context-summarization" aria-label="ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization">ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization</a></li>
                <li>
                    <a href="#agentic-context-engineering-evolving-contexts-for-self-improving-language-models" aria-label="Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models">Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models</a></li>
                <li>
                    <a href="#mem-%ce%b1-learning-memory-construction-via-reinforcement-learning" aria-label="Mem-α: Learning Memory Construction via Reinforcement Learning">Mem-α: Learning Memory Construction via Reinforcement Learning</a></li>
                <li>
                    <a href="#deepseek-ocr-contexts-optical-compression" aria-label="DeepSeek-OCR: Contexts Optical Compression">DeepSeek-OCR: Contexts Optical Compression</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%8e%a8%e8%8d%90%e5%86%85%e5%ae%b9" aria-label="推荐内容">推荐内容</a></li>
                <li>
                    <a href="#%e5%bd%b1%e9%9f%b3%e8%ae%b0%e5%bd%95" aria-label="影音记录">影音记录</a><ul>
                        
                <li>
                    <a href="#%e7%b2%be%e9%80%89%e6%ad%8c%e5%8d%95" aria-label="精选歌单">精选歌单</a></li>
                <li>
                    <a href="#live%e6%bc%94%e5%87%ba" aria-label="Live演出">Live演出</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%b9%a6%e9%98%85%e8%af%bb%e6%91%98%e5%bd%95" aria-label="书&amp;阅读摘录">书&amp;阅读摘录</a><ul>
                        
                <li>
                    <a href="#not-another-workflow-builderhttpsbloglangchaincomnot-another-workflow-builder" aria-label="Not Another Workflow Builder">Not Another Workflow Builder</a></li>
                <li>
                    <a href="#%e5%85%b3%e4%ba%8e-ai-infra-%e7%9a%84%e4%b8%80%e5%88%87httpswwwwoshipmcomai6253851html" aria-label="关于 AI Infra 的一切">关于 AI Infra 的一切</a></li>
                <li>
                    <a href="#building-in-the-ai-era-the-heygen-wayhttpsxcomjoshua_xu_status1978837502787219578" aria-label="Building in the AI Era: The HeyGen Way">Building in the AI Era: The HeyGen Way</a></li>
                <li>
                    <a href="#ai%e8%90%bd%e5%9c%b0%e4%b8%ba%e4%bb%80%e4%b9%88%e8%bf%99%e4%b9%88%e9%9a%behttpsxcom0xshellywangstatus1980195228368986310" aria-label="AI落地为什么这么难？">AI落地为什么这么难？</a></li>
                <li>
                    <a href="#%e5%af%b9%e6%9d%8e%e6%83%b3%e7%9a%84%e7%ac%ac%e4%ba%8c%e6%ac%a13%e5%b0%8f%e6%97%b6%e8%ae%bf%e8%b0%88%e6%99%ba%e6%85%a7%e6%98%af%e6%88%91%e4%bb%ac%e5%92%8c%e4%b8%87%e7%89%a9%e7%9a%84%e5%85%b3%e7%b3%bbhttpsmpweixinqqcomslr6p9mz9pwhdjklevyijaw" aria-label="对李想的第二次3小时访谈：智慧是我们和万物的关系">对李想的第二次3小时访谈：智慧是我们和万物的关系</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="值得关注的模型和新技术">值得关注的模型和新技术<a hidden class="anchor" aria-hidden="true" href="#值得关注的模型和新技术">#</a></h1>
<p><a href="https://openai.com/zh-Hans-CN/index/sora-2/">OpenAI-Sora</a></p>
<p><a href="https://www.anthropic.com/news/skills">Anthropic introduced Agent Skills</a></p>
<p><a href="https://arxiv.org/html/2510.18234v1">DeepSeek-OCR</a></p>
<p><a href="https://huggingface.co/MiniMaxAI/MiniMax-M2">MiniMaxAI/MiniMax-M2</a></p>
<p><a href="https://huggingface.co/zai-org/GLM-4.6">zai-org/GLM-4.6</a></p>
<p><a href="https://openai.com/zh-Hans-CN/index/introducing-chatgpt-atlas/">ChatGPT Atlas</a></p>
<p>OCR-VL系列：</p>
<p><a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL">PaddlePaddle/PaddleOCR-VL</a></p>
<p><a href="https://huggingface.co/allenai/olmOCR-2-7B-1025">allenai/olmOCR-2-7B-1025</a></p>
<p><a href="https://huggingface.co/datalab-to/chandra">datalab-to/chandra</a></p>
<p><a href="https://huggingface.co/lightonai/LightOnOCR-1B-1025">lightonai/LightOnOCR-1B-1025</a></p>
<p><a href="https://huggingface.co/nanonets/Nanonets-OCR2-3B">nanonets/Nanonets-OCR2-3B</a></p>
<h1 id="值得关注的开源项目">值得关注的开源项目<a hidden class="anchor" aria-hidden="true" href="#值得关注的开源项目">#</a></h1>
<p><a href="https://github.com/volcengine/MineContext">MineContext</a>： MineContext is your proactive context-aware AI partner（Context-Engineering+ChatGPT Pulse）</p>
<p><a href="https://github.com/ruc-datalab/DeepAnalyze">DeepAnalyze</a>: agentic LLM for autonomous data science.</p>
<p><a href="https://github.com/SalesforceAIResearch/enterprise-deep-research">Enterprise Deep Research</a></p>
<p><a href="https://github.com/Pokee-AI/PokeeResearchOSS">PokeeResearch-7B Agent</a>: Pokee Deep Research Model Open Source Repo</p>
<h1 id="值得关注的研究和论文">值得关注的研究和论文<a hidden class="anchor" aria-hidden="true" href="#值得关注的研究和论文">#</a></h1>
<h2 id="reasoningbank-scaling-agent-self-evolving-with-reasoning-memory"><strong>ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory</strong><a hidden class="anchor" aria-hidden="true" href="#reasoningbank-scaling-agent-self-evolving-with-reasoning-memory">#</a></h2>
<p>arXiv:<a href="https://arxiv.org/abs/2509.25140">2509.25140</a></p>
<p>ReasoningBank： 从成功与失败经验中提炼可迁移推理策略的记忆框架</p>
<p>不再存储原始轨迹或仅成功流程，而是提取<strong>结构化、可复用的推理单元</strong>（title + description + content）</p>
<p>并提出Memory-aware Test-Time Scaling (MaTTS)，Test-Time Scaling 与记忆系统结合，更多探索 → 更丰富对比信号 → 更高质量记忆 → 更好指导未来探索， 采用两种扩展方式：Parallel Scaling（多轨迹对比，提取稳定策略）和Sequential Scaling （单轨迹内自我修正）</p>
<p><strong>具体实现</strong></p>
<p><img loading="lazy" src="https://arxiv.org/html/2509.25140v1/x2.png" alt=""  />
</p>
<p><strong>1.</strong> ReasoningBank</p>
<p>使用 LLM-as-a-Judge 自动判断轨迹是“成功”还是“失败”， 对成功轨迹：提取“成功策略”， 对失败轨迹：提取“失败教训”或“反面警示”， 输出结构化记忆项， 如：</p>
<blockquote>
<p>Title: “使用搜索功能定位商品” Description: “当页面无直接链接时，优先使用搜索框” Content: “在购物网站中，若未找到目标商品，应使用顶部搜索栏输入关键词，而非盲目点击分类”</p>
</blockquote>
<p>而后对当前任务embedding，通过语义检索召回相关top-k</p>
<p>将检索到的记忆项注入系统提示</p>
<p>每完成一个任务，自动提取新记忆项，直接追加到记忆库</p>
<ol start="2">
<li>MaTTS 实现</li>
</ol>
<p><img loading="lazy" src="https://arxiv.org/html/2509.25140v1/x3.png" alt=""  />
</p>
<p>Parallel Scaling： 对同一任务生成 k 条轨迹， self-contrast识别哪些策略导致成功，哪些导致失败，提炼出更鲁棒、可迁移的记忆项</p>
<p>Sequential Scaling： 在单条轨迹内进行多次self-refinement， 每次修正后记录中间推理过程，作为记忆信号</p>
<h2 id="resum-unlocking-long-horizon-search-intelligence-via-context-summarization">ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization<a hidden class="anchor" aria-hidden="true" href="#resum-unlocking-long-horizon-search-intelligence-via-context-summarization">#</a></h2>
<p>arXiv:<a href="https://arxiv.org/abs/2509.13313">2509.13313</a></p>
<p><strong>ReAct</strong>会将每一轮的“思考-动作-观察”全部追加到上下文中，导致随着搜索轮次增加，上下文迅速膨胀，最终超出模型的上下文长度限制</p>
<p><strong>ReSum 的核心目标</strong>：通过<strong>周期性上下文压缩（摘要）</strong>，让智能体在不丢失关键信息的前提下，突破上下文长度限制。</p>
<p><strong>创新点：</strong></p>
<ul>
<li>在 ReAct 基础上，<strong>周期性调用摘要工具</strong>，将历史对话压缩为结构化摘要（包含已确认证据 + 信息缺口 + 下一步方向），用摘要 + 原始问题组成“压缩状态”，重启推理，从而<strong>绕过上下文长度限制</strong></li>
<li>不是直接用通用大模型做摘要，而是<strong>专门训练一个30B参数的模型</strong></li>
<li>专门设计 <strong>ReSum-GRPO</strong>算法：自动将长轨迹按摘要点<strong>分段</strong>，每段作为独立训练样本，将<strong>整个轨迹的最终奖励（是否答对）广播到所有分段</strong></li>
</ul>
<h2 id="agentic-context-engineering-evolving-contexts-for-self-improving-language-models">Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models<a hidden class="anchor" aria-hidden="true" href="#agentic-context-engineering-evolving-contexts-for-self-improving-language-models">#</a></h2>
<p>arXiv:<a href="https://arxiv.org/abs/2510.04618">2510.04618</a></p>
<p><img loading="lazy" src="https://arxiv.org/html/2510.04618v1/x3.png" alt=""  />
</p>
<p>在context adaptation中，无需微调权重、仅通过动态演化上下文即可自我改进的框架，即<strong>ACE（Agentic Context Engineering）</strong></p>
<ul>
<li><strong>Generator + Reflector + Curator：</strong> Generator（生成推理轨迹），Reflector（从成功/失败中提炼具体经验），以及Curator（将经验以结构化“增量条目”形式合并进上下文，避免整体重写）</li>
<li>上下文不是整体重写，而是以bullet points形式局部更新，每个条目包含元数据和内容</li>
<li><strong>Grow-and-Refine：</strong> 新条目追加，旧条目可更新（如增加计数），并定期通过语义嵌入去重，保持上下文精炼、无冗余，同时支持长期扩展</li>
</ul>
<h2 id="mem-α-learning-memory-construction-via-reinforcement-learning">Mem-α: Learning Memory Construction via Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#mem-α-learning-memory-construction-via-reinforcement-learning">#</a></h2>
<p>arXiv:<a href="https://arxiv.org/abs/2509.25911">2509.25911</a></p>
<p>如何让 LLM 代理<strong>自主学会高效、结构化地构建和管理复杂记忆系统</strong></p>
<p><img loading="lazy" src="https://arxiv.org/html/2509.25911v1/figures/training_framework.png" alt=""  />
</p>
<p>提出基于RL的记忆构建框架 Mem-α，不再依赖预设指令，而是通过**与环境交互 + 奖励反馈，**奖励信号直接来自下游任务表现</p>
<p><strong>记忆架构：</strong></p>
<ul>
<li><strong>核心记忆（Core）</strong>：512 token 的摘要，始终在上下文中，只支持 update，强制保持摘要简洁。</li>
<li><strong>语义记忆（Semantic）</strong>：存储事实性知识，支持增删改。</li>
<li><strong>情节记忆（Episodic）</strong>：按时间戳存储事件，支持时序推理。</li>
<li>每个组件配备专用操作工具（insert/update/delete）</li>
</ul>
<p>所有操作通过结构化函数调用完成：</p>
<pre tabindex="0"><code>memory_insert(memory_type=&#39;semantic&#39;, content=&#39;用户喜欢安静的环境&#39;)
</code></pre><p>RL设计：</p>
<ul>
<li><strong>状态</strong>：当前记忆状态 ℳₜ₋₁ + 当前输入块 cₜ</li>
<li><strong>动作</strong>：在每一步 t，模型可执行多个记忆操作（如 insert/update/delete），每个操作是结构化函数调用</li>
<li><strong>奖励</strong>：
<ul>
<li><strong>r₁（正确性）</strong>：用 RAG 从最终记忆中回答问题，根据准确率打分。</li>
<li><strong>r₂（工具格式）</strong>：检查函数调用是否符合规范（如参数类型、格式）。</li>
<li><strong>r₃（压缩率）</strong>：鼓励记忆长度远小于原始输入长度（1 - lₘ/l꜀）。</li>
<li><strong>r₄（语义有效性）</strong>：用 Qwen3-32B 作为“裁判”判断操作是否语义合理</li>
</ul>
</li>
<li><strong>优化算法： GRPO</strong></li>
</ul>
<h2 id="deepseek-ocr-contexts-optical-compression">DeepSeek-OCR: Contexts Optical Compression<a hidden class="anchor" aria-hidden="true" href="#deepseek-ocr-contexts-optical-compression">#</a></h2>
<p><a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf">https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf</a></p>
<p>通过Contexts Optical Compression将<strong>将文本信息“压缩”到视觉信息中。</strong></p>
<p>用比原始数字文本少得多的vision tokens来表示，从而实现信息压缩率。</p>
<p><strong>视觉编码器DeepEncoder</strong></p>
<ul>
<li>将以Window Attention为主的SAM模型和以Global Attention为主的CLIP串联起来</li>
<li>在两者之间加入一个2层卷积层，以实现16倍下采样</li>
</ul>
<p><strong>Decoder</strong></p>
<p>DeepSeek-3B-MoE</p>
<p>数据集</p>
<ul>
<li><strong>OCR 1.0数据</strong>：包含3000万页多语言文档（粗略/精细标注）和场景文字图像，用于训练基础的文字识别能力。</li>
<li><strong>OCR 2.0数据</strong>：包含图表、化学分子式、平面几何图形等复杂结构化图像的解析数据，提升模型的深度解析能力。</li>
<li><strong>通用视觉数据</strong>：用于保留模型对自然图像的理解能力，如图像描述、目标检测等。</li>
<li><strong>纯文本数据</strong>：用于维持和增强模型的语言能力</li>
</ul>
<p>采用两阶段训练：</p>
<ul>
<li><strong>第一阶段</strong>：单独训练DeepEncoder，使其学会从图像中提取有效的特征。</li>
<li><strong>第二阶段</strong>：将训练好的DeepEncoder与DeepSeek-3B-MoE解码器连接起来，进行端到端的联合训练。训练时，编码器的部分参数被冻结，以稳定训练过程。</li>
</ul>
<h1 id="推荐内容">推荐内容<a hidden class="anchor" aria-hidden="true" href="#推荐内容">#</a></h1>
<p><a href="https://github.com/datawhalechina/hello-agents">Hello-Agents</a>: 从零开始的智能体原理与实践教程</p>
<p>deeplearningAI: <a href="learn.deeplearning.ai/courses/agentic-ai">agentic-ai-course</a></p>
<p>Huggingface: <a href="https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#introduction">The Smol Training Playbook</a></p>
<hr>
<h1 id="影音记录">影音记录<a hidden class="anchor" aria-hidden="true" href="#影音记录">#</a></h1>
<h2 id="精选歌单">精选歌单<a hidden class="anchor" aria-hidden="true" href="#精选歌单">#</a></h2>
<iframe data-testid="embed-iframe" style="border-radius:12px" src="https://open.spotify.com/embed/playlist/7vFcDK6K0tTftyWqPcHvWc?utm_source=generator" width="100%" height="450" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>
<h2 id="live演出">Live演出<a hidden class="anchor" aria-hidden="true" href="#live演出">#</a></h2>
<p>10.17 Valentina Lisitsa钢琴独奏｜东艺</p>
<h1 id="书阅读摘录">书&amp;阅读摘录<a hidden class="anchor" aria-hidden="true" href="#书阅读摘录">#</a></h1>
<h2 id="not-another-workflow-builderhttpsbloglangchaincomnot-another-workflow-builder"><a href="https://blog.langchain.com/not-another-workflow-builder/"><strong>Not Another Workflow Builder</strong></a><a hidden class="anchor" aria-hidden="true" href="#not-another-workflow-builderhttpsbloglangchaincomnot-another-workflow-builder">#</a></h2>
<blockquote>
<p>An LLM agent runs tools in a loop to achieve a goal.</p>
</blockquote>
<blockquote>
<p>There’s two main reasons people are interested in this:</p>
<ol>
<li>Many companies are more resource constrained on engineering talent than others</li>
<li>Non-technical users are the ones who know what agents to build / what they should do</li>
</ol>
</blockquote>
<blockquote>
<p><strong>The issue with visual workflow builders:
1.Visual workflow builders are not “low” barrier to entry.
2.Complex tasks quickly get too complicated to manage in a visual builder.</strong></p>
</blockquote>
<blockquote>
<p>I think agents (prompt + tools) should be strictly easier to create in a no-code way than workflows.</p>
</blockquote>
<h2 id="关于-ai-infra-的一切httpswwwwoshipmcomai6253851html"><a href="https://www.woshipm.com/ai/6253851.html"><strong>关于 AI Infra 的一切</strong></a><a hidden class="anchor" aria-hidden="true" href="#关于-ai-infra-的一切httpswwwwoshipmcomai6253851html">#</a></h2>
<blockquote>
<p>算法人只有两年的保质期，两年后 Ta 把自己的聪明才智发挥完了，就会陷入思维定势，反而跟不上后面的新东西了。</p>
</blockquote>
<blockquote>
<p>所以这里可以给从业者一个建议，就是不要做夹在模型和硬件中间的那个人。</p>
</blockquote>
<blockquote>
<p>这正是我认为需要被纠正的观念。前面提到过，Infra 实际上是可以对模型效果有正向影响的，而不仅仅是只能降本。</p>
</blockquote>
<blockquote>
<p>模型其实由算法、Infra 和数据这个铁三角决定。三者缺一不可，必须协同。</p>
</blockquote>
<blockquote>
<p>所以实际上比较合理的组织架构是，让 Infra 人去设计模型结构，因为 Infra 人最知道该怎么提高效率、节省成本，让数据的人去负责刷模型的点数和 benchmark 分数，因为他们最懂怎么喂模型，而算法人应该主要负责训练的范式革新。</p>
</blockquote>
<h2 id="building-in-the-ai-era-the-heygen-wayhttpsxcomjoshua_xu_status1978837502787219578"><a href="https://x.com/joshua_xu_/status/1978837502787219578"><strong>Building in the AI Era: The HeyGen Way</strong></a><a hidden class="anchor" aria-hidden="true" href="#building-in-the-ai-era-the-heygen-wayhttpsxcomjoshua_xu_status1978837502787219578">#</a></h2>
<blockquote>
<p>We move fast AND be the absolute best… moving fast allows us to build better quality… our success metric: the average video quality any user can achieve.</p>
</blockquote>
<blockquote>
<p>Why 2 months? This aligns with model upgrade cycles… 2-month roadmap… 6-12 month strategic bets… Daily shipping</p>
</blockquote>
<blockquote>
<p>Day 1: Define hypothesis… Day 2: Build MVP… Days 3-5: Ship… Week 2: Analyze… Failure with learning = victory</p>
</blockquote>
<blockquote>
<p>Speed Is Everything… Embrace the Technology Wave… Disagree and Commit… When in doubt, ship an experiment</p>
</blockquote>
<blockquote>
<p>Growth teams are built for a different game… In the AI era, code is cheap. Impact is valuable</p>
</blockquote>
<h2 id="ai落地为什么这么难httpsxcom0xshellywangstatus1980195228368986310"><a href="https://x.com/0xShellywang/status/1980195228368986310/">AI落地为什么这么难？</a><a hidden class="anchor" aria-hidden="true" href="#ai落地为什么这么难httpsxcom0xshellywangstatus1980195228368986310">#</a></h2>
<blockquote>
<p>千万别，不要向老板卖降本的方案。
降本是个陷阱。 老板说要降本， 其实要的是利润。</p>
</blockquote>
<blockquote>
<p>AI to B 落地，往往会遇到三方博弈：老板要利润，但说不清路径。 中层要业绩，不敢碰组织。 员工要饭碗，会软抵抗。</p>
</blockquote>
<blockquote>
<p>但还有第三条路：找一个“无主之地”。不是取代现有岗位， 是做现在没人做的事，做增收，或者能帮采用部门，扩大预算或者人头的事。听起来好像跟老板原来降本的需求南辕北辙。但却是真实发生的案例。</p>
</blockquote>
<blockquote>
<p>AI落地的关键， 不是证明 AI 比人强。是让人觉得： 我用了 AI，我更强。</p>
</blockquote>
<blockquote>
<p>别动存量，找增量。</p>
</blockquote>
<blockquote>
<p>别替代人， 武装人。</p>
</blockquote>
<h2 id="对李想的第二次3小时访谈智慧是我们和万物的关系httpsmpweixinqqcomslr6p9mz9pwhdjklevyijaw"><a href="https://mp.weixin.qq.com/s/lR6p9mz9PWhdJklEvYijAw"><strong>对李想的第二次3小时访谈：智慧是我们和万物的关系</strong></a><a hidden class="anchor" aria-hidden="true" href="#对李想的第二次3小时访谈智慧是我们和万物的关系httpsmpweixinqqcomslr6p9mz9pwhdjklevyijaw">#</a></h2>
<blockquote>
<p>人类并不擅长处理特别复杂的信息——人类要做“熵减”，而不是“熵增”。人类之所以发明方法论、创造工具，本质是为了减少能量消耗——因为大脑运作需要能量。</p>
</blockquote>
<p>今天的人工智能，处理通用信息表现不错，但一旦进入专业领域，往往不如人。这是因为，这些领域需要更高质量的数据和更专业的CoT</p>
<p>今天Chatbot，或者Reasoning，做更长链条思考和推理任务时，也能看到一系列严重的问题——几乎所有人，还是把它当成“信息工具”</p>
<blockquote>
</blockquote>
<blockquote>
<p>还是要站在用户价值的角度，如果大家拼命使用AI，为AI投资，但我的工作时长没减少，结果也没变好，问题到底在哪？
今天通过对话（Chatbot），无论文本多么长，它都是“信息工具”，对大家是参考作用。
什么时候真正改善工作成果和减少工作时长？它必须变成“生产工具”。</p>
</blockquote>
<blockquote>
<p>另外当我Action做完以后，我还会做一个Diffusion（扩散）预测，下边会发生什么样的时长的一个场景。这个主要根据性能，会做出4到8秒Diffusion的轨迹和环境预测</p>
</blockquote>
<blockquote>
<p>五年内没有通用Agent，
但会有Agent OS</p>
</blockquote>
<blockquote>
<p>先做好公司内部的Agent，因为公司内部需要有很多Agent——客服Agent、编程Agent</p>
</blockquote>
<blockquote>
<p>搭建出一个有效的Agent OS，让每个专业的人在上面开发自己所在领域的Agent</p>
</blockquote>
<blockquote>
<p>你不可能让智能商业团队做客服Agent，应该是客服团队借助这个Agent OS，开发出自己专业的客服Agent</p>
</blockquote>
<blockquote>
<p>汽车会从智能终端变成人工智能终端。这是我们要做的。</p>
</blockquote>
<blockquote>
<p>今天，工厂也面临这样的机会。很多人想的是，造一个人形机器人进工厂替代人，但我认为有两个问题：
工厂里人的成本占比并没有大家想象的那么高，替代起来并不划算；
这些就业岗位长期来看还是必要的</p>
</blockquote>
<blockquote>
<p>只关注替代人，我认为是狭隘的。</p>
</blockquote>
<blockquote>
<p>我们没有把握住用户的需求，我们会消失；
我们没有掌握最好的产品和技术，我们会消失；
我们的组织能力方面出现了巨大的问题，我们也会消失</p>
</blockquote>
<blockquote>
<p>这些所有的讨论、争执、吵架，背后有一个更重要的一点——是能量。</p>
<p>当人和人之间的能量始终存在的时候，这些争执、讨论、吵架就是一个更完善的大脑。
当这些能量消失的时候，这些争执、讨论、不同想法就会变成内耗。</p>
</blockquote>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://niraya666.github.io/tags/%E6%9C%88%E5%88%8A/">月刊</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-10 月刊 on x"
            href="https://x.com/intent/tweet/?text=2025-10%20%e6%9c%88%e5%88%8a&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-10-%25E6%259C%2588%25E5%2588%258A%2f&amp;hashtags=%e6%9c%88%e5%88%8a">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-10 月刊 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-10-%25E6%259C%2588%25E5%2588%258A%2f&amp;title=2025-10%20%e6%9c%88%e5%88%8a&amp;summary=2025-10%20%e6%9c%88%e5%88%8a&amp;source=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-10-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-10 月刊 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-10-%25E6%259C%2588%25E5%2588%258A%2f&title=2025-10%20%e6%9c%88%e5%88%8a">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-10 月刊 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-10-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-10 月刊 on whatsapp"
            href="https://api.whatsapp.com/send?text=2025-10%20%e6%9c%88%e5%88%8a%20-%20https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-10-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-10 月刊 on telegram"
            href="https://telegram.me/share/url?text=2025-10%20%e6%9c%88%e5%88%8a&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-10-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-10 月刊 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=2025-10%20%e6%9c%88%e5%88%8a&u=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-10-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="utterances">
  <script src="https://utteranc.es/client.js"
        repo="https://github.com/Niraya666/niraya666.github.io.git"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>


<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: {'[+]': ['ams']}
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://niraya666.github.io/">LZY Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
