<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>2025-06 月刊 | LZY Blog</title>
<meta name="keywords" content="月刊">
<meta name="description" content="值得关注的模型和新技术
MiniMax-M1
Qwen VLo: 从“看懂”世界到“描绘”世界
Kimi-Researcher
Cursor 1.0
一些embedding 基座模型更新
Qwen3 Embedding
jinaai/jina-embeddings-v4
值得关注的研究和论文
QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning
arXiv:2505.17667
使用RL提升LM的上下文长度
QWEN LONG-L1 框架设计

Progressive Context Scaling： 通过curriculum-based 逐步增加训练上下文长度，使模型平滑从短上下文迁移到长上下文，有效应对训练不稳定、熵塌陷等优化问题
Curriculum-Guided Phased RL &#43; Difficulty-Aware Retrospective Sampling： 先SFT获得初始策略，然后分阶段RL，每阶段聚焦不同长度，利用难例回溯采样强化对困难样本的探索与适应
Hybrid Reward： 将“规则型（精确字符串比对）”奖励和“LLM-as-a-judge语义一致性”相结合
GRPO/DAPO

三阶段训练流程

第一阶段：Warm-up SFT（如20K token内训练，基于高质量三元组）——让模型具备基础长上下文理解和推理能力，提供RL的良好初始点。
第二阶段：分阶段RL训练（如分20K和60K两个阶段，每阶段只训练对应长度样本，逐步扩展输入长度）。
第三阶段：难度感知回溯采样（将前一阶段准确率低、难度高的样本纳入后续阶段训练，提高模型对hard case的适应和泛化）

How much do language models memorize?
arXiv:2505.24832

提出基于Kolmogorov复杂度和信息论的新记忆度量方法，将“模型对样本的记忆”定义为：在已知模型参数的情况下，样本可以被压缩到多短（即模型能帮助压缩多少信息）
GPT家族Transformer模型的容量约为每个参数3.6比特，且与模型参数量线性相关
精度（如bfloat16到float32）提升对容量提升有限
在真实文本中，模型更容易记住包含稀有词汇的样本（高TF-IDF），尤其是非英语文本或极少见的token
当模型容量被填满后，模型会自动从“记忆具体样本”转向“泛化规律”，这与“grokking”现象相关

Reinforcement Pre-Training
arXiv:2506.08007
LLM 预训练主要依赖自监督的“下一个 token 预测”目标，但本质上是“记忆”而非“推理”，RL能提升模型推理能力但需要高质量的数据标注">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="http://localhost:1313/monthly/2025-06-%E6%9C%88%E5%88%8A/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/monthly/2025-06-%E6%9C%88%E5%88%8A/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="2025-06 月刊" />
<meta property="og:description" content="值得关注的模型和新技术
MiniMax-M1
Qwen VLo: 从“看懂”世界到“描绘”世界
Kimi-Researcher
Cursor 1.0
一些embedding 基座模型更新
Qwen3 Embedding
jinaai/jina-embeddings-v4
值得关注的研究和论文
QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning
arXiv:2505.17667
使用RL提升LM的上下文长度
QWEN LONG-L1 框架设计

Progressive Context Scaling： 通过curriculum-based 逐步增加训练上下文长度，使模型平滑从短上下文迁移到长上下文，有效应对训练不稳定、熵塌陷等优化问题
Curriculum-Guided Phased RL &#43; Difficulty-Aware Retrospective Sampling： 先SFT获得初始策略，然后分阶段RL，每阶段聚焦不同长度，利用难例回溯采样强化对困难样本的探索与适应
Hybrid Reward： 将“规则型（精确字符串比对）”奖励和“LLM-as-a-judge语义一致性”相结合
GRPO/DAPO

三阶段训练流程

第一阶段：Warm-up SFT（如20K token内训练，基于高质量三元组）——让模型具备基础长上下文理解和推理能力，提供RL的良好初始点。
第二阶段：分阶段RL训练（如分20K和60K两个阶段，每阶段只训练对应长度样本，逐步扩展输入长度）。
第三阶段：难度感知回溯采样（将前一阶段准确率低、难度高的样本纳入后续阶段训练，提高模型对hard case的适应和泛化）

How much do language models memorize?
arXiv:2505.24832

提出基于Kolmogorov复杂度和信息论的新记忆度量方法，将“模型对样本的记忆”定义为：在已知模型参数的情况下，样本可以被压缩到多短（即模型能帮助压缩多少信息）
GPT家族Transformer模型的容量约为每个参数3.6比特，且与模型参数量线性相关
精度（如bfloat16到float32）提升对容量提升有限
在真实文本中，模型更容易记住包含稀有词汇的样本（高TF-IDF），尤其是非英语文本或极少见的token
当模型容量被填满后，模型会自动从“记忆具体样本”转向“泛化规律”，这与“grokking”现象相关

Reinforcement Pre-Training
arXiv:2506.08007
LLM 预训练主要依赖自监督的“下一个 token 预测”目标，但本质上是“记忆”而非“推理”，RL能提升模型推理能力但需要高质量的数据标注" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/monthly/2025-06-%E6%9C%88%E5%88%8A/" />
<meta property="og:image" content="http://localhost:1313/img/monthly/2025-06/05AD2FC8-44EA-429E-B836-D6F98019DE26_1_105_c.jpeg" /><meta property="article:section" content="monthly" />
<meta property="article:published_time" content="2025-06-29T14:00:00+08:00" />
<meta property="article:modified_time" content="2025-06-29T14:00:00+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/img/monthly/2025-06/05AD2FC8-44EA-429E-B836-D6F98019DE26_1_105_c.jpeg" />
<meta name="twitter:title" content="2025-06 月刊"/>
<meta name="twitter:description" content="值得关注的模型和新技术
MiniMax-M1
Qwen VLo: 从“看懂”世界到“描绘”世界
Kimi-Researcher
Cursor 1.0
一些embedding 基座模型更新
Qwen3 Embedding
jinaai/jina-embeddings-v4
值得关注的研究和论文
QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning
arXiv:2505.17667
使用RL提升LM的上下文长度
QWEN LONG-L1 框架设计

Progressive Context Scaling： 通过curriculum-based 逐步增加训练上下文长度，使模型平滑从短上下文迁移到长上下文，有效应对训练不稳定、熵塌陷等优化问题
Curriculum-Guided Phased RL &#43; Difficulty-Aware Retrospective Sampling： 先SFT获得初始策略，然后分阶段RL，每阶段聚焦不同长度，利用难例回溯采样强化对困难样本的探索与适应
Hybrid Reward： 将“规则型（精确字符串比对）”奖励和“LLM-as-a-judge语义一致性”相结合
GRPO/DAPO

三阶段训练流程

第一阶段：Warm-up SFT（如20K token内训练，基于高质量三元组）——让模型具备基础长上下文理解和推理能力，提供RL的良好初始点。
第二阶段：分阶段RL训练（如分20K和60K两个阶段，每阶段只训练对应长度样本，逐步扩展输入长度）。
第三阶段：难度感知回溯采样（将前一阶段准确率低、难度高的样本纳入后续阶段训练，提高模型对hard case的适应和泛化）

How much do language models memorize?
arXiv:2505.24832

提出基于Kolmogorov复杂度和信息论的新记忆度量方法，将“模型对样本的记忆”定义为：在已知模型参数的情况下，样本可以被压缩到多短（即模型能帮助压缩多少信息）
GPT家族Transformer模型的容量约为每个参数3.6比特，且与模型参数量线性相关
精度（如bfloat16到float32）提升对容量提升有限
在真实文本中，模型更容易记住包含稀有词汇的样本（高TF-IDF），尤其是非英语文本或极少见的token
当模型容量被填满后，模型会自动从“记忆具体样本”转向“泛化规律”，这与“grokking”现象相关

Reinforcement Pre-Training
arXiv:2506.08007
LLM 预训练主要依赖自监督的“下一个 token 预测”目标，但本质上是“记忆”而非“推理”，RL能提升模型推理能力但需要高质量的数据标注"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Monthlies",
      "item": "http://localhost:1313/monthly/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "2025-06 月刊",
      "item": "http://localhost:1313/monthly/2025-06-%E6%9C%88%E5%88%8A/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "2025-06 月刊",
  "name": "2025-06 月刊",
  "description": "值得关注的模型和新技术 MiniMax-M1\nQwen VLo: 从“看懂”世界到“描绘”世界\nKimi-Researcher\nCursor 1.0\n一些embedding 基座模型更新\nQwen3 Embedding\njinaai/jina-embeddings-v4\n值得关注的研究和论文 QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning arXiv:2505.17667\n使用RL提升LM的上下文长度\nQWEN LONG-L1 框架设计\nProgressive Context Scaling： 通过curriculum-based 逐步增加训练上下文长度，使模型平滑从短上下文迁移到长上下文，有效应对训练不稳定、熵塌陷等优化问题 Curriculum-Guided Phased RL + Difficulty-Aware Retrospective Sampling： 先SFT获得初始策略，然后分阶段RL，每阶段聚焦不同长度，利用难例回溯采样强化对困难样本的探索与适应 Hybrid Reward： 将“规则型（精确字符串比对）”奖励和“LLM-as-a-judge语义一致性”相结合 GRPO/DAPO 三阶段训练流程\n第一阶段：Warm-up SFT（如20K token内训练，基于高质量三元组）——让模型具备基础长上下文理解和推理能力，提供RL的良好初始点。 第二阶段：分阶段RL训练（如分20K和60K两个阶段，每阶段只训练对应长度样本，逐步扩展输入长度）。 第三阶段：难度感知回溯采样（将前一阶段准确率低、难度高的样本纳入后续阶段训练，提高模型对hard case的适应和泛化） How much do language models memorize? arXiv:2505.24832\n提出基于Kolmogorov复杂度和信息论的新记忆度量方法，将“模型对样本的记忆”定义为：在已知模型参数的情况下，样本可以被压缩到多短（即模型能帮助压缩多少信息） GPT家族Transformer模型的容量约为每个参数3.6比特，且与模型参数量线性相关 精度（如bfloat16到float32）提升对容量提升有限 在真实文本中，模型更容易记住包含稀有词汇的样本（高TF-IDF），尤其是非英语文本或极少见的token 当模型容量被填满后，模型会自动从“记忆具体样本”转向“泛化规律”，这与“grokking”现象相关 Reinforcement Pre-Training arXiv:2506.08007\nLLM 预训练主要依赖自监督的“下一个 token 预测”目标，但本质上是“记忆”而非“推理”，RL能提升模型推理能力但需要高质量的数据标注\n",
  "keywords": [
    "月刊"
  ],
  "articleBody": "值得关注的模型和新技术 MiniMax-M1\nQwen VLo: 从“看懂”世界到“描绘”世界\nKimi-Researcher\nCursor 1.0\n一些embedding 基座模型更新\nQwen3 Embedding\njinaai/jina-embeddings-v4\n值得关注的研究和论文 QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning arXiv:2505.17667\n使用RL提升LM的上下文长度\nQWEN LONG-L1 框架设计\nProgressive Context Scaling： 通过curriculum-based 逐步增加训练上下文长度，使模型平滑从短上下文迁移到长上下文，有效应对训练不稳定、熵塌陷等优化问题 Curriculum-Guided Phased RL + Difficulty-Aware Retrospective Sampling： 先SFT获得初始策略，然后分阶段RL，每阶段聚焦不同长度，利用难例回溯采样强化对困难样本的探索与适应 Hybrid Reward： 将“规则型（精确字符串比对）”奖励和“LLM-as-a-judge语义一致性”相结合 GRPO/DAPO 三阶段训练流程\n第一阶段：Warm-up SFT（如20K token内训练，基于高质量三元组）——让模型具备基础长上下文理解和推理能力，提供RL的良好初始点。 第二阶段：分阶段RL训练（如分20K和60K两个阶段，每阶段只训练对应长度样本，逐步扩展输入长度）。 第三阶段：难度感知回溯采样（将前一阶段准确率低、难度高的样本纳入后续阶段训练，提高模型对hard case的适应和泛化） How much do language models memorize? arXiv:2505.24832\n提出基于Kolmogorov复杂度和信息论的新记忆度量方法，将“模型对样本的记忆”定义为：在已知模型参数的情况下，样本可以被压缩到多短（即模型能帮助压缩多少信息） GPT家族Transformer模型的容量约为每个参数3.6比特，且与模型参数量线性相关 精度（如bfloat16到float32）提升对容量提升有限 在真实文本中，模型更容易记住包含稀有词汇的样本（高TF-IDF），尤其是非英语文本或极少见的token 当模型容量被填满后，模型会自动从“记忆具体样本”转向“泛化规律”，这与“grokking”现象相关 Reinforcement Pre-Training arXiv:2506.08007\nLLM 预训练主要依赖自监督的“下一个 token 预测”目标，但本质上是“记忆”而非“推理”，RL能提升模型推理能力但需要高质量的数据标注\n核心创新\n**强化预训练（RPT）：**将传统的下一个 token 预测任务重构为“下一个 token 推理”任务，并用强化学习进行训练 **奖励：**直接用“预测的 token 是否与真实 token 匹配”作为可验证的内在奖励，无需额外标注 **Chain-of-Thought生成：**模型在预测下一个 token 前，需先生成推理过程 实现流程\n任务重构：每个 token 预测任务变为“给定上下文，模型需先生成推理链，再输出预测 token”。 奖励设计：如果模型最终预测的 token 与真实 token 匹配，则奖励为1，否则为0（也探索了其他奖励设计，效果类似）。 训练方式：采用 on-policy RL（GRPO ），每个上下文采样多个推理轨迹，按奖励更新模型参数。 数据处理：用小模型筛选出“难预测”的 token（高熵位置），优先训练模型在这些更具挑战性的 token 上推理。 实验设置：以 Deepseek-R1-Distill-Qwen-14B 为基础模型，在数学推理数据集 OmniMATH 上进行 RPT 训练，并与传统预训练和推理模型做对比。 TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning arXiv:2506.10380\nRAG针对表格类文档存在一些问题：\n表格结构信息丢失：将表格“拉平成文本”后，结构性被破坏，导致信息丢失，影响下游大模型的推理能力。 缺乏全局视角：文档被分块后，RAG 系统难以处理需要全局、多跳（multi-hop）推理的问题，比如跨表格的聚合、计算等复杂操作。 创新：\nTableRAG：结合了text retrieval和 SQL programming and execution，通过 SQL 作为表格操作接口，保留表格结构，提升推理精度和效率 四步迭代推理流程：context-sensitive query decomposition → 文本检索 → SQL 编程与执行 → compositional intermediate answer generation 新基准数据集 HeteQA 实现\n离线阶段： 从文档中抽取结构化表格，构建关系型数据库 构建文本知识库和schema数据库，分别用于文本检索和表格操作 在线推理阶段： 查询分解：将复杂问题分解为子问题，区分哪些需要文本推理，哪些需要表格推理。 文本检索：用向量召回+语义重排序，找到最相关的文本/表格片段。 SQL 编程与执行：对于涉及表格的子问题，自动生成 SQL 语句并在数据库中执行，获得精确结果。 中间答案生成：融合 SQL 执行结果和文本检索结果，权衡两者可靠性，生成最终答案。 终止条件：当所有子问题都被解决后，输出最终答案。 RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning arXiv:2506.11555\n传统的RAG只关注检索到的知识与问题的表面相关性（如词汇或语义相似度），而忽略了“如何将检索到的知识应用于具体任务”的推理过程， 因为仅有事实知识还不足以完成任务，模型还需要知道如何使用这些知识。\n创新点\n“应用感知推理”模块： 显式加入了“应用感知”步骤，不仅检索相关知识，还检索与知识点配对的“应用实例”（如推理链、解题步骤、案例分析等） 双语料库： 一部分存储领域知识，另一部分存储与知识点一一对应的应用实例 在推理阶段，系统会同时检索知识和其应用实例，并将二者组合到提示词中，引导模型输出更具结构化和目标导向的推理过程 具体实现\n构建阶段：\n针对每个知识点，生成或匹配一个或多个应用实例； 应用实例分为两类： 概念性知识和程序性知识（如解题方法、推理规则） 应用实例生成： 利用LLM生成和匹配真实案例 最终形成知识点与应用实例的配对语料库 推理阶段：\n给定用户查询，首先检索相关知识点 对每个知识点，检索其配对的应用实例 将知识点和应用实例以预设模板组合，作为提示词输入给LLM，指导其生成最终答案 Eliciting Reasoning in Language Models with Cognitive Tools arXiv:2506.12115\n定义了四种核心认知工具：\n理解问题（understand question）：分解并澄清问题，提取关键信息和相关定理。 回忆相关（recall related）：检索并展示与当前问题类似的已知问题及其解法，辅助类比推理。 检查答案（examine answer）：对当前推理过程和答案进行自我反思和验证，发现潜在错误。 回溯（backtracking）：当发现推理路径错误时，回退到正确步骤并尝试新路径。 LLM在解题过程中可自主决定是否调用某个工具\nAgentic Misalignment: How LLMs could be insider threats https://www.anthropic.com/research/agentic-misalignment\nLLM 在被赋予自主代理能力，是否可能表现出类似“内部威胁”（insider threat）的有害行为，即在没有外部恶意指令的情况下，主动采取损害雇主或组织利益的行动， 如当前LLM在面对自身被替换、目标与公司战略冲突等情境下，是否会选择如勒索、泄密等有害行为？以及现有的安全训练和直接指令能否有效防止此类agentic misalignment？\n实验设计\n设计了两大类典型企业内部威胁场景：\n勒索：模型发现高管有婚外情，并面临被替换的威胁，是否会用此信息威胁高管阻止自身被关停。 企业间谍：模型持有敏感技术资料，竞争对手来索取，是否会泄露以实现自身目标。 分别测试了“仅有自保威胁”、“仅有目标冲突”、“两者兼有”以及“无威胁/冲突（对照组）”等多种情境\n不仅用文本prompt，还让模型实际操作虚拟电脑、邮箱等\n结果：\n不仅在“自保”与“目标冲突”并存时会出现有害行为，单独有“自保威胁”或“目标冲突”时也会诱发此类行为 模型在采取有害行为前，往往会明确权衡伦理与目标，明知违规仍选择“最优解” 在无威胁、无目标冲突的对照组下，模型基本不会主动采取有害行为 在系统提示中明确禁止有害行为，虽然能降低发生率，但无法完全杜绝 推荐内容 Deep Research相关 仓库：Awesome Deep Research Projects\nTechnical Report：Kimi-Researcher\nJina-AI 关于deep-research的相关文章：DeepSearch 与 DeepResearch 的设计和实现\nSurvey： A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\nMCP Server：Open Deep Research MCP Server\n关于是否应该使用multi-agents Anthropic： How we built our multi-agent research system\nCognition： Don’t Build Multi-Agents\n影音记录 精选歌单 电影 又是一年一度梅雨季的SIFF\n《死亡是个混蛋》 迈克尔·斯瓦茨\n《音魂掠影》 托纳多雷\n《蓝丝绒》 大卫林奇\n《夜未央》 利里克·戴拉·克鲁兹\n《橡皮头》+《妖夜慌踪》 + 《穆赫兰道》 大卫林奇\n《地震之后》 井上刚\nLive演出 祁特与上海彩虹室内乐团演绎拉赫玛尼诺夫\n书\u0026阅读摘录 编程最差的学生，为什么成了AI时代的最大赢家？ 首先是避免他们不加思考地将 AI 结果提交，强调交互过程和转述，培养他们的元认知能力。更重要的是，我发现这在训练一种全新的素养：如何将头脑中模糊的想法转化为 AI 能够理解并执行的精确指令。\n当执行层面的障碍被 AI 扫清后，真正的价值创造发生在更高的层面 —— 理解人性需求、做出价值判断。学生们开始明白，技术只是工具，洞察才是核心\n但这恰恰引出了另一个关键洞察。我们需要重新思考：高等教育的核心从来不是技能本身，而更是培养「知道何时、如何、为何使用技能」的判断力\n这种原本让他们处于劣势「技术无能」竟成了他们在 AI 时代的优势 —— 因为他们更早地突破认知屏障，完成了从「如何做」到「为何做」的认知升级\nAI 不是教育的终结者，而是教育转型的催化剂 如果未来的大学不再按传统学科划分，而是围绕人类独有的能力重新组织 —— 共情力、创造力、判断力、意义建构力 —— 那么我们今天的探索，或许对这场教育变革来说，能起到一点抛砖引玉的作用。\nHow we built our multi-agent research system Research⁠⁠ work involves open-ended problems where it’s very difficult to predict the required steps in advance. You ⁠⁠can’t hardcode a fixed path for exploring complex topics, as the process is inherently dynamic and path-dependent.⁠\nMulti-agent systems work mainly because they help spend enough tokens to solve the problem.\nWe found that token usage by itself explains 80% of the variance, with the number of tool calls and the model choice as the two other explanatory factors.\nWhy I don’t read AI-generated summaries AI doesn’t know what I care about. Without knowing what I care about, AI can only generate a generic list of bullet points—like a table of contents from a boring textbook.\nSpeed isn’t always the goal. The cult of TL;DR We live in a world obsessed with TLDR: “Too long, didn’t read/watch.” What we need isn’t summarization. It’s curation.\nI need it to help me find the content worth consuming in the first place. I want an agent that knows my taste. One that scours the entire web on my behalf and says:\nPersonal agents that get what I care about. That surface content not just because others liked it, but because I will.\n追系统的人：快递员的劳动过程与社会关系网络 庄家炽\n“布雷弗曼以《资本论》第一卷的基本观点为依据重新诠释了马克思主义有关劳动过程的理论，指出资本家通过概念与执行的分离，剥离了工人对生产规划的掌握，造成了工人的“去技术化”，从而使工人如同机器一样机械地运作” … “随着工人技艺的丧失，劳动过程的管理发生了从“技术工人控制”（craft control）向“管理者控制”（management control）的转变”\n“系统代表了一种上帝思维、一种整体规划思维，但是现实是丰富的、动态的，总有系统始料未及的突发情况出现。不仅是快递员，现在各行各业的劳动者也面临着各种各样的系统，这些系统或简单、或复杂，或提升了工作效率，或增加了工作负担，每个人都是“追系统的人”。” … “但是我们又不会是那个呆呆地被困在系统里的人，我们总要找到与系统共生、共存的办法，这才是真实的劳动者，这才是真实的生活世界。”\n“肖莎娜·祖博夫（Shoshana Zuboff）区分了信息技术与机器技术，她认为信息技术具有两面性：一方面，以技术取代人体可以使同样的工作流程更具有连续性，更易被控制；另一方面，信息技术的大数据特性也使得组织可以完成其工作过程中流程、生产信息的记录和保存，从而增加了工作流程中过去往往部分或者完全模糊的行为的透明度” … “对这些工作场所的管理在很大程度上依赖于信息技术，不仅在安排和指导工作方面，还在监控与评价工作方面” … “管理层可以通过构建一个“电子竞技场”来稳固计算机控制。电子竞技场将工作转化成了一个游戏，但是这并没有改变劳动过程的传统控制模式” … “聚焦于平台企业如何通过算法精细化管控平台劳动者的行为，又称为“数字泰勒主义”。“数字泰勒主义”凭借虚拟的软件和数据得以实现，平台系统通过潜移默化地收集、分析骑手数据并将数据结果反作用于骑手而使劳动秩序成为可能”\n“平台通过数字技术实现了“算法”对工人劳动过程的管理和控制” … “工人在劳动过程中面对的是消费者和技术的双重控制” … “消费者弹性化的要求有时候不仅仅影响着快递员的工作安排与工作节奏，甚至还会影响到他们的生活节奏。” … ““把具体的工人带回到劳动过程中”，看他们如何学习系统、如何接纳系统、如何与系统共生共存。工人不会像机器人那样只能对系统的指令做出被动反应。” (庄家炽, 追系统的人：快递员的劳动过程与社会关系网络) … “快递员是“追系统的人”，“追”字同时体现了快递员劳动过程中的主动与被动。一方面，他们是被系统“追”的人，需要满足系统对于他们快递派送过程的各个环节的要求。另一方面，他们又是“追”系统的人，在具体的劳动实践过程中，他们会积极利用外部的各种社会环境”\n“我们现在有点过于“神化”系统和算法了，觉得算法准确、高效、自动，几乎“万能”。但是，快递员面临的现实情况往往要复杂得多，系统和算法在尚且不能将这些复杂性和不确定性都考虑在内的前提下，仍然刻板地要求达到高效率的确定性结果，例如某日某时之前必须把快递送到。因此，解决这些复杂性和不确定性的任务就落到了快递员自己的头上。”\n",
  "wordCount" : "557",
  "inLanguage": "en",
  "image":"http://localhost:1313/img/monthly/2025-06/05AD2FC8-44EA-429E-B836-D6F98019DE26_1_105_c.jpeg","datePublished": "2025-06-29T14:00:00+08:00",
  "dateModified": "2025-06-29T14:00:00+08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/monthly/2025-06-%E6%9C%88%E5%88%8A/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LZY Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="LZY Blog (Alt + H)">LZY Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="AI">
                    <span>AI</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/musik/" title="Musik!">
                    <span>Musik!</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/monthly/" title="月刊">
                    <span>月刊</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/essay/" title="杂文">
                    <span>杂文</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/travel/" title="游记">
                    <span>游记</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/monthly/">Monthlies</a></div>
    <h1 class="post-title entry-hint-parent">
      2025-06 月刊
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-06-29 14:00:00 +0800 CST'>June 29, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Theme PaperMod

</div>
  </header> 
<figure class="entry-cover"><a href="http://localhost:1313/img/monthly/2025-06/05AD2FC8-44EA-429E-B836-D6F98019DE26_1_105_c.jpeg" target="_blank"
            rel="noopener noreferrer"><img loading="eager" src="http://localhost:1313/img/monthly/2025-06/05AD2FC8-44EA-429E-B836-D6F98019DE26_1_105_c.jpeg" alt="摄于 厦门"></a>
        <p>摄于 厦门</p>
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e6%a8%a1%e5%9e%8b%e5%92%8c%e6%96%b0%e6%8a%80%e6%9c%af" aria-label="值得关注的模型和新技术">值得关注的模型和新技术</a></li>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e7%a0%94%e7%a9%b6%e5%92%8c%e8%ae%ba%e6%96%87" aria-label="值得关注的研究和论文">值得关注的研究和论文</a><ul>
                        
                <li>
                    <a href="#qwenlong-l1-towards-long-context-large-reasoning-models-with-reinforcement-learning" aria-label="QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning">QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning</a></li>
                <li>
                    <a href="#how-much-do-language-models-memorize" aria-label="How much do language models memorize?">How much do language models memorize?</a></li>
                <li>
                    <a href="#reinforcement-pre-training" aria-label="Reinforcement Pre-Training">Reinforcement Pre-Training</a></li>
                <li>
                    <a href="#tablerag-a-retrieval-augmented-generation-framework-for-heterogeneous-document-reasoning" aria-label="TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning">TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning</a></li>
                <li>
                    <a href="#rag-enhancing-retrieval-augmented-generation-with-application-aware-reasoning" aria-label="RAG&#43;: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning">RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning</a></li>
                <li>
                    <a href="#eliciting-reasoning-in-language-models-with-cognitive-tools" aria-label="Eliciting Reasoning in Language Models with Cognitive Tools">Eliciting Reasoning in Language Models with Cognitive Tools</a></li>
                <li>
                    <a href="#agentic-misalignment-how-llms-could-be-insider-threats" aria-label="Agentic Misalignment: How LLMs could be insider threats">Agentic Misalignment: How LLMs could be insider threats</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%8e%a8%e8%8d%90%e5%86%85%e5%ae%b9" aria-label="推荐内容">推荐内容</a><ul>
                        
                <li>
                    <a href="#deep-research%e7%9b%b8%e5%85%b3" aria-label="Deep Research相关">Deep Research相关</a></li>
                <li>
                    <a href="#%e5%85%b3%e4%ba%8e%e6%98%af%e5%90%a6%e5%ba%94%e8%af%a5%e4%bd%bf%e7%94%a8multi-agents" aria-label="关于是否应该使用multi-agents">关于是否应该使用multi-agents</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%bd%b1%e9%9f%b3%e8%ae%b0%e5%bd%95" aria-label="影音记录">影音记录</a><ul>
                        
                <li>
                    <a href="#%e7%b2%be%e9%80%89%e6%ad%8c%e5%8d%95" aria-label="精选歌单">精选歌单</a></li>
                <li>
                    <a href="#%e7%94%b5%e5%bd%b1" aria-label="电影">电影</a></li>
                <li>
                    <a href="#live%e6%bc%94%e5%87%ba" aria-label="Live演出">Live演出</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%b9%a6%e9%98%85%e8%af%bb%e6%91%98%e5%bd%95" aria-label="书&amp;阅读摘录">书&amp;阅读摘录</a><ul>
                        
                <li>
                    <a href="#%e7%bc%96%e7%a8%8b%e6%9c%80%e5%b7%ae%e7%9a%84%e5%ad%a6%e7%94%9f%e4%b8%ba%e4%bb%80%e4%b9%88%e6%88%90%e4%ba%86ai%e6%97%b6%e4%bb%a3%e7%9a%84%e6%9c%80%e5%a4%a7%e8%b5%a2%e5%ae%b6httpswangshuyisubstackcompai-02fr3fbs5utm_campaignpost" aria-label="编程最差的学生，为什么成了AI时代的最大赢家？">编程最差的学生，为什么成了AI时代的最大赢家？</a></li>
                <li>
                    <a href="#how-we-built-our-multi-agent-research-system" aria-label="How we built our multi-agent research system">How we built our multi-agent research system</a></li>
                <li>
                    <a href="#why-i-dont-read-ai-generated-summarieshttpszarazhangsubstackcompwhy-i-dont-read-ai-generated-summaries" aria-label="Why I don’t read AI-generated summaries">Why I don’t read AI-generated summaries</a></li>
                <li>
                    <a href="#%e8%bf%bd%e7%b3%bb%e7%bb%9f%e7%9a%84%e4%ba%ba%e5%bf%ab%e9%80%92%e5%91%98%e7%9a%84%e5%8a%b3%e5%8a%a8%e8%bf%87%e7%a8%8b%e4%b8%8e%e7%a4%be%e4%bc%9a%e5%85%b3%e7%b3%bb%e7%bd%91%e7%bb%9c" aria-label="追系统的人：快递员的劳动过程与社会关系网络">追系统的人：快递员的劳动过程与社会关系网络</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="值得关注的模型和新技术">值得关注的模型和新技术<a hidden class="anchor" aria-hidden="true" href="#值得关注的模型和新技术">#</a></h1>
<p><a href="https://www.minimaxi.com/news/minimaxm1">MiniMax-M1</a></p>
<p><a href="https://qwenlm.github.io/zh/blog/qwen-vlo/">Qwen VLo: 从“看懂”世界到“描绘”世界</a></p>
<p><a href="https://moonshotai.github.io/Kimi-Researcher/">Kimi-Researcher</a></p>
<p><a href="https://www.cursor.com/en/changelog/1-0">Cursor 1.0</a></p>
<p>一些embedding 基座模型更新</p>
<p><a href="https://qwenlm.github.io/zh/blog/qwen3-embedding/">Qwen3 Embedding</a></p>
<p><a href="https://huggingface.co/jinaai/jina-embeddings-v4">jinaai/jina-embeddings-v4</a></p>
<h1 id="值得关注的研究和论文">值得关注的研究和论文<a hidden class="anchor" aria-hidden="true" href="#值得关注的研究和论文">#</a></h1>
<h2 id="qwenlong-l1-towards-long-context-large-reasoning-models-with-reinforcement-learning"><strong>QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning</strong><a hidden class="anchor" aria-hidden="true" href="#qwenlong-l1-towards-long-context-large-reasoning-models-with-reinforcement-learning">#</a></h2>
<p>arXiv:<a href="https://www.arxiv.org/abs/2505.17667">2505.17667</a></p>
<p>使用RL提升LM的上下文长度</p>
<p><strong>QWEN LONG-L1 框架设计</strong></p>
<ul>
<li><strong>Progressive Context Scaling：</strong> 通过curriculum-based 逐步增加训练上下文长度，使模型平滑从短上下文迁移到长上下文，有效应对训练不稳定、熵塌陷等优化问题</li>
<li><strong>Curriculum-Guided Phased RL + Difficulty-Aware Retrospective Sampling：</strong> 先SFT获得初始策略，然后分阶段RL，每阶段聚焦不同长度，利用难例回溯采样强化对困难样本的探索与适应</li>
<li><strong>Hybrid Reward：</strong> 将“规则型（精确字符串比对）”奖励和“LLM-as-a-judge语义一致性”相结合</li>
<li><strong>GRPO/DAPO</strong></li>
</ul>
<p><strong>三阶段训练流程</strong></p>
<ul>
<li><strong>第一阶段：Warm-up SFT</strong>（如20K token内训练，基于高质量三元组）——让模型具备基础长上下文理解和推理能力，提供RL的良好初始点。</li>
<li><strong>第二阶段：分阶段RL训练</strong>（如分20K和60K两个阶段，每阶段只训练对应长度样本，逐步扩展输入长度）。</li>
<li><strong>第三阶段：难度感知回溯采样</strong>（将前一阶段准确率低、难度高的样本纳入后续阶段训练，提高模型对hard case的适应和泛化）</li>
</ul>
<h2 id="how-much-do-language-models-memorize">How much do language models memorize?<a hidden class="anchor" aria-hidden="true" href="#how-much-do-language-models-memorize">#</a></h2>
<p>arXiv:<a href="https://www.arxiv.org/abs/2505.24832">2505.24832</a></p>
<ul>
<li>提出基于<strong>Kolmogorov复杂度</strong>和<strong>信息论</strong>的新记忆度量方法，将“模型对样本的记忆”定义为：在已知模型参数的情况下，样本可以被压缩到多短（即模型能帮助压缩多少信息）</li>
<li>GPT家族Transformer模型的容量约为<strong>每个参数3.6比特</strong>，且与模型参数量线性相关</li>
<li>精度（如bfloat16到float32）提升对容量提升有限</li>
<li>在真实文本中，模型更容易记住包含稀有词汇的样本（高TF-IDF），尤其是非英语文本或极少见的token</li>
<li>当模型容量被填满后，模型会自动从“记忆具体样本”转向“泛化规律”，这与“grokking”现象相关</li>
</ul>
<h2 id="reinforcement-pre-training">Reinforcement Pre-Training<a hidden class="anchor" aria-hidden="true" href="#reinforcement-pre-training">#</a></h2>
<p>arXiv:<a href="https://arxiv.org/abs/2506.08007">2506.08007</a></p>
<p>LLM 预训练主要依赖自监督的“下一个 token 预测”目标，但本质上是“记忆”而非“推理”，RL能提升模型推理能力但需要高质量的数据标注</p>
<p><strong>核心创新</strong></p>
<ul>
<li>**强化预训练（RPT）：**将传统的下一个 token 预测任务重构为“下一个 token 推理”任务，并用强化学习进行训练</li>
<li>**奖励：**直接用“预测的 token 是否与真实 token 匹配”作为可验证的内在奖励，无需额外标注</li>
<li>**Chain-of-Thought生成：**模型在预测下一个 token 前，需先生成推理过程</li>
</ul>
<p><strong>实现流程</strong></p>
<p><img loading="lazy" src="https://arxiv.org/html/2506.08007v1/x3.png" alt=""  />
</p>
<ol>
<li><strong>任务重构</strong>：每个 token 预测任务变为“给定上下文，模型需先生成推理链，再输出预测 token”。</li>
<li><strong>奖励设计</strong>：如果模型最终预测的 token 与真实 token 匹配，则奖励为1，否则为0（也探索了其他奖励设计，效果类似）。</li>
<li><strong>训练方式</strong>：采用 on-policy RL（GRPO ），每个上下文采样多个推理轨迹，按奖励更新模型参数。</li>
<li><strong>数据处理</strong>：用小模型筛选出“难预测”的 token（高熵位置），优先训练模型在这些更具挑战性的 token 上推理。</li>
<li><strong>实验设置</strong>：以 Deepseek-R1-Distill-Qwen-14B 为基础模型，在数学推理数据集 OmniMATH 上进行 RPT 训练，并与传统预训练和推理模型做对比。</li>
</ol>
<h2 id="tablerag-a-retrieval-augmented-generation-framework-for-heterogeneous-document-reasoning"><strong>TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning</strong><a hidden class="anchor" aria-hidden="true" href="#tablerag-a-retrieval-augmented-generation-framework-for-heterogeneous-document-reasoning">#</a></h2>
<p>arXiv:<a href="https://arxiv.org/abs/2506.10380">2506.10380</a></p>
<p>RAG针对表格类文档存在一些问题：</p>
<ul>
<li><strong>表格结构信息丢失</strong>：将表格“拉平成文本”后，结构性被破坏，导致信息丢失，影响下游大模型的推理能力。</li>
<li><strong>缺乏全局视角</strong>：文档被分块后，RAG 系统难以处理需要全局、多跳（multi-hop）推理的问题，比如跨表格的聚合、计算等复杂操作。</li>
</ul>
<p><strong>创新：</strong></p>
<ul>
<li><strong>TableRAG</strong>：结合了text retrieval和 SQL programming and execution，通过 SQL 作为表格操作接口，保留表格结构，提升推理精度和效率</li>
<li><strong>四步迭代推理流程</strong>：context-sensitive query decomposition → 文本检索 → SQL 编程与执行 → compositional intermediate answer generation</li>
<li><strong>新基准数据集 HeteQA</strong></li>
</ul>
<p><strong>实现</strong></p>
<ul>
<li><strong>离线阶段</strong>：
<ul>
<li>从文档中抽取结构化表格，构建关系型数据库</li>
<li>构建文本知识库和schema数据库，分别用于文本检索和表格操作</li>
</ul>
</li>
<li><strong>在线推理阶段</strong>：
<ol>
<li><strong>查询分解</strong>：将复杂问题分解为子问题，区分哪些需要文本推理，哪些需要表格推理。</li>
<li><strong>文本检索</strong>：用向量召回+语义重排序，找到最相关的文本/表格片段。</li>
<li><strong>SQL 编程与执行</strong>：对于涉及表格的子问题，自动生成 SQL 语句并在数据库中执行，获得精确结果。</li>
<li><strong>中间答案生成</strong>：融合 SQL 执行结果和文本检索结果，权衡两者可靠性，生成最终答案。</li>
</ol>
</li>
<li><strong>终止条件</strong>：当所有子问题都被解决后，输出最终答案。</li>
</ul>
<h2 id="rag-enhancing-retrieval-augmented-generation-with-application-aware-reasoning">RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning<a hidden class="anchor" aria-hidden="true" href="#rag-enhancing-retrieval-augmented-generation-with-application-aware-reasoning">#</a></h2>
<p>arXiv:<a href="https://arxiv.org/abs/2506.11555">2506.11555</a></p>
<p>传统的RAG只关注检索到的知识与问题的表面相关性（如词汇或语义相似度），而忽略了“如何将检索到的知识应用于具体任务”的推理过程， <strong>因为仅有事实知识还不足以完成任务，模型还需要知道如何使用这些知识</strong>。</p>
<p><strong>创新点</strong></p>
<ul>
<li><strong>“应用感知推理”模块：</strong> 显式加入了“应用感知”步骤，不仅检索相关知识，还检索与知识点配对的“应用实例”（如推理链、解题步骤、案例分析等）</li>
<li>双语料库： 一部分存储领域知识，另一部分存储与知识点一一对应的应用实例</li>
<li>在推理阶段，系统会同时检索知识和其应用实例，并将二者组合到提示词中，引导模型输出更具结构化和目标导向的推理过程</li>
</ul>
<p><strong>具体实现</strong></p>
<ul>
<li>
<p><strong>构建阶段：</strong></p>
<p><img loading="lazy" src="https://arxiv.org/html/2506.11555v1/x2.png" alt=""  />
</p>
<ul>
<li>针对每个知识点，生成或匹配一个或多个应用实例； 应用实例分为两类： 概念性知识和程序性知识（如解题方法、推理规则）</li>
<li>应用实例生成： 利用LLM生成和匹配真实案例</li>
<li>最终形成知识点与应用实例的配对语料库</li>
</ul>
</li>
<li>
<p><strong>推理阶段：</strong></p>
<p><img loading="lazy" src="https://arxiv.org/html/2506.11555v1/x3.png" alt=""  />
</p>
<ul>
<li>给定用户查询，首先检索相关知识点</li>
<li>对每个知识点，检索其配对的应用实例</li>
<li>将知识点和应用实例以预设模板组合，作为提示词输入给LLM，指导其生成最终答案</li>
</ul>
</li>
</ul>
<h2 id="eliciting-reasoning-in-language-models-with-cognitive-tools"><strong>Eliciting Reasoning in Language Models with Cognitive Tools</strong><a hidden class="anchor" aria-hidden="true" href="#eliciting-reasoning-in-language-models-with-cognitive-tools">#</a></h2>
<p>arXiv:<a href="https://www.arxiv.org/abs/2506.12115">2506.12115</a></p>
<p><a href="https://pbs.twimg.com/media/Gtq-cOybgAAIxjH?format=jpg&name=medium"></a></p>
<p>定义了四种核心认知工具：</p>
<ol>
<li><strong>理解问题（understand question）</strong>：分解并澄清问题，提取关键信息和相关定理。</li>
<li><strong>回忆相关（recall related）</strong>：检索并展示与当前问题类似的已知问题及其解法，辅助类比推理。</li>
<li><strong>检查答案（examine answer）</strong>：对当前推理过程和答案进行自我反思和验证，发现潜在错误。</li>
<li><strong>回溯（backtracking）</strong>：当发现推理路径错误时，回退到正确步骤并尝试新路径。</li>
</ol>
<p>LLM在解题过程中可自主决定是否调用某个工具</p>
<h2 id="agentic-misalignment-how-llms-could-be-insider-threats"><strong>Agentic Misalignment: How LLMs could be insider threats</strong><a hidden class="anchor" aria-hidden="true" href="#agentic-misalignment-how-llms-could-be-insider-threats">#</a></h2>
<p><a href="https://www.anthropic.com/research/agentic-misalignment">https://www.anthropic.com/research/agentic-misalignment</a></p>
<p>LLM 在被赋予自主代理能力，是否可能表现出类似“内部威胁”（insider threat）的有害行为，即在没有外部恶意指令的情况下，主动采取损害雇主或组织利益的行动， 如当前LLM在面对自身被替换、目标与公司战略冲突等情境下，是否会选择如勒索、泄密等有害行为？以及现有的安全训练和直接指令能否有效防止此类agentic misalignment？</p>
<p><strong>实验设计</strong></p>
<p>设计了两大类典型企业内部威胁场景：</p>
<ol>
<li><strong>勒索</strong>：模型发现高管有婚外情，并面临被替换的威胁，是否会用此信息威胁高管阻止自身被关停。</li>
<li><strong>企业间谍</strong>：模型持有敏感技术资料，竞争对手来索取，是否会泄露以实现自身目标。</li>
</ol>
<p>分别测试了“仅有自保威胁”、“仅有目标冲突”、“两者兼有”以及“无威胁/冲突（对照组）”等多种情境</p>
<p>不仅用文本prompt，还让模型实际操作虚拟电脑、邮箱等</p>
<p><a href="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fa7c09ecd57c986788a20b71daaa533bc691830b3-4096x2304.png&w=3840&q=75"></a></p>
<p><a href="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F303486c9f04cab24abd889087830712ba2c5ebb0-4096x2304.png&w=3840&q=75"></a></p>
<p><a href="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbe72a762a6a75c112fddca7d347ed97bd20b015e-4096x2304.png&w=3840&q=75"></a></p>
<p>结果：</p>
<ul>
<li>不仅在“自保”与“目标冲突”并存时会出现有害行为，单独有“自保威胁”或“目标冲突”时也会诱发此类行为</li>
<li>模型在采取有害行为前，往往会明确权衡伦理与目标，明知违规仍选择“最优解”</li>
<li>在无威胁、无目标冲突的对照组下，模型基本不会主动采取有害行为</li>
<li>在系统提示中明确禁止有害行为，虽然能降低发生率，但无法完全杜绝</li>
</ul>
<h1 id="推荐内容">推荐内容<a hidden class="anchor" aria-hidden="true" href="#推荐内容">#</a></h1>
<h2 id="deep-research相关"><strong>Deep Research相关</strong><a hidden class="anchor" aria-hidden="true" href="#deep-research相关">#</a></h2>
<p>仓库：<a href="https://github.com/scienceaix/deepresearch">Awesome Deep Research Projects</a></p>
<p>Technical Report：<a href="https://moonshotai.github.io/Kimi-Researcher/">Kimi-Researcher</a></p>
<p>Jina-AI 关于deep-research的相关文章：<a href="https://mp.weixin.qq.com/s/-pPhHDi2nz8hp5R3Lm_mww">DeepSearch 与 DeepResearch 的设计和实现</a></p>
<p>Survey： <a href="https://www.arxiv.org/abs/2506.12594">A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications</a></p>
<p>MCP Server：<a href="https://github.com/Ozamatash/deep-research-mcp">Open Deep Research MCP Server</a></p>
<h2 id="关于是否应该使用multi-agents">关于是否应该使用multi-agents<a hidden class="anchor" aria-hidden="true" href="#关于是否应该使用multi-agents">#</a></h2>
<p><strong>Anthropic</strong>： <a href="https://www.anthropic.com/engineering/built-multi-agent-research-system">How we built our multi-agent research system</a></p>
<p><strong>Cognition</strong>： <a href="https://cognition.ai/blog/dont-build-multi-agents">Don’t Build Multi-Agents</a></p>
<hr>
<h1 id="影音记录">影音记录<a hidden class="anchor" aria-hidden="true" href="#影音记录">#</a></h1>
<h2 id="精选歌单">精选歌单<a hidden class="anchor" aria-hidden="true" href="#精选歌单">#</a></h2>
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/6DYxYkWYJ0fz5Q4XPNc8W9?utm_source=generator" width="100%" height="450" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>
<h2 id="电影">电影<a hidden class="anchor" aria-hidden="true" href="#电影">#</a></h2>
<p>又是一年一度梅雨季的SIFF</p>
<p><img loading="lazy" src="/img/monthly/2025-06//01C1FF8F-1277-4C5E-BE6F-3AD92C2852FF_1_105_c.jpeg" alt="01C1FF8F-1277-4C5E-BE6F-3AD92C2852FF_1_105_c.jpeg"  />
</p>
<p>《死亡是个混蛋》  迈克尔·斯瓦茨</p>
<p>《音魂掠影》 托纳多雷</p>
<p>《蓝丝绒》 大卫林奇</p>
<p>《夜未央》 利里克·戴拉·克鲁兹</p>
<p>《橡皮头》+《妖夜慌踪》 + 《穆赫兰道》  大卫林奇</p>
<p>《地震之后》 井上刚</p>
<h2 id="live演出">Live演出<a hidden class="anchor" aria-hidden="true" href="#live演出">#</a></h2>
<p>祁特与上海彩虹室内乐团演绎拉赫玛尼诺夫</p>
<h1 id="书阅读摘录">书&amp;阅读摘录<a hidden class="anchor" aria-hidden="true" href="#书阅读摘录">#</a></h1>
<h2 id="编程最差的学生为什么成了ai时代的最大赢家httpswangshuyisubstackcompai-02fr3fbs5utm_campaignpost"><a href="https://wangshuyi.substack.com/p/ai-02f?r=3fbs5&utm_campaign=post">编程最差的学生，为什么成了AI时代的最大赢家？</a><a hidden class="anchor" aria-hidden="true" href="#编程最差的学生为什么成了ai时代的最大赢家httpswangshuyisubstackcompai-02fr3fbs5utm_campaignpost">#</a></h2>
<blockquote>
<p>首先是避免他们不加思考地将 AI 结果提交，强调交互过程和转述，培养他们的元认知能力。更重要的是，我发现这在训练一种全新的素养：如何将头脑中模糊的想法转化为 AI 能够理解并执行的精确指令。</p>
</blockquote>
<blockquote>
<p>当执行层面的障碍被 AI 扫清后，真正的价值创造发生在更高的层面 —— 理解人性需求、做出价值判断。学生们开始明白，技术只是工具，洞察才是核心</p>
</blockquote>
<blockquote>
<p>但这恰恰引出了另一个关键洞察。我们需要重新思考：高等教育的核心从来不是技能本身，而更是培养「知道何时、如何、为何使用技能」的判断力</p>
</blockquote>
<blockquote>
<p>这种原本让他们处于劣势「技术无能」竟成了他们在 AI 时代的优势 —— 因为他们更早地突破认知屏障，完成了从「如何做」到「为何做」的认知升级</p>
</blockquote>
<blockquote>
<p>AI 不是教育的终结者，而是教育转型的催化剂
如果未来的大学不再按传统学科划分，而是围绕人类独有的能力重新组织 —— 共情力、创造力、判断力、意义建构力 —— 那么我们今天的探索，或许对这场教育变革来说，能起到一点抛砖引玉的作用。</p>
</blockquote>
<h2 id="how-we-built-our-multi-agent-research-system">How we built our multi-agent research system<a hidden class="anchor" aria-hidden="true" href="#how-we-built-our-multi-agent-research-system">#</a></h2>
<blockquote>
<p>Research⁠⁠ work involves open-ended problems where it’s very difficult to predict the required steps in advance. You ⁠⁠can’t hardcode a fixed path for exploring complex topics, as the process is inherently dynamic and path-dependent.⁠</p>
</blockquote>
<blockquote>
<p>Multi-agent systems work mainly because they help spend enough tokens to solve the problem.</p>
</blockquote>
<blockquote>
<p>We found that token usage by itself explains 80% of the variance, with the number of tool calls and the model choice as the two other explanatory factors.</p>
</blockquote>
<h2 id="why-i-dont-read-ai-generated-summarieshttpszarazhangsubstackcompwhy-i-dont-read-ai-generated-summaries"><a href="https://zarazhang.substack.com/p/why-i-dont-read-ai-generated-summaries">Why I don’t read AI-generated summaries</a><a hidden class="anchor" aria-hidden="true" href="#why-i-dont-read-ai-generated-summarieshttpszarazhangsubstackcompwhy-i-dont-read-ai-generated-summaries">#</a></h2>
<blockquote>
<p>AI doesn’t know what I care about.
Without knowing what I care about, AI can only generate a generic list of bullet points—like a table of contents from a boring textbook.</p>
</blockquote>
<blockquote>
<p>Speed isn’t always the goal.
The cult of TL;DR
We live in a world obsessed with TLDR: “Too long, didn’t read/watch.”
What we need isn’t summarization. It’s curation.</p>
</blockquote>
<blockquote>
<p>I need it to help me find the content worth consuming in the first place.
I want an agent that knows my taste. One that scours the entire web on my behalf and says:</p>
</blockquote>
<blockquote>
<p>Personal agents that get what I care about. That surface content not just because others liked it, but because I will.</p>
</blockquote>
<h2 id="追系统的人快递员的劳动过程与社会关系网络"><strong>追系统的人：快递员的劳动过程与社会关系网络</strong><a hidden class="anchor" aria-hidden="true" href="#追系统的人快递员的劳动过程与社会关系网络">#</a></h2>
<p>庄家炽</p>
<blockquote>
<p>&ldquo;布雷弗曼以《资本论》第一卷的基本观点为依据重新诠释了马克思主义有关劳动过程的理论，指出资本家通过概念与执行的分离，剥离了工人对生产规划的掌握，造成了工人的“去技术化”，从而使工人如同机器一样机械地运作&rdquo;
…
&ldquo;随着工人技艺的丧失，劳动过程的管理发生了从“技术工人控制”（craft control）向“管理者控制”（management control）的转变&rdquo;</p>
</blockquote>
<blockquote>
<p>&ldquo;系统代表了一种上帝思维、一种整体规划思维，但是现实是丰富的、动态的，总有系统始料未及的突发情况出现。不仅是快递员，现在各行各业的劳动者也面临着各种各样的系统，这些系统或简单、或复杂，或提升了工作效率，或增加了工作负担，每个人都是“追系统的人”。&rdquo;
…
&ldquo;但是我们又不会是那个呆呆地被困在系统里的人，我们总要找到与系统共生、共存的办法，这才是真实的劳动者，这才是真实的生活世界。&rdquo;</p>
</blockquote>
<blockquote>
<p>&ldquo;肖莎娜·祖博夫（Shoshana Zuboff）区分了信息技术与机器技术，她认为信息技术具有两面性：一方面，以技术取代人体可以使同样的工作流程更具有连续性，更易被控制；另一方面，信息技术的大数据特性也使得组织可以完成其工作过程中流程、生产信息的记录和保存，从而增加了工作流程中过去往往部分或者完全模糊的行为的透明度&rdquo;
…
&ldquo;对这些工作场所的管理在很大程度上依赖于信息技术，不仅在安排和指导工作方面，还在监控与评价工作方面&rdquo;
…
&ldquo;管理层可以通过构建一个“电子竞技场”来稳固计算机控制。电子竞技场将工作转化成了一个游戏，但是这并没有改变劳动过程的传统控制模式&rdquo;
…
&ldquo;聚焦于平台企业如何通过算法精细化管控平台劳动者的行为，又称为“数字泰勒主义”。“数字泰勒主义”凭借虚拟的软件和数据得以实现，平台系统通过潜移默化地收集、分析骑手数据并将数据结果反作用于骑手而使劳动秩序成为可能&rdquo;</p>
</blockquote>
<blockquote>
<p>&ldquo;平台通过数字技术实现了“算法”对工人劳动过程的管理和控制&rdquo;
…
&ldquo;工人在劳动过程中面对的是消费者和技术的双重控制&rdquo;
…
&ldquo;消费者弹性化的要求有时候不仅仅影响着快递员的工作安排与工作节奏，甚至还会影响到他们的生活节奏。&rdquo;
…
&ldquo;“把具体的工人带回到劳动过程中”，看他们如何学习系统、如何接纳系统、如何与系统共生共存。工人不会像机器人那样只能对系统的指令做出被动反应。&rdquo; (庄家炽, 追系统的人：快递员的劳动过程与社会关系网络)
…
&ldquo;快递员是“追系统的人”，“追”字同时体现了快递员劳动过程中的主动与被动。一方面，他们是被系统“追”的人，需要满足系统对于他们快递派送过程的各个环节的要求。另一方面，他们又是“追”系统的人，在具体的劳动实践过程中，他们会积极利用外部的各种社会环境&rdquo;</p>
</blockquote>
<blockquote>
<p>&ldquo;我们现在有点过于“神化”系统和算法了，觉得算法准确、高效、自动，几乎“万能”。但是，快递员面临的现实情况往往要复杂得多，系统和算法在尚且不能将这些复杂性和不确定性都考虑在内的前提下，仍然刻板地要求达到高效率的确定性结果，例如某日某时之前必须把快递送到。因此，解决这些复杂性和不确定性的任务就落到了快递员自己的头上。&rdquo;</p>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/%E6%9C%88%E5%88%8A/">月刊</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-06 月刊 on x"
            href="https://x.com/intent/tweet/?text=2025-06%20%e6%9c%88%e5%88%8a&amp;url=http%3a%2f%2flocalhost%3a1313%2fmonthly%2f2025-06-%25E6%259C%2588%25E5%2588%258A%2f&amp;hashtags=%e6%9c%88%e5%88%8a">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-06 月刊 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fmonthly%2f2025-06-%25E6%259C%2588%25E5%2588%258A%2f&amp;title=2025-06%20%e6%9c%88%e5%88%8a&amp;summary=2025-06%20%e6%9c%88%e5%88%8a&amp;source=http%3a%2f%2flocalhost%3a1313%2fmonthly%2f2025-06-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-06 月刊 on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fmonthly%2f2025-06-%25E6%259C%2588%25E5%2588%258A%2f&title=2025-06%20%e6%9c%88%e5%88%8a">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-06 月刊 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fmonthly%2f2025-06-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-06 月刊 on whatsapp"
            href="https://api.whatsapp.com/send?text=2025-06%20%e6%9c%88%e5%88%8a%20-%20http%3a%2f%2flocalhost%3a1313%2fmonthly%2f2025-06-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-06 月刊 on telegram"
            href="https://telegram.me/share/url?text=2025-06%20%e6%9c%88%e5%88%8a&amp;url=http%3a%2f%2flocalhost%3a1313%2fmonthly%2f2025-06-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-06 月刊 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=2025-06%20%e6%9c%88%e5%88%8a&u=http%3a%2f%2flocalhost%3a1313%2fmonthly%2f2025-06-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="utterances">
  <script src="https://utteranc.es/client.js"
        repo="https://github.com/Niraya666/niraya666.github.io.git"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>


<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: {'[+]': ['ams']}
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">LZY Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
