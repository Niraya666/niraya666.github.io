<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>2025-02 月刊 | LZY Blog</title>
<meta name="keywords" content="月刊">
<meta name="description" content="值得关注的模型和新技术
o3-mini
OpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。
OpenAI o3-mini
QwQ-Max-Preview
QwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。

我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max

官方blog：&hellip; QwQ-Max-Preview
Claude 3.7 Sonnet
Claude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。
Claude 3.7 Sonnet and Claude Code
Grok-3
Grok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、&ldquo;Think&quot;模式分步推理，以及&quot;Big Brain&quot;模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium&#43;订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://niraya666.github.io/monthly/2025-02-%E6%9C%88%E5%88%8A/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://niraya666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://niraya666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://niraya666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://niraya666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://niraya666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://niraya666.github.io/monthly/2025-02-%E6%9C%88%E5%88%8A/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="2025-02 月刊" />
<meta property="og:description" content="值得关注的模型和新技术
o3-mini
OpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。
OpenAI o3-mini
QwQ-Max-Preview
QwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。

我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max

官方blog：&hellip; QwQ-Max-Preview
Claude 3.7 Sonnet
Claude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。
Claude 3.7 Sonnet and Claude Code
Grok-3
Grok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、&ldquo;Think&quot;模式分步推理，以及&quot;Big Brain&quot;模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium&#43;订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://niraya666.github.io/monthly/2025-02-%E6%9C%88%E5%88%8A/" />
<meta property="og:image" content="https://niraya666.github.io/img/monthly/2025-02%20%E6%9C%88%E5%88%8A%201a52555697de80d9a84dfc8c32fcb6cf/0E5F0FD2-B499-49AD-AEE5-69D8F7E6CEBD_1_105_c.jpeg" /><meta property="article:section" content="monthly" />
<meta property="article:published_time" content="2025-02-28T11:00:00+08:00" />
<meta property="article:modified_time" content="2025-02-28T11:00:00+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://niraya666.github.io/img/monthly/2025-02%20%E6%9C%88%E5%88%8A%201a52555697de80d9a84dfc8c32fcb6cf/0E5F0FD2-B499-49AD-AEE5-69D8F7E6CEBD_1_105_c.jpeg" />
<meta name="twitter:title" content="2025-02 月刊"/>
<meta name="twitter:description" content="值得关注的模型和新技术
o3-mini
OpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。
OpenAI o3-mini
QwQ-Max-Preview
QwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。

我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max

官方blog：&hellip; QwQ-Max-Preview
Claude 3.7 Sonnet
Claude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。
Claude 3.7 Sonnet and Claude Code
Grok-3
Grok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、&ldquo;Think&quot;模式分步推理，以及&quot;Big Brain&quot;模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium&#43;订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Monthlies",
      "item": "https://niraya666.github.io/monthly/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "2025-02 月刊",
      "item": "https://niraya666.github.io/monthly/2025-02-%E6%9C%88%E5%88%8A/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "2025-02 月刊",
  "name": "2025-02 月刊",
  "description": "值得关注的模型和新技术 o3-mini OpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。\nOpenAI o3-mini\nQwQ-Max-Preview QwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。\n我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max\n官方blog：\u0026hellip; QwQ-Max-Preview\nClaude 3.7 Sonnet Claude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。\nClaude 3.7 Sonnet and Claude Code\nGrok-3 Grok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、\u0026ldquo;Think\u0026quot;模式分步推理，以及\u0026quot;Big Brain\u0026quot;模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium+订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。\n",
  "keywords": [
    "月刊"
  ],
  "articleBody": "值得关注的模型和新技术 o3-mini OpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。\nOpenAI o3-mini\nQwQ-Max-Preview QwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。\n我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max\n官方blog：… QwQ-Max-Preview\nClaude 3.7 Sonnet Claude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。\nClaude 3.7 Sonnet and Claude Code\nGrok-3 Grok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、“Think\"模式分步推理，以及\"Big Brain\"模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium+订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。\nGrok-3: The Next Evolution in AI by xAI\nDeepseek OpenSourceWeek FlashMLA FlashMLA\n功能描述：FlashMLA是一个高效的解码内核，专为多头潜在注意力（MLA）设计，优化Hopper GPU上的变长序列处理。它支持BF16和分页KV缓存（块大小64），已在生产环境中使用。性能指标：在H800 SXM5 GPU上（使用CUDA 12.6），FlashMLA实现3000 GB/s内存带宽和580 TFLOPS计算吞吐量，分别在内存绑定和计算绑定场景下表现出色。性能提升与资源优化：这些指标表明FlashMLA显著提升了解码任务的GPU性能，特别是在处理长序列时减少了内存和计算开销。相比传统内核，其高吞吐量显示出对GPU资源的充分利用。 功能描述：FlashMLA是一个高效的解码内核，专为多头潜在注意力（MLA）设计，优化Hopper GPU上的变长序列处理。它支持BF16和分页KV缓存（块大小64），已在生产环境中使用。 性能指标：在H800 SXM5 GPU上（使用CUDA 12.6），FlashMLA实现3000 GB/s内存带宽和580 TFLOPS计算吞吐量，分别在内存绑定和计算绑定场景下表现出色。 性能提升与资源优化：这些指标表明FlashMLA显著提升了解码任务的GPU性能，特别是在处理长序列时减少了内存和计算开销。相比传统内核，其高吞吐量显示出对GPU资源的充分利用。 DeepEP DeepEP\n功能描述：DeepEP是一个为混合专家（MoE）和专家并行（EP）设计的通信库，提供高吞吐量和低延迟的all-to-all GPU内核。它支持FP8运算，并包括不对称域带宽转发的优化内核，适用于训练和推理预填充任务。对于延迟敏感的推理解码，提供纯RDMA的低延迟内核。此外，它引入了基于钩子的通信-计算重叠方法，不占用流式多处理器（SM）资源。 性能指标：虽然具体量化数据有限，但其设计目标是最大化InfiniBand和NVLink带宽利用，减少通信延迟，尤其在分布式训练中。 性能提升与资源优化：DeepEP通过高效的通信管理减少了数据传输时间，提升了分布式系统的整体性能，特别是在大规模GPU集群中，优化了资源利用率。 DeepGEMM DeepGEMM\n功能描述：DeepGEMM是一个FP8通用矩阵乘法（GEMM）库，支持密集和MoE分组GEMM，专为Hopper张量核心设计。它使用CUDA编写，通过轻量级即时编译（JIT）模块在运行时编译内核，无需安装时的复杂编译。核心逻辑仅约300行代码，采用CUDA核心两级累积解决FP8精度问题，借鉴CUTLASS和CuTe的概念但避免依赖其复杂模板。 性能指标：在Hopper GPU上，DeepGEMM实现1350+ TFLOPS的FP8计算性能，超越大多数专家调优内核，尤其在各种矩阵尺寸上表现优异。 性能提升与资源优化：FP8的低精度计算减少了内存使用和计算时间，DeepGEMM的高性能显示出对GPU计算资源的有效利用，特别适合训练和推理中的矩阵运算。 DualPipe DualPipe\n功能描述：DualPipe是一个双向管道并行算法，旨在在DeepSeek-V3和R1的训练中实现计算-通信重叠。它通过创新的调度减少管道气泡，将前向和后向计算与通信阶段完全重叠，特别是在跨节点专家并行中隐藏通信开销。它充分利用InfiniBand和NVLink带宽，优化内存占用，无需昂贵的张量并行（TP）。 性能指标：DeepSeek-V3的训练仅需278.8万H800 GPU小时，相比其他模型（如OpenAI的GPT-4，训练成本约1亿美元），显示出显著的效率提升。论文提到，通过DualPipe，计算-通信比保持恒定时，可实现近零的all-to-all通信开销。 性能提升与资源优化：DualPipe通过减少通信瓶颈和优化管道调度，显著降低了训练时间和计算资源需求，提升了大规模GPU集群的利用率。 EPLB EPLB\n功能描述：EPLB用于专家并行（EP）场景，通过复制负载重的专家并启发式地分配到不同GPU，确保负载均衡。由于DeepSeek-V3采用组限制专家路由，EPLB尝试将同一组的专家放置在同一节点，减少节点间数据流量。其算法已在eplb.py中开源，便于再现和部署。 性能指标：具体量化数据有限，但其设计目标是通过负载均衡优化GPU利用率，减少训练中的瓶颈。 性能提升与资源优化：EPLB通过防止GPU过载或未充分利用，确保资源的高效分配，特别是在专家负载不均时，提升训练效率和稳定性。 值得关注的研究和论文 Deliberative Alignment https://openai.com/index/deliberative-alignment/\nhttps://arxiv.org/abs/2412.16339\n针对o1-like reasoning 模型的安全对齐方法\n当前模型的安全性问题 即便经过safety training，仍可以被jailbreak 模型的响应时间过快，导致它们不能充分评估所有可能的风险和复杂情况 模型并没有直接学习如何确保安全，而是通过标注的数据来间接理解应如何回应 Deliberative alignment 解决了： 模型响应速度过快、缺乏对安全规范的深入理解 通过直接学习safety specifications，使模型不仅仅记住安全规范的内容，还能在实际应用时根据上下文仔细考虑这些规范 在推理阶段对安全规范进行复杂的推理 Methods 对齐训练使用过程性监督和结果性监督的结合\ntrain an o-style model for helpfulness, without any safety-relevant data then build a dataset of (prompt, completion) pairs where the CoTs in the completions reference the specifications. inserting the relevant safety specification text for each conversation in the system prompt, generating model completions, and then removing the system prompts from the data SFT on this dataset then use RL to train the model to use its CoT more effectively 模型自动从安全规范和已分类的安全提示中生成训练数据，而不需要人工标注的回答 Trading inference-time compute for adversarial robustness | OpenAI Trading inference-time compute for adversarial robustness\npaper\n研究推理模型在推理阶段增加计算资源，对于对抗性攻击（adversarial attacks）的影响\n实验发现：\n推理时间计算的增加提升了对抗性鲁棒性：在大多数情况下，随着推理时间计算的增加，攻击成功率趋近于零。这表明，允许模型在推理阶段花费更多计算资源进行推理，可以有效提高其对抗性鲁棒性；\n无需对抗性训练即可提升鲁棒性： 仅通过增加推理时间计算来提升模型的鲁棒性，而无需专门的微调训练；\n推理时间计算的局限性： 在某些特定攻击下，增加推理时间计算可能不会带来显著的安全性提升，甚至可能导致新的安全漏洞；\n此外，文中还介绍了两种针对推理计算扩展模型的新型攻击方式：\nThink Less 攻击：攻击者诱导模型减少推理时间，从而降低其对抗性鲁棒性 Nerd Sniping 攻击：攻击者诱导模型在推理过程中浪费计算资源，使其在无关紧要的推理任务上消耗过多计算能力，可能导致安全漏洞。 Constitutional Classifiers: Defending against universal jailbreaks ｜Anthropic 为提升LLMs对 universal jailbreaks（通用越狱攻击）的防御能力，Anthropic 研究团队提出了一种名为 Constitutional Classifiers 的方法。\nhttps://www.anthropic.com/research/constitutional-classifiers\n解决的问题 LLMs 易受越狱攻击影响： 现有的 LLMs 仍然容易受到 prompt engineering攻击，例如使用 超长输入 或 特殊格式（如大小写混合）来绕过安全限制； 现有防御机制的局限性：传统的安全机制往往难以检测 通用越狱攻击（universal jailbreaks），即能够绕过多个安全限制的攻击方式 安全性与可用性之间的权衡：许多安全机制会导致 过度拒绝（over-refusal），即模型拒绝回答大量无害的查询，从而影响用户体验 亮点 基于“宪法”规则的分类器：采用 Constitutional AI 的理念，使用一套预定义的“宪法”规则来指导模型的安全性，这些规则明确划分了允许和禁止的内容，并用于训练分类器。 输入与输出双重分类器：该系统包含 输入分类器（检测并拦截恶意输入）和 输出分类器（实时监测模型生成的内容，并在检测到有害信息时立即终止响应） 合成数据增强训练：利用 Claude 生成大量合成数据以提高分类器的泛化能力 实时流式预测：支持 流式预测，即在生成每个 token 时评估其潜在的有害性，从而在检测到风险时立即终止响应，而无需等待完整的输出生成 结论 显著降低越狱成功率（从 86% 降至 4.4%） 保持较低的误拒率（仅增加 0.38%） 计算成本适中（增加 23.7%） Qwen2.5-1M Tech report\nBlog\n开源的长上下文模型， 上下文扩展到1M；包含Qwen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M两个版本；\n方法 预训练阶段： 从Qwen2.5 的checkpoint 开始 （上下文4K） 将RoPE基础频率 从10,000 提高到10,000,000 SFT阶段：分两个阶段 第一阶段：仅在短指令（32K）上微调 第二阶段：混合短指令（32K）和长指令（256K）进行训练 RL阶段：在短文本（8K）上训练，能够泛化；最终获得256K instruction-tuning 模型 长度外推： 引入 Dual Chunk Attention (DCA)， 以解决训练过程中未见过的，Query和Key间相对位置距离过大问题 稀疏注意力机制，以提升推理速度： 分块预填充（Chunked Prefill）， MLP层激活权重的显存使用量可减少96.7% 稀疏性优化 注意项 Qwen2.5-7B-Instruct-1M 至少需要120GB\nQwen2.5-14B-Instruct-1M 至少需要320GB\nLarge Language Diffusion Models (LLaDA) paper\n基于Diffusion Models的新型LLM；能够在多个基准测试中与 LLaMA3 8B 竞争；\n不同于传统的LLM，采用Autoregressive的方式，即从左到右预测下一个token，LLaDA采用掩码扩散（Masked Diffusion）方法: 训练时，LLaDA 通过前向过程（Forward Process）随机掩盖输入文本中的 token，并在反向过程（Reverse Process）中预测被掩盖的 token， 从而允许 LLaDA 在双向上下文中进行推理，而不像自回归模型那样受限于单向信息流；\nLLaDA 采用标准的预训练 + SFT范式，并在2.3 万亿个 token上进行训练，随后使用450 万对数据进行 SFT\nMercury, the first commercial-scale diffusion large language model 由 Inception Labs 开发的首个商业级扩散大语言模型\nMercury 在 NVIDIA H100 GPU 上的推理速度超过 1000 tokens/sec，相比于 GPT-4o Mini（59 tokens/sec）和 Claude 3.5 Haiku（61 tokens/sec），提升了 10-20 倍 这使得 Mercury 比传统 LLM 便宜 5-10 倍，适用于大规模商业应用 Mercury 适用于代码生成（Mercury Coder）、对话 AI、企业自动化等多个领域 免费试用：Mercury Coder 目前已上线，可在 Inception Labs Playground 试用 Trade-offs of Diffusion Language Models 但也存在一些显著的权衡，例如高推理成本、在链式推理（CoT）中仍需多次迭代以及延迟的流式输出\nThoughts Are All Over the Place: On the Underthinking of o1-Like LLMs paper\no1-like 模型通过扩展推理计算能力来提升推理深度，但存在着underthinking的问题，即它们在推理过程中频繁切换思路，而未能充分探索有前景的推理路径，导致推理深度不足，影响最终的准确性；\n对此研究者引入 Underthinking Score，用于衡量模型在错误答案中的 token 效率，以量化思维切换对推理质量的影响；\n为了缓解Underthinking问题，研究者提出了一种新的解码策略 TIP（Thought Switching Penalty），其核心思想是：\n在解码过程中对思维切换施加惩罚，鼓励模型在当前推理路径上进行更深入的探索，而非过早切换 TIP 通过调整 logits 进行惩罚，减少模型在短时间内频繁改变推理方向的可能性 Optimizing Large Language Model Training Using FP4 Quantization paper\n由于FP4格式的动态范围和表示能力有限，直接将LLMs量化到这种低比特格式通常会导致模型性能大幅下降\n通过以下两个关键创新点来应对挑战\n可微分量化估计器（Differentiable Quantization Estimator, DGE）：为了精确的权重更新，提出了一种可微分量化估计器，以减少FP4计算中的梯度更新误差。 异常值夹紧和补偿策略（Outlier Clamping and Compensation, OCC）：为了防止在LLMs训练过程中常见的激活值异常值导致的量化精度损失，提出了一种异常值夹紧和补偿策略。 FP4训练框架能够在保持与BF16和FP8相近的准确性的同时，有效地扩展到高达130亿参数的LLMs，并在最多1000亿个训练token上进行训练\n推荐内容 一些仓库 专注于强化学习（RL）提升大语言模型（LLM）推理能力的仓库： github.com/bruno686/Awesome-RL-based-LLM-Reasoning\n适合LLMs的抓取数据工具集合：\ngithub.com/patrickloeber/llm-data-scrapers\nMCP服务器列表： github.com/punkpeye/awesome-mcp-servers\n开源小红书图文采集工具： github.com/JoeanAmier/XHS-Downloader\n基于 LLM 的智能字幕助手： github.com/WEIFENG2333/VideoCaptioner\n阅读和学习 Reasoning best practices ｜ OpenAI： openAI 官方的reasoning模型使用指南\nalphaxiv：可在 arXiv 论文上进行评论讨论，选择文章内容同AI讨论\nAI Agents Course｜ huggingface：huggingface 官方的AI Agent课程\n影音记录 精选歌单 Live演出 02.16 Sigur Rós TOKYO GARDEN THEATER\n电影 哪吒之魔童闹海 ⭐️⭐️⭐️⭐️\n书\u0026阅读摘录 《逃走的人》 李颖迪 谈论工作的意义似乎早就过时了，太热情了甚至显得傻。“工作就是工作。”这才是正确的态度。我们说起工作，说的是绩效和KPI，不是它的乐趣、意义和自我实现。当时仍在新冠流行期间，它更加剧了某种困顿感和停滞感。但我们其实也害怕真的停下—离开既定轨道，比如辞职了，之后还能找到下一份工作吗？就这样迟疑着，踌躇着，不满意想走，想走又不敢走。明明还“年轻”，按照教科书上的说法，这不应该正是踌躇满志的时候吗？\n2022年，中国城市的房子往往每平方米一万元上下——在北京，这个数字是四万（海淀、朝阳等地甚至每平方米九万），上海、深圳也差不多——在城市买房，往往意味着贷款，动辄几百万。年轻人买房等于交出人生的主动权：未来几十年运转于一场数字游戏般的任务，上班，赚钱，还房贷。但向往城市，就不得不挤上这条令人望而生畏的漫长轨道。\n人们对此有不同的看法。有人认为，低廉的房价将源源不断地吸引年轻人来到鹤岗，从而形成新的活力。\n但另一个人说，人们在城市里购房，购买的只是那一套简单的钢筋水泥么？\n他接着说，不，人们购买的是希望。“房价走低不可能带来希望。没有希望，这里的房价才会走低。”\n去鹤岗，也不能说他就此自由了，要看怎么定义“自由”——自由不是想干什么就干什么吧—他停了下——自由是不想干什么就不干什么。但他又换了一个说法，也可能人生都这样，还能烂到哪里去？\none minute blog: the triangle of talent The weird thing is: as a manager, you’ll spend 90% of your time dealing with the employees on the left (problem employees) or the middle (average joes). But it’s the people on the right side (the star employees) that create 90% of the value in the company.\nCEOs - how many level 5s do you have on your team? If you’re just started (cofounders) - you should have 100% level 5 If you’re a small team (5-20 people) - you should have at least 30% If you’re a medium team (20-150 people) - shoot for 15% If you’re a big company (150+ people) - 5-10%… but honestly just quit, big companies are no fun\nSuno创始人访谈：至少对音乐来说，Scaling Law不是万灵药｜Bolt荐阅 音乐与文字非常不同，不能简单的认为在 AI 的发展过程中，语音只是比文字发展的慢了一点，认为 Scaling law 会解决所有问题。音乐是非常主观的领域，要把音乐的模型训练的好、做出好的产品，还需要很多其他的技能。\n一个国家的工程师与律师的比例直接影响经济增长。工程师越多，经济增长越快；律师越多，经济增长越慢\n音乐行业当前有一种“蛋糕就这么大，大家来分”的心态，大家都在争夺有限的财富，导致不公平的分配。如果我们能够一起把蛋糕做大，那么一切都会变得更加容易\n这首先需要让每个人都能享受音乐创作的乐趣\nDeepseek R1可能找到了超越人类的办法 这就导致了预训练撞墙的事实：模型体积虽然增加了 10 倍，但我们已经无法获得比现在多 10 倍的高质量数据了。\nRLHF 本质上是一种讨好人类的训练方式，它让模型输出符合人类偏好，但同时它扼杀了超越人类的可能性。\n使用强化学习（RL）来训练模型思维链成为了所有人的新共识\n未来的模型对人类标注需求会越来越少。\n未来的 Reasoning 模型可以收集用户和模型聊天时 AI 生成的思维链来训练\n硅谷创业教父Paul Graham：为什么你一直在拖延真正想做的事？ “要么在拖延中痛苦，要么让自己行动起来，花费的功夫是一样的。”\n我们为什么害怕“开始”？ 障碍1：缺少经验 障碍2：他人的质疑 障碍3：自我怀疑\n让开始开始的 9 种策略 策略1：用乐观的心态看待事物 策略2：适度的过度自信 策略3：适度的“无知” 策略4：寻找同路人 策略5：关注进步的速度，而非当前的位置 策略6：降低期望值 策略7：选择容易上手的工具 策略8：将失败当成学习的过程 策略9：改变你的衡量标准\n最好的工作是自由市场中持续学习者的创造性表达 “最好的工作是自由市场中持续学习者的创造性表达”（The best jobs are creative expressions of continuous learners in free markets）这句话出自《纳瓦尔宝典》(The Almanackof Naval Ravikant)\n特有知识更多关注你的内在天赋，你真正的好奇心和热情，这种知识通常极富技术性和创造性，因此不能被外包或自动化。\n更舒服的哲学是“做一个制造者，制造人们想要的有趣的东西，练好你的手艺，合适的人最终会找到你。”\n关键是你得迈出第一步——开始创作，成为那1%的内容创作者\n一句话总结普通人如何建立自己的财富之路：产品化你自己\nThe AI Architect — Bret Taylor “Across our customer base, we are seeing a new role emerge - the role of the AI architect. These leaders are responsible for helping define, manage and evolve their company’s AI agent over time. They come from a variety of both technical and business backgrounds, and we think that every company will have one or many AI architects managing their AI agent and related experience.”\n“There’s a lot of power in combining product and engineering into as few people as possible… few great things have been created by committee.” …And it’s almost impossible to specify the requirements of a product when you’re not sure of the limitations of the technology itself.”\nOne thing I just, I just observe is that I think the early Google days had this interesting mix of PM and engineer, which I think you are, you didn’t, you didn’t wait for PM to tell you these are my, this is my PRD.\nIs your job as a maker of software to author a code in an editor? I would argue no just like a generation ago. Your job wasn’t to punch cards in a punch card That is not what your job is. Your job is to produce digital something, whatever it is, what is the purpose of the software that you’re making? Your job is to produce that. And so I think that like our jobs will change rapidly and meaningfully, but I think the idea that like our job is to type in a\nThey didn’t type faster or produce more code. They did the right thing in the right market, the right time.\n都没人看我为什么还要写博客？ 你拿着相机在城市里穿行，看见一幕——光影交错，人情味流露。你按下快门。 没人关心。 但你并不是为了别人去做。你做，是因为你看到了什么。 写博客也是如此。你写，因为你思考，因为你观察，因为你需要一个“出口”来安放这些想法。 有人看吗？有就算赚到。没有也没关系。创作这件事，本身就已经完成了它的意义。 这才是重点所在。\n写博客能让你理清思绪，让你的观点更明晰。你会更谨慎地组织语言，避免废话——说实话，你是为自己写。如果连你自己都提不起兴趣，那别人就更没有理由了\n",
  "wordCount" : "993",
  "inLanguage": "en",
  "image":"https://niraya666.github.io/img/monthly/2025-02%20%E6%9C%88%E5%88%8A%201a52555697de80d9a84dfc8c32fcb6cf/0E5F0FD2-B499-49AD-AEE5-69D8F7E6CEBD_1_105_c.jpeg","datePublished": "2025-02-28T11:00:00+08:00",
  "dateModified": "2025-02-28T11:00:00+08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://niraya666.github.io/monthly/2025-02-%E6%9C%88%E5%88%8A/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LZY Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://niraya666.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niraya666.github.io/" accesskey="h" title="LZY Blog (Alt + H)">LZY Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niraya666.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/musik/" title="musik!">
                    <span>musik!</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/monthly/" title="月刊">
                    <span>月刊</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/essay/" title="杂文">
                    <span>杂文</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/travel/" title="游记">
                    <span>游记</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://niraya666.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://niraya666.github.io/monthly/">Monthlies</a></div>
    <h1 class="post-title entry-hint-parent">
      2025-02 月刊
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-02-28 11:00:00 +0800 CST'>February 28, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Theme PaperMod

</div>
  </header> 
<figure class="entry-cover"><a href="https://niraya666.github.io/img/monthly/2025-02%20%E6%9C%88%E5%88%8A%201a52555697de80d9a84dfc8c32fcb6cf/0E5F0FD2-B499-49AD-AEE5-69D8F7E6CEBD_1_105_c.jpeg" target="_blank"
            rel="noopener noreferrer"><img loading="eager" src="https://niraya666.github.io/img/monthly/2025-02%20%E6%9C%88%E5%88%8A%201a52555697de80d9a84dfc8c32fcb6cf/0E5F0FD2-B499-49AD-AEE5-69D8F7E6CEBD_1_105_c.jpeg" alt="摄于 东京 浅草文化观光中心"></a>
        <p>摄于 东京 浅草文化观光中心</p>
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e6%a8%a1%e5%9e%8b%e5%92%8c%e6%96%b0%e6%8a%80%e6%9c%af" aria-label="值得关注的模型和新技术">值得关注的模型和新技术</a><ul>
                        
                <li>
                    <a href="#o3-mini" aria-label="o3-mini">o3-mini</a></li>
                <li>
                    <a href="#qwq-max-preview" aria-label="QwQ-Max-Preview">QwQ-Max-Preview</a></li>
                <li>
                    <a href="#claude-37-sonnet" aria-label="Claude 3.7 Sonnet">Claude 3.7 Sonnet</a></li>
                <li>
                    <a href="#grok-3" aria-label="Grok-3">Grok-3</a></li>
                <li>
                    <a href="#deepseek-opensourceweek" aria-label="Deepseek OpenSourceWeek">Deepseek OpenSourceWeek</a><ul>
                        
                <li>
                    <a href="#flashmla" aria-label="FlashMLA">FlashMLA</a></li>
                <li>
                    <a href="#deepep" aria-label="DeepEP">DeepEP</a></li>
                <li>
                    <a href="#deepgemm" aria-label="DeepGEMM">DeepGEMM</a></li>
                <li>
                    <a href="#dualpipe" aria-label="DualPipe">DualPipe</a></li>
                <li>
                    <a href="#eplb" aria-label="EPLB">EPLB</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%e5%80%bc%e5%be%97%e5%85%b3%e6%b3%a8%e7%9a%84%e7%a0%94%e7%a9%b6%e5%92%8c%e8%ae%ba%e6%96%87" aria-label="值得关注的研究和论文">值得关注的研究和论文</a><ul>
                        
                <li>
                    <a href="#deliberative-alignment" aria-label="Deliberative Alignment">Deliberative Alignment</a><ul>
                        
                <li>
                    <a href="#%e5%bd%93%e5%89%8d%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%ae%89%e5%85%a8%e6%80%a7%e9%97%ae%e9%a2%98" aria-label="当前模型的安全性问题">当前模型的安全性问题</a></li>
                <li>
                    <a href="#deliberative-alignment-%e8%a7%a3%e5%86%b3%e4%ba%86" aria-label="Deliberative alignment 解决了：">Deliberative alignment 解决了：</a></li>
                <li>
                    <a href="#methods" aria-label="Methods">Methods</a></li></ul>
                </li>
                <li>
                    <a href="#trading-inference-time-compute-for-adversarial-robustness--openai" aria-label="Trading inference-time compute for adversarial robustness | OpenAI">Trading inference-time compute for adversarial robustness | OpenAI</a></li>
                <li>
                    <a href="#constitutional-classifiers-defending-against-universal-jailbreaks-anthropic" aria-label="Constitutional Classifiers: Defending against universal jailbreaks ｜Anthropic">Constitutional Classifiers: Defending against universal jailbreaks ｜Anthropic</a><ul>
                        
                <li>
                    <a href="#%e8%a7%a3%e5%86%b3%e7%9a%84%e9%97%ae%e9%a2%98" aria-label="解决的问题">解决的问题</a></li>
                <li>
                    <a href="#%e4%ba%ae%e7%82%b9" aria-label="亮点">亮点</a></li>
                <li>
                    <a href="#%e7%bb%93%e8%ae%ba" aria-label="结论">结论</a></li></ul>
                </li>
                <li>
                    <a href="#qwen25-1m" aria-label="Qwen2.5-1M">Qwen2.5-1M</a><ul>
                        
                <li>
                    <a href="#%e6%96%b9%e6%b3%95" aria-label="方法">方法</a></li>
                <li>
                    <a href="#%e6%b3%a8%e6%84%8f%e9%a1%b9" aria-label="注意项">注意项</a></li></ul>
                </li>
                <li>
                    <a href="#large-language-diffusion-models-llada" aria-label="Large Language Diffusion Models (LLaDA)">Large Language Diffusion Models (LLaDA)</a><ul>
                        
                <li>
                    <a href="#mercury-the-first-commercial-scale-diffusion-large-language-modelhttpswwwinceptionlabsainews" aria-label="Mercury, the first commercial-scale diffusion large language model">Mercury, the first commercial-scale diffusion large language model</a></li>
                <li>
                    <a href="#trade-offs-of-diffusion-language-modelshttpsxcomarankomatsuzakistatus1895183757961011285" aria-label="Trade-offs of Diffusion Language Models">Trade-offs of Diffusion Language Models</a></li></ul>
                </li>
                <li>
                    <a href="#thoughts-are-all-over-the-place-on-the-underthinking-of-o1-like-llms" aria-label="Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs">Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</a></li>
                <li>
                    <a href="#optimizing-large-language-model-training-using-fp4-quantization" aria-label="Optimizing Large Language Model Training Using FP4 Quantization">Optimizing Large Language Model Training Using FP4 Quantization</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%8e%a8%e8%8d%90%e5%86%85%e5%ae%b9" aria-label="推荐内容">推荐内容</a><ul>
                        
                <li>
                    <a href="#%e4%b8%80%e4%ba%9b%e4%bb%93%e5%ba%93" aria-label="一些仓库">一些仓库</a></li>
                <li>
                    <a href="#%e9%98%85%e8%af%bb%e5%92%8c%e5%ad%a6%e4%b9%a0" aria-label="阅读和学习">阅读和学习</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%bd%b1%e9%9f%b3%e8%ae%b0%e5%bd%95" aria-label="影音记录">影音记录</a><ul>
                        
                <li>
                    <a href="#%e7%b2%be%e9%80%89%e6%ad%8c%e5%8d%95" aria-label="精选歌单">精选歌单</a></li>
                <li>
                    <a href="#live%e6%bc%94%e5%87%ba" aria-label="Live演出">Live演出</a></li>
                <li>
                    <a href="#%e7%94%b5%e5%bd%b1" aria-label="电影">电影</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%b9%a6%e9%98%85%e8%af%bb%e6%91%98%e5%bd%95" aria-label="书&amp;阅读摘录">书&amp;阅读摘录</a><ul>
                        
                <li>
                    <a href="#%e9%80%83%e8%b5%b0%e7%9a%84%e4%ba%ba-%e6%9d%8e%e9%a2%96%e8%bf%aa" aria-label="《逃走的人》 李颖迪">《逃走的人》 李颖迪</a></li>
                <li>
                    <a href="#one-minute-blog-the-triangle-of-talenthttpsshaanbeehiivcompone-minute-blog-the-triangle-of-talent" aria-label="one minute blog: the triangle of talent">one minute blog: the triangle of talent</a></li>
                <li>
                    <a href="#suno%e5%88%9b%e5%a7%8b%e4%ba%ba%e8%ae%bf%e8%b0%88%e8%87%b3%e5%b0%91%e5%af%b9%e9%9f%b3%e4%b9%90%e6%9d%a5%e8%af%b4scaling-law%e4%b8%8d%e6%98%af%e4%b8%87%e7%81%b5%e8%8d%afbolt%e8%8d%90%e9%98%85httpsmpweixinqqcomsacbplmg8d4fzpjwq5twyfa" aria-label="Suno创始人访谈：至少对音乐来说，Scaling Law不是万灵药｜Bolt荐阅">Suno创始人访谈：至少对音乐来说，Scaling Law不是万灵药｜Bolt荐阅</a></li>
                <li>
                    <a href="#deepseek-r1%e5%8f%af%e8%83%bd%e6%89%be%e5%88%b0%e4%ba%86%e8%b6%85%e8%b6%8a%e4%ba%ba%e7%b1%bb%e7%9a%84%e5%8a%9e%e6%b3%95httpsmazzzystarcom20250130chatgpt-to-deepseek-r1-zh" aria-label="Deepseek R1可能找到了超越人类的办法">Deepseek R1可能找到了超越人类的办法</a></li>
                <li>
                    <a href="#%e7%a1%85%e8%b0%b7%e5%88%9b%e4%b8%9a%e6%95%99%e7%88%b6paul-graham%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bd%a0%e4%b8%80%e7%9b%b4%e5%9c%a8%e6%8b%96%e5%bb%b6%e7%9c%9f%e6%ad%a3%e6%83%b3%e5%81%9a%e7%9a%84%e4%ba%8b" aria-label="硅谷创业教父Paul Graham：为什么你一直在拖延真正想做的事？">硅谷创业教父Paul Graham：为什么你一直在拖延真正想做的事？</a></li>
                <li>
                    <a href="#%e6%9c%80%e5%a5%bd%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%98%af%e8%87%aa%e7%94%b1%e5%b8%82%e5%9c%ba%e4%b8%ad%e6%8c%81%e7%bb%ad%e5%ad%a6%e4%b9%a0%e8%80%85%e7%9a%84%e5%88%9b%e9%80%a0%e6%80%a7%e8%a1%a8%e8%be%behttpsmpweixinqqcomssjngirohcfkhrzlou4eizw" aria-label="最好的工作是自由市场中持续学习者的创造性表达">最好的工作是自由市场中持续学习者的创造性表达</a></li>
                <li>
                    <a href="#the-ai-architect--bret-taylorhttpswwwlatentspacepbret" aria-label="The AI Architect — Bret Taylor">The AI Architect — Bret Taylor</a></li>
                <li>
                    <a href="#%e9%83%bd%e6%b2%a1%e4%ba%ba%e7%9c%8b%e6%88%91%e4%b8%ba%e4%bb%80%e4%b9%88%e8%bf%98%e8%a6%81%e5%86%99%e5%8d%9a%e5%ae%a2httpsbaoyuiotranslationswhy-blog-if-nobody-reads-it" aria-label="都没人看我为什么还要写博客？">都没人看我为什么还要写博客？</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="值得关注的模型和新技术">值得关注的模型和新技术<a hidden class="anchor" aria-hidden="true" href="#值得关注的模型和新技术">#</a></h1>
<h2 id="o3-mini">o3-mini<a hidden class="anchor" aria-hidden="true" href="#o3-mini">#</a></h2>
<p>OpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。</p>
<p><a href="https://openai.com/index/openai-o3-mini/">OpenAI o3-mini</a></p>
<h2 id="qwq-max-preview">QwQ-Max-Preview<a hidden class="anchor" aria-hidden="true" href="#qwq-max-preview">#</a></h2>
<p>QwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。</p>
<blockquote>
<p>我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max</p>
</blockquote>
<p>官方blog：<a href="https://qwenlm.github.io/zh/blog/qwq-max-preview/"><think>&hellip;</think> QwQ-Max-Preview</a></p>
<h2 id="claude-37-sonnet">Claude 3.7 Sonnet<a hidden class="anchor" aria-hidden="true" href="#claude-37-sonnet">#</a></h2>
<p>Claude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。</p>
<p><a href="https://www.anthropic.com/news/claude-3-7-sonnet?utm_source=partner-aws&utm_medium=referral&utm_campaign=sonnet_3-7_launch">Claude 3.7 Sonnet and Claude Code</a></p>
<h2 id="grok-3">Grok-3<a hidden class="anchor" aria-hidden="true" href="#grok-3">#</a></h2>
<p>Grok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、&ldquo;Think&quot;模式分步推理，以及&quot;Big Brain&quot;模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium+订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。</p>
<p><a href="https://opencv.org/blog/grok-3/">Grok-3: The Next Evolution in AI by xAI</a></p>
<h2 id="deepseek-opensourceweek">Deepseek OpenSourceWeek<a hidden class="anchor" aria-hidden="true" href="#deepseek-opensourceweek">#</a></h2>
<h3 id="flashmla">FlashMLA<a hidden class="anchor" aria-hidden="true" href="#flashmla">#</a></h3>
<p><a href="https://github.com/deepseek-ai/FlashMLA">FlashMLA</a></p>
<ul>
<li>功能描述：FlashMLA是一个高效的解码内核，专为多头潜在注意力（MLA）设计，优化Hopper GPU上的变长序列处理。它支持BF16和分页KV缓存（块大小64），已在生产环境中使用。性能指标：在H800 SXM5 GPU上（使用CUDA 12.6），FlashMLA实现3000 GB/s内存带宽和580 TFLOPS计算吞吐量，分别在内存绑定和计算绑定场景下表现出色。性能提升与资源优化：这些指标表明FlashMLA显著提升了解码任务的GPU性能，特别是在处理长序列时减少了内存和计算开销。相比传统内核，其高吞吐量显示出对GPU资源的充分利用。</li>
<li>功能描述：FlashMLA是一个高效的解码内核，专为多头潜在注意力（MLA）设计，优化Hopper GPU上的变长序列处理。它支持BF16和分页KV缓存（块大小64），已在生产环境中使用。</li>
<li>性能指标：在H800 SXM5 GPU上（使用CUDA 12.6），FlashMLA实现3000 GB/s内存带宽和580 TFLOPS计算吞吐量，分别在内存绑定和计算绑定场景下表现出色。</li>
<li>性能提升与资源优化：这些指标表明FlashMLA显著提升了解码任务的GPU性能，特别是在处理长序列时减少了内存和计算开销。相比传统内核，其高吞吐量显示出对GPU资源的充分利用。</li>
</ul>
<h3 id="deepep">DeepEP<a hidden class="anchor" aria-hidden="true" href="#deepep">#</a></h3>
<p><a href="https://github.com/deepseek-ai/DeepEP">DeepEP</a></p>
<ul>
<li>功能描述：DeepEP是一个为混合专家（MoE）和专家并行（EP）设计的通信库，提供高吞吐量和低延迟的all-to-all GPU内核。它支持FP8运算，并包括不对称域带宽转发的优化内核，适用于训练和推理预填充任务。对于延迟敏感的推理解码，提供纯RDMA的低延迟内核。此外，它引入了基于钩子的通信-计算重叠方法，不占用流式多处理器（SM）资源。</li>
<li>性能指标：虽然具体量化数据有限，但其设计目标是最大化InfiniBand和NVLink带宽利用，减少通信延迟，尤其在分布式训练中。</li>
<li>性能提升与资源优化：DeepEP通过高效的通信管理减少了数据传输时间，提升了分布式系统的整体性能，特别是在大规模GPU集群中，优化了资源利用率。</li>
</ul>
<h3 id="deepgemm">DeepGEMM<a hidden class="anchor" aria-hidden="true" href="#deepgemm">#</a></h3>
<p><a href="https://github.com/deepseek-ai/DeepGEMM">DeepGEMM</a></p>
<ul>
<li>功能描述：DeepGEMM是一个FP8通用矩阵乘法（GEMM）库，支持密集和MoE分组GEMM，专为Hopper张量核心设计。它使用CUDA编写，通过轻量级即时编译（JIT）模块在运行时编译内核，无需安装时的复杂编译。核心逻辑仅约300行代码，采用CUDA核心两级累积解决FP8精度问题，借鉴CUTLASS和CuTe的概念但避免依赖其复杂模板。</li>
<li>性能指标：在Hopper GPU上，DeepGEMM实现1350+ TFLOPS的FP8计算性能，超越大多数专家调优内核，尤其在各种矩阵尺寸上表现优异。</li>
<li>性能提升与资源优化：FP8的低精度计算减少了内存使用和计算时间，DeepGEMM的高性能显示出对GPU计算资源的有效利用，特别适合训练和推理中的矩阵运算。</li>
</ul>
<h3 id="dualpipe">DualPipe<a hidden class="anchor" aria-hidden="true" href="#dualpipe">#</a></h3>
<p><a href="https://github.com/deepseek-ai/DualPipe">DualPipe</a></p>
<ul>
<li>功能描述：DualPipe是一个双向管道并行算法，旨在在DeepSeek-V3和R1的训练中实现计算-通信重叠。它通过创新的调度减少管道气泡，将前向和后向计算与通信阶段完全重叠，特别是在跨节点专家并行中隐藏通信开销。它充分利用InfiniBand和NVLink带宽，优化内存占用，无需昂贵的张量并行（TP）。</li>
<li>性能指标：DeepSeek-V3的训练仅需278.8万H800 GPU小时，相比其他模型（如OpenAI的GPT-4，训练成本约1亿美元），显示出显著的效率提升。论文提到，通过DualPipe，计算-通信比保持恒定时，可实现近零的all-to-all通信开销。</li>
<li>性能提升与资源优化：DualPipe通过减少通信瓶颈和优化管道调度，显著降低了训练时间和计算资源需求，提升了大规模GPU集群的利用率。</li>
</ul>
<h3 id="eplb">EPLB<a hidden class="anchor" aria-hidden="true" href="#eplb">#</a></h3>
<p><a href="https://github.com/deepseek-ai/EPLB">EPLB</a></p>
<ul>
<li>功能描述：EPLB用于专家并行（EP）场景，通过复制负载重的专家并启发式地分配到不同GPU，确保负载均衡。由于DeepSeek-V3采用组限制专家路由，EPLB尝试将同一组的专家放置在同一节点，减少节点间数据流量。其算法已在eplb.py中开源，便于再现和部署。</li>
<li>性能指标：具体量化数据有限，但其设计目标是通过负载均衡优化GPU利用率，减少训练中的瓶颈。</li>
<li>性能提升与资源优化：EPLB通过防止GPU过载或未充分利用，确保资源的高效分配，特别是在专家负载不均时，提升训练效率和稳定性。</li>
</ul>
<h1 id="值得关注的研究和论文">值得关注的研究和论文<a hidden class="anchor" aria-hidden="true" href="#值得关注的研究和论文">#</a></h1>
<h2 id="deliberative-alignment">Deliberative Alignment<a hidden class="anchor" aria-hidden="true" href="#deliberative-alignment">#</a></h2>
<p><a href="https://openai.com/index/deliberative-alignment/">https://openai.com/index/deliberative-alignment/</a></p>
<p><a href="https://arxiv.org/abs/2412.16339">https://arxiv.org/abs/2412.16339</a></p>
<p><img loading="lazy" src="https://images.ctfassets.net/kftzwdyauwt9/4RhWQZNZEjSlfiBGktzqgo/0d52931621797aba33f251d4ad9502d4/Diagram_2_D_L.svg?w=3840&amp;q=80" alt=""  />
</p>
<p>针对o1-like reasoning 模型的安全对齐方法</p>
<h3 id="当前模型的安全性问题">当前模型的安全性问题<a hidden class="anchor" aria-hidden="true" href="#当前模型的安全性问题">#</a></h3>
<ul>
<li>即便经过<strong>safety training</strong>，仍可以被jailbreak</li>
<li>模型的响应时间过快，导致它们不能充分评估所有可能的风险和复杂情况</li>
<li>模型并没有直接学习如何确保安全，而是通过标注的数据来间接理解应如何回应</li>
</ul>
<h3 id="deliberative-alignment-解决了"><strong>Deliberative alignment 解决了</strong>：<a hidden class="anchor" aria-hidden="true" href="#deliberative-alignment-解决了">#</a></h3>
<ul>
<li>模型响应速度过快、缺乏对安全规范的深入理解</li>
<li>通过直接学习<strong>safety specifications</strong>，使模型不仅仅记住安全规范的内容，还能在实际应用时根据上下文仔细考虑这些规范</li>
<li>在推理阶段对安全规范进行复杂的推理</li>
</ul>
<h3 id="methods">Methods<a hidden class="anchor" aria-hidden="true" href="#methods">#</a></h3>
<p>对齐训练使用过程性监督和结果性监督的结合</p>
<ol>
<li><strong>train an o-style model for helpfulness, without any safety-relevant data</strong></li>
<li><strong>then build a dataset of (prompt, completion) pairs where the CoTs in the completions reference the specifications.</strong></li>
<li><strong>inserting the relevant safety specification text for each conversation in the system prompt, generating model completions, and then removing the system prompts from the data</strong></li>
<li><strong>SFT on this dataset</strong></li>
<li><strong>then use RL to train the model to use its CoT more effectively</strong></li>
</ol>
<p><img loading="lazy" src="https://images.ctfassets.net/kftzwdyauwt9/29SBBhTk4WRN3UA4xJbHWH/b8e23d0a2cb900aef7d75e220cf3ca1c/SFT_Data_Generation_Light.svg?w=3840&amp;q=80" alt=""  />
</p>
<p><img loading="lazy" src="https://images.ctfassets.net/kftzwdyauwt9/4X6l1dd24Ww2XviAcVhVEf/1a8485f1ecd99187bca5998715f2d61d/RL_Training_Light.svg?w=3840&amp;q=80" alt=""  />
</p>
<ul>
<li>模型自动从安全规范和已分类的安全提示中生成训练数据，而不需要人工标注的回答</li>
</ul>
<h2 id="trading-inference-time-compute-for-adversarial-robustness--openai">Trading inference-time compute for adversarial robustness | OpenAI<a hidden class="anchor" aria-hidden="true" href="#trading-inference-time-compute-for-adversarial-robustness--openai">#</a></h2>
<p><a href="https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/">Trading inference-time compute for adversarial robustness</a></p>
<p><a href="https://arxiv.org/abs/2501.18841">paper</a></p>
<p>研究推理模型在推理阶段增加计算资源，对于对抗性攻击（adversarial  attacks）的影响</p>
<p>实验发现：</p>
<p><strong>推理时间计算的增加提升了对抗性鲁棒性</strong>：在大多数情况下，随着推理时间计算的增加，攻击成功率趋近于零。这表明，允许模型在推理阶段花费更多计算资源进行推理，可以有效提高其对抗性鲁棒性；</p>
<p><strong>无需对抗性训练即可提升鲁棒性</strong>： 仅通过增加推理时间计算来提升模型的鲁棒性，而无需专门的微调训练；</p>
<p><strong>推理时间计算的局限性</strong>： 在某些特定攻击下，增加推理时间计算可能不会带来显著的安全性提升，甚至可能导致新的安全漏洞；</p>
<p>此外，文中还介绍了两种针对推理计算扩展模型的新型攻击方式：</p>
<ul>
<li><strong>Think Less 攻击</strong>：攻击者诱导模型减少推理时间，从而降低其对抗性鲁棒性</li>
<li><strong>Nerd Sniping 攻击</strong>：攻击者诱导模型在推理过程中浪费计算资源，使其在无关紧要的推理任务上消耗过多计算能力，可能导致安全漏洞。</li>
</ul>
<h2 id="constitutional-classifiers-defending-against-universal-jailbreaks-anthropic">Constitutional Classifiers: Defending against universal jailbreaks ｜Anthropic<a hidden class="anchor" aria-hidden="true" href="#constitutional-classifiers-defending-against-universal-jailbreaks-anthropic">#</a></h2>
<p>为提升LLMs对 <strong>universal jailbreaks</strong>（通用越狱攻击）的防御能力，Anthropic 研究团队提出了一种名为 <strong>Constitutional Classifiers</strong> 的方法。</p>
<p><a href="https://www.anthropic.com/research/constitutional-classifiers">https://www.anthropic.com/research/constitutional-classifiers</a></p>
<h3 id="解决的问题"><strong>解决的问题</strong><a hidden class="anchor" aria-hidden="true" href="#解决的问题">#</a></h3>
<ul>
<li><strong>LLMs 易受越狱攻击影响</strong>： 现有的 LLMs 仍然容易受到 <strong>prompt engineering</strong>攻击，例如使用 <strong>超长输入</strong> 或 <strong>特殊格式</strong>（如大小写混合）来绕过安全限制；</li>
<li><strong>现有防御机制的局限性</strong>：传统的安全机制往往难以检测 <strong>通用越狱攻击</strong>（universal jailbreaks），即能够绕过多个安全限制的攻击方式</li>
<li><strong>安全性与可用性之间的权衡</strong>：许多安全机制会导致 <strong>过度拒绝</strong>（over-refusal），即模型拒绝回答大量无害的查询，从而影响用户体验</li>
</ul>
<h3 id="亮点"><strong>亮点</strong><a hidden class="anchor" aria-hidden="true" href="#亮点">#</a></h3>
<ul>
<li><strong>基于“宪法”规则的分类器</strong>：采用 <strong>Constitutional AI</strong> 的理念，使用一套预定义的“宪法”规则来指导模型的安全性，这些规则明确划分了允许和禁止的内容，并用于训练分类器。</li>
<li><strong>输入与输出双重分类器</strong>：该系统包含 <strong>输入分类器</strong>（检测并拦截恶意输入）和 <strong>输出分类器</strong>（实时监测模型生成的内容，并在检测到有害信息时立即终止响应）</li>
<li><strong>合成数据增强训练</strong>：利用 Claude 生成大量<strong>合成数据</strong>以提高分类器的泛化能力</li>
<li><strong>实时流式预测</strong>：支持 <strong>流式预测</strong>，即在生成每个 token 时评估其潜在的有害性，从而在检测到风险时立即终止响应，而无需等待完整的输出生成</li>
</ul>
<h3 id="结论">结论<a hidden class="anchor" aria-hidden="true" href="#结论">#</a></h3>
<ul>
<li><strong>显著降低越狱成功率</strong>（从 86% 降至 4.4%）</li>
<li><strong>保持较低的误拒率</strong>（仅增加 0.38%）</li>
<li><strong>计算成本适中</strong>（增加 23.7%）</li>
</ul>
<h2 id="qwen25-1m">Qwen2.5-1M<a hidden class="anchor" aria-hidden="true" href="#qwen25-1m">#</a></h2>
<p><a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf">Tech report</a></p>
<p><a href="https://qwenlm.github.io/zh/blog/qwen2.5-1m/">Blog</a></p>
<p>开源的长上下文模型， 上下文扩展到1M；包含Qwen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M两个版本；</p>
<h3 id="方法">方法<a hidden class="anchor" aria-hidden="true" href="#方法">#</a></h3>
<ul>
<li>预训练阶段：
<ul>
<li>从Qwen2.5 的checkpoint 开始 （上下文4K）</li>
<li>将RoPE基础频率 从10,000 提高到10,000,000</li>
</ul>
</li>
<li>SFT阶段：分两个阶段
<ul>
<li>第一阶段：仅在短指令（32K）上微调</li>
<li>第二阶段：混合短指令（32K）和长指令（256K）进行训练</li>
</ul>
</li>
<li>RL阶段：在短文本（8K）上训练，能够泛化；最终获得256K instruction-tuning 模型</li>
<li>长度外推：
<ul>
<li>引入 <a href="https://arxiv.org/abs/2402.17463">Dual Chunk Attention</a> (DCA)， 以解决训练过程中未见过的，Query和Key间相对位置距离过大问题</li>
</ul>
</li>
<li>稀疏注意力机制，以提升推理速度：
<ul>
<li>分块预填充（Chunked Prefill）， MLP层激活权重的显存使用量可减少96.7%</li>
<li>稀疏性优化</li>
</ul>
</li>
</ul>
<h3 id="注意项">注意项<a hidden class="anchor" aria-hidden="true" href="#注意项">#</a></h3>
<p>Qwen2.5-7B-Instruct-1M 至少需要120GB</p>
<p>Qwen2.5-14B-Instruct-1M 至少需要320GB</p>
<h2 id="large-language-diffusion-models-llada"><strong>Large Language Diffusion Models (LLaDA)</strong><a hidden class="anchor" aria-hidden="true" href="#large-language-diffusion-models-llada">#</a></h2>
<p><a href="https://arxiv.org/abs/2502.09992">paper</a></p>
<p>基于Diffusion Models的新型LLM；能够在多个基准测试中与 LLaMA3 8B 竞争；</p>
<p>不同于传统的LLM，采用Autoregressive的方式，即从左到右预测下一个token，LLaDA采用掩码扩散（<strong>Masked Diffusion</strong>）方法: 训练时，LLaDA 通过<strong>前向过程（Forward Process）随机掩盖输入文本中的 token，并在反向过程（Reverse Process）<strong>中预测被掩盖的 token， 从而允许 LLaDA 在</strong>双向上下文</strong>中进行推理，而不像自回归模型那样受限于单向信息流；</p>
<p>LLaDA 采用标准的<strong>预训练 + SFT范式，并在</strong>2.3 万亿个 token上进行训练，随后使用<strong>450 万对数据</strong>进行 SFT</p>
<h3 id="mercury-the-first-commercial-scale-diffusion-large-language-modelhttpswwwinceptionlabsainews"><a href="https://www.inceptionlabs.ai/news">Mercury, the first commercial-scale diffusion large language model</a><a hidden class="anchor" aria-hidden="true" href="#mercury-the-first-commercial-scale-diffusion-large-language-modelhttpswwwinceptionlabsainews">#</a></h3>
<p>由 <strong>Inception Labs</strong> 开发的<strong>首个商业级扩散大语言模型</strong></p>
<ul>
<li>Mercury 在 <strong>NVIDIA H100 GPU</strong> 上的推理速度超过 <strong>1000 tokens/sec</strong>，相比于 GPT-4o Mini（59 tokens/sec）和 Claude 3.5 Haiku（61 tokens/sec），提升了 <strong>10-20 倍</strong></li>
<li>这使得 Mercury <strong>比传统 LLM 便宜 5-10 倍</strong>，适用于大规模商业应用</li>
<li>Mercury 适用于<strong>代码生成（Mercury Coder）、对话 AI、企业自动化</strong>等多个领域</li>
<li><strong>免费试用</strong>：Mercury Coder 目前已上线，可在 <a href="https://chat.inceptionlabs.ai/">Inception Labs Playground</a> 试用</li>
</ul>
<h3 id="trade-offs-of-diffusion-language-modelshttpsxcomarankomatsuzakistatus1895183757961011285"><a href="https://x.com/arankomatsuzaki/status/1895183757961011285">Trade-offs of Diffusion Language Models</a><a hidden class="anchor" aria-hidden="true" href="#trade-offs-of-diffusion-language-modelshttpsxcomarankomatsuzakistatus1895183757961011285">#</a></h3>
<p>但也存在一些显著的权衡，例如高推理成本、在链式推理（CoT）中仍需多次迭代以及延迟的流式输出</p>
<h2 id="thoughts-are-all-over-the-place-on-the-underthinking-of-o1-like-llms"><strong>Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</strong><a hidden class="anchor" aria-hidden="true" href="#thoughts-are-all-over-the-place-on-the-underthinking-of-o1-like-llms">#</a></h2>
<p><a href="https://arxiv.org/abs/2501.18585">paper</a></p>
<p>o1-like 模型通过扩展推理计算能力来提升推理深度，但存在着<strong>underthinking</strong>的问题，即它们在推理过程中频繁切换思路，而未能充分探索有前景的推理路径，导致推理深度不足，影响最终的准确性；</p>
<p>对此研究者引入 <strong>Underthinking Score</strong>，用于衡量模型在错误答案中的 token 效率，以量化思维切换对推理质量的影响；</p>
<p>为了缓解<strong>Underthinking</strong>问题，研究者提出了一种新的解码策略 <strong>TIP（Thought Switching Penalty）</strong>，其核心思想是：</p>
<ul>
<li><strong>在解码过程中对思维切换施加惩罚</strong>，鼓励模型在当前推理路径上进行更深入的探索，而非过早切换</li>
<li><strong>TIP 通过调整 logits 进行惩罚</strong>，减少模型在短时间内频繁改变推理方向的可能性</li>
</ul>
<h2 id="optimizing-large-language-model-training-using-fp4-quantization">Optimizing Large Language Model Training Using FP4 Quantization<a hidden class="anchor" aria-hidden="true" href="#optimizing-large-language-model-training-using-fp4-quantization">#</a></h2>
<p><a href="https://arxiv.org/abs/2501.17116">paper</a></p>
<p>由于FP4格式的动态范围和表示能力有限，直接将LLMs量化到这种低比特格式通常会导致模型性能大幅下降</p>
<p>通过以下两个关键创新点来应对挑战</p>
<ol>
<li>可微分量化估计器（Differentiable Quantization Estimator, DGE）：为了精确的权重更新，提出了一种可微分量化估计器，以减少FP4计算中的梯度更新误差。</li>
<li>异常值夹紧和补偿策略（Outlier Clamping and Compensation, OCC）：为了防止在LLMs训练过程中常见的激活值异常值导致的量化精度损失，提出了一种异常值夹紧和补偿策略。</li>
</ol>
<p>FP4训练框架能够在保持与BF16和FP8相近的准确性的同时，有效地扩展到高达130亿参数的LLMs，并在最多1000亿个训练token上进行训练</p>
<h1 id="推荐内容">推荐内容<a hidden class="anchor" aria-hidden="true" href="#推荐内容">#</a></h1>
<h2 id="一些仓库">一些仓库<a hidden class="anchor" aria-hidden="true" href="#一些仓库">#</a></h2>
<p>专注于强化学习（RL）提升大语言模型（LLM）推理能力的仓库： <a href="http://github.com/bruno686/Awesome-RL-based-LLM-Reasoning">github.com/bruno686/Awesome-RL-based-LLM-Reasoning</a></p>
<p>适合LLMs的抓取数据工具集合：</p>
<p><a href="http://github.com/patrickloeber/llm-data-scrapers">github.com/patrickloeber/llm-data-scrapers</a></p>
<p>MCP服务器列表：
<a href="http://github.com/punkpeye/awesome-mcp-servers">github.com/punkpeye/awesome-mcp-servers</a></p>
<p>开源小红书图文采集工具：
<a href="http://github.com/JoeanAmier/XHS-Downloader">github.com/JoeanAmier/XHS-Downloader</a></p>
<p>基于 LLM 的智能字幕助手：
<a href="http://github.com/WEIFENG2333/VideoCaptioner">github.com/WEIFENG2333/VideoCaptioner</a></p>
<h2 id="阅读和学习">阅读和学习<a hidden class="anchor" aria-hidden="true" href="#阅读和学习">#</a></h2>
<p><a href="https://platform.openai.com/docs/guides/reasoning-best-practices">Reasoning best practices ｜ OpenAI</a>： openAI 官方的reasoning模型使用指南</p>
<p><a href="https://www.alphaxiv.org/explore">alphaxiv</a>：可在 arXiv 论文上进行评论讨论，选择文章内容同AI讨论</p>
<p><a href="https://huggingface.co/learn/agents-course/unit0/introduction">AI Agents Course｜ huggingface</a>：huggingface 官方的AI Agent课程</p>
<hr>
<h1 id="影音记录">影音记录<a hidden class="anchor" aria-hidden="true" href="#影音记录">#</a></h1>
<h2 id="精选歌单">精选歌单<a hidden class="anchor" aria-hidden="true" href="#精选歌单">#</a></h2>
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/41b4KBs9GjxnWFUywnu3dt?utm_source=generator" width="100%" height="450" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>
<h2 id="live演出">Live演出<a hidden class="anchor" aria-hidden="true" href="#live演出">#</a></h2>
<p>02.16 <strong>Sigur Rós</strong> TOKYO GARDEN THEATER</p>
<p><img loading="lazy" src="/img/monthly/2025-02%20%E6%9C%88%E5%88%8A%201a52555697de80d9a84dfc8c32fcb6cf/0B36CD88-7168-4433-81E9-B6A8B01C15A2_1_105_c.jpeg" alt="0B36CD88-7168-4433-81E9-B6A8B01C15A2_1_105_c.jpeg"  />
</p>
<h2 id="电影">电影<a hidden class="anchor" aria-hidden="true" href="#电影">#</a></h2>
<p>哪吒之魔童闹海 ⭐️⭐️⭐️⭐️</p>
<h1 id="书阅读摘录">书&amp;阅读摘录<a hidden class="anchor" aria-hidden="true" href="#书阅读摘录">#</a></h1>
<h2 id="逃走的人-李颖迪">《<strong>逃走的人</strong>》 李颖迪<a hidden class="anchor" aria-hidden="true" href="#逃走的人-李颖迪">#</a></h2>
<blockquote>
<p>谈论工作的意义似乎早就过时了，太热情了甚至显得傻。“工作就是工作。”这才是正确的态度。我们说起工作，说的是绩效和KPI，不是它的乐趣、意义和自我实现。当时仍在新冠流行期间，它更加剧了某种困顿感和停滞感。<strong>但我们其实也害怕真的停下—离开既定轨道，比如辞职了，之后还能找到下一份工作吗？就这样迟疑着，踌躇着，不满意想走，想走又不敢走</strong>。明明还“年轻”，按照教科书上的说法，这不应该正是踌躇满志的时候吗？</p>
</blockquote>
<blockquote>
<p>2022年，中国城市的房子往往每平方米一万元上下——在北京，这个数字是四万（海淀、朝阳等地甚至每平方米九万），上海、深圳也差不多——在城市买房，往往意味着贷款，动辄几百万。<strong>年轻人买房等于交出人生的主动权：未来几十年运转于一场数字游戏般的任务，上班，赚钱，还房贷</strong>。但向往城市，就不得不挤上这条令人望而生畏的漫长轨道。</p>
</blockquote>
<blockquote>
<p>人们对此有不同的看法。有人认为，低廉的房价将源源不断地吸引年轻人来到鹤岗，从而形成新的活力。</p>
<p>但另一个人说，人们在城市里购房，购买的只是那一套简单的钢筋水泥么？</p>
<p>他接着说，不，人们购买的是希望。“房价走低不可能带来希望。没有希望，这里的房价才会走低。”</p>
</blockquote>
<blockquote>
<p>去鹤岗，也不能说他就此自由了，要看怎么定义“自由”——<strong>自由不是想干什么就干什么吧—他停了下——自由是不想干什么就不干什么</strong>。但他又换了一个说法，也可能人生都这样，还能烂到哪里去？</p>
</blockquote>
<h2 id="one-minute-blog-the-triangle-of-talenthttpsshaanbeehiivcompone-minute-blog-the-triangle-of-talent"><a href="https://shaan.beehiiv.com/p/one-minute-blog-the-triangle-of-talent">one minute blog: the triangle of talent</a><a hidden class="anchor" aria-hidden="true" href="#one-minute-blog-the-triangle-of-talenthttpsshaanbeehiivcompone-minute-blog-the-triangle-of-talent">#</a></h2>
<blockquote>
<p>The weird thing is: as a manager, you’ll spend 90% of your time dealing with the employees on the left (problem employees) or the middle (average joes).
But it’s the people on the right side (the star employees) that create 90% of the value in the company.</p>
</blockquote>
<p><img loading="lazy" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/1082187e-9850-4f5f-9f36-91f275f33a6f/Level_1_-_Useless__1_.png?t=1738950742" alt=""  />
</p>
<blockquote>
<p>CEOs - how many level 5s do you have on your team?
If you’re just started (cofounders) - you should have 100% level 5
If you&rsquo;re a small team (5-20 people) - you should have at least 30%
If you’re a medium team (20-150 people) - shoot for 15%
If you’re a big company (150+ people) - 5-10%&hellip; but honestly just quit, big companies are no fun</p>
</blockquote>
<h2 id="suno创始人访谈至少对音乐来说scaling-law不是万灵药bolt荐阅httpsmpweixinqqcomsacbplmg8d4fzpjwq5twyfa"><a href="https://mp.weixin.qq.com/s/aCbPlmg8D4FZpJwq5TWyfA">Suno创始人访谈：至少对音乐来说，Scaling Law不是万灵药｜Bolt荐阅</a><a hidden class="anchor" aria-hidden="true" href="#suno创始人访谈至少对音乐来说scaling-law不是万灵药bolt荐阅httpsmpweixinqqcomsacbplmg8d4fzpjwq5twyfa">#</a></h2>
<blockquote>
<p>音乐与文字非常不同，不能简单的认为在 AI 的发展过程中，语音只是比文字发展的慢了一点，认为 Scaling law 会解决所有问题。音乐是非常主观的领域，要把音乐的模型训练的好、做出好的产品，还需要很多其他的技能。</p>
</blockquote>
<blockquote>
<p>一个国家的工程师与律师的比例直接影响经济增长。工程师越多，经济增长越快；律师越多，经济增长越慢</p>
</blockquote>
<blockquote>
<p>音乐行业当前有一种“蛋糕就这么大，大家来分”的心态，大家都在争夺有限的财富，导致不公平的分配。如果我们能够一起把蛋糕做大，那么一切都会变得更加容易</p>
</blockquote>
<blockquote>
<p>这首先需要让每个人都能享受音乐创作的乐趣</p>
</blockquote>
<h2 id="deepseek-r1可能找到了超越人类的办法httpsmazzzystarcom20250130chatgpt-to-deepseek-r1-zh"><a href="https://mazzzystar.com/2025/01/30/chatgpt-to-deepseek-r1-zh/"><strong>Deepseek R1可能找到了超越人类的办法</strong></a><a hidden class="anchor" aria-hidden="true" href="#deepseek-r1可能找到了超越人类的办法httpsmazzzystarcom20250130chatgpt-to-deepseek-r1-zh">#</a></h2>
<blockquote>
<p>这就导致了预训练撞墙的事实：模型体积虽然增加了 10 倍，但我们已经无法获得比现在多 10 倍的高质量数据了。</p>
</blockquote>
<blockquote>
<p>RLHF 本质上是一种讨好人类的训练方式，它让模型输出符合人类偏好，但同时它扼杀了超越人类的可能性。</p>
</blockquote>
<blockquote>
<p>使用强化学习（RL）来训练模型思维链成为了所有人的新共识</p>
</blockquote>
<blockquote>
<p>未来的模型对人类标注需求会越来越少。</p>
</blockquote>
<blockquote>
<p>未来的 Reasoning 模型可以收集用户和模型聊天时 AI 生成的思维链来训练</p>
</blockquote>
<h2 id="硅谷创业教父paul-graham为什么你一直在拖延真正想做的事">硅谷创业教父Paul Graham：为什么你一直在拖延真正想做的事？<a hidden class="anchor" aria-hidden="true" href="#硅谷创业教父paul-graham为什么你一直在拖延真正想做的事">#</a></h2>
<blockquote>
<p>“要么在拖延中痛苦，要么让自己行动起来，花费的功夫是一样的。”</p>
</blockquote>
<blockquote>
<p>我们为什么害怕“开始”？
障碍1：缺少经验
障碍2：他人的质疑
障碍3：自我怀疑</p>
</blockquote>
<blockquote>
<p>让开始开始的 9 种策略
策略1：用乐观的心态看待事物
策略2：适度的过度自信
策略3：适度的“无知”
策略4：寻找同路人
策略5：关注进步的速度，而非当前的位置
策略6：降低期望值
策略7：选择容易上手的工具
策略8：将失败当成学习的过程
策略9：改变你的衡量标准</p>
</blockquote>
<h2 id="最好的工作是自由市场中持续学习者的创造性表达httpsmpweixinqqcomssjngirohcfkhrzlou4eizw"><a href="https://mp.weixin.qq.com/s/SjngIRohcfKHRZLOu4eizw"><strong>最好的工作是自由市场中持续学习者的创造性表达</strong></a><a hidden class="anchor" aria-hidden="true" href="#最好的工作是自由市场中持续学习者的创造性表达httpsmpweixinqqcomssjngirohcfkhrzlou4eizw">#</a></h2>
<blockquote>
<p>“最好的工作是自由市场中持续学习者的创造性表达”（The best jobs are creative expressions of continuous learners in free markets）这句话出自《纳瓦尔宝典》(The Almanackof Naval Ravikant)</p>
</blockquote>
<blockquote>
<p>特有知识更多关注你的内在天赋，你真正的好奇心和热情，这种知识通常极富技术性和创造性，因此不能被外包或自动化。</p>
</blockquote>
<blockquote>
<p>更舒服的哲学是“做一个制造者，制造人们想要的有趣的东西，练好你的手艺，合适的人最终会找到你。”</p>
</blockquote>
<blockquote>
<p>关键是你得迈出第一步——开始创作，成为那1%的内容创作者</p>
</blockquote>
<blockquote>
<p>一句话总结普通人如何建立自己的财富之路：产品化你自己</p>
</blockquote>
<h2 id="the-ai-architect--bret-taylorhttpswwwlatentspacepbret"><a href="https://www.latent.space/p/bret"><strong>The AI Architect — Bret Taylor</strong></a><a hidden class="anchor" aria-hidden="true" href="#the-ai-architect--bret-taylorhttpswwwlatentspacepbret">#</a></h2>
<blockquote>
<p>“<em>Across our customer base, <strong>we are seeing a new role emerge - the role of the AI architect</strong>. These leaders are responsible for helping define, manage and evolve their company&rsquo;s AI agent over time. They come from a variety of both technical and business backgrounds, and we think that every company will have one or many AI architects managing their AI agent and related experience.”</em></p>
</blockquote>
<blockquote>
<p>“<em>There&rsquo;s a lot of power in <strong>combining product and engineering into as few people as possible</strong>… few great things have been created by committee.</em>”
&hellip;<em><strong>And it&rsquo;s almost impossible to specify the requirements of a product when you&rsquo;re not sure of the limitations of the technology itself.”</strong></em></p>
</blockquote>
<blockquote>
<p>One thing I just, I just observe is that I think the early Google days had this interesting mix of PM and engineer, which I think you are, you didn&rsquo;t, you didn&rsquo;t wait for PM to tell you these are my, this is my PRD.</p>
</blockquote>
<blockquote>
<p>Is your job as a maker of software to author a code in an editor? I would argue no just like a generation ago. Your job wasn&rsquo;t to punch cards in a punch card That is not what your job is. Your job is to produce digital something, whatever it is, what is the purpose of the software that you&rsquo;re making?
Your job is to produce that. And so I think that like our jobs will change rapidly and meaningfully, but I think the idea that like our job is to type in a</p>
</blockquote>
<blockquote>
<p>They didn&rsquo;t type faster or produce more code. They did the right thing in the right market, the right time.</p>
</blockquote>
<h2 id="都没人看我为什么还要写博客httpsbaoyuiotranslationswhy-blog-if-nobody-reads-it"><a href="https://baoyu.io/translations/why-blog-if-nobody-reads-it"><strong>都没人看我为什么还要写博客？</strong></a><a hidden class="anchor" aria-hidden="true" href="#都没人看我为什么还要写博客httpsbaoyuiotranslationswhy-blog-if-nobody-reads-it">#</a></h2>
<blockquote>
<p>你拿着相机在城市里穿行，看见一幕——光影交错，人情味流露。你按下快门。 没人关心。 但你并不是为了别人去做。你做，是因为你看到了什么。 写博客也是如此。你写，因为你思考，因为你观察，因为你需要一个“出口”来安放这些想法。 有人看吗？有就算赚到。没有也没关系。创作这件事，本身就已经完成了它的意义。 这才是重点所在。</p>
</blockquote>
<blockquote>
<p>写博客能让你理清思绪，让你的观点更明晰。你会更谨慎地组织语言，避免废话——说实话，你是为自己写。如果连你自己都提不起兴趣，那别人就更没有理由了</p>
</blockquote>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://niraya666.github.io/tags/%E6%9C%88%E5%88%8A/">月刊</a></li>
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-02 月刊 on x"
            href="https://x.com/intent/tweet/?text=2025-02%20%e6%9c%88%e5%88%8a&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-02-%25E6%259C%2588%25E5%2588%258A%2f&amp;hashtags=%e6%9c%88%e5%88%8a">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-02 月刊 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-02-%25E6%259C%2588%25E5%2588%258A%2f&amp;title=2025-02%20%e6%9c%88%e5%88%8a&amp;summary=2025-02%20%e6%9c%88%e5%88%8a&amp;source=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-02-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-02 月刊 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-02-%25E6%259C%2588%25E5%2588%258A%2f&title=2025-02%20%e6%9c%88%e5%88%8a">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-02 月刊 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-02-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-02 月刊 on whatsapp"
            href="https://api.whatsapp.com/send?text=2025-02%20%e6%9c%88%e5%88%8a%20-%20https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-02-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-02 月刊 on telegram"
            href="https://telegram.me/share/url?text=2025-02%20%e6%9c%88%e5%88%8a&amp;url=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-02-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025-02 月刊 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=2025-02%20%e6%9c%88%e5%88%8a&u=https%3a%2f%2fniraya666.github.io%2fmonthly%2f2025-02-%25E6%259C%2588%25E5%2588%258A%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="utterances">
  <script src="https://utteranc.es/client.js"
        repo="https://github.com/Niraya666/niraya666.github.io.git"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>


<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: {'[+]': ['ams']}
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://niraya666.github.io/">LZY Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
