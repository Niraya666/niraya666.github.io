<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Agent on LZY Blog</title>
    <link>https://niraya666.github.io/tags/agent/</link>
    <description>Recent content in Agent on LZY Blog</description>
    <image>
      <title>LZY Blog</title>
      <url>https://niraya666.github.io/images/papermod-cover.png</url>
      <link>https://niraya666.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 25 Jun 2024 17:00:00 +0800</lastBuildDate>
    <atom:link href="https://niraya666.github.io/tags/agent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Agent学习笔记： 如何验证模型的tool-using能力</title>
      <link>https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B%E7%9A%84tool-using%E8%83%BD%E5%8A%9B/</link>
      <pubDate>Tue, 25 Jun 2024 17:00:00 +0800</pubDate>
      <guid>https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B%E7%9A%84tool-using%E8%83%BD%E5%8A%9B/</guid>
      <description>本文将简单介绍如何评价LLM的tool-using 能力。
引言 在工具使用评估方面，过去的研究主要有以下几种思路：
对比工具使用和纯LLM在基准测试上的分数：例如Toolformer和LATM。
在Toolformer研究中，通过下游任务如语言模型评估基准测试、数学推理任务和问答任务来验证工具使用的有效性。
LATM则采用了来自BigBench的六个数据集进行评估。
测试工具使用的准确率和响应质量：例如API-Bank。
在评估过程中，首先初始化评估系统，确保每个API的数据库包含默认值。然后，将预测的API调用与手动标注的API调用进行比较，以确定它们的一致性。响应评估则使用ROUGE-L指标。 利用LLM对工具使用的效果进行评价：例如Tool-bench。
two evaluation metrics:
Pass Rate: Calculates the proportion of successfully completing an instruction within limited OpenAI API calls.
Preference: Measured by comparing two answers (action sequences) for a given instruction.We pre-define a set of criteria for a better answer, which are organized as prompts for ChatGPT.
构造虚拟运行环境，测试代理与环境的交互结果：例如ToolAlpaca。
利用LLM模拟环境（用户代理和助手代理），并使用GPT-4对ToolAlpaca模型进行机器评估，评估其使用各种未见工具的能力。 对于绝大多数企业和垂直场景下， 其中思路1需要构建额外的测试集成本比较高（但还是有必要的）， 而思路4构造虚拟运行环境实际上并不现实； 所以还是选择思路2，外加通过思路3辅助判断；换句话说， 根据场景，构造工具列表和工具调用的ground-truth（包括函数名， 和传入参数） ，在存在歧义时，采用ROUGE评价响应质量， 或者使用LLM判断响应结果并评价。
顺带提一下Langchain 项目中有关Agent的tool-using能力测试的内容， 不过由于Langchain项目又臭又长，且有很大的局限性， 这里只讨论其思路。</description>
    </item>
    <item>
      <title>基于大语言模型的 Agent：科普向</title>
      <link>https://niraya666.github.io/posts/%E5%9F%BA%E4%BA%8E%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84-agent%E7%A7%91%E6%99%AE%E5%90%91/</link>
      <pubDate>Mon, 13 May 2024 16:00:00 +0800</pubDate>
      <guid>https://niraya666.github.io/posts/%E5%9F%BA%E4%BA%8E%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84-agent%E7%A7%91%E6%99%AE%E5%90%91/</guid>
      <description>写在开头 本文是基于最近组内技术交流的文字稿整理。
What is Agent？ 在探讨复杂的人工智能技术之前，让我们先回顾一下生活中的一个简单例子：扫地机器人。这种智能设备在房间中自主导航，避开障碍物，寻找最有效的清洁路径。它就是一个现实生活中的Agent——一个可以自主决策和行动的实体。
在人工智能领域，Agent指的是任何可以感知其环境并根据感知结果做出决策的实体。这些决策旨在完成某些任务或达成特定的目标。Agent的行为可以简单如游戏里的机器人，也可以复杂如自动驾驶汽车。
开始于强化学习 在强化学习中， 我们往往能见到agent的概念。强化学习是一种机器学习方法，它教导Agent通过试错法找到最佳行动路径。就像训练小狗一样，我们通过奖励来引导Agent做出正确的决策。Agent的目标是在与环境的交互中寻找最优策略。理想情况下，如果Agent能够获取足够多的真实环境数据，它就能找到最佳解决方案。然而，由于真实环境的复杂性，完全模拟真实世界是不现实的。
目前，强化学习主要适用于环境简单、问题定义明确的场景，如围棋或视频游戏。这种方法在虚拟环境中通过大量试错来探索解决方案，这种方法虽然有效，但缺乏灵活性和高效性。与人类学习新技能的方式相比，强化学习的效率远低。人们通常通过少量的尝试就能迅速掌握新技能，而强化学习可能需要成千上万次的试错。
这时候，如果agent具有大脑就好了。
将LLMs作为大脑: 赋能智能Agent的关键技术 相较于基于强化学习的Agent，人类的优势在于我们天生具备的记忆能力和逻辑判断能力，甚至包括反思和从经验中学习的能力。这些能力使得我们能够通过极少的试错迅速适应和掌握新技能。
语言模型（LLMs）为AI领域带来了革命性的变化。LLMs通过其深度学习的新范式，以及在思维链和自然语言理解方面的强大能力，预示着Agent将拥有更强大的学习和迁移能力。这种能力的提升将使得创建广泛应用且实用的Agent成为可能。
虽然LLM是否真正具备了推理能力仍然存疑，但LLM的出现无疑改变了很多。以COT（Chain of Thought）为例，这种模型通过打印解题的中间步骤，加强了在数学和逻辑推理方面的能力，减少了幻觉的出现。这一突破性的工作揭示了LLM在不同场景下的强大&amp;quot;in-context learning&amp;quot;能力，以及其在未经微调的情况下的泛化能力。
将LLM视为AI Agent的大脑，为自动化系统提供了一种全新的构思方式。这种基于LLM的agent系统综合了规划、记忆、工具使用和行动的能力，通过API调用与外部世界互动，显示出了前所未有的灵活性和效率。
根据LLM Powered Autonomous Agents一文对LLM-based agent系统的定义， agent需要具备的基本能力：规划+记忆+工具+行动。
规划能力：将复杂的任务分解成小任务， 并管理每一个子任务的目标， 并从过去的失败中反思，以吸取经验。
记忆：LLM的上下文长度有限， 通过额外的记忆系统以提升LLM作为大脑的能力。
工具调用&amp;amp;行动：LLM通过API调用的方式，执行任务， 与外界交互，而不是只是输出文字。
探索AI代理的独特能力：人类与单一LLM无法比拟 AI系统的主要优势在于它们的规模和效率。这些系统能够执行以下任务，超越人类能力：
大规模数据处理：AI能够高效地分析和处理超出人类理解范围的数据量。
无需休息的持续操作：AI系统可以不间断地运行，而无需像人类那样休息和恢复。
超快速计算：AI可以迅速执行复杂的计算，处理速度和效率远超人类。
AI代理与单一LLM的不同:
根据Andrew Ng在讲座中分享的内容，使用相对“简单”模型的代理工作流程（例如GPT-3.5）在实际应用中往往能够超越使用“智能”模型（如GPT-4）的零次提示。这说明在特定场景下，选择适当的AI模型和策略可能比单一的高级模型更有效。
AI代理在决策制定中的应用也显示出其独特的优势。它们可以在没有情感偏见的情况下，基于大量数据做出快速且精确的决策。这种能力在需要快速响应和高精确度的领域尤为重要，如金融交易和紧急响应系统。
Agent的规划和思维过程 AI Agent在处理复杂任务时，通过将大任务分解成小任务来提高效率。此外，自我反思能力允许Agent从过去的行动中学习，通过评估过去的决策来改善未来的表现。
CoT，Chain of Thought， Wei et al. 2022。 即“思维链”，是一种使Agent逐步思考的方法。它通过要求模型展示解决问题的中间步骤来加强其逻辑推理能力，从而提高决策的质量和准确性。
Tree of Thoughts， ToT (Yao et al. 2023)
尽管语言模型在许多领域表现出色，但在需要复杂规划和全局决策的任务上，它们的能力受到了限制。ToT框架应运而生，旨在通过一个结构化的思考和评估过程来弥补这一缺陷。
ToT框架借鉴了人类心理学中的双系统决策理论，通过整合快速直觉判断和慢速深思熟虑的决策过程，极大地提升了模型的决策能力。这一框架通过自我评估的方式，允许模型在面对多种可能的决策路径时，能够进行有效的选择和全局优化。
ToT框架旨在克服现有语言模型在处理需要复杂规划或搜索任务的局限。它通过结构化的思想树来探索和评估不同的决策路径。ToT允许模型在考虑多个推理路径时自我评估其选择，以做出最佳的决策。此外，ToT结合了语言生成和搜索算法（如BFS和DFS），使模型能够在进行决策时前瞻和回溯，以实现全局最优选择。
prompt example：
cot_prompt = &amp;#39;&amp;#39;&amp;#39; Write a coherent passage of 4 short paragraphs.</description>
    </item>
    <item>
      <title>Agent学习笔记：OpenAI Function Calling完全指南</title>
      <link>https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0openai-function-calling%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/</link>
      <pubDate>Sun, 28 Apr 2024 15:00:00 +0800</pubDate>
      <guid>https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0openai-function-calling%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/</guid>
      <description>写在最开始 当我们在讨论基于大型语言模型（LLM-based）的智能代理（agent）时，我们究竟在谈论什么？根据Lilian W在其文章《LLM Powered Autonomous Agents》中的讨论，一个智能代理需要具备几个核心能力：规划（Planning）、记忆（Memory）、以及工具使用（Tool use）。特别地，工具使用方面的进展，得益于OpenAI在API中提供的function calling功能，为我们开启了新的可能性。
OpenAI function calling，作为智能代理与外部工具交互的基本方式，对于每位从业者来说都是必备技能。随着技术的发展，我们期望的不只是能与我们对话的LLM，而是能够辅助我们使用各种工具、做出决策的智能伙伴。
不过需要特别指出的是，最近OpenAI在Chat Completions API中已经废弃了“函数（function）”的使用，转而采用“工具（tool）”。这一变更旨在拓宽LLM集成的功能范围，为更复杂的交互模式铺平道路，如构建能够相互作用的多代理系统。
尽管如此，由于语言习惯的原因，本文中仍然会使用function calling的术语来描述OpenAI的tool using功能，因为“function calling”的说法已经深入人心了。
核心内容概览
Function Calling的定义：解释什么是function calling，以及它在智能代理工作中的作用。
OpenAI Cookbook示例：提供实际的function calling示例，帮助读者理解其在实际应用中的用途。
开源LLM的Tool Using：探索如何在开源大型语言模型中实现工具使用，以及LLM在tool using的时候经历了什么。
评价与训练：讨论如何评价开源模型的工具使用能力，以及如何训练LLM进行有效的工具使用。
鉴于整理笔记的速度远赶不上更新的速度， 会将第四部份作为单独的部分整理。
何为function calling 一句话解释：function calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。
具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。
function calling的应用范围广泛，如
创建智能助手：通过调用外部API回答问题。
转换指令：将自然语言指令转换成API调用指令。
数据提取：从文本中提取结构化数据。
function calling的过程涵盖了从定义函数集、通过模型生成遵循自定义模式的JSON对象字符串，到在代码中解析这个字符串并调用相应函数的全过程。这一连串操作不仅自动化了交互过程，还确保了执行操作的安全性和准确性。
一些常见的问题 JSON mode json mode 和tool-using 有什么关系？有了json mode 还需要用到tool-using吗？
从json mode 的本质， 更多的是在system prompt 增加一句类似“请以json格式输出”之类的话，然后在LLM输出时增加json结果检查和格式转换。在使用时只需要在client.chat.completions.create 中 增加response_format={ &amp;quot;type&amp;quot;: &amp;quot;json_object&amp;quot; } 即可。
那么json mode 什么时候会用到呢？一般在做文本提取，内容提取时可以使用；以RAG场景为例， 当我们希望LLM能够帮我们对用户的query进行改写时，我们肯定是希望模型能够返回干净的json格式改写结果，这样的结果可以直接使用，而不是在模型输出一些内容后，如：</description>
    </item>
  </channel>
</rss>
