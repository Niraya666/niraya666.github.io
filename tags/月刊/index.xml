<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>月刊 on LZY Blog</title>
    <link>https://niraya666.github.io/tags/%E6%9C%88%E5%88%8A/</link>
    <description>Recent content in 月刊 on LZY Blog</description>
    <image>
      <title>LZY Blog</title>
      <url>https://niraya666.github.io/images/papermod-cover.png</url>
      <link>https://niraya666.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 29 Mar 2025 17:00:00 +0800</lastBuildDate>
    <atom:link href="https://niraya666.github.io/tags/%E6%9C%88%E5%88%8A/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2025-03 月刊</title>
      <link>https://niraya666.github.io/monthly/2025-03-%E6%9C%88%E5%88%8A/</link>
      <pubDate>Sat, 29 Mar 2025 17:00:00 +0800</pubDate>
      <guid>https://niraya666.github.io/monthly/2025-03-%E6%9C%88%E5%88%8A/</guid>
      <description>&lt;h1 id=&#34;值得关注的模型和新技术&#34;&gt;值得关注的模型和新技术&lt;/h1&gt;
&lt;h2 id=&#34;deepseek-v3-0324&#34;&gt;DeepSeek V3 0324&lt;/h2&gt;
&lt;p&gt;DeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-V3&#34;&gt;Huggingface&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;qwq-32b&#34;&gt;QwQ-32B&lt;/h2&gt;
&lt;p&gt;由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://qwenlm.github.io/zh/blog/qwq-32b/&#34;&gt;Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Qwen/QwQ-32B&#34;&gt;Huggingface&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;qwen25-omni&#34;&gt;Qwen2.5 Omni&lt;/h2&gt;
&lt;p&gt;Qwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://qwenlm.github.io/zh/blog/qwen2.5-omni/&#34;&gt;Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Omni-7B&#34;&gt;Huggingface&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;gemma-3&#34;&gt;Gemma 3&lt;/h2&gt;
&lt;p&gt;Gemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.google/technology/developers/gemma-3/&#34;&gt;Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/google/gemma-3-27b-it&#34;&gt;Huggingface&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;phi-4-multimodal&#34;&gt;Phi-4-multimodal&lt;/h2&gt;
&lt;p&gt;具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/&#34;&gt;Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/microsoft/Phi-4-multimodal-instruct&#34;&gt;Huggingface&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;qwen25-vl-32b-instruct&#34;&gt;Qwen2.5-VL-32B-Instruct&lt;/h2&gt;
&lt;p&gt;32B参数量版本的Qwen2.5-VL多模态模型；&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;72B too big for VLM? 7B not strong enough! Teh you should use 32B model!&lt;/p&gt;</description>
    </item>
    <item>
      <title>2025-02 月刊</title>
      <link>https://niraya666.github.io/monthly/2025-02-%E6%9C%88%E5%88%8A/</link>
      <pubDate>Fri, 28 Feb 2025 11:00:00 +0800</pubDate>
      <guid>https://niraya666.github.io/monthly/2025-02-%E6%9C%88%E5%88%8A/</guid>
      <description>&lt;h1 id=&#34;值得关注的模型和新技术&#34;&gt;值得关注的模型和新技术&lt;/h1&gt;
&lt;h2 id=&#34;o3-mini&#34;&gt;o3-mini&lt;/h2&gt;
&lt;p&gt;OpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://openai.com/index/openai-o3-mini/&#34;&gt;OpenAI o3-mini&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;qwq-max-preview&#34;&gt;QwQ-Max-Preview&lt;/h2&gt;
&lt;p&gt;QwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;官方blog：&lt;a href=&#34;https://qwenlm.github.io/zh/blog/qwq-max-preview/&#34;&gt;&lt;think&gt;&amp;hellip;&lt;/think&gt; QwQ-Max-Preview&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;claude-37-sonnet&#34;&gt;Claude 3.7 Sonnet&lt;/h2&gt;
&lt;p&gt;Claude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.anthropic.com/news/claude-3-7-sonnet?utm_source=partner-aws&amp;utm_medium=referral&amp;utm_campaign=sonnet_3-7_launch&#34;&gt;Claude 3.7 Sonnet and Claude Code&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;grok-3&#34;&gt;Grok-3&lt;/h2&gt;
&lt;p&gt;Grok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、&amp;ldquo;Think&amp;quot;模式分步推理，以及&amp;quot;Big Brain&amp;quot;模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium+订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。&lt;/p&gt;</description>
    </item>
    <item>
      <title>2025-01 月刊</title>
      <link>https://niraya666.github.io/monthly/2025-01-%E6%9C%88%E5%88%8A/</link>
      <pubDate>Mon, 27 Jan 2025 18:00:00 +0800</pubDate>
      <guid>https://niraya666.github.io/monthly/2025-01-%E6%9C%88%E5%88%8A/</guid>
      <description>&lt;h1 id=&#34;值得关注的新模型&#34;&gt;值得关注的新模型&lt;/h1&gt;
&lt;h2 id=&#34;deepseek-r1&#34;&gt;Deepseek R1&lt;/h2&gt;
&lt;p&gt;DeepSeek R1通过&lt;strong&gt;纯强化学习（RL）框架&lt;/strong&gt;实现了推理能力的突破，首次验证了无需依赖传统监督微调（SFT）或蒙特卡洛树搜索（MCTS）等复杂方法，仅通过两阶段RL优化即可显著提升模型逻辑推理性能。其综合能力直接对标OpenAI的o1模型，在数学（MATH-500达97.3%）、代码生成等核心指标上实现部分超越，同时&lt;strong&gt;全面开源模型权重、训练技术文档及6个蒸馏版本（1.5B-70B）&lt;/strong&gt;，使开发者可灵活适配不同算力场景。尤为引人注目的是，该模型在训练中展现出&lt;strong&gt;自发反思与多步骤验证能力&lt;/strong&gt;，研究者观察到其通过内部反馈机制主动修正推理路径的“顿悟时刻”，揭示了AI系统自我优化的新可能。此外，其研发成本仅为560万美元（基于2048块H800 GPU），相比同类模型降低1-2个数量级。&lt;/p&gt;
&lt;p&gt;技术报告: &lt;a href=&#34;https://arxiv.org/abs/2501.12948&#34;&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Huggingface: &lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-R1&#34;&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一些基于和关于R1的开源复刻工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open-R1: &lt;a href=&#34;https://huggingface.co/blog/open-r1&#34;&gt;&lt;strong&gt;Open-R1: a fully open reproduction of DeepSeek-R1&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RAGEN: &lt;a href=&#34;https://github.com/ZihanWang314/ragen/tree/main&#34;&gt;&lt;strong&gt;RAGEN: A General-Purpose Reasoning Agent Training Framework&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kimi-k1&#34;&gt;Kimi K1&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Kimi K1的核心创新在于通过强化学习驱动的多模态架构，首次实现端到端的视觉与推理深度融合。&lt;/strong&gt; 该模型突破传统分阶段处理模式，直接将图像输入与逻辑推演结合，支持模糊图像解析、手写题识别等复杂场景，并引入反思机制修正推理错误；其两阶段训练框架（预训练+强化学习规模化优化）显著提升思维链生成质量，使模型在数学、物理、化学等跨学科测试中超越国际主流视觉模型（如OpenAI o1）。&lt;/p&gt;
&lt;p&gt;Arxiv: &lt;a href=&#34;https://arxiv.org/abs/2501.12599&#34;&gt;Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;qwen25-max&#34;&gt;Qwen2.5-Max&lt;/h2&gt;
&lt;p&gt;Qwen2.5-Max 是一款经过大规模预训练的专家混合（MoE）模型，训练数据量超过 20 万亿 tokens，并通过监督微调（SFT）和人类反馈强化学习（RLHF）进行优化。该模型在多个基准测试中表现优异，包括在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等评估中超越了 DeepSeek V3，并在 MMLU-Pro 大学水平知识测试中展现出竞争力，同时在与其他领先模型（如 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的对比中也表现突出。Qwen2.5-Max 已集成到 Qwen Chat，支持对话交互和功能体验，同时其 API（模型名称为 qwen-max-2025-01-25）已上线，用户可通过注册阿里云账户并激活相关服务进行调用。&lt;/p&gt;
&lt;p&gt;Blog: &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-max/&#34;&gt;https://qwenlm.github.io/blog/qwen2.5-max/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;qwen25-vl&#34;&gt;Qwen2.5-VL&lt;/h2&gt;
&lt;p&gt;Qwen2.5-VL是由阿里云通义千问团队于2025年1月28日推出的新一代视觉语言模型，专注于提升多模态处理与视觉理解能力。该模型具备五大核心功能：通过增强的视觉理解可识别常见物体并解析图像中的文本、图表及布局；作为视觉代理支持跨设备多步骤操作（如电脑修图、手机订票）；可精准定位超过1小时长视频的特定事件片段；通过生成边界框/点实现视觉定位并输出标准化JSON坐标；针对发票/表格等扫描件提供结构化数据输出，赋能金融商业场景。其创新架构采用动态FPS采样与时间维度mRoPE对齐技术，实现视频动态分辨率处理与时间序列学习，同时通过窗口注意力机制和SwiGLU-RMSNorm优化ViT编码器，保持与Qwen2.5大语言模型架构统一。模型提供3B、7B、72B三种参数规模。&lt;/p&gt;
&lt;p&gt;Blog: &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-vl/&#34;&gt;https://qwenlm.github.io/blog/qwen2.5-vl/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Huggingface: &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&#34;&gt;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;qwen25-1m&#34;&gt;Qwen2.5-1M&lt;/h2&gt;
&lt;p&gt;Qwen2.5-1M系列模型是由Qwen团队于2025年1月27日发布的开源大语言模型，包含Qwen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M两个版本，其核心突破在于支持高达100万Token的上下文处理能力。该模型通过三阶段技术实现长上下文支持：首先在预训练阶段采用Adjusted Base Frequency方法将RoPE基础频率提升至1000万，通过渐进式扩展策略将上下文从4K扩展到256K；随后在监督微调阶段采用短指令（32K）与长指令（256K）混合训练策略，平衡长短任务性能；最终通过创新性Dual Chunk Attention技术将上下文扩展至百万量级，该技术通过重映射超大相对位置值解决位置编码外推难题。为应对超长序列处理挑战，模型引入分块预填充技术（32K分块）降低显存消耗，结合稀疏注意力优化使百万级序列的精度损失显著降低，并通过算子效率优化和动态分块流水线并行实现3.2-6.7倍的预填充加速。在性能表现上，该系列模型不仅能在百万Token的&amp;quot;大海捞针&amp;quot;任务中精准检索信息，在RULER、LV-Eval等复杂长文本理解任务中超越前代128K版本，同时在短文本任务上保持与128K版本相当的性能，其中14B版本在保持GPT-4o-mini八倍上下文长度的前提下实现了相近的短文本处理能力。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
