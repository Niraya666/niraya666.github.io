<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM on LZY Blog</title>
    <link>https://niraya666.github.io/tags/llm/</link>
    <description>Recent content in LLM on LZY Blog</description>
    <image>
      <title>LZY Blog</title>
      <url>https://niraya666.github.io/images/papermod-cover.png</url>
      <link>https://niraya666.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 21 Aug 2024 14:49:00 +0800</lastBuildDate>
    <atom:link href="https://niraya666.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM 输出限制：Structured Outputs、受限编码和提示词工程</title>
      <link>https://niraya666.github.io/posts/llm-%E8%BE%93%E5%87%BA%E9%99%90%E5%88%B6structured-outputs%E5%8F%97%E9%99%90%E7%BC%96%E7%A0%81%E5%92%8C%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B/</link>
      <pubDate>Wed, 21 Aug 2024 14:49:00 +0800</pubDate>
      <guid>https://niraya666.github.io/posts/llm-%E8%BE%93%E5%87%BA%E9%99%90%E5%88%B6structured-outputs%E5%8F%97%E9%99%90%E7%BC%96%E7%A0%81%E5%92%8C%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B/</guid>
      <description>在使用大型语言模型（LLM）时，我们常常面临一个挑战：如何从模型输出中准确提取自己所需的信息。例如，当我们希望模型输出 JSON 格式的数据时，由于模型生成的内容并不总是稳定，可能需要额外编写大量的正则表达式来匹配并提取其中的有效信息。然而，由于 LLM 的能力，导致其输出结构并不永远可靠。
现阶段， 让LLM按要求生成特定格式文本的主要方法有几种种：
微调：使模型的输出遵循特定格式
OpenAI Json-mode/Structured Outputs/function-calling: 这些功能允许模型生成更严格、结构化的输出，但受限于openAI平台。
格式约束：在decoding阶段进行约束，限制模型的输出，
Prompt Engineering： 最简单的办法，但不稳定。
多阶段prompting： 通过多个步骤的提示逐步引导模型生成所需的格式。
本文将聚焦在Structured Outputs， 受限编码， 和prompt-engineering的角度，探讨它们在生成特定格式文本中的应用和效果。
Json Mode 仅特定模型和平台支持
以openAI 为例， 在openai.chat.completions.create 参数中增加response_format={&amp;quot;type&amp;quot;:&amp;quot;json_object&amp;quot;} 即可（具体参见：response_format ）。
需要在prompt中要求输出json格式
不能保证完全按要求的格式结构输出
但非100%成功率，存在一些需要额外检测和适当处理的edge case。
Handling edge cases 根据OpenAI官方文档提供的处理方案 https://platform.openai.com/docs/guides/structured-outputs/json-mode we_did_not_specify_stop_tokens = True try: response = client.chat.completions.create( model=&amp;#34;gpt-3.5-turbo-0125&amp;#34;, messages=[ {&amp;#34;role&amp;#34;: &amp;#34;system&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;You are a helpful assistant designed to output JSON.&amp;#34;}, {&amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;Who won the world series in 2020?</description>
    </item>
  </channel>
</rss>
