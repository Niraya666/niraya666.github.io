<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RAG-Toolkits | LZY Blog</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://niraya666.github.io/tags/rag-toolkits/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://niraya666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://niraya666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://niraya666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://niraya666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://niraya666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://niraya666.github.io/tags/rag-toolkits/index.xml">
<link rel="alternate" hreflang="en" href="https://niraya666.github.io/tags/rag-toolkits/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="RAG-Toolkits" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://niraya666.github.io/tags/rag-toolkits/" />
<meta property="og:image" content="https://niraya666.github.io/images/papermod-cover.png" />


<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://niraya666.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="RAG-Toolkits"/>
<meta name="twitter:description" content=""/>

</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niraya666.github.io/" accesskey="h" title="LZY Blog (Alt + H)">LZY Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niraya666.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/posts/" title="AI">
                    <span>AI</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/musik/" title="Musik!">
                    <span>Musik!</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/monthly/" title="月刊">
                    <span>月刊</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/essay/" title="杂文">
                    <span>杂文</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/travel/" title="游记">
                    <span>游记</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/travel-map/" title="足迹">
                    <span>足迹</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header"><div class="breadcrumbs"><a href="https://niraya666.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://niraya666.github.io/tags/">Tags</a></div>
  <h1>
    RAG-Toolkits
    <a href="/tags/rag-toolkits/index.xml" title="RSS" aria-label="RSS">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
        stroke-linecap="round" stroke-linejoin="round" height="23">
        <path d="M4 11a9 9 0 0 1 9 9" />
        <path d="M4 4a16 16 0 0 1 16 16" />
        <circle cx="5" cy="19" r="1" />
      </svg>
    </a>
  </h1>
</header>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：有了Copali系列模型，我们还需要OCR吗？
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p> 现阶段的RAG是个缝合怪。
写在开头 如果回顾RAG那短暂的历史，其最初主要是为了解决模型知识的动态更新和降低模型幻觉而设计的，核心假设是知识存在于离散的、纯文本的段落中。然而，随着多模态大模型能力的提升，依然沿用着这种“先解析、再分块、后嵌入”的旧思想来处理蕴含丰富视觉信息的文档，这套当初的设计放在现在就显得有些“刻舟求剑”，甚至成了一种历史包袱。这便引出了我们今天要探讨的核心问题：为了让模型“读懂”图文并茂的复杂文档，我们真的需要如此繁琐的预处理吗？
现阶段RAG中存在的问题 先细数一下当前主流RAG架构中，所存在的普遍问题：
依赖专门的OCR模型，layout分析模型对文档进行信息提取：一个典型的PDF文档处理流程可能需要 layout分析模型，OCR模型，表格识别模型等。多模型则增加了系统的复杂度和延迟，而且每一步都存在一定的信息损失。
不同文件格式需要不同的处理方式：PDF、Word、Markdown、PPTX… 每一种文件类型都需要一套不同的解析策略。这导致解析系统变得越来越臃肿，维护成本上升。
图像信息的丢失：传统的RAG几乎是纯文本的。当遇到图像、复杂表格、流程图时，信息丢失是必然的。即便引入多模态模型做Image-Captioning，将图像转译成一段描述性文字，其信息的精确度和完整度也远远无法与原始图像相比。
需要针对性调整分块策略和索引设计：往往需要为不同类型的文档、不同类型的场景，设计分块策略和索引结构。是按固定大小分块，还是按章节分块？对于一篇论文和一份产品手册，最佳策略显然不同。这种高度定制化的设计，使得RAG系统难以扩展和迁移，架构过于复杂。
在前作中，曾展望过VLM在RAG领域的巨大潜力，特别是构建一个End-to-End的RAG系统。这个设想的目标，正是为了彻底推翻上述复杂的“预处理”，让信息处理回归简洁和高效。
为什么现阶段End-to-End的RAG是可行的？ 端到端的RAG，其核心思想是“所见即所得”。不再费力地将文档“翻译”成模型能理解的纯文本格式，而是直接将文档的“视觉形态”——即文档页面图像，直接输入给VLM。
为什么这个方案是可行的？
VLM的VQA能力强于OCR：必须承认，在复杂文档的理解上，VLM的能力强于其OCR能力的。与其在OCR这条路上“象牙里雕花”，将复杂文档转换成纯文本形式， 不如直接将文档页面视为一张图片，利用VLM的VQA能力直接在图像上进行信息定位和回答，这不仅跳过了中间繁琐的步骤，更从根本上避免了信息在多次转换中的损失。
Late Interaction模型的出现：以ColBERT为代表的Late Interaction模型，提供全新的思路。传统的向量检索是将整个文本块压缩成一个单一的向量，这无疑会丢失大量细节。而ColBERT则是为文本块中的每个Token都生成一个向量。检索时，则通过MaxSim操作计算相关性分数。 而这个思想可以迁移到视觉领域：将图像中的每个Patch作为向量化的基本单位，而不是对整张图片构建索引（CLIP）。 同时文档的每一页天然就是一个分块后VLM的输入单元，省去了设计Chunking策略的步骤。
模型能力和基础设施的成熟：主流的VLM已经支持多张高分辨率图片作为输入，并且拥有足够长的上下文窗口来处理整个文档。主流的LLM推理框架也支持了多图像输入，这也为端到端的文档级理解提供了基础。
Copali系列模型 Contextualized Late Interaction Over PaliGemma （Copali）便是该系列的开山之作。其核心思想是ColBERT在图像数据上的扩展。（ColBERT相关内容可参考前作）
选用 PaliGemma-3B 作为基础模型（以SigLIP作为vision-encoder，通过一个线性层将图像patch映射至LLM，即Gemma-2B的嵌入空间），类似ColBERT，在LLM的输出后再增加一个投影层，从而实现多向量表示。
文档侧（构建索引时）：将一张文档页面图像输入到ColPali中。直接通过视觉编码器将其转换为一系列图像patch（1024），这些嵌入随后被送入LLM，最终通过一个投影层，为每个图像块生成一个128维向量。这样，一页文档就被表示为一个bag-of-embeddings。
查询侧（检索）：用户输入的文本查询（query）被送入模型的语言模型部分，同样为查询中的每个token生成一个向量
在检索时，使用类似ColBERT的MaxSim操作来计算查询与文档页面的相关性分数
训练阶段，在vidore/colpali_train_set 118K个query-页面对上做对比学习获得。
同时为了系统性地评估富视觉文档的检索能力，作者创建了一个名为ViDoRe (Visual Document Retrieval Benchmark) 的综合性评测基准。
除了以PaliGemma 为based 的模型外，还有将基础模型换成Qwen2-VL 和Qwen2.5-VL 的ColQwen2模型
Model Score on ViDoRe 🏆 License Comments vidore/colpali 81.3 Gemma • Based on google/paligemma-3b-mix-448.
• Checkpoint used in the ColPali paper. vidore/colpali-v1.1 81.5 Gemma • Based on google/paligemma-3b-mix-448.
• Fix right padding for queries. vidore/colpali-v1.2 83.9 Gemma • Similar to vidore/colpali-v1.1. vidore/colpali-v1.3 84.8 Gemma • Similar to vidore/colpali-v1.2.
• Trained with a larger effective batch size of 256 batch size for 3 epochs. vidore/colqwen2-v0.1 87.3 Apache 2.0 • Based on Qwen/Qwen2-VL-2B-Instruct.
• Supports dynamic resolution.
• Trained using 768 image patches per page and an effective batch size of 32. vidore/colqwen2-v1.0 89.3 Apache 2.0 • Similar to vidore/colqwen2-v0.1, but trained with more powerful GPUs and with a larger effective batch size (256). vidore/colqwen2.5-v0.1 88.8 Apache 2.0 • Based on Qwen/Qwen2 5-VL-3B-Instruct
• Supports dynamic resolution.
• Trained using 768 image patches per page and an effective batch size of 32. vidore/colqwen2.5-v0.2 89.4 Apache 2.0 • Similar to vidore/colqwen2.5-v0.1, but trained with slightly different hyper parameters vidore/colSmol-256M 80.1 Apache 2.0 • Based on HuggingFaceTB/SmolVLM-256M-Instruct. vidore/colSmol-500M 82.3 Apache 2.0 • Based on HuggingFaceTB/SmolVLM-500M-Instruct. （From colpali）
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-09-22 18:56:00 +0800 CST'>September 22, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：有了Copali系列模型，我们还需要OCR吗？" href="https://niraya666.github.io/posts/rag-toolkits-copali/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：ColBERT
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p> 前一段时间跑比赛时，苦于上分陷入瓶颈，于是便祭出了ML经典ensemble大法，其中一个分支便是选择的ColBERT，没想到，竟成了破局的关键；也由此补齐了我对“迟交互”类模型的认知。
在典型的检索系统中，我们常用的架构是 Retrieval &#43; Reranking两阶段:
检索阶段使用轻量级的方法（如BM25或dense vector retriever）从海量语料中快速筛选出几百个候选文档，优先考虑速度和召回率；排序阶段则使用cross-encoder基于 query-document 全量交互，重新给候选集打分，得到精准的最终排序。
这么做虽运行良好，但存在瓶颈：第一阶段检索质量的限制意味着大量低质量候选被送入昂贵的第二阶段，而cross-encoder在大规模候选集上的计算成本奇高， 高相关的文档如果没能被第一阶段召回，就无法被 reranker “捞回来”。
而multi-vector模型正好填补了这个检索流水线中的“空白中间层”——比 embedding 更精准，比 cross-encoder 更高效。相比单向量 embedding 模型将整个chunk压缩为一个向量，multi-vector模型保留了 **token 级别的表示，**可以捕捉到局部与 query 高度匹配的片段，也是能缓解“语义鸿沟”的原因。
而ColBERT 便是经典的多向量模型/late-interaction代表。
（可能在本文中会将ColBERT/late-interaction/multi-vector-model 混为一谈，毕竟只是实例-机制-类别 的关系 ）
不同的“交互”层级 （借用ColBERT论文中的插图）
想要理解late-interaction，需要先理解过去的不同“交互”形式；
Representation-based Similarity （no-interaction，embedding） 也就是双塔， query和document独立计算出一个向量，比较两个向量的相似度获得分数S；在生成各自的向量前， Query 和 Document 之间没有任何信息交互，所有复杂的语义都被压缩进了一个单一的向量中（也就存在信息压缩的损失和瓶颈）
优点速度极快，但缺点也很明显，精度有限，所有信息被压缩到一个向量中。
All-to-all Interaction （cross-encoder） 目前主流基于BERT 实现的Reranker / Cross-Encoder
将查询和文档拼接成 [CLS] query_tokens [SEP] document_tokens [SEP]
输入到 BERT 模型中，[CLS] 特殊标记最终输出向量，将向量输入一个linear layer中， 从而获得获得一个0～1间的标量值。
因为是在入口处做的交互，于是也被称为“Early, Deep, All-to-all Interaction”
目前而言效果比较好的排序方案，代价是计算速度慢
Late Interaction （ColBERT） 查询和文档是独立、并行地通过 BERT 编码器进行编码，生成各自的token级别向量表示
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-08-12 20:56:00 +0800 CST'>August 12, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：ColBERT" href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1colbert/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：基于多模态大模型的文档解析方案（2025版）
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p>Updated on 2025-03-29: Add SmolDocling &amp; VLM Summary
技术迭代速度之快令人惊叹，前作 RAG工具箱：文档解析与表格处理 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。
本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。
过去的技术栈总结 在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决&#34;garbage in, garbage out&#34;的输入质量问题，后续处理环节将难以发挥应有价值。
传统文档解析技术长期受限于以下核心痛点：
结构化信息缺失：无法准确识别文档标题、副标题等层级结构
特殊内容处理薄弱：数学公式、专业符号解析准确率低下
复杂表格解析困境：跨页表格、合并单元格等场景支持不足
图像信息提取瓶颈：扫描文档、手写体识别效果欠佳
版式适应性问题：多栏布局、影印版本等文档格式兼容性差
从技术角度，过去文档解析的底层逻辑和框架：
纯文本解析: PyPDF, PyMuPDF只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力
OCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；
基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；
随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。
Benchmark 为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。
现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench
OCRBench、OCRBench-V2
OmniDocBench
CC-OCR
…and more
（关于benchmark的具体内容见附录）
这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选
当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。
不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。
根据benchmark和实际测试结果，目前几个值得关注的开源VLM：
Qwen2.5-VL
Phi-4-multimodal
Llama 3.2 Vision
olmocr
and more …
Qwen2.5-VL系列模型 cookbook
Blog
Technical Report
这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。
在Technical Report 中一些和document-parse有关的内容：
在第一阶段视觉预训练中（仅训练ViT），针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p&gt;）、表格（&lt;table&gt;）、图表（&lt;div class=&#34;chart&#34;&gt;）、公式（&lt;div class=&#34;formula&#34;&gt;）、图像标注（&lt;div class=&#34;image caption&#34;&gt;）、OCR文本（&lt;div class=&#34;image ocr&#34;&gt;）、乐谱（&lt;div class=&#34;music sheet&#34;&gt;）、化学式（&lt;div class=&#34;chemical formula&#34;&gt;）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-12 16:44:00 +0800 CST'>March 12, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：基于多模态大模型的文档解析方案（2025版）" href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：RAG Tutorial for Beginner
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p>一些适合入门的RAG材料
原理、科普 Retrieval Augmented Generation (RAG) for LLMs
Langchain|Retrieval augmented generation (RAG)
A Practitioners Guide to Retrieval Augmented Generation (RAG)
RAG for Beginners: The Complete Guide to Retrieval Augmented Generation
What is Retrieval Augmented Generation (RAG)?
Retrieval augmented generation (RAG)
思考题：
Why RAG？ RAG解决了什么问题？RAG和SFT如何选择？RAG的优势？
什么样的问题是RAG无法解决的？什么样的数据适合使用RAG
为什么要做chunking？chunk-size受那些因素制约？
什么是embedding？向量库在做什么？一定要做语义匹配吗？什么是reranking？
如何evaluate 检索效果的好坏 ？
RAG中，LLMs起到的作用？
Hands-On 前置任务 获取模型服务：
获得LLMs供应商API： 推荐 OpenRouter（仅LLMs，需要梯子，境外信用卡或Crypto），SilconFlow（LLMs&#43;embedding,无需梯子,有送token），Groq（仅LLMs, 需梯子，速度快，免费）
或采用本地部署： 推荐Ollama
例子,采用openRouter API实现LLM对话：
from openai import OpenAI client = OpenAI( base_url=&#34;https://openrouter.ai/api/v1&#34;, api_key=&#34;&lt;OPENROUTER_API_KEY&gt;&#34;, ) completion = client.chat.completions.create( model=&#34;openai/gpt-4o&#34;, messages=[ { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the meaning of life?&#34; } ] ) print(completion.choices[0].message.content) 例子：基于本地ollama启动的deepseek R1（蒸馏版本）对话
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-11 19:24:00 +0800 CST'>March 11, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：RAG Tutorial for Beginner" href="https://niraya666.github.io/posts/rag-tutorial-for-beginner/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：Query Enhancement
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p>引言 首先，从最简单的RAG（Retrieval-Augmented Generation）谈起。
RAG结合了信息检索与生成模型，先通过用户的原始问题query 在知识库中检索与相关的文档，再利用生成模型（如ChatGPT）生成回答。最简单的RAG版本通常被称为Naive-RAG，虽然Naive-RAG能够处理一些简单的问题，但在真实场景下，用户提出的问题往往更加复杂多样，远远超出系统最初设计的预期范围。
在用户与RAG系统交互时，往往会遇到以下几种常见问题：
表达模糊不清：用户在描述问题时，往往难以精准表达自己的需求。他们可能仅使用几个词组或片段，导致系统难以理解用户的真实意图。
依赖上下文：用户的问题经常依赖之前的对话或背景信息，而系统只接收到当前的孤立问题，缺少关键的上下文支持，无法给出准确的答案。
复杂多层次问题：某些问题本身复杂，包含多个子问题或需要推理、逻辑分析。Naive-RAG依赖简单的关键词匹配或语义匹配，无法很好地处理这类复杂的需求。
面对这些挑战，Naive-RAG的局限性变得明显：仅仅依靠用户的原始query无法应对多样化的提问场景。那么，如何提升RAG系统的检索能力，增强对复杂问题的处理呢？这时就需要引入Query-Enhancement技术。
Query-Enhancement，顾名思义，是对用户的原始query进行增强处理。它的核心思想是：不要直接使用用户的原始query进行相关文档的检索，而是通过某种技术手段对query进行改写、扩展或优化，从而生成一个更适合检索的query。这种技术在不同场景下有不同的命名，如query rewrite或query reformulation，但核心目标都是一致的——根据用户的初始输入，生成一个更符合检索需求、更能提升查询准确性的新query。
通过Query-Enhancement，可以解决许多Naive-RAG无法处理的问题。例如，针对模糊或简短的提问，增强后的query可以通过添加上下文或关键词来丰富信息内容；对于复杂的多层次问题，增强后的query可以拆解为多个子问题，逐步进行检索与推理。
本文将深入探讨不同的Query-Enhancement技术方案，并分析这些技术如何根据原始query生成更高效的查询，帮助RAG系统在复杂场景下提供更精确的答案。
Query Rewrite Because the original query can not be always optimal to retrieve for the LLM, especially in the real world… we first prompt an LLM to rewrite the queries, then conduct retrieval-augmented reading.
Query Rewrite的目的是将用户模糊、不明确或不完整的查询转换为更适合检索的形式，以提升检索效率和准确性。
仅使用原始query的缺点：
查询常常模糊、不具体或不完整，导致难以找到有效的检索结果。
缺乏明确术语，无法有效从大型数据集中提取相关信息。
对话上下文中的查询含义因对话历史而异，同样的表达在不同的上下文中可能有不同的意图。
输入文本与真正需要检索的知识之间不可避免地存在差距。这种差距限制了检索性能，增加了对检索能力增强和提示工程的依赖。
为了解决这些问题，检索器之前增加了一步对输入进行改写的过程，填补给定输入与检索需求之间的差距。这一步利用了LLM本身的能力对原始查询进行有针对性的改写。
比如，来自RAG_Techniques 的这段prompt：
query_rewrite_template = &#34;&#34;&#34;You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information. Original query: {original_query} Rewritten query:&#34;&#34;&#34; 使LLM可以更加系统地将用户的原始查询改写为更符合检索需求的形式。
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-10-31 16:04:00 +0800 CST'>October 31, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：Query Enhancement" href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1query-enhancement/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：检索
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p> If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.
— from Build a search engine, not a vector DB
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-07-25 17:08:00 +0800 CST'>July 25, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：检索" href="https://niraya666.github.io/posts/rag%E6%A3%80%E7%B4%A2/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：文本分块
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p>为什么要进行文本分块？ 大型语言模型（LLM）和嵌入式模型都有其处理文本的长度限制。为了有效处理超出这些限制的长文本，我们需要进行文本分块。
如何进行文本分块 **块分割戒律：**我们的目标不仅仅是为了划分数据块。更重要的是，我们要以一种便于日后检索和提取价值的格式来整理我们的数据。
文本分块的根本目的不仅仅是简单地将数据分割成块。更重要的是，我们要以便于检索和提取价值的方式来组织这些数据块。以下是几种常见的分块逻辑：
句子分割（Sentence Splitting） 这种方法只是简单地将文本切分成N个字符的片段，不考虑内容或形式的连贯性。
text = &#34;...&#34; # 你的文本 from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter( separator = &#34;\n\n&#34;, chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 递归字符文本分割（Recursive Character Text Splitting） 这种方法首先尝试用一个分隔符将文本切分成小块。如果初始尝试未能达到预期的块大小或结构，就递归地用不同的分隔符重新分割结果块，直到得到满意的结果。
text = &#34;...&#34; # 你的文本 from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 根据langchain 的默认分隔条件 [&#34;\n\n&#34;, &#34;\n&#34;, &#34; &#34;, &#34;&#34;] ,也就是会将text根据该分割条件的顺序（两个换行-&gt;一个换行-&gt;空格）将文本进行递归分割。
针对特定文档的分割方法（Document Specific Splitting） 例如针对Markdown或Python代码的分割策略，这些策略根据文档的结构来进行分割，如类定义或Markdown标题。
langchain 提供了一些常见文档的分割方法：
mardown的分割逻辑
[ # First, try to split along Markdown headings (starting with level 2) &#34;\n#{1,6} &#34;, # Note the alternative syntax for headings (below) is not handled here # Heading level 2 # --------------- # End of code block &#34;```\n&#34;, # Horizontal lines &#34;\n\\*\\*\\*&#43;\n&#34;, &#34;\n---&#43;\n&#34;, &#34;\n___&#43;\n&#34;, # Note that this splitter doesn&#39;t handle horizontal lines defined # by *three or more* of ***, ---, or ___, but this is not handled &#34;\n\n&#34;, &#34;\n&#34;, &#34; &#34;, &#34;&#34;, ] python的分割逻辑：
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-05-15 16:11:00 +0800 CST'>May 15, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：文本分块" href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%9C%AC%E5%88%86%E5%9D%97/"></a>
</article>

<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img loading="lazy" src="https://niraya666.github.io/img/post-cover/rag_toolkits_2.JPG" alt="RAG工具箱：文档解析与表格处理">
</figure>
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：文档解析与表格处理
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p> 引言 在信息化时代，数据和知识的快速提取变得尤为重要。特别是对于学术和技术文档，如何从格式丰富且结构复杂的PDF文件中准确地提取信息，是提高研究效率的关键。本文将探讨几种先进的技术方法，用于优化学术论文的PDF解析及其表格内容的处理。
对于文档解析，langchain 和 llama_index 提供的 document loader 能够支持多种文件类型，覆盖绝大多数文档格式的需求。但对于pdf解析而言还存在着一些挑战。
人类与机器的阅读差异 尽管PDF格式适合人类阅读，提供了优秀的视觉体验和格式保真，但它并不适合机器阅读。PDF文件通常包含复杂的布局和多样的内容元素，如多列布局、公式和表格，这些都给自动解析带来了挑战。
常见的PDF解析问题 使用传统的PDF解析库可能遇到多种问题：
多列布局导致的文本流读取错误。
公式和表格的解析效果差，难以正确提取信息。
解析过程中结构化信息（如标题和列表）的丢失。
影印版PDF的文本无法被标准OCR工具识别。
高级解析技术 根据unstractued提供的方案，文档解析可以大致分为两种方法：结构化解析和基于图像的解析。结构化解析侧重于从文档中提取文字和其它字符数据。而图像基解析则包括以下两种技术：
OCR技术：首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字。
基于Transformer的端到端解析：使用深度学习模型直接将图像转换为相应的文字内容。在这方面，Dount 和 Nougat 模型表现出色，尤其是 Nougat 能够将图像中的文章整理成Markdown或Latex格式，非常适合需要保留结构信息（如标题层级、公式和表格）的场景。
只可惜Nougat只针对英文的学术论文做了训练，但遇到其他语言如中文论文的效果就不是那么的好了。
必须强调一点的是，Markdown格式因其简洁性和易于解析的特点，被广泛认为是LLM（大型语言模型）友好的文档格式。Markdown通过明确的标记语法，帮助模型更好地理解文档结构和内容，从而提高信息提取的准确性和效率。特别在存在大量公式，表格的学术论文场景下，Markdown可能是更合适的格式选择。
快速上手：使用Nougat将pdf解析成适合LLM读取的markdown 依赖按照
!pip install -q pymupdf python-Levenshtein nltk !pip install -q git&#43;https://github.com/huggingface/transformers.git Load model and processor
from transformers import AutoProcessor, VisionEncoderDecoderModel import torch processor = AutoProcessor.from_pretrained(&#34;facebook/nougat-base&#34;) model = VisionEncoderDecoderModel.from_pretrained(&#34;facebook/nougat-base&#34;) device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34; model.to(device) 将pdf转成图像
from typing import Optional, List import io import fitz from pathlib import Path def rasterize_paper( pdf: Path, outpath: Optional[Path] = None, dpi: int = 96, return_pil=False, pages=None, ) -&gt; Optional[List[io.BytesIO]]: &#34;&#34;&#34; Rasterize a PDF file to PNG images. Args: pdf (Path): The path to the PDF file. outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None. dpi (int, optional): The output DPI. Defaults to 96. return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False. pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None. Returns: Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None. &#34;&#34;&#34; pillow_images = [] if outpath is None: return_pil = True try: if isinstance(pdf, (str, Path)): pdf = fitz.open(pdf) if pages is None: pages = range(len(pdf)) for i in pages: page_bytes: bytes = pdf[i].get_pixmap(dpi=dpi).pil_tobytes(format=&#34;PNG&#34;) if return_pil: pillow_images.append(io.BytesIO(page_bytes)) else: with (outpath / (&#34;%02d.png&#34; % (i &#43; 1))).open(&#34;wb&#34;) as f: f.write(page_bytes) except Exception: pass if return_pil: return pillow_images from transformers import StoppingCriteria, StoppingCriteriaList from collections import defaultdict class RunningVarTorch: def __init__(self, L=15, norm=False): self.values = None self.L = L self.norm = norm def push(self, x: torch.Tensor): assert x.dim() == 1 if self.values is None: self.values = x[:, None] elif self.values.shape[1] &lt; self.L: self.values = torch.cat((self.values, x[:, None]), 1) else: self.values = torch.cat((self.values[:, 1:], x[:, None]), 1) def variance(self): if self.values is None: return if self.norm: return torch.var(self.values, 1) / self.values.shape[1] else: return torch.var(self.values, 1) class StoppingCriteriaScores(StoppingCriteria): def __init__(self, threshold: float = 0.015, window_size: int = 200): super().__init__() self.threshold = threshold self.vars = RunningVarTorch(norm=True) self.varvars = RunningVarTorch(L=window_size) self.stop_inds = defaultdict(int) self.stopped = defaultdict(bool) self.size = 0 self.window_size = window_size @torch.no_grad() def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor): last_scores = scores[-1] self.vars.push(last_scores.max(1)[0].float().cpu()) self.varvars.push(self.vars.variance()) self.size &#43;= 1 if self.size &lt; self.window_size: return False varvar = self.varvars.variance() for b in range(len(last_scores)): if varvar[b] &lt; self.threshold: if self.stop_inds[b] &gt; 0 and not self.stopped[b]: self.stopped[b] = self.stop_inds[b] &gt;= self.size else: self.stop_inds[b] = int( min(max(self.size, 1) * 1.15 &#43; 150 &#43; self.window_size, 4095) ) else: self.stop_inds[b] = 0 self.stopped[b] = False return all(self.stopped.values()) and len(self.stopped) &gt; 0 将pdf转成markdown
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-04-20 17:00:00 +0800 CST'>April 20, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：文档解析与表格处理" href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">RAG工具箱：评估RAG系统的方法论
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h2>
  </header>
  <div class="entry-content">
    <p>写在最前面 在过去的几个月中，我主要致力于与RAG（检索增强生成模型）相关的研究和实验工作。通过深入地分析众多论文和项目代码，我发现虽然在RAG领域取得基本成就相对容易，但要实现出色的成果却异常困难。
此系列文章旨在分享我在RAG研究中的心得和挑战。正如我们从OpenAI的开发者日活动中看到的，即使是在这个领域的领头羊也在不断试验和努力以寻求突破。
在我们深入探索RAG系统提升的各种方法之前，有一个基本问题需要解决：我们如何准确地评估一个RAG系统的性能？这就像是在机器学习或深度学习任务中没有给定的测试集和评价标准，我们难以判断实验的成功与否。
因此，本系列的第一篇文章将聚焦于介绍RAG系统的评价方法、相关指标以及测试框架，为我们接下来的探索设定明确的标准和目标。
测试框架 以下是一些测试框架，为RAG系统评估提供了强大的支持。
TruLens TruLens提供了一个独特的视角来评估和跟踪大型语言模型（LLM）实验，通过一系列创新的功能和方法，帮助开发者和研究人员更深入地了解模型性能和行为。
TruLens的反馈功能（Feedback Functions）是其核心概念之一，提供了一种程序化的方法来评估应用的运行表现。这些函数从“可扩展性”和“有意义性”两个维度出发，考虑评估的范围，旨在为用户提供有价值的反馈，帮助他们理解和改进他们的LLM应用。
在RAG应用中，提供准确的上下文信息至关重要，以避免生成不真实的“幻觉”答案。TruLens采用了创新性的RAG三元组评估方法，专门针对RAG架构的每个环节进行幻觉风险评估，确保模型的每个部分都能有效地工作，从而减少误导信息的产生。
上下文相关性（Context Relevance） 上下文相关性是评估RAG应用的第一步，确保每一段检索到的上下文都与提出的查询紧密相关。TruLens利用序列化记录的结构来评估上下文的相关性，这是一个关键的步骤，确保模型在正确的信息上生成回答。
真实性（Groundedness） 在检索到的上下文信息的基础上，大型语言模型将生成答案。TruLens强调了独立验证每个回答的重要性，以确保它们基于可靠信息，并且能够在检索到的上下文中找到支持的证据。这一步骤是确保模型回答的真实性和可靠性的关键。
答案相关性（Answer Relevance） 最后，评估需要确保最终回答有效地解答了原始问题，这通过评估应用的最终回答与用户输入的相关性来实现。这一过程确保了模型的输出不仅是真实的，而且是对用户查询有用的。
TruLens还提出了“诚实、无害和有帮助”的评估原则（Honest, Harmless, and Helpful Evaluations），这些原则旨在确保LLM应用在提供帮助的同时，也是安全和可信的。
Ragas Ragas框架专为评估检索增强生成（RAG）系统而设计，定义了四个核心评估指标：上下文相关性（context_relevancy）、上下文回溯（context_recall）、忠实度（faithfulness）和答案相关性（answer_relevancy）。这些指标共同构成了Ragas的评分体系，提供了一个全面的方法来评价RAG系统的性能。
此外，Ragas巧妙地利用大语言模型（LLM）进行无参考评估，有效降低了评估成本。通过这种方法，Ragas能够提供一种既经济又有效的方式来评估RAG系统，尤其是在处理大量数据和复杂查询时。
其他测试框架
DeepEval
DeepEval How to Evaluate RAG Applications in CI/CD Pipelines with DeepEval
ARES
github: https://github.com/stanford-futuredata/ARES
Paper: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems
LangChain Evals
Llama Index Evals
UpTrain
数据 在评估数据集时，不必依赖人工标注的标准答案，而是通过底层的大语言模型 (LLM) 来进行评估。
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-04-08 11:00:00 +0800 CST'>April 8, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Theme PaperMod</footer>
  <a class="entry-link" aria-label="post link to RAG工具箱：评估RAG系统的方法论" href="https://niraya666.github.io/posts/rag_toolkit_eval/"></a>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2026 <a href="https://niraya666.github.io/">LZY Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
