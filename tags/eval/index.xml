<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Eval on LZY Blog</title>
    <link>https://niraya666.github.io/tags/eval/</link>
    <description>Recent content in Eval on LZY Blog</description>
    <image>
      <title>LZY Blog</title>
      <url>https://niraya666.github.io/images/papermod-cover.png</url>
      <link>https://niraya666.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 19 Jan 2026 21:08:00 +0800</lastBuildDate>
    <atom:link href="https://niraya666.github.io/tags/eval/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>如何构建可靠的 Agent 评估体系：读 Anthropic 技术博客有感</title>
      <link>https://niraya666.github.io/posts/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84-ai-agent-%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB-%E8%AF%BB-anthropic-%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E6%9C%89%E6%84%9F/</link>
      <pubDate>Mon, 19 Jan 2026 21:08:00 +0800</pubDate>
      <guid>https://niraya666.github.io/posts/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E5%8F%AF%E9%9D%A0%E7%9A%84-ai-agent-%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB-%E8%AF%BB-anthropic-%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E6%9C%89%E6%84%9F/</guid>
      <description>&lt;p&gt;读完 Anthropic 的一篇博客&lt;a href=&#34;https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents&#34;&gt; &lt;em&gt;Demystifying evals for AI agents&lt;/em&gt;&lt;/a&gt;，深有感触。&lt;/p&gt;
&lt;p&gt;结合文章内容和最近的工程实践，整理了一些关于构建 Agent 评估体系的笔记，和一些思考。&lt;/p&gt;
&lt;h2 id=&#34;为什么需要-evals&#34;&gt;为什么需要 Evals？&lt;/h2&gt;
&lt;p&gt;Evals 的本质不是为了打分，而是为了让开发可量化。&lt;/p&gt;
&lt;p&gt;在缺乏评估体系时，开发团队往往面临着极大的不确定性：无论是优化 Prompt 还是更换基座模型，都难以判断这些变更是真正解决了问题，还是在引入新问题的同时破坏了原有的能力。&lt;/p&gt;
&lt;p&gt;在早期的 Demo 阶段，依赖人工测试或许能快速验证想法。然而，Agent 系统不同于单次交互的对话模型，它包含多步骤推理和动态工具调用。随着系统复杂度的增加，手动测试的覆盖率和回归效率将难以为继，团队很容易陷入“由于缺乏前置检测，只能依赖线上反馈被动修复 Bug”的恶性循环。&lt;/p&gt;
&lt;p&gt;此外，Evals 也是产品与研发之间的通用语言。将模糊的主观感受，转化为准确率、响应延迟和 Token 成本等可度量的工程指标。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbd42e7b2f3e9bb5218142796d3ede4816588dec0-4584x2834.png&amp;amp;w=3840&amp;amp;q=75&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;目前的评估方式，依旧还是三大件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;人工评估&lt;/strong&gt;：最准确但最贵。通常用来作为Golden，用于校准自动评估的准确性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;代码基准 (Code-based)&lt;/strong&gt;：适用于有明确唯一答案的场景（如数学计算、代码运行）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-as-a-judge&lt;/strong&gt;：目前的主流。用更强的模型去评估其他模型的输出。&lt;strong&gt;关键点&lt;/strong&gt;：需要经过人类专家的校准。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;（注：关于评估的具体细节与最佳实践，推荐参考 Hugging Face  的 &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/spaces/OpenEvals/evaluation-guidebook#the-model-builder-perspective-am-i-building-a-strong-model&#34;&gt;The LLM Evaluation Guidebook&lt;/a&gt;&lt;/strong&gt; ）&lt;/p&gt;
&lt;h2 id=&#34;agent-eval-和llm-eval-的不同&#34;&gt;Agent Eval 和LLM Eval 的不同&lt;/h2&gt;
&lt;p&gt;相比于单纯的 RAG 或 LLM 评估，Agent 的评估复杂度要高得多。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从“测模型”到“测系统”&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;正如&lt;a href=&#34;https://epoch.ai/gradient-updates/why-benchmarking-is-hard&#34;&gt;Why benchmarking is hard&lt;/a&gt;，中提到的“许多变动的部件和自由度都会影响最终结果”。在 Agent 开发中，&lt;/p&gt;
&lt;p&gt;我们测试的永远不是“裸模型”，而是 &lt;strong&gt;“模型 + Harness”&lt;/strong&gt; 这个整体。&lt;/p&gt;
&lt;p&gt;这里的Harness 分成两大部分：Eval Harness（我们的评估系统本身）+ Agent Harness (Scaffold)。&lt;/p&gt;</description>
    </item>
    <item>
      <title>从下半场开始，对于评估的重新思考: 一些概念</title>
      <link>https://niraya666.github.io/posts/eval_1/</link>
      <pubDate>Wed, 11 Jun 2025 15:04:00 +0800</pubDate>
      <guid>https://niraya666.github.io/posts/eval_1/</guid>
      <description>&lt;h2 id=&#34;引子&#34;&gt;引子&lt;/h2&gt;
&lt;p&gt;前一段时间，OpenAI 研究员姚顺雨在一篇广受关注的文章《&lt;a href=&#34;https://ysymyth.github.io/The-Second-Half/&#34;&gt;The Second Half&lt;/a&gt;》中提出：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“I think we should fundamentally re-think evaluation. It means not just to create new and harder benchmarks, but to fundamentally question existing evaluation setups and create new ones, so that we are forced to invent new methods beyond the working recipe.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;指出，AI 发展的“下半场”已经到来，而其中一个核心议题，就是对 evaluation的重新思考。我们需要的不再是单纯地创造更难的 benchmark，然后在这些 benchmark 上“刷分”，而是要更加关注评测的实用性、创新性，以及其与真实世界的契合度。&lt;/p&gt;
&lt;p&gt;过去，我们往往过于关注模型本身、训练方法以及各种fancy的技术手段，却忽略了模型与真实世界的交互和落地应用。而 evaluation，正是连接模型能力与实际需求的关键环节。&lt;/p&gt;
&lt;p&gt;本文最初的出发点，是梳理 &lt;a href=&#34;https://github.com/huggingface/evaluation-guidebook/tree/main&#34;&gt;Hugging Face Evaluation &lt;/a&gt;系列文章中的一些要点。需要说明的是，HF 的文章主要聚焦于如何评测LLM的能力，考虑到其成文的时间，其中部分内容在当前 LLM 能力飞速提升的背景下，显得有些滞后。但这也为我们提供了一个契机，从后来者的视角重新思考 evaluation 的意义，并尝试将这些理念应用到更加复杂和多样化的 AI 系统中，如RAG和AI-agent。&lt;/p&gt;
&lt;p&gt;在机器学习和深度学习的流程中，Evaluation是衡量模型性能的核心环节。它贯穿于模型开发的始终：从训练阶段的实时监控，到上线前的最终验证，再到部署后的持续追踪。&lt;br&gt;
&lt;br&gt;
简单来说，评估的目标是回答两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型是否有效？&lt;/strong&gt;（性能指标）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模型是否可靠？&lt;/strong&gt;（鲁棒性、泛化能力）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以经典的猫狗分类任务为例，在训练前，我们可能将数据集划分为trainingset，evalset，和testset （经典的7:2:1）保证训练数据与测试数据无重叠；对于分类问题，可能选择Accuracy、Precision和Recall 作为metrics，在训练过程中监控模型在evalset上的情况（是否存在overfitting之类的问题）；训练结束后，在testset上验证模型的最终性能，判断是否达到预期目标。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
