<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CoT on LZY Blog</title>
    <link>http://localhost:1313/tags/cot/</link>
    <description>Recent content in CoT on LZY Blog</description>
    <image>
      <title>LZY Blog</title>
      <url>http://localhost:1313/images/papermod-cover.png</url>
      <link>http://localhost:1313/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 05 Aug 2025 20:12:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/cot/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>拒绝“想太多”：大模型Thinking Budget控制方案解析</title>
      <link>http://localhost:1313/posts/thinking-budget-0805/</link>
      <pubDate>Tue, 05 Aug 2025 20:12:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/thinking-budget-0805/</guid>
      <description>&lt;h2 id=&#34;为什么需要thinking-budget&#34;&gt;为什么需要thinking-budget&lt;/h2&gt;
&lt;p&gt;你是否也曾见过一个LLM在深度思考中逐渐陷入了自我怀疑的无限循环中呢？&lt;/p&gt;
&lt;p&gt;虽然CoT一定程度上提升模型回答的准确率，reasoning-mode也逐渐成为各大开源模型中的必需品。但新的问题也随之而来：失控的思考。LLM常常会表现出overthinking的问题，大量冗余猜测，和自我否定，无一不增加token预算，这些问题在小参数量的、采用蒸馏获得的reasoning模型中尤为常见。&lt;/p&gt;
&lt;p&gt;那么一个很自然的想法：为模型的思考设定一个预算，对于简单问题可以选择比较少的预算，而难题可以使用较大的预算。&lt;/p&gt;
&lt;h2 id=&#34;主流模型厂商所提供的推理控制&#34;&gt;主流模型厂商所提供的推理控制&lt;/h2&gt;
&lt;p&gt;会发现，在主流的模型提供商的API中和应用服务中，普遍有对于推理长度的控制手段&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: 提供了&lt;code&gt;reasoning.effort&lt;/code&gt;等参数来暗示模型的思考深度（针对o1，o3系列模型）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking&#34;&gt;Anthropic&lt;/a&gt;&lt;/strong&gt;: 允许通过&lt;code&gt;budget_tokens&lt;/code&gt;或&lt;a href=&#34;https://www.anthropic.com/engineering/claude-code-best-practices&#34;&gt;特定提示词&lt;/a&gt;（如 &amp;ldquo;think&amp;rdquo; &amp;lt; &amp;ldquo;think hard&amp;rdquo; &amp;lt; &amp;ldquo;think harder&amp;rdquo; &amp;lt; &amp;ldquo;ultrathink.&amp;quot;）来触发不同级别的思考&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.google.dev/gemini-api/docs/thinking?hl=zh-cn#set-budget&#34;&gt;Gemini&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://www.alibabacloud.com/help/en/model-studio/deep-thinking#6f0633b9cdts1&#34;&gt;Qwen&lt;/a&gt;&lt;/strong&gt;: 直接在API中提供了明确的&lt;code&gt;thinking_budget&lt;/code&gt;参数设置&lt;/p&gt;
&lt;h2 id=&#34;开源实现budget-forcing&#34;&gt;开源实现：Budget Forcing&lt;/h2&gt;
&lt;p&gt;那么对于开发者而言，特别是本地化部署的LLM要如何实现thinking-budget？&lt;/p&gt;
&lt;p&gt;最直观的方法便是使用prompt，但效果并不好，特别当模型在训练过程中没有特别针对性训练，明文化要求模型“用更少的token进行思考” 一般并不起什么作用。相比之下Qwen3系列的混合thinking模型中，在训练阶段加入了&lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt;的标记，于是在推理过程中在system-prompt或者user-prompt中加入&lt;code&gt;/no_think&lt;/code&gt;能够从think模式切换。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; &lt;strong&gt;(Section 4.3)&lt;/strong&gt;&amp;ldquo;To better integrate the two modes and enable users to dynamically switch the model’s thinking process, we design chat templates for Qwen3, as shown in Table 9. Specifically, for samples in thinking mode and non-thinking mode, we introduce &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no think&lt;/code&gt; flags in the user query or system message, respectively. This allows the model to follow the user’s input and select the appropriate thinking mode accordingly.&amp;rdquo;&lt;br&gt;
…&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
