<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Alignment on LZY Blog</title>
    <link>https://niraya666.github.io/tags/alignment/</link>
    <description>Recent content in Alignment on LZY Blog</description>
    <image>
      <title>LZY Blog</title>
      <url>https://niraya666.github.io/images/papermod-cover.png</url>
      <link>https://niraya666.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 23 Jan 2025 15:21:00 +0800</lastBuildDate>
    <atom:link href="https://niraya666.github.io/tags/alignment/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>RLHF 之路：强化学习复习之上篇</title>
      <link>https://niraya666.github.io/posts/rlhf-%E4%B9%8B%E8%B7%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E4%B9%8B%E4%B8%8A%E7%AF%87/</link>
      <pubDate>Thu, 23 Jan 2025 15:21:00 +0800</pubDate>
      <guid>https://niraya666.github.io/posts/rlhf-%E4%B9%8B%E8%B7%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E4%B9%8B%E4%B8%8A%E7%AF%87/</guid>
      <description>&lt;h2 id=&#34;写在前面&#34;&gt;写在前面&lt;/h2&gt;
&lt;p&gt;决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。&lt;/p&gt;
&lt;p&gt;学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。
于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）&lt;/p&gt;
&lt;p&gt;主要的教材来自：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;a href=&#34;http://incompleteideas.net/book/RLbook2020.pdf&#34;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;B站UP主 shuhuai008 的系列推导视频&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇笔记将包含以下的内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MDP&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DP&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Monte Carlo Methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TD方法&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;马尔可夫决策过程markov-decision-processmdp&#34;&gt;马尔可夫决策过程(Markov Decision Process，MDP)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;相关概念&#34;&gt;相关概念&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;随机变量（Random Variance）&lt;/strong&gt;： ( $X, \ y, \ x \perp y$)，随机变量之间存在的独立性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;随机过程（Stochastic Process）&lt;/strong&gt;： ${S_t}_{t=1}^{\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Markov链/过程（Markov Chain/Process）&lt;/strong&gt;：强调了&lt;strong&gt;Markov性质&lt;/strong&gt;（Markov Property），即&lt;strong&gt;未来的状态仅依赖于当前状态而与过去无关&lt;/strong&gt;，形式化地表示为：&lt;/p&gt;
&lt;p&gt;$$
P(S_{t+1}|S_t, S_{t-1}, &amp;hellip;,S_1) = P(S_{t+1}|S_{t})
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;状态空间模型（State Space Model）：&lt;/strong&gt; Markov Chain + Observation； 如 HMM， Kalman Filter，particle Filter。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
