[{"content":"这是我的个人介绍页面。你可以在这里写下你的背景信息、兴趣爱好、专业技能等等。\n","permalink":"https://niraya666.github.io/about/","summary":"这是我的个人介绍页面。你可以在这里写下你的背景信息、兴趣爱好、专业技能等等。","title":"About"},{"content":"2024-春夏之交 大阪\n旅行时间2天3夜\n在多邻国上学了快一年的散装日语，受到了看大河剧、日本电影和玩日本游戏的影响，我决定利用刚获得的三年多次入境签证，以及51假期的空闲时间，去日本看看。考虑到时间限制，我选择了关西的交通枢纽和经济中心——大阪，作为这次日本之行的第一站。\n对于日本，尤其是大阪，我的第一印象是热门景区和场所人非常多。这主要是因为赶上了日本的10天黄金周，同时由于日元汇率低迷，大量外国人涌入日本消费和旅游。然而，在居民区，特别是我所住的浪速区，情况则大为不同，晚上和清晨几乎见不到几个人，给人一种非常萧条的感觉。中国人会说，这叫做“缺乏人气”。此外，随处可见的大乌鸦加深了这种萧条感。或许这也可能是宫崎英高在《黑魂》系列中乌鸦的灵感来源吧。在日本，乌鸦似乎被视为吉祥的象征。\n游玩篇 出了民宿，我的第一站是不远处的难波八阪神社。这里以其狮子头形状的大狮子殿闻名，成为了一处热门的打卡地点。据说，这个狮子头具有驱除灾难和带来好运的力量。\n经过两站地下铁，即可到达动物园前站， 这里是新世界的入口。\n新世界，曾是日本向西方看齐、力图“脱亚入欧”时的产物。在20世纪初，日本大量吸纳西方文化元素，并在大阪建立了这一街区。然而时光流转，21世纪的今天，这里更像是一个保存良好的“旧世界”。街区内充满昭和时代的气息，从游戏厅、炸串店到浮夸的店面招牌，甚至粉红色影院，处处透露出时代的印记。\n作为泡沫经济时期产物， 粉红影院现在主要吸引了一部分中老年男性和非传统性别的人士。影院的“学生半价”标志显得有些讽刺，同时也透露出一种时代变迁的哀愁。鉴于这类场所的特殊性，不推荐旅行者单独前往。\n新世界的标志性建筑通天阁，则见证了区域的多重历史。这座铁塔最初在1912年仿照巴黎的埃菲尔铁塔建造，但在二战中因为“献纳”政策而被拆除，用其材料支持军工。通天阁的现代版本建于1956年，为了迎接万博会而重建，塔身醒目地印有日立的广告，成为了大阪的新地标。\n西成区，与新世界仅一路之隔，是大阪的旧城区。这个区域以较复杂的治安情况、流浪汉的较高出现率，以及成人娱乐业而知名。其中，著名的酒吧一条街和在中文互联网上广为人知的飞田新地均位于此地。游客在此区域应特别小心，避免随意使用手机或拍照，以尊重当地文化和增加个人安全。\n往新世界东北边走， 则能到达天王寺车站，商圈，以及以此命名此地的四天王寺。这里的天王寺车站作为从关西机场到大阪市区的两条主要电车线路之一JR西日本的终点站，同前面走过的西成区和新世界相比， 自然非常繁华。\n天王寺站前\nJR西日本机场线之HARUKA， 以Hello- Kitty涂装著称。\n日本的许多古迹因二战的轰炸和历史上的大地震而稀少，多数是在原址上修复或重建的。四天王寺就是这样的例子，它是日本历史悠久的寺庙之一，最初由飞鸟时期的圣德太子创建。虽然历经多次战火，我们今天所见的四天王寺主要是1957年后的重建。\n前往天王寺途中，偶遇的一心寺。\n在天王寺附近，我意外地发现了一个旧书集市。在那里，我用400日元买到了一本1994年的旧书。虽然还有很多书我想买想看，但考虑到海关的限制和行李空间的限制，我只能忍痛放弃。\n不可免俗的道顿堀格利高小人\n蟹道乐\n夜幕降临， 抵达梅田，乘坐hep five摩天轮，体验大阪夜景。\nhep five摩天轮\n第二天，前往大阪历史博物馆和大阪城， 以及shooping。\n大阪城\n大阪城始建于1583年，由丰臣秀吉下令建造。大阪城的天守阁如今是博物馆，展示了丰臣秀吉的生平、战国时期的武器和铠甲，以及大阪城的历史变迁。\n如果对于历史感兴趣的小伙伴， 大阪城公园边上的大阪历史博物馆也值得逛一逛。 大阪历史博物馆展示了古代大阪作为日本重要商业城市的发展过程，以及现代大阪的城市化进程。参观路线从10楼一路到7楼，6楼为特别展。\n日式连锁中餐王将的碳水加碳水再加碳水的定食\n剩下的shopping时间，逛了堂吉柯德，在西斋心桥发现一家二手唱片店\n位于 Big Step 大楼的地下一层，可惜根据网友的评论，似乎店面越来越小，东西越来越少了。\n似乎在日本不同城市，优衣库会有限定的特别款式， 在大阪则是御好烧（大阪烧）和章鱼烧的图案。\n美国村边上的小自由女神像\n攻略篇 入境 Visit Japan Web （VJW）为了替代入境卡而设计的， 需要在入境前提前申报，在入境时扫码即可。和纸质入境卡相比， 至少从我的观察而言， 使用VJW会更加快速，而没有存在需要排队的情况。\n交通 在大阪，从关西机场到市区有两条主要的铁路线路，南海电铁和JR西日本。两条线路隶属于不同的公司运营，在使用购票机购票时需要注意区分。平均时间在半个小时到一个小时不等， 票价在970到1800日元间，取决于所乘坐车次和制定席与否。除了特急列车（Rapi:t \u0026amp; HARUKA）,均和国内乘坐地铁没有太大差别。\n对于特急列车， 可以在网上买票，通过扫二维码进站，或者是在自助购票机上操作，以及通过刷西瓜卡进站，在候车处附近的机器上购买特急券。理论上有售票员会查票。\n赶了6:00的第一班Rapi:t alpha\n吃 居酒屋会在刚入座上小菜，也叫お通し（o-to-shi）,这部分一般是按人头收费，价格在330日元左右，一般是强制收取不给退的。\n日本餐厅一般是不提供打包服务的， 因为内用和外带属于不同执照，并且会因为害怕顾客吃坏肚子店家要负责任， 所以在日本餐厅，店家对于打包吃剩的食物十分抗拒。\n点餐时， 特别在旅游城市， 一般都会有多国语言版本或者是带图片版本的菜单， 有的店家甚至有扫码点单，可谓是非常友好的。所以， 只需要会几个日语单词即可，如\n日语单词 意思 罗马音 注文 点单 chūmon 会計 结账 kaikei メニュー 菜单 menyū 一つ 一个 hitotsu 二つ 两个 futatsu 三つ 三个 mittsu 少々 少量 shōshō 大盛り 大份 ōmori おすすめ 推荐 osusume トイレ 洗手间 toire 在便利店消费时，有几个常用的日语单词和表达可以帮助你更方便地交流。例如，店员可能会询问你是否需要袋子（ふくろ，fukuro）或加热食物（温め，atatame）。在不需要这些服务时，可以回答“大丈夫”（daijōbu，没关系）。\n在7-11一般是又店员负责扫码和年龄检查（买烟酒时），扫完后，有顾客在旁边的一台机器上选择付款方式并付款。当然，也可以直接出示Alipay 给店员， 店员会帮你操作的。\n大阪周游卡（Osaka Amazing Pass） or 大阪乐游券（Osaka e-Pass） 目前周游卡已停售(二日券)，一日券购买有效期截止到2024年6月30日。\n其中大阪周游卡需要购买后兑换成实体卡，兑换位置可查看 售票及换票点；而大阪乐游券直接展示二维码即可。二者都支持在Klook上购买。周游卡价格在¥3300包括了市区地铁随意乘坐， 乐游券价格为¥2400，只包括景点。\n用GPT-4 整理了一个周游卡和乐游券差异的表格（不能保证100%正确）：\n地点 (日本語) 地点 (中文) Location Regular fee Osaka Amazing Pass Osaka e-Pass 梅田スカイビル空中庭園展望台 梅田天空大厦空中庭园观景台 梅田 ¥ 1500 ✔️ ✔️ 梅田スカイビル・衣川邦彦天空美術館 梅田天空大厦衣川邦彦天空美术馆 梅田 ¥ 1000 ✔️ ✔️ HEP FIVE観覧車 HEP FIVE摩天轮 梅田 ¥ 600 ✔️ ✔️ 通天閣 通天阁 新世界 ¥ 1000 ✔️ ✔️ 通天閣タワースライダー 通天阁塔滑梯 新世界 ¥ 1000 ✔️ ✔️ 四天王寺 (中心伽藍・本坊庭園) 四天王寺（中心伽蓝、本坊庭园） 四天王寺 ¥ 300 ✔️ ✔️ とんぼりリバークルーズ 道顿堀河巡航 道顿堀 ¥ 1200 ✔️ ✔️ 天王寺動物園 天王寺动物园 四天王寺 ¥ 500 ✔️ ❌ 中之島川クルーズ 中之岛河巡航 北区中之岛 ¥ 1500 ✔️ ✔️ 大阪国立国際美術館 大阪国立国际美术馆 北区中之岛 ¥ 430 ✔️ ❌ 道頓堀ZAZA 道顿堀ZAZA 道顿堀 ¥ 800 ✔️ ✔️ 上方浮世絵館 上方浮世绘馆 难波 ¥ 700 ✔️ ✔️ 大阪歴史博物館 大阪历史博物馆 大阪城 ¥ 600 ✔️ ❌ 大阪平和の殿堂 大阪和平馆 大阪城 ¥ 250 ✔️ ✔️ 大阪城天守閣 大阪城天守阁 大阪城 ¥ 600 ✔️ ❌ ワンダークルーズ 奇迹巡航 ¥ 1200 ✔️ ✔️ 大阪くらしの今昔館 大阪生活今昔馆 天神橋 ¥ 600 ✔️ ✔️ 天保山大観覧車 天保山大观览车 天保山 ¥ 1200 ✔️ ❌ グリオンミュージアム GLION博物馆 天保山 ¥ 1200 ✔️ ✔️ サンタマリア日帰りクルーズ 圣玛丽亚日游巡航 ¥ 1800 ✔️ ✔️ サンタマリア夜景クルーズ 圣玛丽亚夜景巡航 ¥ 2300 ✔️ ✔️ キャプテンライン 队长线 ¥ 1500 ✔️ ✔️ レゴランド・ディスカバリー・センター大阪 大阪乐高探索中心 ¥2800～¥3000 ✔️ ✔️ 咲洲コスモタワー展望台 咲洲宇宙塔展望台 ¥1000 ✔️ ✔️ ボートレース住之江 住之江赛艇 ¥ 1500 ✔️ ✔️ 大阪水上バスアクアライナー 大阪水上巴士Aqua-Liner ¥ 1800 ✔️ ✔️ 桜川お花見クルーズ 樱川赏花巡航 ¥ 1200 ✔️ ✔️ 夕日クルーズ「寄道」 夕阳巡航“YORIMICHI” ¥ 1500 ✔️ ✔️ 大阪城御座船 大阪城御座船 ¥ 1500 ✔️ ✔️ エンタープライズイノベーションミュージアム 创业创新博物馈 ¥ 300 ✔️ ✔️ 大阪城重要文化財櫓 大阪城重要文化财瞭 ¥ 800 ✔️ ❌ 大阪城西の丸庭園 大阪城西之丸庭园 ¥ 200 ✔️ ❌ 咲くやこの花館 咲く家这花馆 ¥ 500 ✔️ ❌ 長居植物園 长居植物园 ¥ 200 ✔️ ❌ 大阪市立自然史博物館 大阪市立自然历史博物馆 ¥ 300 ✔️ ❌ 堺利晶の杜 堺利晶之森 ¥ 300 ✔️ ❌ 堺市博物館 堺市博物馆 ¥ 200 ✔️ ❌ 万博記念公園（自然文化園、日本庭園） 万博纪念公园（自然文化园、日本庭园） ¥ 260 ✔️ ❌ OSAKA WHEEL（日本最大の観覧車） 大阪摩天轮（日本最大） 天保山 ¥ 1000 ✔️ ✔️ 大阪科学館 大阪科学馆 ¥ 400 ✔️ ❌ 桂陵園 桂陵园 ¥ 150 ✔️ ❌ 日向の湯 日向温泉 ¥ 800 ❌ ✔️ 海洋堂ホビーランド 海洋堂模型乐园 ¥ 1000 ❌ ✔️ 忍者トリックハウス（エディオン難波本店） 忍者特技屋（EDION难波本店） ¥ 1000 ❌ ✔️ 道頓堀ミュージアムなみき座 道顿堀剧场 ❌ ✔️ 更多的攻略建议参考：小气少年的部落格 这是我所发现的最完整的日本旅行攻略，成功解决了我的旅行前焦虑。 此外，在推荐一本书：《日本點餐完全圖解》，这本书详细介绍了在日本点餐的注意事项，甚至教会你怎么吃。\n感受篇 在这次的旅途中，我在网上上结识了两位旅游搭子，分别各搭了一天。\nA是一位出生于1998年的年轻人，帅气的外表下藏着近期感情伤痕的痛楚。他独自一人前来关西，试图通过旅行洗涤感情上的创伤。尽管我不愿轻易给人贴上标签，诸如“渣男”之类的称呼，但A的行为难免让人费解：他同时与四五名女生保持联系，不断在Tinder上寻找新的可能，对每一个路过的女性都有自己的评价。然而，从他的话语中，我逐渐感知到他背后隐藏的故事——那些使他变得如此的伤痕累累。通过他，我也间接了解了日本的一些风俗业及他的个人感受体验。\nR则是一位三十多岁的体制内员工，表面上看似普通，内心却充满了对现状的不满和对自由的渴望。他的思想深受那个时代的河殇文化与十年前“意林”文学的影响。在他的眼中，日本社会的方方面面几乎都是模范，而国内的绝大多数则恰恰相反。他甚至考虑过一个大胆的想法：辞去体制内的工作，“润”到日本。用互联网上的用语形容他的思想就是妥妥“反贼”和“润人”，但又何妨。R还是个爱酒如命的人。\n日本人究竟相信什么？道德伦理？法律？还是抽象的社会潜规则 事实上，日本社会的一个显著特点是高度的循规蹈矩。人们极其重视集体，与社会脱节被视为所能想到的最严酷的惩罚之一。也许正是因为日本人对意识形态和教条缺乏关注，他们格外重视外在形象，注重符合各种场合的姿态，因为“面子”比“里子”更重要。这演化成了一种行为模式，即在私下里怎么搞都行，但在公众面前一定要维护自己的“形象”。这意味着日本人本质上追求的是集体认同，极度避免被社会边缘化。\n“日本的情况则不同，原始信仰从未遭到更精致的官方律条的碾压。尽管不同宗教之间的区分不甚清晰，尤其是在最广泛的层面上，但佛寺和神社仍然共存并立。两种教派的仪式人们都参与，尽管不一定同时同地。这或许是因为日本人对意识形态和教条缺乏关注，却格外重视外在，重视符合各种场合的姿态，因为“面子”比“里子”更重要。”\n在这种文化氛围下，个人行为被严格的社会规范和集体利益所约束。脱离集体，被视为严重的社会惩罚，比法律制裁更为严酷。社会关系和集体认同对个人生活至关重要，因此日本人普遍面临巨大的心理压力，必须时刻注意自己的行为是否符合社会期望，以避免被边缘化。\n这种面子文化强调外在形象的维护。日本人非常重视在各种场合的表现和形象管理，不论是在公共场合的礼仪，还是在工作中的表现，维护良好的外在形象是必不可少的。在私下场合，日本人可能会表现出更大的灵活性和自由度，但在公开场合，他们会极力维护自己的公众形象，遵守社会规范和礼仪，以获得集体的认可和尊重。\n不论是飞田新地的挂羊头卖狗肉，还是游戏厅的老虎机，日本人总是有能力绕过所谓的规则，显示出一种极强的变通能力；但在公共区域场合下， 日本人又总能展现出一种死板地对于规则的严格遵守。\n日本社会的这种双重性在于，私下和公开行为的巨大反差。这种现象可以通过对集体认同和避免边缘化的强烈需求来解释。正如《日本之镜：日本文化中的英雄与恶人》一书中所提到的，“制约日本人行为的不是社会规则，而是一套抽象的道德教训。”\n然而，这种“抽象的道德”在中文语境下显得陌生。中国古代追求的是儒家的仁义道德，而日本的道德观念更多来源于神道教的泛神信仰，强调避免在社会上出丑。例如，须佐之男被放逐的故事就是一种道德教训，提醒人们避免类似的羞辱。日本人常说，只要不被逮个正着，避免在社会上出丑，就能够为所欲为，不必担心后果。\n","permalink":"https://niraya666.github.io/posts/%E5%A4%A7%E9%98%AA%E6%B8%B8%E8%AE%B0%E6%97%B6%E5%85%89%E4%BA%A4%E9%94%99%E4%B8%8B%E7%9A%84%E6%97%A5%E6%9C%AC%E6%8E%A2%E7%B4%A2/","summary":"2024-春夏之交 大阪\n旅行时间2天3夜\n在多邻国上学了快一年的散装日语，受到了看大河剧、日本电影和玩日本游戏的影响，我决定利用刚获得的三年多次入境签证，以及51假期的空闲时间，去日本看看。考虑到时间限制，我选择了关西的交通枢纽和经济中心——大阪，作为这次日本之行的第一站。\n对于日本，尤其是大阪，我的第一印象是热门景区和场所人非常多。这主要是因为赶上了日本的10天黄金周，同时由于日元汇率低迷，大量外国人涌入日本消费和旅游。然而，在居民区，特别是我所住的浪速区，情况则大为不同，晚上和清晨几乎见不到几个人，给人一种非常萧条的感觉。中国人会说，这叫做“缺乏人气”。此外，随处可见的大乌鸦加深了这种萧条感。或许这也可能是宫崎英高在《黑魂》系列中乌鸦的灵感来源吧。在日本，乌鸦似乎被视为吉祥的象征。\n游玩篇 出了民宿，我的第一站是不远处的难波八阪神社。这里以其狮子头形状的大狮子殿闻名，成为了一处热门的打卡地点。据说，这个狮子头具有驱除灾难和带来好运的力量。\n经过两站地下铁，即可到达动物园前站， 这里是新世界的入口。\n新世界，曾是日本向西方看齐、力图“脱亚入欧”时的产物。在20世纪初，日本大量吸纳西方文化元素，并在大阪建立了这一街区。然而时光流转，21世纪的今天，这里更像是一个保存良好的“旧世界”。街区内充满昭和时代的气息，从游戏厅、炸串店到浮夸的店面招牌，甚至粉红色影院，处处透露出时代的印记。\n作为泡沫经济时期产物， 粉红影院现在主要吸引了一部分中老年男性和非传统性别的人士。影院的“学生半价”标志显得有些讽刺，同时也透露出一种时代变迁的哀愁。鉴于这类场所的特殊性，不推荐旅行者单独前往。\n新世界的标志性建筑通天阁，则见证了区域的多重历史。这座铁塔最初在1912年仿照巴黎的埃菲尔铁塔建造，但在二战中因为“献纳”政策而被拆除，用其材料支持军工。通天阁的现代版本建于1956年，为了迎接万博会而重建，塔身醒目地印有日立的广告，成为了大阪的新地标。\n西成区，与新世界仅一路之隔，是大阪的旧城区。这个区域以较复杂的治安情况、流浪汉的较高出现率，以及成人娱乐业而知名。其中，著名的酒吧一条街和在中文互联网上广为人知的飞田新地均位于此地。游客在此区域应特别小心，避免随意使用手机或拍照，以尊重当地文化和增加个人安全。\n往新世界东北边走， 则能到达天王寺车站，商圈，以及以此命名此地的四天王寺。这里的天王寺车站作为从关西机场到大阪市区的两条主要电车线路之一JR西日本的终点站，同前面走过的西成区和新世界相比， 自然非常繁华。\n天王寺站前\nJR西日本机场线之HARUKA， 以Hello- Kitty涂装著称。\n日本的许多古迹因二战的轰炸和历史上的大地震而稀少，多数是在原址上修复或重建的。四天王寺就是这样的例子，它是日本历史悠久的寺庙之一，最初由飞鸟时期的圣德太子创建。虽然历经多次战火，我们今天所见的四天王寺主要是1957年后的重建。\n前往天王寺途中，偶遇的一心寺。\n在天王寺附近，我意外地发现了一个旧书集市。在那里，我用400日元买到了一本1994年的旧书。虽然还有很多书我想买想看，但考虑到海关的限制和行李空间的限制，我只能忍痛放弃。\n不可免俗的道顿堀格利高小人\n蟹道乐\n夜幕降临， 抵达梅田，乘坐hep five摩天轮，体验大阪夜景。\nhep five摩天轮\n第二天，前往大阪历史博物馆和大阪城， 以及shooping。\n大阪城\n大阪城始建于1583年，由丰臣秀吉下令建造。大阪城的天守阁如今是博物馆，展示了丰臣秀吉的生平、战国时期的武器和铠甲，以及大阪城的历史变迁。\n如果对于历史感兴趣的小伙伴， 大阪城公园边上的大阪历史博物馆也值得逛一逛。 大阪历史博物馆展示了古代大阪作为日本重要商业城市的发展过程，以及现代大阪的城市化进程。参观路线从10楼一路到7楼，6楼为特别展。\n日式连锁中餐王将的碳水加碳水再加碳水的定食\n剩下的shopping时间，逛了堂吉柯德，在西斋心桥发现一家二手唱片店\n位于 Big Step 大楼的地下一层，可惜根据网友的评论，似乎店面越来越小，东西越来越少了。\n似乎在日本不同城市，优衣库会有限定的特别款式， 在大阪则是御好烧（大阪烧）和章鱼烧的图案。\n美国村边上的小自由女神像\n攻略篇 入境 Visit Japan Web （VJW）为了替代入境卡而设计的， 需要在入境前提前申报，在入境时扫码即可。和纸质入境卡相比， 至少从我的观察而言， 使用VJW会更加快速，而没有存在需要排队的情况。\n交通 在大阪，从关西机场到市区有两条主要的铁路线路，南海电铁和JR西日本。两条线路隶属于不同的公司运营，在使用购票机购票时需要注意区分。平均时间在半个小时到一个小时不等， 票价在970到1800日元间，取决于所乘坐车次和制定席与否。除了特急列车（Rapi:t \u0026amp; HARUKA）,均和国内乘坐地铁没有太大差别。\n对于特急列车， 可以在网上买票，通过扫二维码进站，或者是在自助购票机上操作，以及通过刷西瓜卡进站，在候车处附近的机器上购买特急券。理论上有售票员会查票。\n赶了6:00的第一班Rapi:t alpha\n吃 居酒屋会在刚入座上小菜，也叫お通し（o-to-shi）,这部分一般是按人头收费，价格在330日元左右，一般是强制收取不给退的。\n日本餐厅一般是不提供打包服务的， 因为内用和外带属于不同执照，并且会因为害怕顾客吃坏肚子店家要负责任， 所以在日本餐厅，店家对于打包吃剩的食物十分抗拒。","title":"大阪游记：时光交错下的日本探索"},{"content":"为什么要进行文本分块？ 大型语言模型（LLM）和嵌入式模型都有其处理文本的长度限制。为了有效处理超出这些限制的长文本，我们需要进行文本分块。\n如何进行文本分块 **块分割戒律：**我们的目标不仅仅是为了划分数据块。更重要的是，我们要以一种便于日后检索和提取价值的格式来整理我们的数据。\n文本分块的根本目的不仅仅是简单地将数据分割成块。更重要的是，我们要以便于检索和提取价值的方式来组织这些数据块。以下是几种常见的分块逻辑：\n句子分割（Sentence Splitting） 这种方法只是简单地将文本切分成N个字符的片段，不考虑内容或形式的连贯性。\ntext = \u0026#34;...\u0026#34; # 你的文本 from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter( separator = \u0026#34;\\n\\n\u0026#34;, chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 递归字符文本分割（Recursive Character Text Splitting） 这种方法首先尝试用一个分隔符将文本切分成小块。如果初始尝试未能达到预期的块大小或结构，就递归地用不同的分隔符重新分割结果块，直到得到满意的结果。\ntext = \u0026#34;...\u0026#34; # 你的文本 from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 根据langchain 的默认分隔条件 [\u0026quot;\\n\\n\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;] ,也就是会将text根据该分割条件的顺序（两个换行-\u0026gt;一个换行-\u0026gt;空格）将文本进行递归分割。\n针对特定文档的分割方法（Document Specific Splitting） 例如针对Markdown或Python代码的分割策略，这些策略根据文档的结构来进行分割，如类定义或Markdown标题。\nlangchain 提供了一些常见文档的分割方法：\nmardown的分割逻辑\n[ # First, try to split along Markdown headings (starting with level 2) \u0026#34;\\n#{1,6} \u0026#34;, # Note the alternative syntax for headings (below) is not handled here # Heading level 2 # --------------- # End of code block \u0026#34;```\\n\u0026#34;, # Horizontal lines \u0026#34;\\n\\\\*\\\\*\\\\*+\\n\u0026#34;, \u0026#34;\\n---+\\n\u0026#34;, \u0026#34;\\n___+\\n\u0026#34;, # Note that this splitter doesn\u0026#39;t handle horizontal lines defined # by *three or more* of ***, ---, or ___, but this is not handled \u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;, ] python的分割逻辑：\n[ # First, try to split along class definitions \u0026#34;\\nclass \u0026#34;, \u0026#34;\\ndef \u0026#34;, \u0026#34;\\n\\tdef \u0026#34;, # Now split by the normal type of lines \u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;, ] 表格 重点提示： 后来在我们对数据块进行语义搜索时，直接从表格中匹配嵌入可能会比较困难。开发者常用的做法是，在提取了表格之后，对其进行关键信息提取。然后对这些关键信息的总结进行嵌入。如果这个总结的嵌入与你的搜索目标匹配，那么就可以把原始表格交给你的大语言模型处理。\n语义分块 目前的文本分块方法大多基于结构或字符数量，忽视了文本的实际语义内容。有效的语义分块应当确保内容上相似的数据被归类在一起，这样不仅有助于数据的组织，还能提升后续处理的效率和准确性。\n根据Greg的做法：寻找连续句子间的分割点， 即从第一句话开始，获取其嵌入，然后与第二句进行比较，以此类推。在寻找嵌入距离较大的“分割点”。如果距离超过了一定阈值，就认为这标志着新的语义段落的开始。\n可以通过以下步骤来进行语义分块：\n文档初始处理：首先将你的文档分割成句子，每个句子通常包含关于一个主题的单一观点。\n句子嵌入：利用语言模型获取每个句子的嵌入表示。\n句子比较与分块：从第一句话开始，逐句比较嵌入的相似度。当相邻句子之间的嵌入距离超过预设的阈值时，标记为新的语义段落的开始。\n聚类相似句子：将语义上接近的句子聚集成块，同时保持句子的原始顺序。\n伪代码示例\narticle = \u0026#34;你的完整文章\u0026#34; chunks = [article[:1]] # 默认第一部分为一个独立分块 # 文章逐句审查 for sentence in article[1:]: if is_similar(sentence, chunks[-1][-1]): chunks[-1].append(sentence) # 如果新句子与当前块的最后一个句子相似，则加入当前块 else: chunks.append([sentence]) # 如果不相似，则创建新的分块 # 持续这个过程直至文章末尾 # 在这种方法中，is_similar 函数需要定义为比较句子嵌入之间的距离，并判断它们是否足够接近。 特殊分块以及后处理 不见得一定要在召回后，返回原始的chunks。\nSummary(摘要) 处理和存储大量文档的原始数据块往往不是最高效的方法。通过为每个文档创建摘要并进行嵌入，我们不仅可以减少处理的数据量，还能提高检索的速度和准确性。\nhttps://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary\ncreate a summary for each document, embed that along with (or instead of) the document.\nimport uuid from langchain_core.documents import Document from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI chain = ( {\u0026#34;doc\u0026#34;: lambda x: x.page_content} | ChatPromptTemplate.from_template(\u0026#34;Summarize the following document:\\n\\n{doc}\u0026#34;) | ChatOpenAI(max_retries=0) | StrOutputParser() ) summaries = chain.batch(docs, {\u0026#34;max_concurrency\u0026#34;: 5}) # The vectorstore to use to index the child chunks vectorstore = Chroma(collection_name=\u0026#34;summaries\u0026#34;, embedding_function=OpenAIEmbeddings()) # The storage layer for the parent documents store = InMemoryByteStore() id_key = \u0026#34;doc_id\u0026#34; # The retriever (empty to start) retriever = MultiVectorRetriever( vectorstore=vectorstore, byte_store=store, id_key=id_key, ) doc_ids = [str(uuid.uuid4()) for _ in docs] summary_docs = [ Document(page_content=s, metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries) ] retriever.vectorstore.add_documents(summary_docs) retriever.docstore.mset(list(zip(doc_ids, docs))) Hypothetical questions HyDE\nhypothetical_document_embeddings\nHyDE (Hypothetical Document Embeddings)\nHyDE（Hypothetical Document Embeddings，假设性文档嵌入），通过生成假设性的答案并对这些生成的文档进行嵌入，而非嵌入原始文本。这种方法特别适用于用户问题简短且与需要查找的原始文本在语义上相似度不高的情况。\n当用户的问题与知识库中的原始文本在语义上相差较大时，传统的文档检索方法可能无法有效地匹配到相关内容。HyDE通过使用大型语言模型（LLMs）根据用户问题生成格式上类似于知识库中文本的假设性答案，从而提升匹配的准确率。\n尽管HyDE可以提升检索的准确性，但这种方法也会一定程度上增加检索的耗时。为了平衡检索准确率和响应速度，可以考虑使用更强大的重排模型（reranking models）。这些模型能够在检索后的阶段进一步优化结果，通过精准地评估和排序生成的假设性答案，以提供最相关的回答。\nParent Document Retriever (PDR) from Advanced RAG Techniques: an Illustrated Overview\nA.K.A Multi-size-chunks(Parent Document Retriever/ parent-child chunks retriever/ Auto Merging Retriever)\n核心思想是，尽管小片段的数据更容易在语义上与短小的用户查询匹配，但这些小片段往往缺乏提供全面回答所需的足够上下文。PDR的解决方案是在检索时优先获取这些小片段，然后识别并返回包含这些小片段的更大父文档，以此为基础进行问题的回答。\n在使用embedding模型召回时，尽可能的将对应文本拆成句子级别，因为往往人的问题都是比较短的句子，这样在语义匹配上能够较好的匹配上；但是在使用LLM做回答时，由需要足够的上下文让模型能够有足够多的背景进行回答；因此在召回的时候倾向于使用small-chunks用于匹配，再通过某些聚合方式合并成一个具备相对完整上下文的较大chunks用作模型回答之背景；\n在Langchain中提供了Parent Document Retriever这一方法。\nDuring retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n使用方法：\nfrom langchain.retrievers import ParentDocumentRetriever from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter # Retrieving larger chunks # This text splitter is used to create the parent documents parent_splitter = RecursiveCharacterTextSplitter(separators=\u0026#34;\\n\u0026#34;,chunk_size=2000) # This text splitter is used to create the child documents # It should create documents smaller than the parent child_splitter = RecursiveCharacterTextSplitter(separators=\u0026#34;\\n\u0026#34;,chunk_size=400) # define your docs = embeddings, and vectorstore # docs = loader.load(...) # emebeddings = OpenAIEmbeddings(...) # vectorstore = Chroma(...) # The storage layer for the parent documents store = InMemoryStore() retriever = ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter, ) retriever.add_documents(docs) 而在llama_index中也提供了类似的方法。\n# 创建较小的子块 from llama_index.schema import IndexNode sub_chunk_sizes = [256, 512, 864] sub_node_parsers = [ SimpleNodeParser.from_defaults(chunk_size=c) for c in sub_chunk_sizes ] all_nodes = [] for base_node in base_nodes: for n in sub_node_parsers: sub_nodes = n.get_nodes_from_documents([base_node]) sub_inodes = [IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes] all_nodes.extend(sub_inodes) original_node = IndexNode.from_text_node(base_node, base_node.node_id) all_nodes.append(original_node) all_nodes_dict = {n.node_id: n for n in all_nodes} # 创建索引、检索器和查询引擎 vector_index_chunk = VectorStoreIndex( all_nodes, service_context=service_context ) vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2) retriever_chunk = RecursiveRetriever( \u0026#34;vector\u0026#34;, retriever_dict={\u0026#34;vector\u0026#34;: vector_retriever_chunk}, node_dict=all_nodes_dict, verbose=True, ) query_engine_chunk = RetrieverQueryEngine.from_args( retriever_chunk, service_context=service_context ) response = query_engine_chunk.query( \u0026#34;Can you tell me about the key concepts for safety finetuning\u0026#34; ) print(str(response)) Sentence Window Retrieval 为了更好地分析找到的语境，我们在检索到的最相关单句之前后各扩展 k 个句子，然后把这个扩展后的语境送给 LLM 进行推理\nfrom Advanced RAG Techniques: an Illustrated Overview\n分块可视化小工具 推荐一个可视化chunk-spliting 工具chunkviz\n推荐阅读 Text splitting (chunking) for RAG applications\n5_Levels_Of_Text_Splitting\nlangchain:Semantic Chunking\nChunking Strategies for LLM Applications\nAdvanced RAG Techniques: an Illustrated Overview\nRAG系列05：基于语义的Chunk分割\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%9C%AC%E5%88%86%E5%9D%97/","summary":"为什么要进行文本分块？ 大型语言模型（LLM）和嵌入式模型都有其处理文本的长度限制。为了有效处理超出这些限制的长文本，我们需要进行文本分块。\n如何进行文本分块 **块分割戒律：**我们的目标不仅仅是为了划分数据块。更重要的是，我们要以一种便于日后检索和提取价值的格式来整理我们的数据。\n文本分块的根本目的不仅仅是简单地将数据分割成块。更重要的是，我们要以便于检索和提取价值的方式来组织这些数据块。以下是几种常见的分块逻辑：\n句子分割（Sentence Splitting） 这种方法只是简单地将文本切分成N个字符的片段，不考虑内容或形式的连贯性。\ntext = \u0026#34;...\u0026#34; # 你的文本 from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter( separator = \u0026#34;\\n\\n\u0026#34;, chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 递归字符文本分割（Recursive Character Text Splitting） 这种方法首先尝试用一个分隔符将文本切分成小块。如果初始尝试未能达到预期的块大小或结构，就递归地用不同的分隔符重新分割结果块，直到得到满意的结果。\ntext = \u0026#34;...\u0026#34; # 你的文本 from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 根据langchain 的默认分隔条件 [\u0026quot;\\n\\n\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;] ,也就是会将text根据该分割条件的顺序（两个换行-\u0026gt;一个换行-\u0026gt;空格）将文本进行递归分割。\n针对特定文档的分割方法（Document Specific Splitting） 例如针对Markdown或Python代码的分割策略，这些策略根据文档的结构来进行分割，如类定义或Markdown标题。","title":"RAG工具箱：文本分块"},{"content":"写在开头 本文是基于最近组内技术交流的文字稿整理。\nWhat is Agent？ 在探讨复杂的人工智能技术之前，让我们先回顾一下生活中的一个简单例子：扫地机器人。这种智能设备在房间中自主导航，避开障碍物，寻找最有效的清洁路径。它就是一个现实生活中的Agent——一个可以自主决策和行动的实体。\n在人工智能领域，Agent指的是任何可以感知其环境并根据感知结果做出决策的实体。这些决策旨在完成某些任务或达成特定的目标。Agent的行为可以简单如游戏里的机器人，也可以复杂如自动驾驶汽车。\n开始于强化学习 在强化学习中， 我们往往能见到agent的概念。强化学习是一种机器学习方法，它教导Agent通过试错法找到最佳行动路径。就像训练小狗一样，我们通过奖励来引导Agent做出正确的决策。Agent的目标是在与环境的交互中寻找最优策略。理想情况下，如果Agent能够获取足够多的真实环境数据，它就能找到最佳解决方案。然而，由于真实环境的复杂性，完全模拟真实世界是不现实的。\n目前，强化学习主要适用于环境简单、问题定义明确的场景，如围棋或视频游戏。这种方法在虚拟环境中通过大量试错来探索解决方案，这种方法虽然有效，但缺乏灵活性和高效性。与人类学习新技能的方式相比，强化学习的效率远低。人们通常通过少量的尝试就能迅速掌握新技能，而强化学习可能需要成千上万次的试错。\n这时候，如果agent具有大脑就好了。\n将LLMs作为大脑: 赋能智能Agent的关键技术 相较于基于强化学习的Agent，人类的优势在于我们天生具备的记忆能力和逻辑判断能力，甚至包括反思和从经验中学习的能力。这些能力使得我们能够通过极少的试错迅速适应和掌握新技能。\n语言模型（LLMs）为AI领域带来了革命性的变化。LLMs通过其深度学习的新范式，以及在思维链和自然语言理解方面的强大能力，预示着Agent将拥有更强大的学习和迁移能力。这种能力的提升将使得创建广泛应用且实用的Agent成为可能。\n虽然LLM是否真正具备了推理能力仍然存疑，但LLM的出现无疑改变了很多。以COT（Chain of Thought）为例，这种模型通过打印解题的中间步骤，加强了在数学和逻辑推理方面的能力，减少了幻觉的出现。这一突破性的工作揭示了LLM在不同场景下的强大\u0026quot;in-context learning\u0026quot;能力，以及其在未经微调的情况下的泛化能力。\n将LLM视为AI Agent的大脑，为自动化系统提供了一种全新的构思方式。这种基于LLM的agent系统综合了规划、记忆、工具使用和行动的能力，通过API调用与外部世界互动，显示出了前所未有的灵活性和效率。\n根据LLM Powered Autonomous Agents一文对LLM-based agent系统的定义， agent需要具备的基本能力：规划+记忆+工具+行动。\n规划能力：将复杂的任务分解成小任务， 并管理每一个子任务的目标， 并从过去的失败中反思，以吸取经验。\n记忆：LLM的上下文长度有限， 通过额外的记忆系统以提升LLM作为大脑的能力。\n工具调用\u0026amp;行动：LLM通过API调用的方式，执行任务， 与外界交互，而不是只是输出文字。\n探索AI代理的独特能力：人类与单一LLM无法比拟 AI系统的主要优势在于它们的规模和效率。这些系统能够执行以下任务，超越人类能力：\n大规模数据处理：AI能够高效地分析和处理超出人类理解范围的数据量。\n无需休息的持续操作：AI系统可以不间断地运行，而无需像人类那样休息和恢复。\n超快速计算：AI可以迅速执行复杂的计算，处理速度和效率远超人类。\nAI代理与单一LLM的不同:\n根据Andrew Ng在讲座中分享的内容，使用相对“简单”模型的代理工作流程（例如GPT-3.5）在实际应用中往往能够超越使用“智能”模型（如GPT-4）的零次提示。这说明在特定场景下，选择适当的AI模型和策略可能比单一的高级模型更有效。\nAI代理在决策制定中的应用也显示出其独特的优势。它们可以在没有情感偏见的情况下，基于大量数据做出快速且精确的决策。这种能力在需要快速响应和高精确度的领域尤为重要，如金融交易和紧急响应系统。\nAgent的规划和思维过程 AI Agent在处理复杂任务时，通过将大任务分解成小任务来提高效率。此外，自我反思能力允许Agent从过去的行动中学习，通过评估过去的决策来改善未来的表现。\nCoT，Chain of Thought， Wei et al. 2022。 即“思维链”，是一种使Agent逐步思考的方法。它通过要求模型展示解决问题的中间步骤来加强其逻辑推理能力，从而提高决策的质量和准确性。\nTree of Thoughts， ToT (Yao et al. 2023)\n尽管语言模型在许多领域表现出色，但在需要复杂规划和全局决策的任务上，它们的能力受到了限制。ToT框架应运而生，旨在通过一个结构化的思考和评估过程来弥补这一缺陷。\nToT框架借鉴了人类心理学中的双系统决策理论，通过整合快速直觉判断和慢速深思熟虑的决策过程，极大地提升了模型的决策能力。这一框架通过自我评估的方式，允许模型在面对多种可能的决策路径时，能够进行有效的选择和全局优化。\nToT框架旨在克服现有语言模型在处理需要复杂规划或搜索任务的局限。它通过结构化的思想树来探索和评估不同的决策路径。ToT允许模型在考虑多个推理路径时自我评估其选择，以做出最佳的决策。此外，ToT结合了语言生成和搜索算法（如BFS和DFS），使模型能够在进行决策时前瞻和回溯，以实现全局最优选择。\nprompt example：\ncot_prompt = \u0026#39;\u0026#39;\u0026#39; Write a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} Make a plan then write. Your output should be of the following format: Plan: Your plan here. Passage: Your passage here. \u0026#39;\u0026#39;\u0026#39; vote_prompt = \u0026#39;\u0026#39;\u0026#39;Given an instruction and several choices, decide which choice is most promising. Analyze each choice in detail, then conclude in the last line \u0026#34;The best choice is {s}\u0026#34;, where s the integer id of the choice. \u0026#39;\u0026#39;\u0026#39; compare_prompt = \u0026#39;\u0026#39;\u0026#39;Briefly analyze the coherency of the following two passages. Conclude in the last line \u0026#34;The more coherent passage is 1\u0026#34;, \u0026#34;The more coherent passage is 2\u0026#34;, or \u0026#34;The two passages are similarly coherent\u0026#34;. \u0026#39;\u0026#39;\u0026#39; score_prompt = \u0026#39;\u0026#39;\u0026#39;Analyze the following passage, then at the last line conclude \u0026#34;Thus the coherency score is {s}\u0026#34;, where s is an integer from 1 to 10. \u0026#39;\u0026#39;\u0026#39; GoT， Graph of Thoughts，\nhttps://arxiv.org/abs/2308.09687\nhttps://arxiv.org/abs/2401.14295\nGoT（Graph of Thoughts）是在ToT（Tree of Thoughts）基础上的一种进阶，它通过引入图状结构，增强了模型处理复杂推理任务的能力。这种结构允许在生成的思考中进行更为灵活的图状转换，如合并多个思考或在思考间进行循环。\nGoT的核心特点在于其灵活的图状结构，这使得它能够通过合并和循环不同的思考节点来模拟更复杂的推理过程。每个思考节点可以从多个父节点聚合信息，也可以向多个子节点扩展，形成一个密集的思考网络。这种结构特别适合处理那些传统树状模型难以解决的复杂推理和决策任务。\nGoT设计之初就为了处理比ToT更为复杂的推理任务。通过允许广泛的依赖关系和中间步骤的聚合，GoT能够更有效地解决那些需要非线性思考路径的问题。例如，在处理涉及多个相互关联子问题的任务时，GoT的图状结构提供了一种比传统线性或树状结构更为有效的解决方案。\nXoT, Everything of Thoughts, https://arxiv.org/abs/2311.04254\nXoT代表了在AI领域中对复杂思考和决策过程的一次显著革新，它通过结合蒙特卡洛树搜索（MCTS）和大型语言模型求解器（LLM solver）来优化决策路径。\nXoT框架包含以下几个关键步骤和组件：\n预训练的MCTS模块：该模块通过在特定任务上的预训练，获得了有效导航复杂思考路径的能力。\n动态思考搜索：在实际应用中，MCTS模块继续利用其策略和价值网络来探索和优化潜在的思考轨迹。\n思考修正与优化：LLM评估并修正MCTS的输出，确保思考路径的逻辑严密性和有效性。\n决策执行：修正后的思考路径被送回LLM，用于解决具体的问题和挑战。\nReAct (Reason to Act) 是由 Yao et al. 2023 提出的一种新颖的提示框架，旨在通过交错的思考、行动和观察步骤，提高大型语言模型（LLM）的动态推理和计划调整能力。\nReAct通过“思考-行动-观察”结构，使LLM能够生成口头推理迹线和与任务相关的行动。这种结构允许模型动态地创建、维护并调整高层次的行动计划，同时与外部环境（如Wikipedia）互动，引入额外信息以丰富推理。\nReAct的独特特点\n直观且易于设计：ReAct的设计直观，易于实现，并能快速适应不同的应用场景。\n通用性与灵活性：适用于多种任务，能够灵活处理复杂的情境。\n高性能与鲁棒性：提供稳定且有效的推理输出。\n与人类行为一致且可控：增强了模型的可控性，与人类的推理和行动方式更为一致。\nReAct与CoT的对比\n相对于CoT（Chain of Thought），ReAct通过交错的思考、行动和观察步骤，改进了模型的根据性和可信度。然而，这种结构在提高模型实地操作的同时，也略微降低了在推理步骤制定的灵活性。此外，ReAct的有效性很大程度上依赖于成功检索到信息丰富的知识。\nThought: ... Action: ... Observation: ... ... (Repeated many times) Reflexion是由Shinn \u0026amp; Labash 2023提出的一种创新的决策支持框架。它包括三个核心模块：Actor（行动模块）、Evaluator（评估模块）、和Self-Reflection（自我反思模块），旨在通过增强的语言模型反馈机制提升任务处理效率和质量。\n框架组成与工作机制\nActor模块：负责生成针对特定任务的行动指令和文本。\nEvaluator模块：评估Actor的输出，保证行动的适宜性和效果。\nSelf-Reflection模块：反馈生成部分，通过深入分析和总结Actor的表现，指导未来的改进。\nReflexion框架的轻量级设计和对详细反馈的处理能力使其在多个高级任务中表现优异，包括编程、推理和复杂决策。实际应用显示，该框架能显著提升任务处理的正确率和效率。\n思考的快与慢。 在《思考，快与慢》一书中，丹尼尔·卡尼曼提出了人类思考的两种模式：“系统1”和“系统2”。系统1是快速、本能和情感性的，而系统2则更为缓慢、深思熟虑和逻辑性。这两种系统在处理不同类型的记忆和决策时发挥着不同的作用。\nAgent记忆 记忆是人类认知功能的核心，使我们能够从经验中学习并避免重复错误。在人工智能领域，尤其是在开发智能代理（Agent）时，构建有效的记忆系统也显得尤为重要。\nAI系统，尤其是依赖于大型语言模型的Agent，常面临有限的上下文记忆容量问题。这限制了它们在处理持续对话和复杂任务中的表现。这些系统通常受到有限上下文长度的制约，这限制了它们在处理连续对话或复杂任务时的效率。例如，一个没有有效记忆机制的Agent可能会重复犯同样的错误，或者忘记初始的任务目标。\n尽管现代LLMs的上下文长度已经可以处理超过100,000个标记，试图将所有对话历史和相关记忆直接置于模型的上下文中既不经济也不高效。因此，开发一个专门的记忆系统，如检索增强生成（Retrieval-Augmented Generation, RAG）成为提升Agent性能的有效手段。\n如果要为AI构建高效的记忆系统，借鉴和参考人类的记忆系统是必不可少的。人类的记忆系统包含多种类型的记忆，每种记忆都有其独特的生物学基础和功能，这些记忆类型不仅支持我们的日常活动，还是我们进行复杂决策和学习的基础。\n通过了解人类记忆的工作原理，我们可以设计出能够更好地模拟人类认知过程的AI系统。这不仅可以提高机器的任务执行能力，还可以使机器更加自主地适应复杂的环境和需求。\n根据维基百科的定义，人类的记忆系统可细分成：\n工作记忆：支持我们处理日常任务和进行决策。\n工具记忆：帮助我们形成习惯和自动化反应。\n语义记忆：允许我们累积知识和理解世界。\n情节记忆：使我们能够回忆过去的经历和事件。\n动作记忆：通过练习改善我们的物理技能和运动。\n遗忘的机制和认知卸载。记忆的形成经历过滤、压缩、遗忘和重组等环节。在这个过程中，大脑会删除大部分接收到的信息，只保留对当前活动或未来决策最关键的部分。\n建议阅读：\nA Survey on the Memory Mechanism of Large Language Model based Agents\nAgent的工具调用 我们更希望能帮助我们干活的agent 而不是一个只会说话的LLMs。换句话说人实际上并不需要一个聊天的对象，而是帮助解决问题的助手\n如何让LLMs具备工具调用的能力。 根据当前的主要工作， 主要是以openAI 的tool-using 的工作为主（也就是之前的function calling）。\n一句话解释：function calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\n具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\n更详细的tool-using内容，可以查看之前的博文：Agent学习笔记：OpenAI Function Calling完全指南\n除了tool-using意外， 比较重要的工具还有：code interpreter， 在代码开发的场景下， code interpreter充当了RL中的Env角色，让agent能够根据环境的反馈做出对应决策，而不只是生成代码。 除了openAI所提供的code interpreter外， 还有一些开源的code interpreter，如Open Interpreter 可以使用。\n从单Agent到多agent协作 来自斯坦福的虚拟小镇，因其独特的话题性而被广泛讨论。\nGenerative Agents: Interactive Simulacra of Human Behavior\nMetaGPT 在MetaGPT看来，可以将智能体想象成环境中的数字人，其中\n智能体 = 大语言模型（LLM） + 观察 + 思考 + 行动 + 记忆\n多智能体系统可以视为一个智能体社会，其中\n多智能体 = 智能体 + 环境 + 标准流程（SOP） + 通信 + 经济\nCode = SOP(Team) 是核心哲学。我们将SOP具象化，并且用于LLM构成的团队\n一些讨论 LLMs真的具备逻辑推理能力吗 何为推理（reasoning）：是对信息进行处理和推断，形成判断或结论的过程；**推理通常是一个包含多个推断步骤的过程。推理通常被认为需要抽象思维——也就是说，推理的能力不局限于具体的例子，而是更为普遍的。**LLMs的推理能力似乎是”涌现“的结果，而非专门针对性训练，所以LLMs是否真正具备推理能力？从一些结果表明，LLMs似乎是在背答案而非真正的推理。\n在 Can Large Language Models Reason? 这篇文章中作者认为CoT过程中LLM可能是在背答案：\nGPT-4 solved 10 out of 10 problems that had been published before 2021 (GPT-4’s pre-training cutoff date) and zero out of 10 problems that had been published after 2021. 不过，我还是持保留意见的， 作者所观察到的现象和人类很像，更擅长在自己熟悉的领域，因为存在大量的记忆，记忆间存在大量“近道”（shortcut）用于加速推导过程，甚至直接给出答案；所要做的让LLMs去执行“慢思考”而不是“快思考”。\n如果从transformer 的原理上讨论， 一个自回归的模型本身是和推理，逻辑甚至是智能无缘的。\n但人类的语言中， 天生具备了逻辑，或者人类的语言中融入的人类本身的逻辑。那么通过模拟语言的生成，一定程度也能恢复出语言背后的逻辑。\nAgent真的只是商业上的吹捧或者泡沫吗 泡沫的成份一定是有点。但也是未来十年值得做的东西。\n如果把LLM类比成CPU的话， 一个完整的计算机系统，除了CPU以外， 还需要内存，控制，输入输出等组件。不可能让LLM实现所有功能，这并不现实。但通过一定合理的架构，工作流的编排，能够大大提升LLM的准确性，进而加快落地实现。\n传统行业要如何在agent上发力 所有公司都有SOP，和大量存在在员工脑子中的“隐性知识”。 这些“知识”只有转换成agent的工作流（思维方式），或者是LLM的system prompt，才会变得更有价值和意义。现阶段有很多公司在做所谓的知识管理， 本质上是用公司的内部文本作为RAG系统而已，但一个问答系统很难量化出收益，并且对于使用者的要求挺高的。 实际上我认为，发力点应该是将内部人类可使用的工具逐渐API化，标准化以方便agent的工具调用，然后定义智能代理在各种业务场景中的具体任务和行动。这包括从简单的数据检索到复杂的决策制定\nAgent应该从事简单重复劳动还是具有创造性的探索工作 这题让GPT-4来回答吧：\n关于“Agent应该从事简单重复劳动还是具有创造性的探索工作”的问题，我们可以从多个角度进行分析：\n经济效益：从经济角度考虑，如果简单重复的劳动可以被自动化或由机器人执行，这将大幅度降低成本，并提高效率。企业通常会选择自动化这类工作以减少人力成本。然而，对于创造性的探索工作，由于需要不断地创新和解决新问题，这种类型的工作更可能需要人类智能的参与，因此，这种工作对于推动技术进步和经济增长具有重要价值。\n技术发展：随着AI和机器学习技术的发展，越来越多的简单劳动可以被智能系统所取代。例如，数据输入、监控等任务已被自动化系统广泛接管。然而，对于需要高度创造性和判断力的工作，如科学研究、艺术创作、复杂决策制定等，目前的技术还无法完全替代人类。\n人类福祉：从心理学的角度来看，长期从事简单重复的劳动可能导致工作满意度下降，影响心理健康。相反，创造性工作通常能带来更高的个人成就感和职业满意度。因此，鼓励Agent从事创造性探索工作可能更有助于提升整体福祉。\n社会需求和未来趋势：社会对创新和创造的需求日益增加，这种趋势推动了对创造性职业的需求。例如，新兴技术、可持续发展解决方案和个性化产品等领域，都需要大量的创新思维和创造力。因此，社会和市场需求可能更倾向于从事具有创造性的探索工作。\n综上所述，虽然从事简单重复的劳动在某些情况下具有其经济和效率上的优势，但从长远和全面的角度来看，鼓励Agent从事具有创造性的探索工作似乎更具前瞻性和可持续性。这不仅有助于经济和技术的发展，还能改善工作者的心理福祉和职业满意度。\nAgent在制造业良率分析上如何应用 以Yield分析和提升为例，在进行Yield提升和案例分析时，存在大量重复的分析和判断工作，这些可以通过AI进行加速和辅助，而不需人工逐一检查。\n工程师每天可能需要处理超过100个与良率相关的预警。其中约40%可能是由于测试问题、设备故障或是之前已经标注过的案例。这些都需要工程师逐一检查。由于人力有限，他们只能优先审查良率特别低的情况，可能会错过一些潜在的问题。通过使用特定规则和AI Agent，可以自动检查这些数据并作出判断。\n在处理实际案例时，工程师通常会根据案例特征在知识库中查询处理方法，这一过程也可以由Agent自动完成，并缩小后续检查的范围。如果需要进一步分析，工程师会调用不同的工具用于检查，这些分析也可以通过Agent来执行。如果仍无法找到问题，可以提醒人工介入。当Agent成功分析出结果时，还会将分析过程和结果整理成图文并茂的报告，供工程师检查和使用。一旦这些分析被人工审核并采用，它们就会成为高质量的知识，自动加入知识库中供下次使用。\nReading LLM Powered Autonomous Agents\nA busy person’s Intro to AI Agents\n基于大语言模型的AI Agents—Part 1\nChain of Thought\nTree of Thoughts\nGraph of Thoughts\nReAct\nReflexion\nMetaGPT: Meta Programming for Multi-Agent Collaborative Framework\nGenerative Agents: Interactive Simulacra of Human Behavior\nLLM Agents Papers\nAI Agent \u0026amp; 大模型经典论文推荐\nAI Agents are disrupting automation: Current approaches, market solutions and recommendations\nAGI-Edgerunners-LLM-Agents-Papers\nThe Rise and Potential of Large Language Model Based Agents: A Survey\nAwesome AI Agents\nLarge Language Model based Multi-Agents: A Survey of Progress and Challenges\nThe Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n","permalink":"https://niraya666.github.io/posts/%E5%9F%BA%E4%BA%8E%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84-agent%E7%A7%91%E6%99%AE%E5%90%91/","summary":"写在开头 本文是基于最近组内技术交流的文字稿整理。\nWhat is Agent？ 在探讨复杂的人工智能技术之前，让我们先回顾一下生活中的一个简单例子：扫地机器人。这种智能设备在房间中自主导航，避开障碍物，寻找最有效的清洁路径。它就是一个现实生活中的Agent——一个可以自主决策和行动的实体。\n在人工智能领域，Agent指的是任何可以感知其环境并根据感知结果做出决策的实体。这些决策旨在完成某些任务或达成特定的目标。Agent的行为可以简单如游戏里的机器人，也可以复杂如自动驾驶汽车。\n开始于强化学习 在强化学习中， 我们往往能见到agent的概念。强化学习是一种机器学习方法，它教导Agent通过试错法找到最佳行动路径。就像训练小狗一样，我们通过奖励来引导Agent做出正确的决策。Agent的目标是在与环境的交互中寻找最优策略。理想情况下，如果Agent能够获取足够多的真实环境数据，它就能找到最佳解决方案。然而，由于真实环境的复杂性，完全模拟真实世界是不现实的。\n目前，强化学习主要适用于环境简单、问题定义明确的场景，如围棋或视频游戏。这种方法在虚拟环境中通过大量试错来探索解决方案，这种方法虽然有效，但缺乏灵活性和高效性。与人类学习新技能的方式相比，强化学习的效率远低。人们通常通过少量的尝试就能迅速掌握新技能，而强化学习可能需要成千上万次的试错。\n这时候，如果agent具有大脑就好了。\n将LLMs作为大脑: 赋能智能Agent的关键技术 相较于基于强化学习的Agent，人类的优势在于我们天生具备的记忆能力和逻辑判断能力，甚至包括反思和从经验中学习的能力。这些能力使得我们能够通过极少的试错迅速适应和掌握新技能。\n语言模型（LLMs）为AI领域带来了革命性的变化。LLMs通过其深度学习的新范式，以及在思维链和自然语言理解方面的强大能力，预示着Agent将拥有更强大的学习和迁移能力。这种能力的提升将使得创建广泛应用且实用的Agent成为可能。\n虽然LLM是否真正具备了推理能力仍然存疑，但LLM的出现无疑改变了很多。以COT（Chain of Thought）为例，这种模型通过打印解题的中间步骤，加强了在数学和逻辑推理方面的能力，减少了幻觉的出现。这一突破性的工作揭示了LLM在不同场景下的强大\u0026quot;in-context learning\u0026quot;能力，以及其在未经微调的情况下的泛化能力。\n将LLM视为AI Agent的大脑，为自动化系统提供了一种全新的构思方式。这种基于LLM的agent系统综合了规划、记忆、工具使用和行动的能力，通过API调用与外部世界互动，显示出了前所未有的灵活性和效率。\n根据LLM Powered Autonomous Agents一文对LLM-based agent系统的定义， agent需要具备的基本能力：规划+记忆+工具+行动。\n规划能力：将复杂的任务分解成小任务， 并管理每一个子任务的目标， 并从过去的失败中反思，以吸取经验。\n记忆：LLM的上下文长度有限， 通过额外的记忆系统以提升LLM作为大脑的能力。\n工具调用\u0026amp;行动：LLM通过API调用的方式，执行任务， 与外界交互，而不是只是输出文字。\n探索AI代理的独特能力：人类与单一LLM无法比拟 AI系统的主要优势在于它们的规模和效率。这些系统能够执行以下任务，超越人类能力：\n大规模数据处理：AI能够高效地分析和处理超出人类理解范围的数据量。\n无需休息的持续操作：AI系统可以不间断地运行，而无需像人类那样休息和恢复。\n超快速计算：AI可以迅速执行复杂的计算，处理速度和效率远超人类。\nAI代理与单一LLM的不同:\n根据Andrew Ng在讲座中分享的内容，使用相对“简单”模型的代理工作流程（例如GPT-3.5）在实际应用中往往能够超越使用“智能”模型（如GPT-4）的零次提示。这说明在特定场景下，选择适当的AI模型和策略可能比单一的高级模型更有效。\nAI代理在决策制定中的应用也显示出其独特的优势。它们可以在没有情感偏见的情况下，基于大量数据做出快速且精确的决策。这种能力在需要快速响应和高精确度的领域尤为重要，如金融交易和紧急响应系统。\nAgent的规划和思维过程 AI Agent在处理复杂任务时，通过将大任务分解成小任务来提高效率。此外，自我反思能力允许Agent从过去的行动中学习，通过评估过去的决策来改善未来的表现。\nCoT，Chain of Thought， Wei et al. 2022。 即“思维链”，是一种使Agent逐步思考的方法。它通过要求模型展示解决问题的中间步骤来加强其逻辑推理能力，从而提高决策的质量和准确性。\nTree of Thoughts， ToT (Yao et al. 2023)\n尽管语言模型在许多领域表现出色，但在需要复杂规划和全局决策的任务上，它们的能力受到了限制。ToT框架应运而生，旨在通过一个结构化的思考和评估过程来弥补这一缺陷。\nToT框架借鉴了人类心理学中的双系统决策理论，通过整合快速直觉判断和慢速深思熟虑的决策过程，极大地提升了模型的决策能力。这一框架通过自我评估的方式，允许模型在面对多种可能的决策路径时，能够进行有效的选择和全局优化。\nToT框架旨在克服现有语言模型在处理需要复杂规划或搜索任务的局限。它通过结构化的思想树来探索和评估不同的决策路径。ToT允许模型在考虑多个推理路径时自我评估其选择，以做出最佳的决策。此外，ToT结合了语言生成和搜索算法（如BFS和DFS），使模型能够在进行决策时前瞻和回溯，以实现全局最优选择。\nprompt example：\ncot_prompt = \u0026#39;\u0026#39;\u0026#39; Write a coherent passage of 4 short paragraphs.","title":"基于大语言模型的 Agent：科普向"},{"content":"写在最开始 当我们在讨论基于大型语言模型（LLM-based）的智能代理（agent）时，我们究竟在谈论什么？根据Lilian W在其文章《LLM Powered Autonomous Agents》中的讨论，一个智能代理需要具备几个核心能力：规划（Planning）、记忆（Memory）、以及工具使用（Tool use）。特别地，工具使用方面的进展，得益于OpenAI在API中提供的function calling功能，为我们开启了新的可能性。\nOpenAI function calling，作为智能代理与外部工具交互的基本方式，对于每位从业者来说都是必备技能。随着技术的发展，我们期望的不只是能与我们对话的LLM，而是能够辅助我们使用各种工具、做出决策的智能伙伴。\n不过需要特别指出的是，最近OpenAI在Chat Completions API中已经废弃了“函数（function）”的使用，转而采用“工具（tool）”。这一变更旨在拓宽LLM集成的功能范围，为更复杂的交互模式铺平道路，如构建能够相互作用的多代理系统。\n尽管如此，由于语言习惯的原因，本文中仍然会使用function calling的术语来描述OpenAI的tool using功能，因为“function calling”的说法已经深入人心了。\n核心内容概览\nFunction Calling的定义：解释什么是function calling，以及它在智能代理工作中的作用。\nOpenAI Cookbook示例：提供实际的function calling示例，帮助读者理解其在实际应用中的用途。\n开源LLM的Tool Using：探索如何在开源大型语言模型中实现工具使用，以及LLM在tool using的时候经历了什么。\n评价与训练：讨论如何评价开源模型的工具使用能力，以及如何训练LLM进行有效的工具使用。\n鉴于整理笔记的速度远赶不上更新的速度， 会将第四部份作为单独的部分整理。\n何为function calling 一句话解释：function calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\n具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\nfunction calling的应用范围广泛，如\n创建智能助手：通过调用外部API回答问题。\n转换指令：将自然语言指令转换成API调用指令。\n数据提取：从文本中提取结构化数据。\nfunction calling的过程涵盖了从定义函数集、通过模型生成遵循自定义模式的JSON对象字符串，到在代码中解析这个字符串并调用相应函数的全过程。这一连串操作不仅自动化了交互过程，还确保了执行操作的安全性和准确性。\n一些常见的问题 JSON mode json mode 和tool-using 有什么关系？有了json mode 还需要用到tool-using吗？\n从json mode 的本质， 更多的是在system prompt 增加一句类似“请以json格式输出”之类的话，然后在LLM输出时增加json结果检查和格式转换。在使用时只需要在client.chat.completions.create 中 增加response_format={ \u0026quot;type\u0026quot;: \u0026quot;json_object\u0026quot; } 即可。\n那么json mode 什么时候会用到呢？一般在做文本提取，内容提取时可以使用；以RAG场景为例， 当我们希望LLM能够帮我们对用户的query进行改写时，我们肯定是希望模型能够返回干净的json格式改写结果，这样的结果可以直接使用，而不是在模型输出一些内容后，如：\n\u0026#34;\u0026#34;\u0026#34; 好的，以下是我的改写内容： ``` real-rewrite-query ``` \u0026#34;\u0026#34;\u0026#34; 其中包含了一些模型喜欢输出的客套话，此时我们需要通过正则匹配等方法将真正希望使用内容提取出来。而这时候json mode可以直接输出需要的内容， 而跳过了额外的提取步骤。在json mode 出现之前，这样的处理我们也尝试使用过tool-using 的模式，但有点大材小用了。\n很显然， tool-using的真正强大之处并不只是对输出格式进行处理， 而是能够让模型从提供的多个tools中选择需要使用的。\n如何使用OpenAI function calling 天气查询的简单示例 环境配置\n首先，我们需要安装一些必要的Python库。这些库将帮助我们与OpenAI的API进行交互，以及完成一些辅助功能。\n!pip install scipy --quiet !pip install tenacity --quiet !pip install tiktoken --quiet !pip install termcolor --quiet !pip install openai --quiet os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = \u0026#34;...\u0026#34; from openai import OpenAI from tenacity import retry, wait_random_exponential, stop_after_attempt from termcolor import colored client = OpenAI() 工具函数定义\n接下来，我们定义一些实用函数，这些函数旨在方便我们向Chat Completions API发送请求，并管理与跟踪对话状态。\n@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3)) def chat_completion_request(messages, tools=None, tool_choice=None, model=\u0026#34;gpt-3.5-turbo\u0026#34;): try: response = client.chat.completions.create( model=model, messages=messages, tools=tools, tool_choice=tool_choice, ) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e def pretty_print_conversation(messages): role_to_color = { \u0026#34;system\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;function\u0026#34;: \u0026#34;magenta\u0026#34;, } for message in messages: if message[\u0026#34;role\u0026#34;] == \u0026#34;system\u0026#34;: print(colored(f\u0026#34;system: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;user\u0026#34;: print(colored(f\u0026#34;user: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;assistant\u0026#34; and message.get(\u0026#34;function_call\u0026#34;): print(colored(f\u0026#34;assistant: {message[\u0026#39;function_call\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;assistant\u0026#34; and not message.get(\u0026#34;function_call\u0026#34;): print(colored(f\u0026#34;assistant: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;function\u0026#34;: print(colored(f\u0026#34;function ({message[\u0026#39;name\u0026#39;]}): {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) 函数规范定义\n我们还需要创建一些函数规范，以接口化与假设的天气API的交互。\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;The temperature unit to use. Infer this from the users location.\u0026#34;, }, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;format\u0026#34;], }, } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_n_day_weather_forecast\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get an N-day weather forecast\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;The temperature unit to use. Infer this from the users location.\u0026#34;, }, \u0026#34;num_days\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The number of days to forecast\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;format\u0026#34;, \u0026#34;num_days\u0026#34;] }, } }, ] 对话示例\n当我们请求当前天气时， LLM会要求澄清问题，如地址等参数信息：\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like today\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message output:\nChatCompletionMessage(content=\u0026#39;Sure, may I know your current location?\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None) 当补充缺失的信息后， LLM将生成适当的函数参数\nmessages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;I\u0026#39;m in Shanghai, China.\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message output:\nChatCompletionMessage(content=None, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_VdqOOMp9pagf5ho39Y2HmYV4\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;San Francisco, CA\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;,\\n \u0026#34;num_days\u0026#34;: 4\\n}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)]) 完整流程如下：\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;what is the weather going to be like in Glasgow, Scotland over the next x days\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;in 5 days\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message 当然， 我们也可以强制要求使用特定的函数\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Give me a weather report for Toronto, Canada.\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools, tool_choice={\u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;get_n_day_weather_forecast\u0026#34;}} ) chat_response.choices[0].message output:\nChatCompletionMessage(content=None, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_0ldecDpV8Vdq8mGPoUewlue3\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Toronto, Canada\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;,\\n \u0026#34;num_days\u0026#34;: 1\\n}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)]) 并行函数调用\n对于一些特定模型，如**gpt-4-turbo-preview**, gpt-4-0125-preview, gpt-4-1106-preview, gpt-3.5-turbo-0125, and **gpt-3.5-turbo-1106**支持并行函数调用， 允许我们在单个回合中调用多个函数。\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;what is the weather going to be like in San Francisco and Glasgow over the next 4 days\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message.tool_calls assistant_message output:\n[ChatCompletionMessageToolCall(id=\u0026#39;call_tfl8eTCW64sHvHjiiatoYzku\u0026#39;, function=Function(arguments=\u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;San Francisco, CA\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;fahrenheit\u0026#34;, \u0026#34;num_days\u0026#34;: 4}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;), ChatCompletionMessageToolCall(id=\u0026#39;call_bAqj55RygP2Y1T85RHqgskku\u0026#39;, function=Function(arguments=\u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;Glasgow, UK\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;, \u0026#34;num_days\u0026#34;: 4}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)] 实现本地函数调用\n首先我们需要构造两个用于演示的假function\nimport json def get_current_weather(location, format=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Simulates getting the current weather for a given location. The response is hardcoded for demonstration purposes. Args: location (str): The city and state, e.g., San Francisco, CA. format (str, optional): The temperature unit to use. Defaults to \u0026#34;fahrenheit\u0026#34;. Returns: str: JSON string with the current weather data. \u0026#34;\u0026#34;\u0026#34; if \u0026#34;tokyo\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Partly Cloudy\u0026#34;}) elif \u0026#34;san francisco\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;San Francisco\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;72\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Sunny\u0026#34;}) elif \u0026#34;paris\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;Paris\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Rainy\u0026#34;}) else: return json.dumps({\u0026#34;location\u0026#34;: location, \u0026#34;temperature\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Data Unavailable\u0026#34;}) def get_n_day_weather_forecast(location, num_days, format=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Simulates getting an N-day weather forecast for a given location. The response is hardcoded for demonstration purposes. Args: location (str): The city and state, e.g., San Francisco, CA. num_days (int): The number of days to forecast. format (str, optional): The temperature unit to use. Defaults to \u0026#34;fahrenheit\u0026#34;. Returns: str: JSON string with the N-day weather forecast data. \u0026#34;\u0026#34;\u0026#34; # This example just returns a fixed response regardless of the input. # In a real scenario, the response would depend on the location, num_days, and format. forecast = [ {\u0026#34;day\u0026#34;: 1, \u0026#34;temperature\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Sunny\u0026#34;}, {\u0026#34;day\u0026#34;: 2, \u0026#34;temperature\u0026#34;: \u0026#34;18\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Cloudy\u0026#34;}, {\u0026#34;day\u0026#34;: 3, \u0026#34;temperature\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Rainy\u0026#34;} ] # Return only the forecast for the requested number of days. return json.dumps(forecast[:num_days]) available_functions = { \u0026#34;get_current_weather\u0026#34;: get_current_weather, \u0026#34;get_n_day_weather_forecast\u0026#34;: get_n_day_weather_forecast, } 尝试请求天气\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like in Tokyo!\u0026#34;}) chat_response = chat_completion_request(messages, tools=tools, tool_choice=\u0026#34;auto\u0026#34;) assistant_message = chat_response.choices[0].message assistant_message = json.loads(assistant_message.model_dump_json()) assistant_message[\u0026#34;content\u0026#34;] = str(assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;]) #a temporary patch but this should be handled differently # remove \u0026#34;function_call\u0026#34; from assistant message del assistant_message[\u0026#34;function_call\u0026#34;] assistant_message \u0026#34;\u0026#34;\u0026#34; {\u0026#39;content\u0026#39;: \u0026#39;{\\\u0026#39;arguments\\\u0026#39;: \\\u0026#39;{\\\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\\\n}\\\u0026#39;, \\\u0026#39;name\\\u0026#39;: \\\u0026#39;get_current_weather\\\u0026#39;}\u0026#39;, \u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;tool_calls\u0026#39;: [{\u0026#39;id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;function\u0026#39;: {\u0026#39;arguments\u0026#39;: \u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\n}\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;}, \u0026#39;type\u0026#39;: \u0026#39;function\u0026#39;}]} \u0026#34;\u0026#34;\u0026#34; messages.append(assistant_message) 可以看到，LLM返回了json格式的参数信息（其实是str）以及需要调用的function名称， 拥有这些信息之后就可以调用之前定义的函数了\n# get the weather information to pass back to the model function_name_to_call = assistant_message[\u0026#39;tool_calls\u0026#39;][0][\u0026#39;function\u0026#39;][\u0026#39;name\u0026#39;] function_arguments = assistant_message[\u0026#39;tool_calls\u0026#39;][0][\u0026#39;function\u0026#39;][\u0026#39;arguments\u0026#39;] weather = available_functions[function_name_to_call](function_arguments) 将函数执行结果和对话历史返回给LLM\nmessages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;], \u0026#34;content\u0026#34;: weather}) messages \u0026#34;\u0026#34;\u0026#34; [{\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#34;What\u0026#39;s the weather like in Tokyo!\u0026#34;}, {\u0026#39;content\u0026#39;: \u0026#39;{\\\u0026#39;arguments\\\u0026#39;: \\\u0026#39;{\\\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\\\n}\\\u0026#39;, \\\u0026#39;name\\\u0026#39;: \\\u0026#39;get_current_weather\\\u0026#39;}\u0026#39;, \u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;tool_calls\u0026#39;: [{\u0026#39;id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;function\u0026#39;: {\u0026#39;arguments\u0026#39;: \u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\n}\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;}, \u0026#39;type\u0026#39;: \u0026#39;function\u0026#39;}]}, {\u0026#39;role\u0026#39;: \u0026#39;tool\u0026#39;, \u0026#39;tool_call_id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;fahrenheit\u0026#34;}\u0026#39;}] \u0026#34;\u0026#34;\u0026#34; 最终输出结果（for user）\nfinal_response = chat_completion_request(messages, tools=tools) final_response.choices[0].message.content output：\n\u0026#39;The current weather in Tokyo is partly cloudy with a temperature of 10°C (50°F).\u0026#39; 通过function calling 实现sql执行 下载演示所用的sqlite数据\n!wget https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip -O chinook.zip !unzip chinook.zip 连接数据库，和定义一些function\nimport sqlite3 conn = sqlite3.connect(\u0026#34;/content/chinook.db\u0026#34;) print(\u0026#34;Opened database successfully\u0026#34;) def get_table_names(conn): \u0026#34;\u0026#34;\u0026#34;Return a list of table names.\u0026#34;\u0026#34;\u0026#34; table_names = [] tables = conn.execute(\u0026#34;SELECT name FROM sqlite_master WHERE type=\u0026#39;table\u0026#39;;\u0026#34;) for table in tables.fetchall(): table_names.append(table[0]) return table_names def get_column_names(conn, table_name): \u0026#34;\u0026#34;\u0026#34;Return a list of column names.\u0026#34;\u0026#34;\u0026#34; column_names = [] columns = conn.execute(f\u0026#34;PRAGMA table_info(\u0026#39;{table_name}\u0026#39;);\u0026#34;).fetchall() for col in columns: column_names.append(col[1]) return column_names def get_database_info(conn): \u0026#34;\u0026#34;\u0026#34;Return a list of dicts containing the table name and columns for each table in the database.\u0026#34;\u0026#34;\u0026#34; table_dicts = [] for table_name in get_table_names(conn): columns_names = get_column_names(conn, table_name) table_dicts.append({\u0026#34;table_name\u0026#34;: table_name, \u0026#34;column_names\u0026#34;: columns_names}) return table_dicts 获取db 的schema\ndatabase_schema_dict = get_database_info(conn) database_schema_string = \u0026#34;\\n\u0026#34;.join( [ f\u0026#34;Table: {table[\u0026#39;table_name\u0026#39;]}\\nColumns: {\u0026#39;, \u0026#39;.join(table[\u0026#39;column_names\u0026#39;])}\u0026#34; for table in database_schema_dict ] ) 定义tools工具列表\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ask_database\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Use this function to answer user questions about music. Input should be a fully formed SQL query.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; SQL query extracting info to answer the user\u0026#39;s question. SQL should be written using this database schema: {database_schema_string} The query should be returned in plain text, not in JSON. \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, } } ] 定义用于执行sql的function\ndef ask_database(conn, query): \u0026#34;\u0026#34;\u0026#34;Function to query SQLite database with a provided SQL query.\u0026#34;\u0026#34;\u0026#34; try: results = str(conn.execute(query).fetchall()) except Exception as e: results = f\u0026#34;query failed with error: {e}\u0026#34; return results def execute_function_call(message): if message.tool_calls[0].function.name == \u0026#34;ask_database\u0026#34;: query = json.loads(message.tool_calls[0].function.arguments)[\u0026#34;query\u0026#34;] results = ask_database(conn, query) else: results = f\u0026#34;Error: function {message.tool_calls[0].function.name} does not exist\u0026#34; return results messages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Answer user questions by generating SQL queries against the Chinook Music Database.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi, who are the top 5 artists by number of tracks?\u0026#34;}) chat_response = chat_completion_request(messages, tools) assistant_message = chat_response.choices[0].message messages.append(assistant_message) if assistant_message.tool_calls: results = execute_function_call(assistant_message) messages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message.tool_calls[0].id, \u0026#34;name\u0026#34;: assistant_message.tool_calls[0].function.name, \u0026#34;content\u0026#34;: results}) 此时的messages\n[{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Answer user questions by generating SQL queries against the Chinook Music Database.\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Hi, who are the top 5 artists by number of tracks?\u0026#39;}, ChatCompletionMessage(content=\u0026#39;Function(arguments=\\\u0026#39;{\\\\n \u0026#34;query\u0026#34;: \u0026#34;SELECT artists.Name, COUNT(tracks.TrackId) AS num_tracks FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY artists.ArtistId ORDER BY num_tracks DESC LIMIT 5\u0026#34;\\\\n}\\\u0026#39;, name=\\\u0026#39;ask_database\\\u0026#39;)\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_vSLysEQncbGvMgXGhttIow6v\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;query\u0026#34;: \u0026#34;SELECT artists.Name, COUNT(tracks.TrackId) AS num_tracks FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY artists.ArtistId ORDER BY num_tracks DESC LIMIT 5\u0026#34;\\n}\u0026#39;, name=\u0026#39;ask_database\u0026#39;), type=\u0026#39;function\u0026#39;)]), {\u0026#39;role\u0026#39;: \u0026#39;tool\u0026#39;, \u0026#39;tool_call_id\u0026#39;: \u0026#39;call_vSLysEQncbGvMgXGhttIow6v\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;ask_database\u0026#39;, \u0026#39;content\u0026#39;: \u0026#34;[(\u0026#39;Iron Maiden\u0026#39;, 213), (\u0026#39;U2\u0026#39;, 135), (\u0026#39;Led Zeppelin\u0026#39;, 114), (\u0026#39;Metallica\u0026#39;, 112), (\u0026#39;Deep Purple\u0026#39;, 92)]\u0026#34;}] 最终输出：\nfinal_response = chat_completion_request(messages, tools=tools) final_response.choices[0].message.content # output \u0026#34;\u0026#34;\u0026#34; The top 5 artists by number of tracks are: 1. Iron Maiden - 213 tracks 2. U2 - 135 tracks 3. Led Zeppelin - 114 tracks 4. Metallica - 112 tracks 5. Deep Purple - 92 tracks \u0026#34;\u0026#34;\u0026#34; 基于function calling构建智能代理：自动获取与深度解析arXiv学术文章 在这一节中将介绍如何构建一个能够从arXiv上查找论文， 下载分析并总结学术论文的Agent。这个Agent不仅可以帮助用户快速了解特定领域的最新研究动态，而且还能深入分析和总结选定文章的核心内容。\nAgent核心功能\n获取arXiv文章 get_articles 代理通过**arxiv**库搜索关于特定主题的文章，为用户提供简要的文章摘要和链接。\n阅读并总结文章 read_article_and_summarize 利用**PyPDF2**库读取选中文章的PDF文件，代理能够提炼出文章的主要论点、支撑证据和结论。\n环境配置\n!pip install scipy --quiet !pip install tenacity --quiet !pip install tiktoken==0.3.3 --quiet !pip install termcolor --quiet !pip install openai --quiet !pip install arxiv --quiet !pip install pandas --quiet !pip install PyPDF2 --quiet !pip install tqdm --quiet import os import arxiv import ast import concurrent import json import os import pandas as pd import tiktoken from csv import writer from IPython.display import display, Markdown, Latex from openai import OpenAI from PyPDF2 import PdfReader from scipy import spatial from tenacity import retry, wait_random_exponential, stop_after_attempt from tqdm import tqdm from termcolor import colored GPT_MODEL = \u0026#34;gpt-3.5-turbo-0613\u0026#34; EMBEDDING_MODEL = \u0026#34;text-embedding-ada-002\u0026#34; client = OpenAI() 所有下载的论文都被存储在本地**./data/papers目录中，并且每篇文章的详细信息（包括其嵌入向量）都记录在arxiv_library.csv**文件中。\ndirectory = \u0026#39;./data/papers\u0026#39; # Check if the directory already exists if not os.path.exists(directory): # If the directory doesn\u0026#39;t exist, create it and any necessary intermediate directories os.makedirs(directory) print(f\u0026#34;Directory \u0026#39;{directory}\u0026#39; created successfully.\u0026#34;) else: # If the directory already exists, print a message indicating it print(f\u0026#34;Directory \u0026#39;{directory}\u0026#39; already exists.\u0026#34;) # Set a directory to store downloaded papers data_dir = os.path.join(os.curdir, \u0026#34;data\u0026#34;, \u0026#34;papers\u0026#34;) paper_dir_filepath = \u0026#34;./data/arxiv_library.csv\u0026#34; # Generate a blank dataframe where we can store downloaded files df = pd.DataFrame(list()) df.to_csv(paper_dir_filepath) 我们将定义一些utils function用于：\n文章获取与存储：通过**get_articles**函数查询主题相关的文章，系统自动下载文章并记录重要信息及embedding向量。\n文章选择与内容提取：根据用户的查询，系统通过计算embedding向量的相似度来选择最相关的文章，并提取出文章的文本内容。\n内容分块与总结：长文本被分割成多个较小的块，每个块被独立总结。\n汇总总结：所有独立块的总结被汇总成一篇全面的总结，更好地回应用户的查询。\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def embedding_request(text): response = client.embeddings.create(input=text, model=EMBEDDING_MODEL) return response @retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def get_articles(query, library=paper_dir_filepath, top_k=5): \u0026#34;\u0026#34;\u0026#34;This function gets the top_k articles based on a user\u0026#39;s query, sorted by relevance. It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize. \u0026#34;\u0026#34;\u0026#34; client = arxiv.Client() search = arxiv.Search( query = query, max_results = top_k, sort_by = arxiv.SortCriterion.SubmittedDate ) result_list = [] for result in client.results(search): result_dict = {} result_dict.update({\u0026#34;title\u0026#34;: result.title}) result_dict.update({\u0026#34;summary\u0026#34;: result.summary}) # Taking the first url provided result_dict.update({\u0026#34;article_url\u0026#34;: [x.href for x in result.links][0]}) result_dict.update({\u0026#34;pdf_url\u0026#34;: [x.href for x in result.links][1]}) result_list.append(result_dict) # Store references in library file response = embedding_request(text=result.title) file_reference = [ result.title, result.download_pdf(data_dir), response.data[0].embedding, ] # Write to file with open(library, \u0026#34;a\u0026#34;) as f_object: writer_object = writer(f_object) writer_object.writerow(file_reference) f_object.close() return result_list def strings_ranked_by_relatedness( query: str, df: pd.DataFrame, relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), top_n: int = 100, ) -\u0026gt; list[str]: \u0026#34;\u0026#34;\u0026#34;Returns a list of strings and relatednesses, sorted from most related to least.\u0026#34;\u0026#34;\u0026#34; query_embedding_response = embedding_request(query) query_embedding = query_embedding_response.data[0].embedding strings_and_relatednesses = [ (row[\u0026#34;filepath\u0026#34;], relatedness_fn(query_embedding, row[\u0026#34;embedding\u0026#34;])) for i, row in df.iterrows() ] strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True) strings, relatednesses = zip(*strings_and_relatednesses) return strings[:top_n] def read_pdf(filepath): \u0026#34;\u0026#34;\u0026#34;Takes a filepath to a PDF and returns a string of the PDF\u0026#39;s contents\u0026#34;\u0026#34;\u0026#34; # creating a pdf reader object reader = PdfReader(filepath) pdf_text = \u0026#34;\u0026#34; page_number = 0 for page in reader.pages: page_number += 1 pdf_text += page.extract_text() + f\u0026#34;\\nPage Number: {page_number}\u0026#34; return pdf_text # Split a text into smaller chunks of size n, preferably ending at the end of a sentence def create_chunks(text, n, tokenizer): \u0026#34;\u0026#34;\u0026#34;Returns successive n-sized chunks from provided text.\u0026#34;\u0026#34;\u0026#34; tokens = tokenizer.encode(text) i = 0 while i \u0026lt; len(tokens): # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens j = min(i + int(1.5 * n), len(tokens)) while j \u0026gt; i + int(0.5 * n): # Decode the tokens and check for full stop or newline chunk = tokenizer.decode(tokens[i:j]) if chunk.endswith(\u0026#34;.\u0026#34;) or chunk.endswith(\u0026#34;\\n\u0026#34;): break j -= 1 # If no end of sentence found, use n tokens as the chunk size if j == i + int(0.5 * n): j = min(i + n, len(tokens)) yield tokens[i:j] i = j def extract_chunk(content, template_prompt): \u0026#34;\u0026#34;\u0026#34;This function applies a prompt to some input content. In this case it returns a summarized chunk of text\u0026#34;\u0026#34;\u0026#34; prompt = template_prompt + content response = client.chat.completions.create( model=GPT_MODEL, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt}], temperature=0 ) return response.choices[0].message.content def summarize_text(query): \u0026#34;\u0026#34;\u0026#34;This function does the following: - Reads in the arxiv_library.csv file in including the embeddings - Finds the closest file to the user\u0026#39;s query - Scrapes the text out of the file and chunks it - Summarizes each chunk in parallel - Does one final summary and returns this to the user\u0026#34;\u0026#34;\u0026#34; # A prompt to dictate how the recursive summarizations should approach the input paper summary_prompt = \u0026#34;\u0026#34;\u0026#34;Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\u0026#34;\u0026#34;\u0026#34; # If the library is empty (no searches have been performed yet), we perform one and download the results library_df = pd.read_csv(paper_dir_filepath).reset_index() if len(library_df) == 0: print(\u0026#34;No papers searched yet, downloading first.\u0026#34;) get_articles(query) print(\u0026#34;Papers downloaded, continuing\u0026#34;) library_df = pd.read_csv(paper_dir_filepath).reset_index() library_df.columns = [\u0026#34;title\u0026#34;, \u0026#34;filepath\u0026#34;, \u0026#34;embedding\u0026#34;] library_df[\u0026#34;embedding\u0026#34;] = library_df[\u0026#34;embedding\u0026#34;].apply(ast.literal_eval) strings = strings_ranked_by_relatedness(query, library_df, top_n=1) print(\u0026#34;Chunking text from paper\u0026#34;) pdf_text = read_pdf(strings[0]) # Initialise tokenizer tokenizer = tiktoken.get_encoding(\u0026#34;cl100k_base\u0026#34;) results = \u0026#34;\u0026#34; # Chunk up the document into 1500 token chunks chunks = create_chunks(pdf_text, 1500, tokenizer) text_chunks = [tokenizer.decode(chunk) for chunk in chunks] print(\u0026#34;Summarizing each chunk of text\u0026#34;) # Parallel process the summaries with concurrent.futures.ThreadPoolExecutor( max_workers=len(text_chunks) ) as executor: futures = [ executor.submit(extract_chunk, chunk, summary_prompt) for chunk in text_chunks ] with tqdm(total=len(text_chunks)) as pbar: for _ in concurrent.futures.as_completed(futures): pbar.update(1) for future in futures: data = future.result() results += data # Final summary print(\u0026#34;Summarizing into overall summary\u0026#34;) response = client.chat.completions.create( model=GPT_MODEL, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;\u0026#34;\u0026#34;Write a summary collated from this collection of key points extracted from an academic paper. The summary should highlight the core argument, conclusions and evidence, and answer the user\u0026#39;s query. User query: {query} The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions. Key points:\\n{results}\\nSummary:\\n\u0026#34;\u0026#34;\u0026#34;, } ], temperature=0, ) return response 实现一个**Conversation**类，用于支持与API进行多轮对话\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def chat_completion_request(messages, functions=None, model=GPT_MODEL): try: response = client.chat.completions.create( model=model, messages=messages, functions=functions, ) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e class Conversation: def __init__(self): self.conversation_history = [] def add_message(self, role, content): message = {\u0026#34;role\u0026#34;: role, \u0026#34;content\u0026#34;: content} self.conversation_history.append(message) def display_conversation(self, detailed=False): role_to_color = { \u0026#34;system\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;function\u0026#34;: \u0026#34;magenta\u0026#34;, } for message in self.conversation_history: print( colored( f\u0026#34;{message[\u0026#39;role\u0026#39;]}: {message[\u0026#39;content\u0026#39;]}\\n\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]], ) ) 完成以上基本工作， 接下来是agent的核心内容\n定义工具列表 tools\narxiv_functions = [ { \u0026#34;name\u0026#34;: \u0026#34;get_articles\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Use this function to get academic papers from arXiv to answer user questions.\u0026#34;\u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; User query in JSON. Responses should be summarized and should include the article URL reference \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, }, { \u0026#34;name\u0026#34;: \u0026#34;read_article_and_summarize\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Use this function to read whole papers and provide a summary for users. You should NEVER call this function before get_articles has been called in the conversation.\u0026#34;\u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; Description of the article in plain text based on the user\u0026#39;s query \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, } ] 定义function用于工具调用\ndef chat_completion_with_function_execution(messages, functions=[None]): \u0026#34;\u0026#34;\u0026#34;This function makes a ChatCompletion API call with the option of adding functions\u0026#34;\u0026#34;\u0026#34; response = chat_completion_request(messages, functions) full_message = response.choices[0] if full_message.finish_reason == \u0026#34;function_call\u0026#34;: print(f\u0026#34;Function generation requested, calling function\u0026#34;) return call_arxiv_function(messages, full_message) else: print(f\u0026#34;Function not required, responding to user\u0026#34;) return response def call_arxiv_function(messages, full_message): \u0026#34;\u0026#34;\u0026#34;Function calling function which executes function calls when the model believes it is necessary. Currently extended by adding clauses to this if statement.\u0026#34;\u0026#34;\u0026#34; if full_message.message.function_call.name == \u0026#34;get_articles\u0026#34;: try: parsed_output = json.loads( full_message.message.function_call.arguments ) print(\u0026#34;Getting search results\u0026#34;) results = get_articles(parsed_output[\u0026#34;query\u0026#34;]) except Exception as e: print(parsed_output) print(f\u0026#34;Function execution failed\u0026#34;) print(f\u0026#34;Error message: {e}\u0026#34;) messages.append( { \u0026#34;role\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;name\u0026#34;: full_message.message.function_call.name, \u0026#34;content\u0026#34;: str(results), } ) try: print(\u0026#34;Got search results, summarizing content\u0026#34;) response = chat_completion_request(messages) return response except Exception as e: print(type(e)) raise Exception(\u0026#34;Function chat request failed\u0026#34;) elif ( full_message.message.function_call.name == \u0026#34;read_article_and_summarize\u0026#34; ): parsed_output = json.loads( full_message.message.function_call.arguments ) print(\u0026#34;Finding and reading paper\u0026#34;) summary = summarize_text(parsed_output[\u0026#34;query\u0026#34;]) return summary else: raise Exception(\u0026#34;Function does not exist and cannot be called\u0026#34;) arXiv conversation, Start with a system message\npaper_system_message = \u0026#34;\u0026#34;\u0026#34;You are arXivGPT, a helpful assistant pulls academic papers to answer user questions. You summarize the papers clearly so the customer can decide which to read to answer their question. You always provide the article_url and title so the user can understand the name of the paper and click through to access it. Begin!\u0026#34;\u0026#34;\u0026#34; paper_conversation = Conversation() paper_conversation.add_message(\u0026#34;system\u0026#34;, paper_system_message) # Add a user message paper_conversation.add_message(\u0026#34;user\u0026#34;, \u0026#34;Hi, how does PPO reinforcement learning work?\u0026#34;) chat_response = chat_completion_with_function_execution( paper_conversation.conversation_history, functions=arxiv_functions ) assistant_message = chat_response.choices[0].message.content paper_conversation.add_message(\u0026#34;assistant\u0026#34;, assistant_message) display(Markdown(assistant_message)) output:\nFunction generation requested, calling function Getting search results Got search results, summarizing content I found several papers related to PPO reinforcement learning. Here are a few summaries: Title: \u0026#34;Bandit Profit-maximization for Targeted Marketing\u0026#34; Summary: This paper presents near-optimal algorithms for optimizing profit over multiple demand curves, which are dependent on different ancillary variables while maintaining the same price. It is relevant to PPO reinforcement learning as it tackles a sequential profit-maximization problem. Article URL: Link Title: \u0026#34;Inferring potential landscapes: A Schrödinger bridge approach to Maximum Caliber\u0026#34; Summary: This work extends Schrödinger bridges to account for integral constraints along paths, specifically in the context of Maximum Caliber, a Maximum Entropy principle applied in a dynamic context. While not directly related to PPO reinforcement learning, it can provide insights into stochastic dynamics and inference of time-varying potential landscapes. Article URL: Link Title: \u0026#34;a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification\u0026#34; Summary: This paper proposes an architecture-agnostic detection cost function (a-DCF) for evaluating spoofing-robust automatic speaker verification (ASV) systems. Although it does not focus on PPO reinforcement learning, it provides a metric for evaluating ASV systems in the presence of spoofing attacks. Article URL: Link These papers should provide insights into different aspects of reinforcement learning and related topics. Add another user message to induce our system to use the second tool\npaper_conversation.add_message( \u0026#34;user\u0026#34;, \u0026#34;Can you read the PPO sequence generation paper for me and give me a summary\u0026#34;, ) updated_response = chat_completion_with_function_execution( paper_conversation.conversation_history, functions=arxiv_functions ) display(Markdown(updated_response.choices[0].message.content)) output:\nFunction generation requested, calling function Finding and reading paper Chunking text from paper Summarizing each chunk of text 100%|██████████| 4/4 [00:04\u0026lt;00:00, 1.11s/it] Summarizing into overall summary Core Argument: The paper discusses the potential of using a general-purpose large language model (LLM) to learn the structural biophysics of DNA. The authors show that fine-tuning a LLM, specifically chatGPT 3.5-turbo, can enhance its ability to analyze and design DNA sequences and their structures. The study focuses on the formation of secondary structures in DNA, which are governed by base pairing and stacking bonds. The authors propose a method that involves chaining together models fine-tuned for subtasks and using a chain-of-thought approach to improve the model\u0026#39;s performance. Evidence: The authors use the NUPACK software suite to provide data for training and validation. The expert pipeline approach involves using models that have been fine-tuned for subtasks and feeding their outputs into each other. The models perform better when they explicitly consider the nearest neighbor window and the reverse complement of the sequences. The pipeline approach, where a separate model determines the reverse complement and feeds it to another model for secondary structure prediction, enhances the accuracy of the predictions. The performance of the models improves with larger training sets. Conclusions: The study demonstrates the potential of using LLMs to learn DNA structural biophysics. Integrating experimental data and machine learning is important in scientific research. The expert pipeline approach and breaking down the problem into smaller subtasks improve the performance of the models in DNA sequence analysis. The combination of chain-of-thought and model pipeline provides the best results in analysis tasks. The CoT approach, combined with the reverse complement transformation, yields the highest accuracy in design tasks. The addition of an error checking layer further improves accuracy in design tasks. Sequence design is more challenging than analysis, but error correction can compensate for the increased difficulty. Larger training sets benefit design tasks more. Future research directions include exploring chaining smaller models for performance improvement and using an LLM architecture involving both an encoder and decoder for direct sequence comparison. Function calling via open source LLMs 在考虑成本和隐私性的背景下，我们可能会倾向于在开源的大型语言模型（LLM）上实现函数调用功能。目前，有几个框架支持以类似OpenAI API的形式调用工具（tools call）：\nXinference\nText Generation Inference (TGI)\n而在开源大型语言模型（LLM）方面，支持工具调用的主要有：\nLlama-3\nMixtral-8x7B-Instruct-v0.1\nqwent\nchatGLM-6B\nNexusRaven-13B\ngorilla-openfunctions-v1\n等等\n以下以Xinference和chatGLM-6B为例，探索如何通过OpenAI API的形式调用开源模型的函数调用功能。\n环境\n%pip install -U -q xinference[transformers] openai langchain !pip install typing-extensions --upgrade ## Start Local Server !nohup xinference-local \u0026gt; xinference.log 2\u0026gt;\u0026amp;1 \u0026amp; 模型加载\n!xinference launch -u my-llm --model-name chatglm3 --size-in-billions 6 --model-format pytorch ## Interact with the running model import openai messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Who are you?\u0026#34; } ] client = openai.Client(api_key=\u0026#34;empty\u0026#34;, base_url=f\u0026#34;http://0.0.0.0:9997/v1\u0026#34;) client.chat.completions.create( model=\u0026#34;my-llm\u0026#34;, messages=messages, ) # ChatCompletion(id=\u0026#39;chatda6056ac-da01-11ee-b92e-0242ac1c000c\u0026#39;, choices=[Choice(finish_reason=\u0026#39;stop\u0026#39;, index=0, logprobs=None, message=ChatCompletionMessage(content=\u0026#34;I am an AI assistant named ChatGLM3-6B, which is developed based on the language model jointly trained by Tsinghua University KEG Lab and Zhipu AI Company in 2023. My job is to provide appropriate answers and support to users\u0026#39; questions and requests.\u0026#34;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None))], created=1709541198, model=\u0026#39;my-llm\u0026#39;, object=\u0026#39;chat.completion\u0026#39;, system_fingerprint=None, usage=CompletionUsage(completion_tokens=-1, prompt_tokens=-1, total_tokens=-1)) without tool using\ncompletion = client.chat.completions.create( model=\u0026#34;my-llm\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the weather like in London?\u0026#34;}] ) # print(handle_response(completion)) print(completion.choices[0].message.content) \u0026#34;\u0026#34;\u0026#34; London has a temperate climate with warm summers and cool winters. The average temperature during the summer months (June to August) is around 18°C, while the winter months (December to February) are around 6°C. The city experiences heavy rainfall throughout the year, with an annual precipitation of around 350 mm. The average precipitation on the weekends is around 40 mm. London\u0026#39;s cloudy skies are common throughout the year, but they are especially prevalent in December and January. \u0026#34;\u0026#34;\u0026#34; tool using\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;]}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, }, } ] def get_completion(messages, model=\u0026#34;my-llm\u0026#34;, temperature=0, max_tokens=500, tools=None, tool_choice=None): response = client.chat.completions.create( model=model, messages=messages, temperature=temperature, max_tokens=max_tokens, tools=tools, tool_choice=tool_choice ) return response.choices[0].message # Defines a dummy function to get the current weather def get_current_weather(location, unit=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Get the current weather in a given location\u0026#34;\u0026#34;\u0026#34; weather = { \u0026#34;location\u0026#34;: location, \u0026#34;temperature\u0026#34;: \u0026#34;50\u0026#34;, \u0026#34;unit\u0026#34;: unit, } return json.dumps(weather) messages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like in Boston!\u0026#34;}) assistant_message = get_completion(messages, tools=tools, tool_choice=\u0026#34;auto\u0026#34;) assistant_message = json.loads(assistant_message.model_dump_json()) assistant_message[\u0026#34;content\u0026#34;] = str(assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;]) #a temporary patch but this should be handled differently # remove \u0026#34;function_call\u0026#34; from assistant message del assistant_message[\u0026#34;function_call\u0026#34;] messages.append(assistant_message) # get the weather information to pass back to the model weather = get_current_weather(messages[1][\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;arguments\u0026#34;]) messages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;], \u0026#34;content\u0026#34;: weather}) final_response = get_completion(messages, tools=tools) final_response \u0026#34;\u0026#34;\u0026#34; ChatCompletionMessage(content=\u0026#39;The current weather in Boston is 50 degrees Fahrenheit.\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[]) \u0026#34;\u0026#34;\u0026#34; LLM 在执行function calling时经历了什么 可惜我们并不能看到openAI的模型在服务器端发生了什么，但是根据开源的模型和推理框架，我们某种程度上，也能对LLM在执行function calling的背后逻辑一探究竟。\n这部分内容可以从推理框架和开源模型的源码中找到答案。\n根据xinference 的源码：https://github.com/xorbitsai/inference/blob/main/xinference/model/llm/utils.py#L42\n我们主要关注ChatGLM3 和Qwen\n当我们使用以下假设对话时：\nmessages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;今天北京的天气怎么样？\u0026#34; } ] tools = [ { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, } ] 根据GLM的官方文档，则最终给到ChatGLM3模型的prompt应该长这样：\n\u0026lt;|system|\u0026gt; Answer the following questions as best as you can. You have access to the following tools: [ { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, } ] \u0026lt;|user|\u0026gt; 今天北京的天气怎么样？ \u0026lt;|assistant|\u0026gt; 好的，让我们来查看今天的天气 \u0026lt;|assistant|\u0026gt;get_current_weather ```python tool_call(location=\u0026#34;beijing\u0026#34;, unit=\u0026#34;celsius\u0026#34;) ``` \u0026lt;|observation|\u0026gt; {\u0026#34;temperature\u0026#34;: 22} \u0026lt;|assistant|\u0026gt; 根据查询结果，今天北京的气温为 22 摄氏度。 根据xinference 中有关qwen的代码\nelif prompt_style.style_name == \u0026#34;QWEN\u0026#34;: if tools: tool_desc = \u0026#34;\u0026#34;\u0026#34;{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters} Format the arguments as a JSON object.\u0026#34;\u0026#34;\u0026#34; react_instruction = \u0026#34;\u0026#34;\u0026#34;Answer the following questions as best you can. You have access to the following APIs: {tools_text} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tools_name_text}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated zero or more times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin!\u0026#34;\u0026#34;\u0026#34; tools_text = [] tools_name_text = [] for func_info in tools: parameters = [] required_parameters = func_info[\u0026#34;function\u0026#34;][\u0026#34;parameters\u0026#34;].get( \u0026#34;required\u0026#34;, [] ) for name, p in func_info[\u0026#34;function\u0026#34;][\u0026#34;parameters\u0026#34;][ \u0026#34;properties\u0026#34; ].items(): param = dict({\u0026#34;name\u0026#34;: name}, **p) if name in required_parameters: param[\u0026#34;required\u0026#34;] = True parameters.append(param) name = func_info[\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;] desc = func_info[\u0026#34;function\u0026#34;][\u0026#34;description\u0026#34;] tool_string = tool_desc.format( name_for_model=name, name_for_human=name, # Hint: You can add the following format requirements in description: # \u0026#34;Format the arguments as a JSON object.\u0026#34; # \u0026#34;Enclose the code within triple backticks (`) at the beginning and end of the code.\u0026#34; description_for_model=desc, parameters=json.dumps(parameters, ensure_ascii=False), ) tools_text.append(tool_string) tools_name_text.append(name) tools_text_string = \u0026#34;\\n\\n\u0026#34;.join(tools_text) tools_name_text_string = \u0026#34;, \u0026#34;.join(tools_name_text) tool_system = react_instruction.format( tools_text=tools_text_string, tools_name_text=tools_name_text_string, ) else: tool_system = \u0026#34;\u0026#34; ret = f\u0026#34;\u0026lt;|im_start|\u0026gt;system\\n{prompt_style.system_prompt}\u0026lt;|im_end|\u0026gt;\u0026#34; for message in chat_history: role = get_role(message[\u0026#34;role\u0026#34;]) content = message[\u0026#34;content\u0026#34;] ret += prompt_style.intra_message_sep if tools: if role == \u0026#34;user\u0026#34;: if tool_system: content = tool_system + f\u0026#34;\\n\\nQuestion: {content}\u0026#34; tool_system = \u0026#34;\u0026#34; else: content = f\u0026#34;Question: {content}\u0026#34; elif role == \u0026#34;assistant\u0026#34;: tool_calls = message.get(\u0026#34;tool_calls\u0026#34;) if tool_calls: func_call = tool_calls[0][\u0026#34;function\u0026#34;] f_name, f_args = ( func_call[\u0026#34;name\u0026#34;], func_call[\u0026#34;arguments\u0026#34;], ) content = f\u0026#34;Thought: I can use {f_name}.\\nAction: {f_name}\\nAction Input: {f_args}\u0026#34; elif content: content = f\u0026#34;Thought: I now know the final answer.\\nFinal answer: {content}\u0026#34; elif role == \u0026#34;tool\u0026#34;: role = \u0026#34;function\u0026#34; content = f\u0026#34;Observation: {content}\u0026#34; else: raise Exception(f\u0026#34;Unsupported message role: {role}\u0026#34;) if content: content = content.lstrip(\u0026#34;\\n\u0026#34;).rstrip() ret += f\u0026#34;\u0026lt;|im_start|\u0026gt;{role}\\n{content}\u0026lt;|im_end|\u0026gt;\u0026#34; else: ret += f\u0026#34;\u0026lt;|im_start|\u0026gt;{role}\\n\u0026#34; return ret 会稍微复杂一些， 利用了react的COT方式（代码中的react_instruction），要求模型以一系列的**Thought（思考）、Action（行动）、Action Input（行动输入）和Observation**（观察结果）步骤，最终给出问题的答案，以增加正确性。\n假设使用以下工具列表和对话历史：\n# 工具列表 tools = [ { \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;geo_lookup\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Retrieves geographical information.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The query to lookup.\u0026#34; } } } } } ] chat_history = [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the population of Tokyo?\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;tool_calls\u0026#34;: [ { \u0026#34;function\u0026#34;: \u0026#34;geo_lookup\u0026#34;, \u0026#34;arguments\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;Tokyo\u0026#34; } } ], \u0026#34;content\u0026#34;: \u0026#34;The population of Tokyo is about 14 million.\u0026#34; } ] 可以推断出，最终输入Qwen的prompt应该长这样：\nsystem Answer the following questions as best you can. You have access to the following APIs: geo_lookup: Call this tool to interact with the geo_lookup API. What is the geo_lookup API useful for? Retrieves geographical information. Parameters: [{\u0026#34;name\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The query to lookup.\u0026#34;, \u0026#34;required\u0026#34;: true}] Format the arguments as a JSON object. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [geo_lookup] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated zero or more times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! user Question: What is the population of Tokyo? assistant Thought: I can use geo_lookup to find the information. Action: geo_lookup Action Input: {\u0026#34;query\u0026#34;: \u0026#34;Tokyo\u0026#34;} Observation: The population of Tokyo is about 14 million. Thought: I now know the final answer. Final answer: The population of Tokyo is about 14 million. 在这个例子中，用户询问东京的人口数量。助手利用**geo_lookup**工具进行查询，具体的行动步骤包括：\n思考：助手决定可以使用**geo_lookup**工具来查找信息。\n行动：实际调用**geo_lookup**工具。\n行动输入：向工具传递的参数，即查询**\u0026quot;Tokyo\u0026quot;**。\n观察：观察到的结果，这里是东京的人口大约为1400万。\n最终思考：基于观察结果，助手得出了最终答案。\n最终答案：向用户提供的答案，即东京的人口数量。\n更详细的内容，建议看这篇知乎文章：Qwen Function Calling 的对话模板及训练方法总结; 以及qwen的官方文档：ReAct Prompting 示例\n参考 openAI function calling\nHow to call functions with chat models\nHow_to_call_functions_for_knowledge_retrieval\nQwen Function Calling 的对话模板及训练方法总结\nqwen的官方文档\nGLM的官方文档\ntool-using via Groq API\nJson mode in Groq\nOpenAI JSON Mode \u0026amp; Seeding\nOpenAI API Guide: Using JSON Mode\n","permalink":"https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0openai-function-calling%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/","summary":"写在最开始 当我们在讨论基于大型语言模型（LLM-based）的智能代理（agent）时，我们究竟在谈论什么？根据Lilian W在其文章《LLM Powered Autonomous Agents》中的讨论，一个智能代理需要具备几个核心能力：规划（Planning）、记忆（Memory）、以及工具使用（Tool use）。特别地，工具使用方面的进展，得益于OpenAI在API中提供的function calling功能，为我们开启了新的可能性。\nOpenAI function calling，作为智能代理与外部工具交互的基本方式，对于每位从业者来说都是必备技能。随着技术的发展，我们期望的不只是能与我们对话的LLM，而是能够辅助我们使用各种工具、做出决策的智能伙伴。\n不过需要特别指出的是，最近OpenAI在Chat Completions API中已经废弃了“函数（function）”的使用，转而采用“工具（tool）”。这一变更旨在拓宽LLM集成的功能范围，为更复杂的交互模式铺平道路，如构建能够相互作用的多代理系统。\n尽管如此，由于语言习惯的原因，本文中仍然会使用function calling的术语来描述OpenAI的tool using功能，因为“function calling”的说法已经深入人心了。\n核心内容概览\nFunction Calling的定义：解释什么是function calling，以及它在智能代理工作中的作用。\nOpenAI Cookbook示例：提供实际的function calling示例，帮助读者理解其在实际应用中的用途。\n开源LLM的Tool Using：探索如何在开源大型语言模型中实现工具使用，以及LLM在tool using的时候经历了什么。\n评价与训练：讨论如何评价开源模型的工具使用能力，以及如何训练LLM进行有效的工具使用。\n鉴于整理笔记的速度远赶不上更新的速度， 会将第四部份作为单独的部分整理。\n何为function calling 一句话解释：function calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\n具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\nfunction calling的应用范围广泛，如\n创建智能助手：通过调用外部API回答问题。\n转换指令：将自然语言指令转换成API调用指令。\n数据提取：从文本中提取结构化数据。\nfunction calling的过程涵盖了从定义函数集、通过模型生成遵循自定义模式的JSON对象字符串，到在代码中解析这个字符串并调用相应函数的全过程。这一连串操作不仅自动化了交互过程，还确保了执行操作的安全性和准确性。\n一些常见的问题 JSON mode json mode 和tool-using 有什么关系？有了json mode 还需要用到tool-using吗？\n从json mode 的本质， 更多的是在system prompt 增加一句类似“请以json格式输出”之类的话，然后在LLM输出时增加json结果检查和格式转换。在使用时只需要在client.chat.completions.create 中 增加response_format={ \u0026quot;type\u0026quot;: \u0026quot;json_object\u0026quot; } 即可。\n那么json mode 什么时候会用到呢？一般在做文本提取，内容提取时可以使用；以RAG场景为例， 当我们希望LLM能够帮我们对用户的query进行改写时，我们肯定是希望模型能够返回干净的json格式改写结果，这样的结果可以直接使用，而不是在模型输出一些内容后，如：","title":"Agent学习笔记：OpenAI Function Calling完全指南"},{"content":" 引言 在信息化时代，数据和知识的快速提取变得尤为重要。特别是对于学术和技术文档，如何从格式丰富且结构复杂的PDF文件中准确地提取信息，是提高研究效率的关键。本文将探讨几种先进的技术方法，用于优化学术论文的PDF解析及其表格内容的处理。\n对于文档解析，langchain 和 llama_index 提供的 document loader 能够支持多种文件类型，覆盖绝大多数文档格式的需求。但对于pdf解析而言还存在着一些挑战。\n人类与机器的阅读差异 尽管PDF格式适合人类阅读，提供了优秀的视觉体验和格式保真，但它并不适合机器阅读。PDF文件通常包含复杂的布局和多样的内容元素，如多列布局、公式和表格，这些都给自动解析带来了挑战。\n常见的PDF解析问题 使用传统的PDF解析库可能遇到多种问题：\n多列布局导致的文本流读取错误。\n公式和表格的解析效果差，难以正确提取信息。\n解析过程中结构化信息（如标题和列表）的丢失。\n影印版PDF的文本无法被标准OCR工具识别。\n高级解析技术 根据unstractued提供的方案，文档解析可以大致分为两种方法：结构化解析和基于图像的解析。结构化解析侧重于从文档中提取文字和其它字符数据。而图像基解析则包括以下两种技术：\nOCR技术：首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字。\n基于Transformer的端到端解析：使用深度学习模型直接将图像转换为相应的文字内容。在这方面，Dount 和 Nougat 模型表现出色，尤其是 Nougat 能够将图像中的文章整理成Markdown或Latex格式，非常适合需要保留结构信息（如标题层级、公式和表格）的场景。\n只可惜Nougat只针对英文的学术论文做了训练，但遇到其他语言如中文论文的效果就不是那么的好了。\n必须强调一点的是，Markdown格式因其简洁性和易于解析的特点，被广泛认为是LLM（大型语言模型）友好的文档格式。Markdown通过明确的标记语法，帮助模型更好地理解文档结构和内容，从而提高信息提取的准确性和效率。特别在存在大量公式，表格的学术论文场景下，Markdown可能是更合适的格式选择。\n快速上手：使用Nougat将pdf解析成适合LLM读取的markdown 依赖按照\n!pip install -q pymupdf python-Levenshtein nltk !pip install -q git+https://github.com/huggingface/transformers.git Load model and processor\nfrom transformers import AutoProcessor, VisionEncoderDecoderModel import torch processor = AutoProcessor.from_pretrained(\u0026#34;facebook/nougat-base\u0026#34;) model = VisionEncoderDecoderModel.from_pretrained(\u0026#34;facebook/nougat-base\u0026#34;) device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model.to(device) 将pdf转成图像\nfrom typing import Optional, List import io import fitz from pathlib import Path def rasterize_paper( pdf: Path, outpath: Optional[Path] = None, dpi: int = 96, return_pil=False, pages=None, ) -\u0026gt; Optional[List[io.BytesIO]]: \u0026#34;\u0026#34;\u0026#34; Rasterize a PDF file to PNG images. Args: pdf (Path): The path to the PDF file. outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None. dpi (int, optional): The output DPI. Defaults to 96. return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False. pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None. Returns: Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None. \u0026#34;\u0026#34;\u0026#34; pillow_images = [] if outpath is None: return_pil = True try: if isinstance(pdf, (str, Path)): pdf = fitz.open(pdf) if pages is None: pages = range(len(pdf)) for i in pages: page_bytes: bytes = pdf[i].get_pixmap(dpi=dpi).pil_tobytes(format=\u0026#34;PNG\u0026#34;) if return_pil: pillow_images.append(io.BytesIO(page_bytes)) else: with (outpath / (\u0026#34;%02d.png\u0026#34; % (i + 1))).open(\u0026#34;wb\u0026#34;) as f: f.write(page_bytes) except Exception: pass if return_pil: return pillow_images from transformers import StoppingCriteria, StoppingCriteriaList from collections import defaultdict class RunningVarTorch: def __init__(self, L=15, norm=False): self.values = None self.L = L self.norm = norm def push(self, x: torch.Tensor): assert x.dim() == 1 if self.values is None: self.values = x[:, None] elif self.values.shape[1] \u0026lt; self.L: self.values = torch.cat((self.values, x[:, None]), 1) else: self.values = torch.cat((self.values[:, 1:], x[:, None]), 1) def variance(self): if self.values is None: return if self.norm: return torch.var(self.values, 1) / self.values.shape[1] else: return torch.var(self.values, 1) class StoppingCriteriaScores(StoppingCriteria): def __init__(self, threshold: float = 0.015, window_size: int = 200): super().__init__() self.threshold = threshold self.vars = RunningVarTorch(norm=True) self.varvars = RunningVarTorch(L=window_size) self.stop_inds = defaultdict(int) self.stopped = defaultdict(bool) self.size = 0 self.window_size = window_size @torch.no_grad() def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor): last_scores = scores[-1] self.vars.push(last_scores.max(1)[0].float().cpu()) self.varvars.push(self.vars.variance()) self.size += 1 if self.size \u0026lt; self.window_size: return False varvar = self.varvars.variance() for b in range(len(last_scores)): if varvar[b] \u0026lt; self.threshold: if self.stop_inds[b] \u0026gt; 0 and not self.stopped[b]: self.stopped[b] = self.stop_inds[b] \u0026gt;= self.size else: self.stop_inds[b] = int( min(max(self.size, 1) * 1.15 + 150 + self.window_size, 4095) ) else: self.stop_inds[b] = 0 self.stopped[b] = False return all(self.stopped.values()) and len(self.stopped) \u0026gt; 0 将pdf转成markdown\nfrom tqdm import tqdm def trans_pdf_to_markdown(filepath): output = \u0026#34;\u0026#34; images = rasterize_paper(pdf=filepath, return_pil=True) for i in tqdm(range(len(images))): image = Image.open(images[i]) pixel_values = processor(images=image, return_tensors=\u0026#34;pt\u0026#34;).pixel_values # autoregressively generate tokens, with custom stopping criteria (as defined by the Nougat authors) outputs = model.generate(pixel_values.to(device), min_length=1, max_length=3584, bad_words_ids=[[processor.tokenizer.unk_token_id]], return_dict_in_generate=True, output_scores=True, stopping_criteria=StoppingCriteriaList([StoppingCriteriaScores()]), ) generated = processor.batch_decode(outputs[0], skip_special_tokens=True)[0] generated = processor.post_process_generation(generated, fix_markdown=True) output+=generated output+=\u0026#34;\\n\\n\u0026#34; return output filepath = \u0026#39;xxx.pdf\u0026#39; makrdown_ = trans_pdf_to_markdown(filepath) 具体参考Nougat的官方示例。\n表格类数据的RAG问题 表格类目前的解决方案主要有两种：\n对于内容较小的表格，一般采用LLM对表格进行summary， 以table summary构建查询的index，在召回后， 进行表格内容的复原或者根据问题后处理提供至LLM进行生成。\n对于内容较多的表格， 一般采取结构化处理， 也就是存储至数据库， 在查询阶段根据问题匹配对应的表schema， 有模型执行text-to-sql任务，生成sql并执行获得结果，最终根据结果回答内容。\n对于第一种方案，在Langchain 的cookbook中提到了Semi-structured RAG的方法值得借鉴；\n此外在HRoT这篇工作中， 也提到了类似的方法；\n在HRoT这篇工作中， 增加了将table基于问题进行重构的算法，\nType-Aware Table Reconstruction algorithm\n该算法的实现步骤如下：\n问题分类：首先，算法对问题进行分类，判断问题是算术问题还是跨列选择问题。这有助于确定需要从表格中检索的信息类型。\n获取表格和证据：对于算术问题，算法获取与问题相关的表格集合。然后，从这些表格中提取出作为证据的文本和表格片段。\n表格分区：对于每个表格，算法将其分区为多个子表格。这是通过获取表格的跨度列表（Lt）来完成的，该列表包含了表格中每个头部（行和列）的起始和结束位置。\n确定保留的行和列：算法根据问题类型和证据来确定哪些行和列是回答问题所必需的。这是通过分析每个证据的子标题（即表格中的行和列标题）来实现的。\n重建表格：在确定了需要保留的行和列之后，算法根据这些信息重建表格。重建的表格只包含对问题回答有帮助的信息，从而减少了无关信息的干扰。\n在TableQuery 这篇工作中， 利用一系列模块组件，实现了基于LLM对数据库的高效查询。\nTableQuery的架构设计包括以下几个主要模块：\nDatastore：Datastore是一个表格集合，可以通过自然语言查询。这些表格可以是数据库或电子表格目录。除了表格数据，Datastore还包含模式文件，这些文件包含每个表的元数据，如表关键词、列名、列类型、列关键词（用户为列名提供的关键词）等。模式文件可以手动为这些表创建，也可以通过应用各种启发式方法自动生成。\nTable Selector：表选择器根据输入查询从Datastore中选择合适的表。这是通过从输入查询中提取关键词，并找到与问题关键词（以及模式中的表关键词、列名、列关键词等）重叠系数最大的表来完成的。\nKnown Fields Extractor：已知字段提取器提取查询中已经给出值的列。对于这些列中的每一个，也提取了相应的值。这是通过一个预训练的深度学习模型来完成的，该模型用于在自由文本上执行问答。\nUnknown Fields Extractor：未知字段提取器提取需要从选定表中检索值的列。这是通过排除已知字段提取器已经从查询中提取的列，并找到其列关键词与问题关键词重叠系数最大的列来完成的。\nAggregate Function Classifier：一些查询可能需要对选定表中的未知字段的值执行进一步操作。这是通过在生成的SQL查询中包含SQL聚合函数（如COUNT、SUM、MIN、MAX、AVG等）来实现的。聚合函数分类器决定给定输入查询要使用的聚合函数。为了执行这项任务，作者训练了一个两层的神经网络，该网络以使用通用句子编码器编码的查询作为输入，并输出要使用的适当聚合函数（如果有的话）。\nSQL Generator：结合已知字段和值、未知字段和聚合函数来构建SQL查询，当该查询在选定的表上运行时，返回所需的结果。\n关于text-to-sql的更详尽内容， 可以参考这篇综述文章：Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey。 限于篇幅就不展开了。\n参考文献 langchain: Document loaders\nNougat: Neural Optical Understanding for Academic Documents\nlangchain: Semi-structured RAG\nHRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering\nTableQuery： Querying tabular data with natural language\nNatural Language Interfaces for Tabular Data Querying and Visualization: A Survey\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86/","summary":"引言 在信息化时代，数据和知识的快速提取变得尤为重要。特别是对于学术和技术文档，如何从格式丰富且结构复杂的PDF文件中准确地提取信息，是提高研究效率的关键。本文将探讨几种先进的技术方法，用于优化学术论文的PDF解析及其表格内容的处理。\n对于文档解析，langchain 和 llama_index 提供的 document loader 能够支持多种文件类型，覆盖绝大多数文档格式的需求。但对于pdf解析而言还存在着一些挑战。\n人类与机器的阅读差异 尽管PDF格式适合人类阅读，提供了优秀的视觉体验和格式保真，但它并不适合机器阅读。PDF文件通常包含复杂的布局和多样的内容元素，如多列布局、公式和表格，这些都给自动解析带来了挑战。\n常见的PDF解析问题 使用传统的PDF解析库可能遇到多种问题：\n多列布局导致的文本流读取错误。\n公式和表格的解析效果差，难以正确提取信息。\n解析过程中结构化信息（如标题和列表）的丢失。\n影印版PDF的文本无法被标准OCR工具识别。\n高级解析技术 根据unstractued提供的方案，文档解析可以大致分为两种方法：结构化解析和基于图像的解析。结构化解析侧重于从文档中提取文字和其它字符数据。而图像基解析则包括以下两种技术：\nOCR技术：首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字。\n基于Transformer的端到端解析：使用深度学习模型直接将图像转换为相应的文字内容。在这方面，Dount 和 Nougat 模型表现出色，尤其是 Nougat 能够将图像中的文章整理成Markdown或Latex格式，非常适合需要保留结构信息（如标题层级、公式和表格）的场景。\n只可惜Nougat只针对英文的学术论文做了训练，但遇到其他语言如中文论文的效果就不是那么的好了。\n必须强调一点的是，Markdown格式因其简洁性和易于解析的特点，被广泛认为是LLM（大型语言模型）友好的文档格式。Markdown通过明确的标记语法，帮助模型更好地理解文档结构和内容，从而提高信息提取的准确性和效率。特别在存在大量公式，表格的学术论文场景下，Markdown可能是更合适的格式选择。\n快速上手：使用Nougat将pdf解析成适合LLM读取的markdown 依赖按照\n!pip install -q pymupdf python-Levenshtein nltk !pip install -q git+https://github.com/huggingface/transformers.git Load model and processor\nfrom transformers import AutoProcessor, VisionEncoderDecoderModel import torch processor = AutoProcessor.from_pretrained(\u0026#34;facebook/nougat-base\u0026#34;) model = VisionEncoderDecoderModel.from_pretrained(\u0026#34;facebook/nougat-base\u0026#34;) device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model.to(device) 将pdf转成图像\nfrom typing import Optional, List import io import fitz from pathlib import Path def rasterize_paper( pdf: Path, outpath: Optional[Path] = None, dpi: int = 96, return_pil=False, pages=None, ) -\u0026gt; Optional[List[io.","title":"RAG工具箱：文档解析与表格处理"},{"content":"游记：2024-春 昆明大理 昆明 时间似乎停止了 不知为何在昆明有一种回到兰州的感觉，一样的三线省会，似曾相识的破败老城区街道，赫鲁晓夫楼，砖瓦房，和五六十年代的家属大院，似乎时间就停止流动了。我不认为这是种贬义的表达，至少清晨行走在老街区，唤起了我上学时，清晨走出校园到火车站赶火车的尘封记忆。 不过需要提一嘴的是，似乎当前的实体经济，特别是在老城区，不论是这次旅行所看到的，还是之前在故乡和别的城市所看到的，可以说是很糟糕了。目光所及有一半的店面处于关门转让的状态，而开着的那一半，又有近乎三成处于清仓甩卖中。\n古镇PTSD 在全国绝大多数古镇逐渐趋同演化的当下，不知这是否是件悲哀的事。因为去云南省博物馆的途中会路过官渡古镇，遂决定顺便去看看。从古镇的东北边进入至正门而出，这次访问让我有幸同时见证了从破败而真实的古镇到商业化过度开发的古镇的转变。\n在历史长河中被剥夺了名字的人 在昆明这个城市名字的由来中，有一段被历史辗转淡忘的故事。昆明这一名称起源于昆明夷——西汉时期活跃在洱海周边的一个游牧民族。而在这片土地上生活的滇人，一个拥有先进青铜技术的农耕文明，不断地抵抗游牧民族的侵扰。尽管如此，在历史的长河中，正是这些滇人创造了辉煌的文明，却在历史的演进中失去了自己的名字。这不禁让人感受到一种悲剧的讽刺——在自己孕育辉煌的土地上，却被剥夺了命名的权力。这反映出历史的无情，以及文化与权力之间复杂的交织。\n故乡与迁徙 在省博的临时展区，有幸看到了一个关于迁徙和故乡的特别展。 正巧这次来云南的最初目的也是为了探寻人生的下一个迁徙地。\n今天，流动成为常态。我们祖辈所拥有的那种一生居于一地的安稳逐渐被打破。很多人离开熟悉的地方，到另一处，与来自天南地北的人一起，不知不觉将异乡生活成故乡。\n而提到故乡，你会想到什么？是那方伴你长大的土地，是老屋里围炉团聚的家人，是家乡菜的味道和浓浓的乡音，是家门口熟悉的街道、树林和田地，是小时候玩耍的院子，和那群如今已各奔东西的伙伴。无论走多远，想到那一草一木，一人一景，就安宁。\n这也许就是人们一直以来追寻的栖息地。在远方，在心里，它叫“香格里拉”。\n现代的我们因为各种原因，在故乡与异乡之间辗转，继续着“迁徙与流动”。展览策划过程中，我们以“故乡”“现居地”“理想地”为题，以“故事+展品”的方式开启活动征集，鼓励不同身份的人群通过不同角度的讲述，分享他们的感受与思考。在近一个月的时间里，我们得到了大家的积极回响。在大家的分享中，我们与不同的故事和记忆相连，也看到因为不同而更加多元、广阔的世界。\n这里展示的文字和物品，是记忆的承载，也是对“故的未来。\n乡”的“好久不见”，它记录着我们的成长，也指引着我们\n大理 这次来大理的初始动机是实地考察和调研“数字游民”这个群体，他们是否真正像想象中的自由，他们是如何工作和生活的， 他们背后的故事又是什么？ 以及他们为什么选择大理？\n感谢Dalihub， 让我有幸认识一群有趣的人。\n来自海边房子\nDalihub的秘密空间\n救火途中的直升机\n自由飞翔\n猜你喜欢是“坏”的吗 在大理的第二天，我便幸运地参与了一场与我的工作密切相关的线下沙龙活动，主题涉及推荐系统。我的工作列表中正好有一项是关于如何防止大型科技公司通过推荐系统作恶的问题。 活动中，主讲人“西雅图大黄蜂”提出了一个观点：所有的算法和技术本质上都是中性的，真正的“恶”是由使用它们的人带来的。作为技术从业者，我们当然不希望看到自己的发明像原子弹一样被用于恶劣的目的。但现实往往很残酷，技术的接受与否很大程度上是由资本决定的，而非我们。\n之前我考虑过一个想法：将推荐系统的召回和排序分开，召回过程保留在服务器端，而排序则转移到用户端，这样一来可以提高用户的隐私保护。技术上并不复杂，真正的挑战来自于资本或大公司缺乏推行此类改变的动力。除非有一天，大多数用户愿意为保护隐私牺牲一定的便利性，否则这种改变还遥遥无期，至少在当前的中国是这样的。\n再次感谢主理人Nian和主讲人魏峰，一次难忘的体验。\n真的躺平吗？不，只是喘口气 选择大理的理由： 成本，气候， 和有趣的人们\n在大理，你可以遇到形形色色的人物：那些选择在此旅居的设计师，决定在退休后移居大理的上海夫妇，因成本考虑而搬来的独立开发者，还有视大理为家的咨询师小姐姐。这些都是构成大理独特社群氛围的不同面貌。\n当你不确定下一步该做什么时，不妨去洱海边走走，那里的美景足以让你放慢脚步，深呼吸，重新找回自己的节奏。在大理，即便是“躺平”，也是一种享受生活、与自然和谐共处的态度。\n洱海边落日\n取舍，及时调整自己的欲望 在网上能看到很多对于大理的劝退文， 无外乎就是过度商业化网红化， 城市规划稀烂，交通不方便，宰客现象严重等等。 以上我都认同， 对于一个习惯了大城市便利的人而言， 初到大理的感受的确是如此。 和在地的小伙伴聊下来，发现及时调整自己的欲望还是挺重要的。\n但是，在早晨拉开窗帘看到洱海的一瞬间， 似乎这一切都是值得的。\n随处可见的丁达尔效应\n去跳海，去发疯！\n西南旅游小Tips 注意防晒， 保湿，加湿器和润唇膏很重要。 尽量避开春季，因为春季是风季，很有可能因为大风而错过苍山的缆车。 大理古城的主干道不值得驻足，真正有趣的东西隐藏在巷子里\n这一切就像是一场梦\n感谢在大理遇到的所有人事物\n下一次再见\n","permalink":"https://niraya666.github.io/posts/%E6%B8%B8%E8%AE%B02024-%E6%98%A5-%E6%98%86%E6%98%8E%E5%A4%A7%E7%90%86/","summary":"游记：2024-春 昆明大理 昆明 时间似乎停止了 不知为何在昆明有一种回到兰州的感觉，一样的三线省会，似曾相识的破败老城区街道，赫鲁晓夫楼，砖瓦房，和五六十年代的家属大院，似乎时间就停止流动了。我不认为这是种贬义的表达，至少清晨行走在老街区，唤起了我上学时，清晨走出校园到火车站赶火车的尘封记忆。 不过需要提一嘴的是，似乎当前的实体经济，特别是在老城区，不论是这次旅行所看到的，还是之前在故乡和别的城市所看到的，可以说是很糟糕了。目光所及有一半的店面处于关门转让的状态，而开着的那一半，又有近乎三成处于清仓甩卖中。\n古镇PTSD 在全国绝大多数古镇逐渐趋同演化的当下，不知这是否是件悲哀的事。因为去云南省博物馆的途中会路过官渡古镇，遂决定顺便去看看。从古镇的东北边进入至正门而出，这次访问让我有幸同时见证了从破败而真实的古镇到商业化过度开发的古镇的转变。\n在历史长河中被剥夺了名字的人 在昆明这个城市名字的由来中，有一段被历史辗转淡忘的故事。昆明这一名称起源于昆明夷——西汉时期活跃在洱海周边的一个游牧民族。而在这片土地上生活的滇人，一个拥有先进青铜技术的农耕文明，不断地抵抗游牧民族的侵扰。尽管如此，在历史的长河中，正是这些滇人创造了辉煌的文明，却在历史的演进中失去了自己的名字。这不禁让人感受到一种悲剧的讽刺——在自己孕育辉煌的土地上，却被剥夺了命名的权力。这反映出历史的无情，以及文化与权力之间复杂的交织。\n故乡与迁徙 在省博的临时展区，有幸看到了一个关于迁徙和故乡的特别展。 正巧这次来云南的最初目的也是为了探寻人生的下一个迁徙地。\n今天，流动成为常态。我们祖辈所拥有的那种一生居于一地的安稳逐渐被打破。很多人离开熟悉的地方，到另一处，与来自天南地北的人一起，不知不觉将异乡生活成故乡。\n而提到故乡，你会想到什么？是那方伴你长大的土地，是老屋里围炉团聚的家人，是家乡菜的味道和浓浓的乡音，是家门口熟悉的街道、树林和田地，是小时候玩耍的院子，和那群如今已各奔东西的伙伴。无论走多远，想到那一草一木，一人一景，就安宁。\n这也许就是人们一直以来追寻的栖息地。在远方，在心里，它叫“香格里拉”。\n现代的我们因为各种原因，在故乡与异乡之间辗转，继续着“迁徙与流动”。展览策划过程中，我们以“故乡”“现居地”“理想地”为题，以“故事+展品”的方式开启活动征集，鼓励不同身份的人群通过不同角度的讲述，分享他们的感受与思考。在近一个月的时间里，我们得到了大家的积极回响。在大家的分享中，我们与不同的故事和记忆相连，也看到因为不同而更加多元、广阔的世界。\n这里展示的文字和物品，是记忆的承载，也是对“故的未来。\n乡”的“好久不见”，它记录着我们的成长，也指引着我们\n大理 这次来大理的初始动机是实地考察和调研“数字游民”这个群体，他们是否真正像想象中的自由，他们是如何工作和生活的， 他们背后的故事又是什么？ 以及他们为什么选择大理？\n感谢Dalihub， 让我有幸认识一群有趣的人。\n来自海边房子\nDalihub的秘密空间\n救火途中的直升机\n自由飞翔\n猜你喜欢是“坏”的吗 在大理的第二天，我便幸运地参与了一场与我的工作密切相关的线下沙龙活动，主题涉及推荐系统。我的工作列表中正好有一项是关于如何防止大型科技公司通过推荐系统作恶的问题。 活动中，主讲人“西雅图大黄蜂”提出了一个观点：所有的算法和技术本质上都是中性的，真正的“恶”是由使用它们的人带来的。作为技术从业者，我们当然不希望看到自己的发明像原子弹一样被用于恶劣的目的。但现实往往很残酷，技术的接受与否很大程度上是由资本决定的，而非我们。\n之前我考虑过一个想法：将推荐系统的召回和排序分开，召回过程保留在服务器端，而排序则转移到用户端，这样一来可以提高用户的隐私保护。技术上并不复杂，真正的挑战来自于资本或大公司缺乏推行此类改变的动力。除非有一天，大多数用户愿意为保护隐私牺牲一定的便利性，否则这种改变还遥遥无期，至少在当前的中国是这样的。\n再次感谢主理人Nian和主讲人魏峰，一次难忘的体验。\n真的躺平吗？不，只是喘口气 选择大理的理由： 成本，气候， 和有趣的人们\n在大理，你可以遇到形形色色的人物：那些选择在此旅居的设计师，决定在退休后移居大理的上海夫妇，因成本考虑而搬来的独立开发者，还有视大理为家的咨询师小姐姐。这些都是构成大理独特社群氛围的不同面貌。\n当你不确定下一步该做什么时，不妨去洱海边走走，那里的美景足以让你放慢脚步，深呼吸，重新找回自己的节奏。在大理，即便是“躺平”，也是一种享受生活、与自然和谐共处的态度。\n洱海边落日\n取舍，及时调整自己的欲望 在网上能看到很多对于大理的劝退文， 无外乎就是过度商业化网红化， 城市规划稀烂，交通不方便，宰客现象严重等等。 以上我都认同， 对于一个习惯了大城市便利的人而言， 初到大理的感受的确是如此。 和在地的小伙伴聊下来，发现及时调整自己的欲望还是挺重要的。\n但是，在早晨拉开窗帘看到洱海的一瞬间， 似乎这一切都是值得的。\n随处可见的丁达尔效应\n去跳海，去发疯！\n西南旅游小Tips 注意防晒， 保湿，加湿器和润唇膏很重要。 尽量避开春季，因为春季是风季，很有可能因为大风而错过苍山的缆车。 大理古城的主干道不值得驻足，真正有趣的东西隐藏在巷子里\n这一切就像是一场梦\n感谢在大理遇到的所有人事物\n下一次再见","title":"游记：2024-春 昆明大理"},{"content":"写在最前面 在过去的几个月中，我主要致力于与RAG（检索增强生成模型）相关的研究和实验工作。通过深入地分析众多论文和项目代码，我发现虽然在RAG领域取得基本成就相对容易，但要实现出色的成果却异常困难。\n此系列文章旨在分享我在RAG研究中的心得和挑战。正如我们从OpenAI的开发者日活动中看到的，即使是在这个领域的领头羊也在不断试验和努力以寻求突破。\n在我们深入探索RAG系统提升的各种方法之前，有一个基本问题需要解决：我们如何准确地评估一个RAG系统的性能？这就像是在机器学习或深度学习任务中没有给定的测试集和评价标准，我们难以判断实验的成功与否。\n因此，本系列的第一篇文章将聚焦于介绍RAG系统的评价方法、相关指标以及测试框架，为我们接下来的探索设定明确的标准和目标。\n测试框架 以下是一些测试框架，为RAG系统评估提供了强大的支持。\nTruLens TruLens提供了一个独特的视角来评估和跟踪大型语言模型（LLM）实验，通过一系列创新的功能和方法，帮助开发者和研究人员更深入地了解模型性能和行为。\nTruLens的反馈功能（Feedback Functions）是其核心概念之一，提供了一种程序化的方法来评估应用的运行表现。这些函数从“可扩展性”和“有意义性”两个维度出发，考虑评估的范围，旨在为用户提供有价值的反馈，帮助他们理解和改进他们的LLM应用。\n在RAG应用中，提供准确的上下文信息至关重要，以避免生成不真实的“幻觉”答案。TruLens采用了创新性的RAG三元组评估方法，专门针对RAG架构的每个环节进行幻觉风险评估，确保模型的每个部分都能有效地工作，从而减少误导信息的产生。\n上下文相关性（Context Relevance） 上下文相关性是评估RAG应用的第一步，确保每一段检索到的上下文都与提出的查询紧密相关。TruLens利用序列化记录的结构来评估上下文的相关性，这是一个关键的步骤，确保模型在正确的信息上生成回答。\n真实性（Groundedness） 在检索到的上下文信息的基础上，大型语言模型将生成答案。TruLens强调了独立验证每个回答的重要性，以确保它们基于可靠信息，并且能够在检索到的上下文中找到支持的证据。这一步骤是确保模型回答的真实性和可靠性的关键。\n答案相关性（Answer Relevance） 最后，评估需要确保最终回答有效地解答了原始问题，这通过评估应用的最终回答与用户输入的相关性来实现。这一过程确保了模型的输出不仅是真实的，而且是对用户查询有用的。\nTruLens还提出了“诚实、无害和有帮助”的评估原则（Honest, Harmless, and Helpful Evaluations），这些原则旨在确保LLM应用在提供帮助的同时，也是安全和可信的。\nRagas Ragas框架专为评估检索增强生成（RAG）系统而设计，定义了四个核心评估指标：上下文相关性（context_relevancy）、上下文回溯（context_recall）、忠实度（faithfulness）和答案相关性（answer_relevancy）。这些指标共同构成了Ragas的评分体系，提供了一个全面的方法来评价RAG系统的性能。\n此外，Ragas巧妙地利用大语言模型（LLM）进行无参考评估，有效降低了评估成本。通过这种方法，Ragas能够提供一种既经济又有效的方式来评估RAG系统，尤其是在处理大量数据和复杂查询时。\n其他测试框架\nDeepEval\nDeepEval How to Evaluate RAG Applications in CI/CD Pipelines with DeepEval\nARES\ngithub: https://github.com/stanford-futuredata/ARES\nPaper: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\nLangChain Evals\nLlama Index Evals\nUpTrain\n数据 在评估数据集时，不必依赖人工标注的标准答案，而是通过底层的大语言模型 (LLM) 来进行评估。\n为了对 RAG 流程进行评估，需要以下几种信息：\nquestion：这是RAG流程的输入，即用户的查询问题。\nanswer：这是由RAG流程生成的答案，也就是输出结果。\ncontexts：这是为了解答question而从外部知识源检索到的相关上下文信息。\n指标 在深入研究检索增强生成（RAG）系统和其他相关技术时，了解和使用正确的评估指标至关重要。以下是几个关键指标，它们帮助我们量化和评估检索系统的效能：\n检索指标 MRR（平均倒数排名） MRR是衡量检索系统性能的一种方法，特别关注于检索结果中的首次正确命中的排名。MRR的高值表示系统能够更频繁地将相关结果排在前列。\n$$ MRR = \\frac{1}{查询数量}\\sum_{i=1}^{查询数量} \\frac{1}{首次正确命中的排名} $$\n这个指标特别有用，因为它直接关注于用户最有可能查看的第一个搜索结果的质量。\n召回率（Recall） 召回率是另一个重要指标，它衡量了系统检索到的相关文档数量与总的相关文档数量之间的比例。高召回率意味着系统能够检索到更多的相关文档。\n$$ Recall = \\frac{检索到的相关文档数量}{总的相关文档数量} $$\nNDCG（标准化折扣累积增益） NDCG（Normalized Discounted Cumulative Gain）是一个在信息检索、推荐系统和机器学习领域常用的评估指标，用于衡量一个系统或模型返回的结果列表的质量。NDCG特别关注于结果的排序质量，即最相关或最有价值的结果是否排在了列表的前面。与其他评估指标相比，NDCG的独特之处在于它考虑了结果的相关性（relevance）不仅是二元的（相关或不相关），而且可以是多级的（例如，从不相关到非常相关的多个级别）。\nEM（精确匹配） EM度量了系统输出的答案与标准答案完全一致的比例，是评估系统准确度的直接方式。在某些场景下，即使是非常小的差异也可能导致答案被视为不正确，这使得EM成为一个严格的评估标准。\n基于大语言模型评估的LLM生成指标 在RAG任务中，对LLM回答的问题主要关注了回答的可验证性（verifiability），即是否严格遵循检索到的上下文如实回答。可验证性由两部分组成：\n高引用召回率（high citation recall）：即所有生成的内容都有充分的引用（外部知识）支持。\n高引用精度（high citation precision）：每个引用是否真的支持生成的内容。\n回答相关性（Answer Relevance） 回答相关性关注的是系统生成的回答与用户提出的问题之间的相关性。理想情况下，回答应该直接且准确地对应于问题，没有偏离主题或提供不相关的信息。\nTruLens中回答相关性的计算方式： TruLens通过提供一种基于LLM的评估方法，允许开发者和研究人员通过编程方式获取对系统生成回答的相关性评估。这种方法利用链式推理（Chain of Thought, CoT）增强理解和推理过程，为评估提供透明度和可解释性。\nfrom trulens_eval.feedback.provider.openai import OpenAI openai_provider = OpenAI() qa_relevance = ( Feedback(openai_provider.relevance_with_cot_reasons, name=\u0026#34;Answer Relevance\u0026#34;) .on_input_output() ) 在relevance_with_cot_reasons方法中，使用聊天完成模型来评估回答对于提示的相关性，并揭示评分背后的推理过程。这种方法不仅考虑回答的内容和长度，而且还评估其是否全面回答了问题，并提供了与问题所有部分相关的上下文信息。\nRAGAS中回答相关性的计算逻辑： RAGAS通过利用LLM重新生成问题（QUESTION_GEN），然后计算这个重新生成的问题与原始问题之间的相似度来评估回答的相关性。这种方法特别关注于系统生成回答的准确性和与原始问题的对应关系。\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_answer_relevance.py QUESTION_GEN = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34; Generate question for the given answer. Answer:\\nThe PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India Question: When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from? Answer:{answer} Question: \u0026#34;\u0026#34;\u0026#34; ) 不过，由于用户提出的问题通常比较简略，使用RAGAS计算出的回答相关性通常较低。\n上下文相关性（Context Relevance） 上下文相关性专注于评估给定上下文（例如从数据库或文档中检索到的信息片段）与用户查询之间的相关性。高相关性的上下文信息为大型语言模型（LLM）提供了生成准确回答的基础。\n在TruLens中上下文相关性的计算方式： 在TruLens，上下文相关性的评估使用了与回答相关性相同的反馈函数，但是采用了不同的选择器来专注于输入（用户问题）和检索到的上下文信息之间的相关性。这通过对输入问题和来源节点中的文本应用.on_input().on(TruLlama.select_source_nodes().node.text)选择器并计算它们的平均相关性得分来实现。\nqs_relevance = ( Feedback(openai_provider.relevance_with_cot_reasons, name=\u0026#34;Context Relevance\u0026#34;) .on_input() .on(TruLlama.select_source_nodes().node.text) .aggregate(np.mean) ) 底层计算逻辑：此过程首先识别出与用户问题直接相关的上下文信息，评估这些信息的相关性，可能会应用链式推理（CoT）方法来提高评分的准确性和深度。最终生成的平均分表示了上下文信息的整体相关性，分值范围从0到1。\n在RAGAS中上下文相关性的计算逻辑： RAGAS采用了一种略有不同的方法来计算上下文相关性。它通过提取与问题相关的上下文句子（candidate sentences），并对这些句子进行自洽性检查。然后，使用提取的句子数量与检索到的上下文中的总句子数量的比率作为评分标准。\n$$ \\text{context relevancy} = \\frac{|S|}{|\\text{Total number of sentences in retrieved context}|} $$\n这个方法通过使用LLM根据问题和上下文，从上下文中提取出能够支持回答的句子，进而计算这些候选句子和上下文的长度占比（这里使用的是词元数量的比值）。\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_context_relevancy.py CONTEXT_RELEVANCE = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;\\ Please extract relevant sentences from the provided context that is absolutely required answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \u0026#34;Insufficient Information\u0026#34;. While extracting candidate sentences you\u0026#39;re not allowed to make any changes to sentences from given context. question:{question} context:\\n{context} candidate sentences:\\n\u0026#34;\u0026#34;\u0026#34; ) 真实性或忠实度（Groundedness or Faithfulness） 在TruLens和RAGAS中，真实性或忠实度的评估旨在衡量生成的声明或回答在事实上的准确性及其对源材料的依赖程度。这一评估标准对于确保生成内容的质量和可信度至关重要。\nTruLens中的Groundedness 在TruLens框架中，groundedness的评估通过检查声明中的每个句子是否在源材料中有支持来进行。这个过程利用大型语言模型（LLM）和链式推理（Chain of Thought, CoT）方法来增强评估的准确性和深度。具体来说，评估方法会将整个声明作为一个整体进行处理，并对声明中的每个句子赋予一个从0到10的评分，0代表没有任何信息重叠，而10代表信息完全重叠。\ngrounded = Groundedness(groundedness_provider=openai_provider) groundedness = ( Feedback(grounded.groundedness_measure_with_cot_reasons, name=\u0026#34;Groundedness\u0026#34;) .on(TruLlama.select_source_nodes().node.text) .on_output() .aggregate(grounded.grounded_statements_aggregator) ) RAGAS中的Faithfulness 而在RAGAS框架中，Faithfulness（忠实度）的概念与TruLens中的Groundedness（真实性）相似，旨在评估生成回答的事实一致性。忠实度得分通过比较生成回答中的声明与给定上下文的一致性来计算，特别是检查回答中的声明是否可以从给定的上下文中推断出来。\n$$ \\text{Faithfulness score} = \\frac{|\\text{Number of claims that can be inferred from given context}|}{|\\text{Total number of claims in the generated answer}|} $$\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_faithfulness.py LONG_FORM_ANSWER_PROMPT = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;\\ Given a question and answer, create one or more statements from each sentence in the given answer. question: Who was Albert Einstein and what is he best known for? answer: He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics. statements:\\nAlbert Einstein was born in Germany.\\nAlbert Einstein was best known for his theory of relativity. question: Cadmium Chloride is slightly soluble in this chemical, it is also called what? answer: alcohol statements:\\nCadmium Chloride is slightly soluble in alcohol. question: Were Shahul and Jithin of the same nationality? answer: They were from different countries. statements:\\nShahul and Jithin were from different countries. question:{question} answer: {answer} statements:\\n\u0026#34;\u0026#34;\u0026#34; # noqa: E501 ) NLI_STATEMENTS_MESSAGE = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34; Prompt: Natural language inference Consider the given context and following statements, then determine whether they are supported by the information present in the context.Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. Context:\\nJohn is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects. statements:\\n1. John is majoring in Biology.\\n2. John is taking a course on Artificial Intelligence.\\n3. John is a dedicated student.\\n4. John has a part-time job.\\n5. John is interested in computer programming.\\n Answer: 1. John is majoring in Biology. Explanation: John\u0026#39;s major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology. Verdict: No. 2. John is taking a course on Artificial Intelligence. Explanation: The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI. Verdict: No. 3. John is a dedicated student. Explanation: The prompt states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication. Verdict: Yes. 4. John has a part-time job. Explanation: There is no information given in the context about John having a part-time job. Therefore, it cannot be deduced that John has a part-time job. Verdict: No. 5. John is interested in computer programming. Explanation: The context states that John is pursuing a degree in Computer Science, which implies an interest in computer programming. Verdict: Yes. Final verdict for each statement in order: No. No. Yes. No. Yes. context:\\n{context} statements:\\n{statements} Answer: 微调模型评估 在评估RAG系统时，虽然GPT-4等大型语言模型（LLM）因其高性能而被广泛使用，但由于成本和本地部署的需要，选择一个有效的本地部署开源模型也成为了许多研究和应用场景的必需。然而，与GPT-4等高级模型相比，这些本地开源模型的能力通常存在差距，特别是在特定领域内的应用效果上。因此，针对特定领域微调一个LLM用于评估变得尤为重要。\n以下是使用Trulens测试框架，基于笔者所在领域的语料库，对同一个RAG系统使用不同的开源LLM进行评价的结果展示。特别关注的是不同验证模型的失败评分率，这里的失败评分率指的是模型无法按照提示给出有效评分的情况（例如，要求在0～1范围内打分，但模型未给出评分或评分超出范围）。\n失败评分率对比\neval model Groundedness-FailRatio Context-Relevance-FailRatio Answer-Relevance-FailRatio Records gpt-3.5-turbo 0.00 0.00 0.00 41 gpt-4-turbo 0.00 0.00 0.00 41 Qwen1.5-14B-chat 0.07 0.05 0.02 41 Qwen1.5-7B-chat 0.10 0.10 0.07 41 zephyr-7b-beta 0.27 0.22 0.22 41 Qwen1.5-4B-chat 0.32 0.15 0.17 41 chatGLM3-6b-32K 0.36 0.71 0.54 41 微调策略和建议 为了确保评价结果的稳定性和准确性，微调模型时使用的数据集应特别关注于测试框架中使用的提示格式。这样做旨在提高模型对特定提示格式的响应能力，从而提高评价的准确度和一致性。\n此外，从测试结果来看，建议至少使用13B以上的模型进行评价。较小的模型可能在遵循指令和评分准确性方面存在挑战，从而影响评价结果的可靠性。\n对于这个场景的微调，一般采用LoRA微调方法即可满足需求。LoRA（Low-Rank Adaptation）是一种有效的微调技术，可以在不大幅增加模型参数的前提下，通过更新模型的少数关键参数来实现性能的提升。这种方法特别适用于需要特定领域知识增强的场景。\n如果能够收集到足够多的人类反馈结果，采用RLHF（Reinforcement Learning from Human Feedback）专门训练一个评价模型也是一个可行的选择。RLHF通过从人类反馈中学习来优化模型的性能，这可以在成本可控的范围内提供更精确的评价结果。\n传统NLP评估 BLEU BLEU（Bilingual Evaluation Understudy）通过计算机器翻译输出与一个或多个人工翻译的参考译文之间的词汇精确度来评价翻译的质量。BLEU的主要目的是自动地评估文本翻译的好坏，尽量接近人类翻译质量评估的结果。\nROUGE ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是自然语言处理（NLP）任务中常用的一种评估指标，尤其在自动文摘（自动摘要）和机器翻译等领域中非常流行。它主要用于评估自动生成的文本与人工编写的参考文本之间的相似度。ROUGE指标通过计算生成文本与参考文本之间的重叠来量度生成文本的质量。\n常见的ROUGE度量方式有：基于n-gram的重叠度量方法(如ROUGE-1, ROUGE-2)。它计算生成文本与参考文本之间共有的n-grams的数量，并以此评估生成文本的质量。以及基于最长公共子序列（LCS） 的 ROUGE-L，它考虑了句子级别的结构相似性，不仅仅是简单的词汇重叠。通过计算最长公共子序列的长度，ROUGE-L能够捕捉到生成文本和参考文本之间的顺序依赖性，从而提供更全面的相似度评估。\n对于n-gram ROUGE有两个变体： 召回率（Recall）：参考文本中与生成文本共有的n-grams数量除以参考文本中的n-grams总数。精确率（Precision）：参考文本中与生成文本共有的n-grams数量除以生成文本中的n-grams总数。以及二者的调和平均 F1-score。\nMore Details：\npaper: ROUGE: A Package for Automatic Evaluation of Summaries\nblog: GenAI model evaluation metric — ROUGE\n例子：\n# !pip install rouge from rouge import Rouge rouge = Rouge() long = \u0026#39;It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us.\u0026#39; short = \u0026#39;It was an age of wisdom, foolishness, belief, Light, Darkness, hope, and despair, with both light and darkness.\u0026#39; scores = rouge.get_scores(short, long) print(scores) # output: \u0026#39;\u0026#39;\u0026#39; [{\u0026#39;rouge-1\u0026#39;: {\u0026#39;r\u0026#39;: 0.39285714285714285, \u0026#39;p\u0026#39;: 0.6470588235294118, \u0026#39;f\u0026#39;: 0.4888888841876543}, \u0026#39;rouge-2\u0026#39;: {\u0026#39;r\u0026#39;: 0.06976744186046512, \u0026#39;p\u0026#39;: 0.17647058823529413, \u0026#39;f\u0026#39;: 0.09999999593888906}, \u0026#39;rouge-l\u0026#39;: {\u0026#39;r\u0026#39;: 0.39285714285714285, \u0026#39;p\u0026#39;: 0.6470588235294118, \u0026#39;f\u0026#39;: 0.4888888841876543}}] \u0026#39;\u0026#39;\u0026#39; where r, p, and f representing for recall, precision, and f_score.\nROUGE指标的缺点：\n依赖于参考文本： 如果参考文本的质量不高或者数量不足，可能会导致评分不准确\n忽略语义信息： 即使两段文本表达相同的意思，但使用了不同的词汇或表达方式，ROUGE评分也可能较低\n无法评价文本的流畅性和一致性\n对长文本的评价能力有限\n无法全面评估信息的新颖性和重要性\n数据集 一些公开的RAG数据集\nHotpotQA (HQA) 数据集链接：HotpotQA at Hugging Face\n特点：基于维基百科的问答数据集，需要阅读多个支持文档来回答和推理问题。问题多样，不局限于任何预先存在的知识库。提供了句子级别的支持以强化LLM的推理需求。最后，提供了新类型的事实对比问题，测试LLMs提取和比较文本中各种实体属性的能力。\nQasper (QASP) 数据集链接：Qasper at Hugging Face\n特点：基于NLP论文的问答数据集，筛选自Semantic Scholar Open Research Corpus (S2ORC)。\nNarrativeQA (NQA) 数据集链接：NarrativeQA at Hugging Face\n特点：NarrativeQA是一个英语语言的故事和相应问题的数据集，旨在测试阅读理解能力，特别是对长文档的理解。\nQuALITY (QLTY) 数据集链接：QuALITY at Hugging Face\n特点：一个基于故事和文章的多项选择问答数据集，来源包括Project Gutenberg和Open American National Corpus等资源。\nPopQA 论文链接：PopQA Paper\n数据集链接：PopQA at Hugging Face\n特点：PopQA是一个大规模的开放领域问答（QA）数据集，包含14k个以实体为中心的QA对。每个问题都是通过使用模板将从Wikidata检索到的知识元组转换而来的。\nTriviaQA 数据集链接：TriviaQA at Hugging Face\n特点：TriviaqQA是一个阅读理解数据集，包含超过650K的问题-答案-证据三元组。TriviaqQA包括由琐事爱好者编写的95K个问题-答案对。\nASQA 数据集链接：ASQA at Hugging Face\n特点：ASQA是第一个专注于含糊事实问题的长形式问答数据集。与以往的长形式答案数据集不同，每个问题都标注了长形式答案和可由生成段落回答的提取式问答对。\nPUBHEALTH 数据集链接：PUBHEALTH at Hugging Face\n特点：一个包含11,832个用于事实检查的声明的数据集，这些声明涉及一系列健康话题，包括生物医学主题（如传染病、干细胞研究）、政府医疗政策（如堕胎、心理健康、妇女健康）以及其他与公共健康相关的故事。\n推荐阅读 RAGAS Paper\n用 RAGAs（检索增强生成评估）评估 RAG（检索增强型生成）应用 [译]\n如何构建高效的 RAG 系统\nEvaluating Verifiability in Generative Search Engines\nBuilding and Evaluating Advanced RAG\nSteps In Evaluating Retrieval Augmented Generation (RAG) Pipelines\nGenAI model evaluation metric — ROUGE\nRetrieval Augmented Generation (RAG) for LLMs\nRAG Evaluation\n","permalink":"https://niraya666.github.io/posts/rag_toolkit_eval/","summary":"写在最前面 在过去的几个月中，我主要致力于与RAG（检索增强生成模型）相关的研究和实验工作。通过深入地分析众多论文和项目代码，我发现虽然在RAG领域取得基本成就相对容易，但要实现出色的成果却异常困难。\n此系列文章旨在分享我在RAG研究中的心得和挑战。正如我们从OpenAI的开发者日活动中看到的，即使是在这个领域的领头羊也在不断试验和努力以寻求突破。\n在我们深入探索RAG系统提升的各种方法之前，有一个基本问题需要解决：我们如何准确地评估一个RAG系统的性能？这就像是在机器学习或深度学习任务中没有给定的测试集和评价标准，我们难以判断实验的成功与否。\n因此，本系列的第一篇文章将聚焦于介绍RAG系统的评价方法、相关指标以及测试框架，为我们接下来的探索设定明确的标准和目标。\n测试框架 以下是一些测试框架，为RAG系统评估提供了强大的支持。\nTruLens TruLens提供了一个独特的视角来评估和跟踪大型语言模型（LLM）实验，通过一系列创新的功能和方法，帮助开发者和研究人员更深入地了解模型性能和行为。\nTruLens的反馈功能（Feedback Functions）是其核心概念之一，提供了一种程序化的方法来评估应用的运行表现。这些函数从“可扩展性”和“有意义性”两个维度出发，考虑评估的范围，旨在为用户提供有价值的反馈，帮助他们理解和改进他们的LLM应用。\n在RAG应用中，提供准确的上下文信息至关重要，以避免生成不真实的“幻觉”答案。TruLens采用了创新性的RAG三元组评估方法，专门针对RAG架构的每个环节进行幻觉风险评估，确保模型的每个部分都能有效地工作，从而减少误导信息的产生。\n上下文相关性（Context Relevance） 上下文相关性是评估RAG应用的第一步，确保每一段检索到的上下文都与提出的查询紧密相关。TruLens利用序列化记录的结构来评估上下文的相关性，这是一个关键的步骤，确保模型在正确的信息上生成回答。\n真实性（Groundedness） 在检索到的上下文信息的基础上，大型语言模型将生成答案。TruLens强调了独立验证每个回答的重要性，以确保它们基于可靠信息，并且能够在检索到的上下文中找到支持的证据。这一步骤是确保模型回答的真实性和可靠性的关键。\n答案相关性（Answer Relevance） 最后，评估需要确保最终回答有效地解答了原始问题，这通过评估应用的最终回答与用户输入的相关性来实现。这一过程确保了模型的输出不仅是真实的，而且是对用户查询有用的。\nTruLens还提出了“诚实、无害和有帮助”的评估原则（Honest, Harmless, and Helpful Evaluations），这些原则旨在确保LLM应用在提供帮助的同时，也是安全和可信的。\nRagas Ragas框架专为评估检索增强生成（RAG）系统而设计，定义了四个核心评估指标：上下文相关性（context_relevancy）、上下文回溯（context_recall）、忠实度（faithfulness）和答案相关性（answer_relevancy）。这些指标共同构成了Ragas的评分体系，提供了一个全面的方法来评价RAG系统的性能。\n此外，Ragas巧妙地利用大语言模型（LLM）进行无参考评估，有效降低了评估成本。通过这种方法，Ragas能够提供一种既经济又有效的方式来评估RAG系统，尤其是在处理大量数据和复杂查询时。\n其他测试框架\nDeepEval\nDeepEval How to Evaluate RAG Applications in CI/CD Pipelines with DeepEval\nARES\ngithub: https://github.com/stanford-futuredata/ARES\nPaper: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\nLangChain Evals\nLlama Index Evals\nUpTrain\n数据 在评估数据集时，不必依赖人工标注的标准答案，而是通过底层的大语言模型 (LLM) 来进行评估。\n为了对 RAG 流程进行评估，需要以下几种信息：\nquestion：这是RAG流程的输入，即用户的查询问题。\nanswer：这是由RAG流程生成的答案，也就是输出结果。\ncontexts：这是为了解答question而从外部知识源检索到的相关上下文信息。","title":"RAG工具箱：评估RAG系统的方法论"},{"content":"Hello-World! 欢迎来到我的博客\n在这里，我将深入探索生成式人工智能的奥秘，同时也会涉猎音乐、电影等领域，分享一些个人的思考和感悟。\n为什么我决定写博客 在生活的纷扰和无尽的日常中，我发现自己一直在与拖延症作斗争。直到今天，我终于下定决心，决定将心中的思绪和感悟记录下来，开启我的博客之旅。\n有几个原因驱使我做出了这个决定。\n首先，岁月不饶人，尤其是经历了新冠疫情之后，我明显感觉到我的记忆力不如以往。过去能够轻松驾驭多重任务的我，如今却常在走入客厅的半路上忘记初衷，或是在浏览器的搜索框前失去了寻找的目的。这种突如其来的迷茫，让我开始思索，我的思绪是否正如秋日里的落叶，悄然飘落。\n其次，在深夜的静思中，我时常回想起坂本龙一那句引人深思的话：“我还能看到几次满月？”这不仅是对时间流逝的感慨，更是一种对生命有限性的深刻体悟。在这有限的时光里，我究竟能留下什么？假如我的时间之沙仅剩下几颗，我的存在又有何意义？我不求答案，但愿通过这些文字，如同在时间的长河中种下一棵树，哪怕是最微小的存在，也能留下自己生命的痕迹。\n最后，我被“数据主义”（Dataism）这一概念深深吸引，它如同一面镜子，映照出在数字时代，我们的数据、思考和情感不仅仅是信息的载体，更是构成我们数字化身份的基石。随着AI的羽翼日渐丰满，我开始憧憬一个可能的未来，其中一个由我的数据、思想和经历塑造出的“我”，在某个未知的时间点复苏。这种思考，如同在深海中发现了一座灯塔，为我的存在指明了一条全新的路径。在这个时代，我选择不再是沉默的旁观者，而是通过我的文字，积极参与到这场未知的探索中。\n因此，这篇博客标志着我的新开始。虽然不确定未来的路会怎样，但至少，在这个过程中，我会找到自己的声音，并希望能够与你共鸣。\n","permalink":"https://niraya666.github.io/posts/helloworld/","summary":"Hello-World! 欢迎来到我的博客\n在这里，我将深入探索生成式人工智能的奥秘，同时也会涉猎音乐、电影等领域，分享一些个人的思考和感悟。\n为什么我决定写博客 在生活的纷扰和无尽的日常中，我发现自己一直在与拖延症作斗争。直到今天，我终于下定决心，决定将心中的思绪和感悟记录下来，开启我的博客之旅。\n有几个原因驱使我做出了这个决定。\n首先，岁月不饶人，尤其是经历了新冠疫情之后，我明显感觉到我的记忆力不如以往。过去能够轻松驾驭多重任务的我，如今却常在走入客厅的半路上忘记初衷，或是在浏览器的搜索框前失去了寻找的目的。这种突如其来的迷茫，让我开始思索，我的思绪是否正如秋日里的落叶，悄然飘落。\n其次，在深夜的静思中，我时常回想起坂本龙一那句引人深思的话：“我还能看到几次满月？”这不仅是对时间流逝的感慨，更是一种对生命有限性的深刻体悟。在这有限的时光里，我究竟能留下什么？假如我的时间之沙仅剩下几颗，我的存在又有何意义？我不求答案，但愿通过这些文字，如同在时间的长河中种下一棵树，哪怕是最微小的存在，也能留下自己生命的痕迹。\n最后，我被“数据主义”（Dataism）这一概念深深吸引，它如同一面镜子，映照出在数字时代，我们的数据、思考和情感不仅仅是信息的载体，更是构成我们数字化身份的基石。随着AI的羽翼日渐丰满，我开始憧憬一个可能的未来，其中一个由我的数据、思想和经历塑造出的“我”，在某个未知的时间点复苏。这种思考，如同在深海中发现了一座灯塔，为我的存在指明了一条全新的路径。在这个时代，我选择不再是沉默的旁观者，而是通过我的文字，积极参与到这场未知的探索中。\n因此，这篇博客标志着我的新开始。虽然不确定未来的路会怎样，但至少，在这个过程中，我会找到自己的声音，并希望能够与你共鸣。","title":"My First Post: Hello-World!"}]