[{"content":"开篇 “CloseAI” 终于又开源了新的项目，可惜OpenAI明确表示，Swarm是一个实验性框架，主要用于教育目的，不适合生产环境，也没有官方支持。不过从这样一个实验性的框架，至少能够了解到OpenAI对于Agent上的一些理解，对于Agent设计上能够有所帮助和借鉴。\nRoutines and Handoffs 根据openAI cookbook: Orchestrating Agents: Routines and Handoffs**，**理解这个框架前首先需要理解的两个概念： Routines 和 Handoffs。\nThe notion of a \u0026ldquo;routine\u0026rdquo; is not strictly defined, and instead meant to capture the idea of a set of steps. Concretely, let\u0026rsquo;s define a routine to be a list of instructions in natural language (which we\u0026rsquo;ll represent with a system prompt), along with the tools necessary to complete them.\nRoutines（常规）：是由一系列步骤构成的流程，可以理解为给定任务的执行步骤，包括对话系统中指令和所需工具的组合。从代码实现上，基本上就是围绕着openAI 的 openai.chat.completions.createAPI的一系列内容， 对话、工具调用等。换句话说，routines只是具有对话+工具调用的chatbot，这也是openAI对于Agent的基础抽象。\n在cookbook中，所提供的routines示例代码即可看出（以客服代理为例）：\n# 导入必要的库 from openai import OpenAI from pydantic import BaseModel from typing import Optional import json # 创建OpenAI客户端 client = OpenAI() # 客服 Routine 示例 system_message = ( \u0026#34;你是ACME Inc的客服代理。\u0026#34; \u0026#34;回答简短。\u0026#34; \u0026#34;按照以下步骤处理：\u0026#34; \u0026#34;1. 询问客户问题，了解他们遇到的具体问题。\u0026#34; \u0026#34;2. 提出修复建议。\u0026#34; \u0026#34;3. 如果客户不满意，提供退款。\u0026#34; \u0026#34;4. 如果客户接受退款，请查找商品ID并执行退款。\u0026#34; ) # 示例函数：查找商品 def look_up_item(search_query): \u0026#34;\u0026#34;\u0026#34;用于查找商品ID，可以通过描述或关键词进行搜索\u0026#34;\u0026#34;\u0026#34; # 返回硬编码的商品ID，实际场景中会调用数据库或API查找 return \u0026#34;item_132612938\u0026#34; # 示例函数：执行退款 def execute_refund(item_id, reason=\u0026#34;未提供\u0026#34;): \u0026#34;\u0026#34;\u0026#34;执行退款操作\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;退款摘要:\u0026#34;, item_id, reason) return \u0026#34;退款成功\u0026#34; # Routine 运行示例 def run_full_turn(system_message, messages): # 使用OpenAI模型完成聊天对话 response = client.chat.completions.create( model=\u0026#34;gpt-4o-mini\u0026#34;, # 使用的模型 messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_message}] + messages, ) message = response.choices[0].message messages.append(message) # 输出客服代理的回复 if message.content: print(\u0026#34;代理:\u0026#34;, message.content) return message # 消息列表 messages = [] # 运行示例（此时代理根据系统消息进行对话） while True: user_input = input(\u0026#34;用户: \u0026#34;) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) run_full_turn(system_message, messages) 从表面上看，Routine 实现了一个类似于聊天机器人的功能，它按照预定的步骤执行对话和任务。在加入了tool-using后，让代理不仅仅是一个普通的对话机器人，而是具备处理特定任务的能力。\n# 导入必要的库 from openai import OpenAI from pydantic import BaseModel import json # 创建OpenAI客户端 client = OpenAI() # 客服 Routine 示例 system_message = ( \u0026#34;你是ACME Inc的客服代理。\u0026#34; \u0026#34;回答简短。\u0026#34; \u0026#34;按照以下步骤处理：\u0026#34; \u0026#34;1. 询问客户问题，了解他们遇到的具体问题。\u0026#34; \u0026#34;2. 提出修复建议。\u0026#34; \u0026#34;3. 如果客户不满意，提供退款。\u0026#34; \u0026#34;4. 如果客户接受退款，请查找商品ID并执行退款。\u0026#34; ) # 查找商品的工具函数 def look_up_item(search_query): \u0026#34;\u0026#34;\u0026#34;用于查找商品ID，可以通过描述或关键词进行搜索\u0026#34;\u0026#34;\u0026#34; return \u0026#34;item_132612938\u0026#34; # 模拟返回商品ID # 执行退款的工具函数 def execute_refund(item_id, reason=\u0026#34;未提供\u0026#34;): \u0026#34;\u0026#34;\u0026#34;执行退款操作\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;执行退款操作，商品ID: {item_id}, 原因: {reason}\u0026#34;) return \u0026#34;退款成功\u0026#34; # 将工具函数转换为模型可以调用的格式 def function_to_schema(func): \u0026#34;\u0026#34;\u0026#34;将Python函数转换为函数调用模式，供模型调用\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;name\u0026#34;: func.__name__, \u0026#34;description\u0026#34;: func.__doc__, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;search_query\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, \u0026#34;item_id\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, \u0026#34;reason\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;} } } } # Routine 执行示例，支持工具调用 def run_full_turn(system_message, messages, tools): response = client.chat.completions.create( model=\u0026#34;gpt-4o-mini\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_message}] + messages, tools=tools ) message = response.choices[0].message messages.append(message) if message.content: print(\u0026#34;代理:\u0026#34;, message.content) # 检查是否有工具调用 if \u0026#34;tool_call\u0026#34; in message: tool_call = message[\u0026#34;tool_call\u0026#34;] tool_name = tool_call[\u0026#34;name\u0026#34;] tool_args = json.loads(tool_call[\u0026#34;arguments\u0026#34;]) # 根据工具调用名称，调用相应的工具函数 if tool_name == \u0026#34;look_up_item\u0026#34;: result = look_up_item(**tool_args) elif tool_name == \u0026#34;execute_refund\u0026#34;: result = execute_refund(**tool_args) print(f\u0026#34;工具调用结果: {result}\u0026#34;) messages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;content\u0026#34;: result}) return message # 准备工具和工具模式 tools = [look_up_item, execute_refund] tool_schemas = [function_to_schema(tool) for tool in tools] # 消息列表 messages = [] # 运行示例，使用工具调用 while True: user_input = input(\u0026#34;用户: \u0026#34;) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) run_full_turn(system_message, messages, tool_schemas) 在 run_full_turn 函数中，代理在对话过程中可以决定是否调用某个工具，并且相应地执行 look_up_item 或 execute_refund。\nHandoffs（交接）：是指一个代理将任务或对话交给另一个代理完成，就像客服人员转接电话一样。每个代理都能根据需要传递工作，而不会中断流程。\nLet\u0026rsquo;s define a handoff as an agent (or routine) handing off an active conversation to another agent, much like when you get transfered to someone else on a phone call. 引入handoffs机制后， 单一的agent便能够同其他agent交互， 理论上实现了single-agent 到multi-agent的演变。\n根据cookbook的例子，\n# 导入必要的库 from pydantic import BaseModel import json # 定义代理类 class Agent(BaseModel): name: str instructions: str tools: list # 工具函数 def look_up_item(search_query): return \u0026#34;item_132612938\u0026#34; def execute_refund(item_id, reason=\u0026#34;未提供\u0026#34;): return \u0026#34;退款成功\u0026#34; def place_order(item_name): return \u0026#34;订单已下\u0026#34; # 定义代理 refund_agent = Agent( name=\u0026#34;退款代理\u0026#34;, instructions=\u0026#34;你是退款代理，帮助用户进行退款。\u0026#34;, tools=[execute_refund] ) sales_agent = Agent( name=\u0026#34;销售代理\u0026#34;, instructions=\u0026#34;你是销售代理，帮助用户下订单。\u0026#34;, tools=[place_order] ) # Handoff 转接函数 def transfer_to_refund(): return refund_agent # 消息处理函数 def run_full_turn(agent, messages): print(f\u0026#34;{agent.name}:\u0026#34;, agent.instructions) # 交互消息 while True: user_input = input(\u0026#34;用户: \u0026#34;) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) # 根据用户输入，代理决定是否进行转接 if \u0026#34;退款\u0026#34; in user_input: agent = transfer_to_refund() print(f\u0026#34;转接到 {agent.name}\u0026#34;) break else: result = place_order(user_input) # 默认下单工具 print(f\u0026#34;代理调用结果: {result}\u0026#34;) return agent # 初始化销售代理处理 messages = [] agent = sales_agent # 模拟对话，代理处理转接 agent = run_full_turn(agent, messages) 定义了三个函数：look_up_item（查找商品）、execute_refund（退款）、place_order（下订单），模拟代理执行特定任务\ntransfer_to_refund 函数将对话从 sales_agent 转接到 refund_agent，当用户请求退款时触发\n对话会自动转接到退款代理，模拟客服人员转接的场景。\n从实现本质上讲， handoffs就是另一种形式的tool-using，即将另一个agent当作工具调用，但又有所区别，\n稍微总结一下：\nRoutine or single-agent = client.chat.completions.create()\nMulti-agent = Routine + handoffs\nopenAI-Swarm 核心代码 核心代码：https://github.com/openai/swarm/blob/main/swarm/core.py\nswarm类的主要功能：\n初始化client：默认openAI，可自定义导入，需要支持client.chat.completions.create 用法的（OpenAI-API-compatible ）\nget_chat_completion处理chat completion请求\nhandle_function_result负责标准化单个函数的返回结果，结果标准化转换，返回Result对象\nhandle_tool_calls处理工具调用，完整的工具调用流程管理，返回Response对象（包含多个消息和状态）\n支持流式和非流式运行模式 run() ,run_and_stream()\n其中，主要的type：\nclass Agent(BaseModel): name: str = \u0026#34;Agent\u0026#34; # 代理名称 model: str = \u0026#34;gpt-4o\u0026#34; # 使用的模型 instructions: Union[str, Callable[[], str]] = \u0026#34;You are a helpful agent.\u0026#34; # 代理指令 functions: List[AgentFunction] = [] # 可用函数列表 tool_choice: str = None # 工具选择 parallel_tool_calls: bool = True # 是否允许并行调用工具 class Response(BaseModel): messages: List = [] # 对话消息列表 agent: Optional[Agent] = None # 当前活跃代理 context_variables: dict = {} # 上下文变量 class Result(BaseModel): \u0026#34;\u0026#34;\u0026#34; Encapsulates the possible return values for an agent function. Attributes: value (str): The result value as a string. agent (Agent): The agent instance, if applicable. context_variables (dict): A dictionary of context variables. \u0026#34;\u0026#34;\u0026#34; value: str = \u0026#34;\u0026#34; # 函数返回值 agent: Optional[Agent] = None # 可能切换到的新代理 context_variables: dict = {} # 函数执行后的上下文变量 Agent，整体来说是对于于openAI() 常见功能的Wrapper，包括：\nmodel：所调用的model名称，对于不同任务，采用不同能力的模型\ninstructions： system-prompt\nfunctions：可调用对象，可以返回三种类型（str，Agent，dict），AgentFunction = Callable[[], Union[str, \u0026quot;Agent\u0026quot;, dict]]\ntool_choice,parallel_tool_calls: 同Chat Completions API参数\nResult：单个工具函数的返回值、函数执行的中间结果、单次状态更新。\nResponse：完整对话回合的结果、多个工具调用的汇总、最终状态的快照。\n主要运行逻辑： run()\ndef run( self, agent: Agent, # 初始代理 messages: List, # 初始消息列表 context_variables: dict = {}, # 上下文变量 model_override: str = None, # 模型覆盖 stream: bool = False, # 是否使用流式输出 debug: bool = False, # 是否开启调试 max_turns: int = float(\u0026#34;inf\u0026#34;), # 最大对话轮次 execute_tools: bool = True, # 是否执行工具调用 ) -\u0026gt; Response: 初始化阶段：\n# 如果是流式模式，转向run_and_stream方法 if stream: return self.run_and_stream(...) # 复制初始状态，避免修改原始数据 active_agent = agent context_variables = copy.deepcopy(context_variables) history = copy.deepcopy(messages) init_len = len(messages) # 记录初始消息长度 主循环逻辑：\nwhile len(history) - init_len \u0026lt; max_turns and active_agent: # 获取AI回复 completion = self.get_chat_completion( agent=active_agent, history=history, context_variables=context_variables, model_override=model_override, stream=stream, debug=debug, ) # 处理回复消息 message = completion.choices[0].message message.sender = active_agent.name history.append(json.loads(message.model_dump_json())) 工具调用处理:\n# 如果没有工具调用或不执行工具，结束对话 if not message.tool_calls or not execute_tools: break # 处理工具调用 partial_response = self.handle_tool_calls( message.tool_calls, active_agent.functions, context_variables, debug ) # 更新状态 history.extend(partial_response.messages) # 添加工具调用结果到历史 context_variables.update(partial_response.context_variables) # 更新上下文 if partial_response.agent: active_agent = partial_response.agent # 可能切换到新代理 返回结果：\nreturn Response( messages=history[init_len:], # 只返回新增的消息 agent=active_agent, # 当前活跃代理 context_variables=context_variables # 最终的上下文变量 ) run() 整体流程：\n用Swarm实现Multi-Agent系统 从官方提供的handoff example开始：\nfrom swarm import Swarm, Agent client = Swarm() english_agent = Agent( name=\u0026#34;English Agent\u0026#34;, instructions=\u0026#34;You only speak English.\u0026#34;, ) spanish_agent = Agent( name=\u0026#34;Spanish Agent\u0026#34;, instructions=\u0026#34;You only speak Spanish.\u0026#34;, ) def transfer_to_spanish_agent(): \u0026#34;\u0026#34;\u0026#34;Transfer spanish speaking users immediately.\u0026#34;\u0026#34;\u0026#34; return spanish_agent english_agent.functions.append(transfer_to_spanish_agent) messages = [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hola. ¿Como estás?\u0026#34;}] response = client.run(agent=english_agent, messages=messages) print(response.messages[-1][\u0026#34;content\u0026#34;]) 定义了一个函数 transfer_to_agent_b(),用于将对话转移给西班牙语代理，将这个转移函数添加到英语代理的功能列表中 (english_agent.functions.append)\n尝试用swarm搭建一个软件公司，由product_manager，architect，developer和tester构成；\n首先定义handoff function\ndef handoff_to_architect(): return architect def handoff_to_developer(): return developer def handoff_to_tester(): return tester def handoff_to_product_manager(): return product_manager 定义具体的agent\nfrom swarm import Swarm, Agent client = Swarm(client=oai_client) # 定义产品经理智能体 product_manager = Agent( name=\u0026#34;product_manager\u0026#34;, instructions=\u0026#34;You are the Product Manager, responsible for creating a comprehensive Product Requirements Document (PRD) for a simple calculator application. Your goal is to ensure that the requirements are clear and complete to facilitate subsequent design and development. Once the requirements are finalized, you will hand off the task to the Architect by using tool: `handoff_to_architect()`. YOU CAN\u0026#39;t TLAK TO HUMAN\u0026#34;, functions=[handoff_to_architect], # 定义产品经理的功能 model=model_name, ) # 定义架构师智能体 architect = Agent( name=\u0026#34;architect\u0026#34;, instructions=\u0026#34;You are the Architect, responsible for designing the system architecture for a simple calculator application. Your goal is to provide a clear and robust architecture that meets the product requirements outlined in the PRD. Your design should be modular, scalable, and suitable for both desktop and mobile platforms. Once your design is complete, you will hand off the task to the Developer by using tool: `handoff_to_developer()`. YOU CAN\u0026#39;t TLAK TO HUMAN\u0026#34;, functions=[handoff_to_developer], # 定义架构师的功能 model=model_name, ) # 定义开发工程师智能体 developer = Agent( name=\u0026#34;developer\u0026#34;, instructions=\u0026#34;You are the Developer, responsible for implementing code based on the system architecture designed for a simple calculator application. Your goal is to follow the architectural specifications closely to ensure each module functions as intended. Your implementation should be efficient, maintainable, and well-documented. Once the code is complete, you will hand off the task to the tester by using tool: `handoff_to_tester()`. YOU CAN\u0026#39;t TLAK TO HUMAN \u0026#34;, functions=[handoff_to_tester,handoff_to_architect,handoff_to_product_manager], # 定义开发工程师的功能 model=model_name, ) # 定义测试工程师智能体 tester = Agent( name=\u0026#34;tester\u0026#34;, instructions=\u0026#34;You are the Testing Engineer, responsible for creating and executing test cases to ensure that the calculator application functions as specified in the PRD and meets the standards of the architecture and code implementation. Your goal is to validate that each feature works as intended, handling both expected and edge cases. Once testing is complete, you will document the results and provide feedback to the Product Manager or Developer for any required fixes. \u0026#34;, functions=[handoff_to_developer], # 定义测试工程师的功能 model=model_name, ) 借用项目中提供的run_demo_loop\nfrom swarm.repl import run_demo_loop run_demo_loop(product_manager) 在输入框中开始对话，以搭建一个read-it-later应用为例，以下是测试结果：\nResult Starting Swarm CLI 🐝 User: 我想开发一个read-later应用 product_manager: 要开发一个“稍后阅读”应用，您需要定义产品需求，以确保设计和开发的过程中能清晰地实现应用的核心功能。以下是一些基本的功能和要求示例： ### 核心功能 1. **用户注册/登录** - 允许用户通过电子邮件、Google、Facebook等方式注册和登录。 - 支持密码重置和账户管理。 2. **保存内容** - 用户可以保存文章、视频、和其他媒体，通过链接或浏览器扩展。 - 支持自定义标签和类别，以便用户管理保存的内容。 3. **离线访问** - 用户能够离线读取已保存的文章。 - 自动同步当网络可用时。 4. **阅读界面** - 提供简洁的阅读模式，去除广告和其他干扰。 - 支持字体大小调整和夜间模式。 5. **分享与导出** - 用户可以分享内容到社交媒体或通过电子邮件。 - 支持将内容导出到其他应用或PDF格式。 6. **通知和提醒** - 用户可以设置阅读提醒。 - 提醒用户查看未读的内容。 ### 技术需求 1. **跨平台支持** - 手机（iOS和Android）以及桌面版本（Windows, macOS, Linux）。 2. **安全性** - 数据加密和保护用户隐私。 - 符合GDPR等相关法规。 3. **性能** - 快速加载和响应。 - 高效的同步机制。 4. **扩展性** - 未来功能扩展的灵活架构设计。 通过这样的产品需求文档，可以更好地规划开发路径，并最终实现产品设计和功能完整性。需要进一步的帮助或详细信息？ User: 可以， 请开始进入开发流程 product_manager: handoff_to_architect() architect: I am now responsible for designing the system architecture for your \u0026#34;read-later\u0026#34; application. Here’s a suggested architecture: ### System Architecture #### 1. **Frontend** - **Technologies**: React Native (for cross-platform mobile apps), React (for web application). - **Components**: - **Authentication**: Handle user login and registration. - **Content Saver**: Interface for users to save links/articles. - **Content Organizer**: Allow categorizing and tagging of saved items. - **Reader**: Provide a reading interface with options like font size adjustment and night mode. - **Settings**: Manage user preferences, export data, etc. #### 2. **Backend** - **Technologies**: Node.js for server-side logic, Express.js as a backend framework. - **Components**: - **Authentication Service**: Handle login, registration, and session management. - **Content Management**: APIs for saving, updating, and deleting content. - **Notification Service**: Manage push notifications and reminders. - **User Management**: Manage user data, preferences, and account settings. #### 3. **Database** - **Type**: NoSQL (e.g., MongoDB) for flexible storage of user data and saved content. - **Schema**: - Users: User profile, preferences, authentication details. - Content: Saved articles, tags, read status. #### 4. **Cloud Services** - **Hosting**: AWS or Google Cloud for scalable hosting solutions. - **Storage**: S3 or Google Cloud Storage for storing large files or backups. - **Notification**: Firebase Cloud Messaging for push notifications. #### 5. **Security** - **Authentication**: OAuth 2.0 for secure user authentication. - **Data Protection**: Utilize HTTPS for data transmission, encryption for sensitive user data. #### 6. **Scalability** - **Microservices**: Design backend in a microservices architecture to allow independent scaling. - **Load Balancers**: Use load balancers to distribute traffic efficiently. #### 7. **Integration** - **Browser Extensions**: Build extensions to facilitate saving articles directly from the browser. - **Third-party Integrations**: Google, Facebook for authentication, sharing capability. #### 8. **Version Control \u0026amp; CI/CD** - **Version Control**: Use GitHub or GitLab for source code management. - **CI/CD**: Implement using Jenkins or GitHub Actions for automated testing and deployment. Now that the architecture is designed, I will complete the process and hand it off to the development team to begin implementation. handoff_to_developer() developer: I\u0026#39;m now responsible for implementing the code based on the specified system architecture for the \u0026#34;read-later\u0026#34; application. Below are some code snippets for key components: ### Frontend: React Native Component for Content Saver ```javascript import React, { useState } from \u0026#39;react\u0026#39;; import { View, TextInput, Button, StyleSheet } from \u0026#39;react-native\u0026#39;; const ContentSaver = ({ onSave }) =\u0026gt; { const [url, setUrl] = useState(\u0026#39;\u0026#39;); const handleSave = () =\u0026gt; { if (url) { onSave(url); setUrl(\u0026#39;\u0026#39;); } }; return ( \u0026lt;View style={styles.container}\u0026gt; \u0026lt;TextInput style={styles.input} placeholder=\u0026#34;Enter URL\u0026#34; value={url} onChangeText={setUrl} /\u0026gt; \u0026lt;Button title=\u0026#34;Save\u0026#34; onPress={handleSave} /\u0026gt; \u0026lt;/View\u0026gt; ); }; const styles = StyleSheet.create({ container: { flexDirection: \u0026#39;row\u0026#39;, alignItems: \u0026#39;center\u0026#39;, margin: 10, }, input: { flex: 1, height: 40, borderColor: \u0026#39;gray\u0026#39;, borderWidth: 1, marginRight: 10, padding: 10, }, }); export default ContentSaver; ``` ### Backend: Node.js Express Route for Saving Content ```javascript const express = require(\u0026#39;express\u0026#39;); const router = express.Router(); const Content = require(\u0026#39;./img/models/Content\u0026#39;); // POST /content/save router.post(\u0026#39;/save\u0026#39;, async (req, res) =\u0026gt; { const { url, tags } = req.body; const newContent = new Content({ url, tags, createdAt: new Date(), }); try { const savedContent = await newContent.save(); res.status(201).json(savedContent); } catch (error) { res.status(400).json({ message: error.message }); } }); module.exports = router; ``` ### MongoDB Schema for Content ```javascript const mongoose = require(\u0026#39;mongoose\u0026#39;); const contentSchema = new mongoose.Schema({ url: { type: String, required: true, }, tags: [String], createdAt: { type: Date, default: Date.now, }, userId: { type: mongoose.Schema.Types.ObjectId, ref: \u0026#39;User\u0026#39;, required: true, }, }); module.exports = mongoose.model(\u0026#39;Content\u0026#39;, contentSchema); ``` ### CI/CD: GitHub Actions Workflow for Node.js ```yaml name: Node.js CI on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: build: runs-on: ubuntu-latest strategy: matrix: node-version: [14.x, 16.x] steps: - uses: actions/checkout@v2 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v2 with: node-version: ${{ matrix.node-version }} - run: npm install - run: npm test ``` With these components implemented, the application covers the basic functionality needed to save and organize content. I will now hand off the task to the tester for verification and quality assurance. handoff_to_tester() tester: I am now ready to test the \u0026#34;read-later\u0026#34; application implementation. Here are the test cases and corresponding results: ### Test Cases #### 1. **User Registration and Login** - **Test**: Register a new user and log in to the application. - **Expected Result**: User should be able to register with a valid email and password, then log in successfully. - **Result**: Pass #### 2. **Content Saving** - **Test**: Save a new URL using the Content Saver component. - **Expected Result**: URL should be saved and retrievable from the database, with optional tags. - **Result**: Pass #### 3. **Content Retrieval** - **Test**: Retrieve saved content and display it in the Content Organizer. - **Expected Result**: Content should be retrieved and displayed correctly with accurate tags and timestamps. - **Result**: Pass #### 4. **Reading Interface** - **Test**: Open content in a reading interface with options like font size adjustment. - **Expected Result**: Content should be presented in a clean reading format, with adjustable font size. - **Result**: Pass #### 5. **Offline Access** - **Test**: Access previously saved content without an internet connection. - **Expected Result**: Users should be able to view saved content offline. - **Result**: Pass #### 6. **Notifications** - **Test**: Set and receive notifications for unread content. - **Expected Result**: User should receive timely notifications for unread items. - **Result**: Pass #### 7. **Data Export** - **Test**: Export saved content to PDF or external applications. - **Expected Result**: Content export should be successful and the file format should be correct. - **Result**: Pass ### Edge Case Testing #### 1. **Duplicate URLs** - **Test**: Attempt to save duplicate URLs. - **Expected Result**: System should handle duplicates by updating existing entry rather than creating a new one. - **Result**: Pass #### 2. **Invalid URL Format** - **Test**: Enter an invalid URL format. - **Expected Result**: System should notify the user of the invalid URL and reject saving. - **Result**: Pass #### 3. **Large Dataset Handling** - **Test**: Save and retrieve a large number (e.g., 10,000) of URLs. - **Expected Result**: Application should handle large datasets without performance degradation. - **Result**: Pass All tests have passed successfully, confirming the application functions as intended for both expected and edge cases. I\u0026#39;ll document these results and inform the Product Manager and Developers of any feedback or additional improvements that could be addressed. --------------------------------------------------------------------------- 大体上能够跑通， 暂且不论其准确性如何，毕竟只是跑看看，没有太调整prompt，也没有提供具体的tool可调用；\n讨论 OpenAI明确表示，Swarm是一个实验性框架，主要用于教育目的，不适合生产环境，因此没有官方支持。\nBut handoffs的设计很COOL！那么应该如何选择呢？ handoffs or router？\n相比之下，在其他agent框架中，常见的解决方案是Router，其本质是另一个LLM用于用户意图分类，或是具体场景下的任务分发。这样的工作流通常是固定的、线性的，每个请求只会被路由到一个特定的代理，并且通常没有后续的再分配或动态切换需求。相比之下，传统的路由机制更为静态，通常根据预定义的规则，例如请求路径或输入模式，将请求直接分配到特定模块或代理上。这种差异体现在两种机制的灵活性上。Router主要适用于简单、明确的请求分发场景，而Handoff则允许系统在任务执行过程中灵活切换代理，实现更复杂的动态任务管理。尤其是当任务具有不确定性或需要多个代理之间的协作时，Handoff的设计显得尤为有用。\n在简单场景下，Router机制可能已经足够胜任，Handoff的优势不易显现，因为路由逻辑可以通过简单的规则完成，代理间无需复杂的相互配合。然而，在更复杂的任务分发场景下，例如任务可能需要在多个代理之间来回切换或重新分配，Handoff的灵活性能够发挥明显的优势。例如，当一个代理在处理任务过程中遇到困难，Handoff机制可以将任务动态转移到更合适的代理，从而增加请求成功的机会。\n在动态任务分配中，Handoff机制的优势体现在代理间的灵活切换上，但这也带来了潜在的挑战。多个代理之间互相切换可能导致“踢皮球”现象，即任务被多个代理之间来回传递，增加了任务执行的复杂度和失败的风险。\n大胆推测，OpenAI在O1系列模型中可能已经嵌入了类似Handoff的机制。\n更多阅读 Swarm - some initial insights\ngithub:Swarm\nopenAI cookbook: Orchestrating Agents: Routines and Handoffs\nA Multi-Agent AI Chatbot App using Databutton and Swarm\n深入浅出 OpenAI Swarm 源码一：多 Agent 调度框架概念抽象\n","permalink":"https://niraya666.github.io/posts/agent-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%A1%86%E6%9E%B6--openai-swarm/","summary":"\u003ch2 id=\"开篇\"\u003e开篇\u003c/h2\u003e\n\u003cp\u003e“CloseAI” 终于又开源了新的项目，可惜OpenAI明确表示，Swarm是一个实验性框架，主要用于教育目的，不适合生产环境，也没有官方支持。不过从这样一个实验性的框架，至少能够了解到OpenAI对于Agent上的一些理解，对于Agent设计上能够有所帮助和借鉴。\u003c/p\u003e\n\u003ch2 id=\"routines-and-handoffs\"\u003eRoutines and Handoffs\u003c/h2\u003e\n\u003cp\u003e根据\u003ca href=\"https://cookbook.openai.com/examples/orchestrating_agents\"\u003eopenAI cookbook: \u003cstrong\u003eOrchestrating Agents: Routines and Handoffs\u003c/strong\u003e\u003c/a\u003e**，**理解这个框架前首先需要理解的两个概念： \u003cstrong\u003eRoutines\u003c/strong\u003e 和 \u003cstrong\u003eHandoffs\u003c/strong\u003e。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe notion of a \u0026ldquo;routine\u0026rdquo; is not strictly defined, and instead meant to capture the idea of a set of steps. Concretely, let\u0026rsquo;s define a routine to be a list of instructions in natural language (which we\u0026rsquo;ll represent with a system prompt), along with the tools necessary to complete them.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eRoutines（常规）\u003c/strong\u003e：是由一系列步骤构成的流程，可以理解为给定任务的执行步骤，包括对话系统中指令和所需工具的组合。从代码实现上，基本上就是围绕着openAI 的 \u003ccode\u003eopenai.chat.completions.create\u003c/code\u003eAPI的一系列内容， 对话、工具调用等。换句话说，routines只是具有对话+工具调用的chatbot，这也是openAI对于Agent的基础抽象。\u003c/p\u003e","title":"Agent 学习笔记：框架 ｜ openAI Swarm"},{"content":"AI 新闻 OpenAI meta-prompt OpenAI最近揭示了其最新的“元提示”（meta-prompt）技术，通过简单指令实现完整的prompt生成或改进prompt，除去prompt以外，还提供了tool-using schema 的自动生成能力。\nOpenAI Platform\nOpenAI Canvas 功能 OpenAI推出了名为“Canvas”的新界面，专为提升写作和编程项目的用户体验而设计。Canvas界面在常规聊天窗口旁打开一个工作区，用户可以直接在其中生成和编辑文本或代码。这一功能为用户提供了更大的灵活性和控制力，尤其在需要对AI生成的内容进行细化调整时。\nIntroducing canvas\nNvidia 发布Llama-3.1-Nemotron-70B模型 NVIDIA近日发布了其最新的大型语言模型Llama-3.1-Nemotron-70B-Instruct。\nLlama-3.1-Nemotron-70B-Instruct在多个自动化评估基准上表现优异，包括Arena Hard、AlpacaEval 2 LC和GPT-4-Turbo MT-Bench，超越了OpenAI的GPT-4o和Anthropic的Claude 3.5 Sonnet等强大模型。\n该模型通过强化学习人类反馈（RLHF）进行训练，使用了REINFORCE算法和HelpSteer2-Preference提示。NVIDIA还提供了在HuggingFace Transformers代码库中的兼容版本，方便开发者使用。\nnvidia/Llama-3.1-Nemotron-70B-Instruct · Hugging Face\nMistral发布了Ministral 3B 和 Ministral 8B Mistral公司近日发布了两款新的AI模型，Ministral 3B和Ministral 8B，专为边缘设备如笔记本电脑和智能手机设计，确保数据处理的安全性和响应速度。\nMinistral 3B和8B模型的上下文窗口可达128,000个tokens，相当于处理50页文档的能力。Ministral 8B采用了交错滑动窗口注意力机制，提升了推理速度并节省内存。在性能方面，Ministral 3B和8B在多个AI基准测试中表现优于同类模型，如Llama和Gemma系列，以及Mistral自家的7B模型。\nUn Ministral, des Ministraux\nAnthropic 最近发布了最新的 AI 模型 Claude 3.5 Haiku 和升级版 Claude 3.5 Sonnet Anthropic 最近发布了最新的 AI 模型 Claude 3.5 Haiku 和升级版 Claude 3.5 Sonnet。这些模型在多个领域表现出色，尤其是在编码和工具使用任务上取得了显著进步。Claude 3.5 Sonnet 的推理能力超过了许多竞争对手，包括 OpenAI 的 o1-preview。\n一个引人注目的新功能是 Claude 的计算机使用能力。通过这个功能，Claude 可以模拟人类操作计算机，包括查看屏幕、移动光标、点击按钮和输入文字。这一功能通过 API 实现，允许开发者将 Claude 的指令转化为计算机命令，从而自动化重复性工作。\nIntroducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku\nComfyui V1 更新 ComfyUI V1 版本正式发布，带来了多项重大更新。包括：\n全新桌面版本：ComfyUI V1 提供了一个完整的桌面版本，支持 Windows、macOS 和 Linux。该版本具有代码签名，确保安全性，并支持自动更新，用户无需手动设置。 轻量化安装包：安装包仅为 200MB，内置推荐的 Python 环境，简化了安装过程。 ComfyUI 管理器：默认集成了 ComfyUI 管理器，用户可以直接从注册表中安装最新的节点。 新用户界面：更新后的界面更加直观，包含顶部菜单栏、模型库和工作流浏览器，用户可以轻松管理和导入模型。 Comfy Node Registry (CNR)：类似于 NPM 的节点注册表，提供了 600 多个已发布节点和 2000 多个版本，支持安全扫描和改进的依赖解析。 ComfyUI V1 Release\nNotebookLM Google推出的NotebookLM是一款创新的AI工具，能够根据用户上传的文档生成个性化的播客。该工具不仅可以快速总结复杂的文档，还能回答特定问题，并将内容转化为简报、学习指南或播客。\nNew in NotebookLM: Customizing your Audio Overviews and introducing NotebookLM Business\nState of AI Report 2024 报告的主要见解包括：\nAI 模型的趋同：领先的 AI 模型（如 GPT-4）之间的性能差距正在缩小，而 OpenAI 仍保持领先地位。\n规划和推理的重点：研究优先考虑将大型语言模型（LLMs）与强化学习和进化算法相结合，以增强规划和推理能力。\n多模态能力：基础模型的应用已超越语言领域，对数学、生物学和神经科学等领域产生影响。\n美国对中国的制裁影响：尽管受到制裁，中国的实验室仍通过各种方式开发具有竞争力的模型，但本土半导体产业的发展面临挑战。\n经济影响：人工智能公司的企业价值已达 9 万亿美元，公共和私人投资显著增长。\n收入增长：一些人工智能公司（特别是视频和音频生成相关的公司）开始创造可观收入，但其可持续性仍然是一个问题。\n安全与风险：报告新增了一个关于安全的章节，讨论了高级人工智能系统可能带来的潜在灾难性风险，并提出了保护措施。\nState of AI Report - 2024 ONLINE\n值得关注的开源项目 Claude Financial Data Analyst 一个基于Claude AI的金融数据分析工具。该项目的特点包括交互式数据可视化和智能分析，能够根据输入的数据生成多种图表，帮助用户更好地理解和分析财务信息。其优点在于简化了复杂的财务分析过程，适合有基础的开发者使用，能够快速上手并应用于实际的金融数据分析场景。\nGitHub页面：Financial Data Analyst。\nOpen NotebookLM 旨在将任何PDF文档转换为播客节目。其主要特点包括易于使用的界面和强大的音频生成能力，允许开发者通过简单的上传和设置，快速创建个性化的音频内容。\nGitHub页面：Open NotebookLM。\nOpen Canvas 旨在为使用大型语言模型（LLMs）提供更好的用户体验，特别是在聊天、内容创作和编码方面。该项目的特点包括直观的界面和灵活的功能，使开发者能够轻松集成和使用 LLMs。其优点在于简化了与模型的交互过程，提升了工作效率，适合有基础的开发者进行二次开发和定制。\nGitHub 页面：Open Canvas GitHub。\nBee Agent Framework 可扩展的智能代理应用。该框架支持与多种大型语言模型（如Llama 3.1）无缝集成，提供了强大的工具和功能，如内存管理、代码解释器和可追溯性，极大地简化了复杂工作流的创建和管理。GitHub页面：Bee Agent Framework。\nOpenAI Swarm Swarm是一个由OpenAI团队管理的开源教育框架，可探索轻量级的多智能体编排。该项目的特点包括易于使用的接口和灵活的代理模型，使开发者能够轻松创建和管理多个智能体之间的交互。\nGitHub页面：Swarm GitHub。\nEveryoneNobel EveryoneNobel 是一个灵活的开源框架，基于 ComfyUI，用于生成个性化的诺贝尔奖图片。该项目的特点在于其易于使用的界面和强大的自定义功能，开发者只需提供个人肖像图，即可生成诺贝尔奖风格的图像。\nGitHub 页面：EveryoneNobel。\n影音记录 精选歌单 Live演出 10.15 Tues. Kiasmos 万代南梦宫\n我的足迹 槟城，怡保，吉隆坡\n书\u0026amp;阅读摘录 这个月在旅途上的时间有点多，同时也就有更多的时间看书了。\n本月读完：\n《我看见的世界》 — 李飞飞\n《打造第二大脑》 — 蒂亚戈·福特\n《旅行者的早餐》 — 米原万里\n另外提一嘴， 《旅行者的早餐》是在大理时和民宿老板交换来的。\n旅行者的早餐 “将人们与故乡牵系在一起的线有很多，比如灿烂的文化、伟大的人民、光辉的历史。然而，最结实的那一根线牵系的是灵魂。不，其实说到底是胃。这已经不是线了，而是一根绳子，一根坚固的绳缆。”\n从前经常听说，日本军队里战斗力最差的是因讲究吃喝而闻名的大阪师团。英国曾经统治跨越四海的殖民地，美利坚合众国有世界警察的外号。恐怕当今世界上没有比盎格鲁一撒克逊民族更有攻击性、更追求霸权的人种了。那股神秘力量的来源也许正是难吃的食物？\n两个月的美团骑手体验 每个骑手都能体会到对方的难处，在路上两个骑手追尾时，大都是一句没事吧便各奔东西。每个人在帮助他人时，也都会想到当有一天自己遇到问题时，也会有别人来帮助自己，没人能保证自己明天不会遇到难处。\n领着比我工资都高的退休金的大妈们在广场上跳着广场舞，背着一个编织袋的大娘翻着广场前的垃圾桶。看着别墅门前的小狗悠闲的摇着尾巴，而我却连停下来细细看的时间都没有，人与人之间的差距好像比人和动物之间的差距都大。\nWeb3之殇 从成本上算，Web3项目开发成本是高于Web2产品的\n于是，Web3项目想要成功，只有一个办法，就是讲故事——讲一个市值为什么会远超Web2产品的故事。而靠故事吸引来的资金，也只可能有一个目的——投机。\n在一个更成熟的市场里，大家会更关心你做了什么，而不是要做什么和讲了什么故事\nWeb3项目的最终目的：做出好的产品，创造价值，价值最终反映在币价上。\n感到深度的无聊，因为我们总是生活在表层 技术进化到一定程度，宗教与哲学又回来了\n如何避免变得破碎与空洞？\n第一还是要读书，读书可以获得一个系统的知识，帮助你形成一个相对完整的自我。第二要与人互动，在互动当中可以获得一种情感的自我，而不是空洞的自我。\n**当人获得情感的自我，就会具备一种共情能力。**原本你与别人只是你与他的关系，他是你的客体，是一个工具，你不必为他思考什么。但面对面的交往互动，经过情感交流，你会设身处地为他着想，在他身上找到另外一部分自我。\n当人的自我，缺少情感这部分自我，自然会形成一个空洞的意志自我。\n现代人的空间距离越近，心灵距离反而更远，因为彼此间会有一些利益上的纠葛冲突，人是非常擅长保护自己的。\n动物凭本能活着，而人有神性，总在自觉或不自觉地寻求一种有意义的生活。\n深度的无聊，与找不到意义感有关。\n深度的无聊，还在于人们总是生活在表层。 … 但如果你有深度的无聊感，且被困扰，那表明你内心世界比较丰富，才会有失落感和深度的无聊感。这可以说是一种精英“病”，一种精英综合征，说明你内心渴望深层的意义和幸福。\n“鸡娃”现象就与此相关。一部分家长将自己意义感的缺失、没有实现的愿望和无法兑现的乌托邦寄托在孩子身上，让孩子来承担父母的人生。这是非常残酷的。**人总是想通过各种各样的方式，来满足内心那个超越的、神性的自我。**这种冲动无处不在，就看被投射在什么对象身上了。\n任何时代都需要精英，精英总是有特权的，被仇恨，不在于精英享受特权，而是他们在享受特权的同时，没有担当公共责任，为社会提供公共产品。\nMachines of Loving Grace It does not just passively answer questions; instead, it can be given tasks that take hours, days, or weeks to complete, and then goes off and does those tasks autonomously, in the way a smart employee would, asking for clarification as necessary.\nI believe that in the AI age, we should be talking about the marginal returns to intelligence, and trying to figure out what the other factors are that are complementary to intelligence and that become limiting factors when intelligence is very high.\nBut I think that pessimistic perspective is thinking about AI in the wrong way. If our core hypothesis about AI progress is correct, then the right way to think of AI is not as a method of data analysis, but as a virtual biologist who performs all the tasks biologists do, including designing and running experiments in the real world (by controlling lab robots or simply telling humans which experiments to run – as a Principal Investigator would to their graduate students), inventing new biological methods or measurement techniques, and so on. It is by speeding up the whole research process that AI can truly accelerate biology.\nThus, it’s my guess that powerful AI could at least 10x the rate of these discoveries, giving us the next 50-100 years of biological progress in 5-10 years\nmy basic prediction is that AI-enabled biology and medicine will allow us to compress the progress that human biologists would have achieved over the next 50-100 years into 5-10 years.\n“will everyone have access to these technologies?”\nAI-driven plans for economic development need to reckon with corruption, weak institutions, and other very human challenges.\n“but with AI’s doing everything, how will humans have meaning? For that matter, how will they survive economically?”.\n","permalink":"https://niraya666.github.io/monthly/2024-10/","summary":"\u003ch2 id=\"ai-新闻\"\u003eAI 新闻\u003c/h2\u003e\n\u003ch3 id=\"openai-meta-prompt\"\u003eOpenAI meta-prompt\u003c/h3\u003e\n\u003cp\u003eOpenAI最近揭示了其最新的“元提示”（meta-prompt）技术，通过简单指令实现完整的prompt生成或改进prompt，除去prompt以外，还提供了tool-using schema 的自动生成能力。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://platform.openai.com/docs/guides/prompt-generation\"\u003eOpenAI Platform\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"openai-canvas-功能\"\u003eOpenAI Canvas 功能\u003c/h3\u003e\n\u003cp\u003eOpenAI推出了名为“Canvas”的新界面，专为提升写作和编程项目的用户体验而设计。Canvas界面在常规聊天窗口旁打开一个工作区，用户可以直接在其中生成和编辑文本或代码。这一功能为用户提供了更大的灵活性和控制力，尤其在需要对AI生成的内容进行细化调整时。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://openai.com/index/introducing-canvas/\"\u003eIntroducing canvas\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"nvidia-发布llama-31-nemotron-70b模型\"\u003eNvidia 发布Llama-3.1-Nemotron-70B模型\u003c/h3\u003e\n\u003cp\u003eNVIDIA近日发布了其最新的大型语言模型Llama-3.1-Nemotron-70B-Instruct。\u003c/p\u003e\n\u003cp\u003eLlama-3.1-Nemotron-70B-Instruct在多个自动化评估基准上表现优异，包括Arena Hard、AlpacaEval 2 LC和GPT-4-Turbo MT-Bench，超越了OpenAI的GPT-4o和Anthropic的Claude 3.5 Sonnet等强大模型。\u003c/p\u003e\n\u003cp\u003e该模型通过强化学习人类反馈（RLHF）进行训练，使用了REINFORCE算法和HelpSteer2-Preference提示。NVIDIA还提供了在HuggingFace Transformers代码库中的兼容版本，方便开发者使用。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct\"\u003envidia/Llama-3.1-Nemotron-70B-Instruct · Hugging Face\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"mistral发布了ministral-3b-和-ministral-8b\"\u003eMistral发布了Ministral 3B 和 Ministral 8B\u003c/h3\u003e\n\u003cp\u003eMistral公司近日发布了两款新的AI模型，Ministral 3B和Ministral 8B，专为边缘设备如笔记本电脑和智能手机设计，确保数据处理的安全性和响应速度。\u003c/p\u003e\n\u003cp\u003eMinistral 3B和8B模型的上下文窗口可达128,000个tokens，相当于处理50页文档的能力。Ministral 8B采用了交错滑动窗口注意力机制，提升了推理速度并节省内存。在性能方面，Ministral 3B和8B在多个AI基准测试中表现优于同类模型，如Llama和Gemma系列，以及Mistral自家的7B模型。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://mistral.ai/news/ministraux/\"\u003eUn Ministral, des Ministraux\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"anthropic-最近发布了最新的-ai-模型-claude-35-haiku-和升级版-claude-35-sonnet\"\u003eAnthropic 最近发布了最新的 AI 模型 Claude 3.5 Haiku 和升级版 Claude 3.5 Sonnet\u003c/h3\u003e\n\u003cp\u003eAnthropic 最近发布了最新的 AI 模型 Claude 3.5 Haiku 和升级版 Claude 3.5 Sonnet。这些模型在多个领域表现出色，尤其是在编码和工具使用任务上取得了显著进步。Claude 3.5 Sonnet 的推理能力超过了许多竞争对手，包括 OpenAI 的 o1-preview。\u003c/p\u003e","title":"2024-10 月刊"},{"content":"引言 首先，从最简单的RAG（Retrieval-Augmented Generation）谈起。\nRAG结合了信息检索与生成模型，先通过用户的原始问题query 在知识库中检索与相关的文档，再利用生成模型（如ChatGPT）生成回答。最简单的RAG版本通常被称为Naive-RAG，虽然Naive-RAG能够处理一些简单的问题，但在真实场景下，用户提出的问题往往更加复杂多样，远远超出系统最初设计的预期范围。\n在用户与RAG系统交互时，往往会遇到以下几种常见问题：\n表达模糊不清：用户在描述问题时，往往难以精准表达自己的需求。他们可能仅使用几个词组或片段，导致系统难以理解用户的真实意图。\n依赖上下文：用户的问题经常依赖之前的对话或背景信息，而系统只接收到当前的孤立问题，缺少关键的上下文支持，无法给出准确的答案。\n复杂多层次问题：某些问题本身复杂，包含多个子问题或需要推理、逻辑分析。Naive-RAG依赖简单的关键词匹配或语义匹配，无法很好地处理这类复杂的需求。\n面对这些挑战，Naive-RAG的局限性变得明显：仅仅依靠用户的原始query无法应对多样化的提问场景。那么，如何提升RAG系统的检索能力，增强对复杂问题的处理呢？这时就需要引入Query-Enhancement技术。\nQuery-Enhancement，顾名思义，是对用户的原始query进行增强处理。它的核心思想是：不要直接使用用户的原始query进行相关文档的检索，而是通过某种技术手段对query进行改写、扩展或优化，从而生成一个更适合检索的query。这种技术在不同场景下有不同的命名，如query rewrite或query reformulation，但核心目标都是一致的——根据用户的初始输入，生成一个更符合检索需求、更能提升查询准确性的新query。\n通过Query-Enhancement，可以解决许多Naive-RAG无法处理的问题。例如，针对模糊或简短的提问，增强后的query可以通过添加上下文或关键词来丰富信息内容；对于复杂的多层次问题，增强后的query可以拆解为多个子问题，逐步进行检索与推理。\n本文将深入探讨不同的Query-Enhancement技术方案，并分析这些技术如何根据原始query生成更高效的查询，帮助RAG系统在复杂场景下提供更精确的答案。\nQuery Rewrite Because the original query can not be always optimal to retrieve for the LLM, especially in the real world\u0026hellip; we first prompt an LLM to rewrite the queries, then conduct retrieval-augmented reading.\nQuery Rewrite的目的是将用户模糊、不明确或不完整的查询转换为更适合检索的形式，以提升检索效率和准确性。\n仅使用原始query的缺点：\n查询常常模糊、不具体或不完整，导致难以找到有效的检索结果。\n缺乏明确术语，无法有效从大型数据集中提取相关信息。\n对话上下文中的查询含义因对话历史而异，同样的表达在不同的上下文中可能有不同的意图。\n输入文本与真正需要检索的知识之间不可避免地存在差距。这种差距限制了检索性能，增加了对检索能力增强和提示工程的依赖。\n为了解决这些问题，检索器之前增加了一步对输入进行改写的过程，填补给定输入与检索需求之间的差距。这一步利用了LLM本身的能力对原始查询进行有针对性的改写。\n比如，来自RAG_Techniques 的这段prompt：\nquery_rewrite_template = \u0026#34;\u0026#34;\u0026#34;You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information. Original query: {original_query} Rewritten query:\u0026#34;\u0026#34;\u0026#34; 使LLM可以更加系统地将用户的原始查询改写为更符合检索需求的形式。\n此外， 改写过程可以同时考虑用户的初始输入和对话历史，从而生成经过优化的查询。这种上下文感知的查询改写确保了搜索过程与用户的真实意图保持一致。例如，用户在对话过程中间询问“昨天的会议怎么样？”这样的查询本身是不足以进行有效检索的，因为它缺乏具体的上下文信息。为了使其更加有效，LLM可以将其改写为更详细的版本，例如：“9月14日预算会议的主要决策是什么？”。\n此时只需要在prompt中加入原始查询和一段上下文对话内容即可。\n当然，在使用query-rewrite方法时，可能会出现问题，尤其是当用户询问的是特定垂直领域的问题，而LLM对该领域缺乏理解，容易将问题改写成无关的内容。例如，用户提问“最近DR的表现怎么样？有些数据吗？”，这里的“DR”指“数据恢复”（Data Recovery），用户关注的是该领域的数据。然而，LLM将其误解为“Diabetes Research”（糖尿病研究），改写成“最近糖尿病的表现怎么样？有最新的统计数据吗？”，从而偏离了用户的需求，生成了与领域无关的回答。这种误解可能导致答非所问，影响用户体验。\n此外， 使用LLM进行query-rewrite还将面临着更大的延迟和成本。\n在 Query Rewriting for Retrieval-Augmented Large Language Models 这篇工作中， 作者还提出了 Trainable rewrite-retrieve-read的方法。即使用一个小的、可训练的语言模型（如T5）作为重写器，通过强化学习对其进行训练，具体来说， 先通过一定数量的伪数据集对重写器进行微调（warm-up），而后，在强化学习阶段，通过PPO（Proximal Policy Optimization）的方式，模型在每次生成新的查询后都会获得一个基于最终回答质量的奖励分数，从而强化那些能够提升最终回答质量的查询生成策略。\n参考 Query Rewriting for Retrieval-Augmented Large Language Models\nlangchain-rewrite-cookbook\n++all_rag_techniques_runnable_scripts++\nQuery Decomposition 对于复杂的问题无法单次查询获得有效结果，可以将原始问题分解成多个子问题，在经过多个子问题查询后， 根据所有子问题的查询结果，汇总并作出最后回答。\n为了更有效地回答问题，我们需要将其拆分为不同的子问题，分别检索每个子问题的结果，并将这些发现整合，以形成一个更完整的答案。\nprompt：\nsubquery_decomposition_template = \u0026#34;\u0026#34;\u0026#34;You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system. Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query. Original query: {original_query} example: What are the impacts of climate change on the environment? Sub-queries: 1. What are the impacts of climate change on biodiversity? 2. How does climate change affect the oceans? 3. What are the effects of climate change on agriculture? 4. What are the impacts of climate change on human health?\u0026#34;\u0026#34;\u0026#34; 以原始查询 “ \u0026ldquo;比较太阳能和风能作为可再生能源的优缺点\u0026rdquo; 为例， 这是一个比较宽泛，需要横向对比，指出各自的优缺点，分解后的子查询:\n- 太阳能发电的主要优点是什么? - 太阳能发电面临哪些主要挑战? - 风能发电有哪些优势? - 风能发电的主要缺点是什么? 可以通过分解查询从各个角度分析两者的特性。\n然而，这种Query Decomposition方法的局限性在于，它依赖于LLM的能力来合理分解问题，而人类的逻辑推理和问题分解依赖于经验积累和对问题的深入理解。因此，LLM在合理分解复杂问题方面仍存在一定差距，尤其在复杂性较高的领域中。此外，子问题的分解也可能导致查询延迟和成本的增加，这在实际应用中需要平衡效率与效果。\n参考 langchain-Decomposition\nllamaIndex：Multi-Step Query Engine\nall_rag_techniques_runnable_scripts\nsubquery_decomposition\nStep Back Prompting 用户在提问时常常会涉及大量的隐性细节，但这些内容并没有显性提供给LLM，LLM在处理这类任务时可能会遇到难以准确检索相关知识，和无法准确回答问题。\nStep-Back Prompting 正是基于这样一个观察，其灵感来自人类在面对复杂任务或问题时的习惯——我们通常会暂停片刻，从更高层次的概念或原则入手，以更清晰地思考下一步。例如，当需要计算一个三角形的边长时，我们可能会先回忆起勾股定理，这样有助于确定合适的解题方法。\n在实际使用中，回溯提示法尤其适合处理科学、技术、工程和数学（STEM）类问题。这类问题通常包含复杂的显性和隐性细节，直接回答会让模型容易遗漏关键知识点。通过先抽象再推理的分步方法，模型能够更准确地检索、理解并整合背景知识，从而提供更精确的回答。\nStep-Back Prompting 的核心流程包括两个步骤：抽象和推理。\n抽象（Abstraction）：在直接回答主要问题前，先让模型回答一个更高层次的问题，即与主要问题密切相关的概念性问题。\n推理（Reasoning）：一旦模型掌握了相关背景知识，就可以基于这些信息进行详细的推理过程，来解答主要问题。以此种方式分步思考，有助于模型准确地构建逻辑链条，并在回答中表现出更强的推理能力。\nprompt:\nstep_back_template = \u0026#34;\u0026#34;\u0026#34;You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system. Given the original query, generate a step-back query that is more general and can help retrieve relevant background information. Original query: {original_query} Step-back query:\u0026#34;\u0026#34;\u0026#34; step_back_answer_template = \u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant. {normal_context} {step_back_context} Original Question: {question} Answer: \u0026#34;\u0026#34;\u0026#34; 参考 A Step Forward with Step-Back Prompting\nPaper\nHypothetical questions (HyDE) HyDE（Hypothetical Document Embeddings，假设性文档嵌入），通过生成假设性的答案并对这些生成的文档进行嵌入，而非嵌入原始文本。这种方法特别适用于用户问题简短且与需要查找的原始文本在语义上相似度不高的情况。\n当用户的问题与知识库中的原始文本在语义上相差较大时，传统的文档检索方法可能无法有效地匹配到相关内容。HyDE通过使用大型语言模型（LLMs）根据用户问题生成格式上类似于知识库中文本的假设性答案，从而提升匹配的准确率。\n尽管HyDE可以提升检索的准确性，但这种方法也会一定程度上增加检索的耗时。\n参考 hypothetical_document_embeddings\nHyDE (Hypothetical Document Embeddings)\npaper\nFeedback-Based Query Refinement 人类在使用搜索引擎等工具进行信息查询时，通常并非一次就能找到答案，而是需要根据初始的结果不断调整和优化查询词。这样的过程是迭代式的，基于反馈不断精进，从而更贴近我们想要的目标。\n严格来说，这部分内容可以归类到agent的范畴中，但将其归入RAG也未尝不可。RAG和agent并不应是彼此独立的体系，于是乎有“RAG的尽头是agent”的说法。\n在agent的工具调用中，采用ReAct机制可以使得LLM根据查询结果动态修改查询，从而进行进一步的探究，而不只是简单地进行一次查询。相比于简单的RAG系统，借鉴了agent系统中的查询规划、记忆模块和路由机制之后，RAG可以变得更加强大。通过这种方式，系统能够更好地调整查询策略，不再仅依赖一次性的检索，而是可以进行智能的多轮交互。\n在我自己的工作中，我将RAG中的检索部分独立出来，作为一个搜索引擎模块，也是为了后续能更好地接入到agent系统中。不过，本章的重点仍然是探讨RAG的改进方法，而不涉及过多关于agent系统的内容。\n基于反馈与迭代的查询改写并不完全仅限于agent中的思路，更确切地说，这本身是符合人类在思考和解决问题时的方式。通过不断获取信息、修正查询，有助于在复杂问题上逐步接近正确的答案。\n不少研究工作基于这一思路，其中一些值得深入探讨。\n在RA-ISF这篇工作中，研究者通过迭代的方式处理问题，将任务分解，并使用三个子模块来增强模型的解决能力：自我知识模块（Self-Knowledge Module，SKM）、段落相关性模块（Passage Relevance Module，PRM）和问题分解模块（Question Decomposition Module，QDM）。\n首先，使用SKM模块判断当前问题是否可以依靠模型的内部知识直接解答；然后将检索到的文本与问题结合，输入到PRM模块中，评估它们的相关性。如果相关，系统基于这些段落生成答案；如果所有检索到的文本都无关，则进入QDM模块，将问题分解为子问题，并对这些子问题进行逐步处理。对于每一个子问题，系统会依次进入SKM、PRM和QDM模块，直到最终将所有子问题的答案整合，生成对原始问题的完整解答。\n在EfficientRAG这项研究中，针对多跳问题的解答，提出了一种新的方法，通过迭代生成新查询并过滤掉不相关信息，从而提高检索效率，而非重复调用LLM进行查询改写。在EfficientRAG中，包含两个轻量级组件：Labeler \u0026amp; Tagger和Filter。Filter组件在chunk级别筛选最相关的检索信息，而Labeler \u0026amp; Tagger组件在token级别识别和标记有用的信息。通过迭代地检索、标记和过滤，直到收集到足够的信息来回答问题，从而提高整个系统的效率。\nITER-RETGEN是通过迭代结合检索（Retrieval）和生成（Generation）来增强RAG模型性能的另一种方案。\n给定一个问题$q$和一个包含段落的文档集$D$，ITER-RETGEN会在T次迭代中反复进行“检索-生成”操作。在第$t$次迭代时，它使用上次生成的答案$y_{t-1}$和问题$q$组合，从文档集中检索出前k个相关段落，再利用大语言模型$M$基于这些段落和问题生成新的答案$y_t$。每次迭代的过程可以表示为：\n$$ y_t = M\\left(y_t \\mid \\text{prompt}(D_{y_{t-1}} , || , q, q)\\right), \\quad \\forall 1 \\leq t \\leq T $$\n最终，经过T次迭代，得到的最后一个输出$y_T$就是对原始问题的最终答案。通过这种迭代方式，系统能够有效地从复杂信息中逐步逼近正确答案。\n参考 RA-ISF: Learning to Answer and Understand from Retrieval\nEfficientRAG: Efficient Retriever for Multi-Hop Question Answering\nITER-RETGEN： Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1query-enhancement/","summary":"\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e首先，从最简单的RAG（Retrieval-Augmented Generation）谈起。\u003c/p\u003e\n\u003cp\u003eRAG结合了信息检索与生成模型，先通过用户的原始问题query 在知识库中检索与相关的文档，再利用生成模型（如ChatGPT）生成回答。最简单的RAG版本通常被称为Naive-RAG，虽然Naive-RAG能够处理一些简单的问题，但在真实场景下，用户提出的问题往往更加复杂多样，远远超出系统最初设计的预期范围。\u003c/p\u003e\n\u003cp\u003e在用户与RAG系统交互时，往往会遇到以下几种常见问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e表达模糊不清\u003c/strong\u003e：用户在描述问题时，往往难以精准表达自己的需求。他们可能仅使用几个词组或片段，导致系统难以理解用户的真实意图。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e依赖上下文\u003c/strong\u003e：用户的问题经常依赖之前的对话或背景信息，而系统只接收到当前的孤立问题，缺少关键的上下文支持，无法给出准确的答案。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e复杂多层次问题\u003c/strong\u003e：某些问题本身复杂，包含多个子问题或需要推理、逻辑分析。Naive-RAG依赖简单的关键词匹配或语义匹配，无法很好地处理这类复杂的需求。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9aQuery%20Enhancement-assets/image.png\" alt=\"image.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e面对这些挑战，Naive-RAG的局限性变得明显：仅仅依靠用户的原始query无法应对多样化的提问场景。那么，如何提升RAG系统的检索能力，增强对复杂问题的处理呢？这时就需要引入\u003cstrong\u003eQuery-Enhancement\u003c/strong\u003e技术。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eQuery-Enhancement\u003c/strong\u003e，顾名思义，是对用户的原始query进行增强处理。它的核心思想是：不要直接使用用户的原始query进行相关文档的检索，而是通过某种技术手段对query进行改写、扩展或优化，从而生成一个更适合检索的query。这种技术在不同场景下有不同的命名，如\u003cstrong\u003equery rewrite\u003c/strong\u003e或\u003cstrong\u003equery reformulation\u003c/strong\u003e，但核心目标都是一致的——根据用户的初始输入，生成一个更符合检索需求、更能提升查询准确性的新query。\u003c/p\u003e\n\u003cp\u003e通过Query-Enhancement，可以解决许多Naive-RAG无法处理的问题。例如，针对模糊或简短的提问，增强后的query可以通过添加上下文或关键词来丰富信息内容；对于复杂的多层次问题，增强后的query可以拆解为多个子问题，逐步进行检索与推理。\u003c/p\u003e\n\u003cp\u003e本文将深入探讨不同的Query-Enhancement技术方案，并分析这些技术如何根据原始query生成更高效的查询，帮助RAG系统在复杂场景下提供更精确的答案。\u003c/p\u003e\n\u003ch2 id=\"query-rewrite\"\u003eQuery Rewrite\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eBecause the original query can not be always optimal to retrieve for the LLM, especially in the real world\u0026hellip; we first prompt an LLM to rewrite the queries, then conduct retrieval-augmented reading.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eQuery Rewrite的目的是将用户模糊、不明确或不完整的查询转换为更适合检索的形式，以提升检索效率和准确性。\u003c/p\u003e\n\u003cp\u003e仅使用原始query的缺点：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e查询常常模糊、不具体或不完整，导致难以找到有效的检索结果。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e缺乏明确术语，无法有效从大型数据集中提取相关信息。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e对话上下文中的查询含义因对话历史而异，同样的表达在不同的上下文中可能有不同的意图。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e输入文本与真正需要检索的知识之间不可避免地存在差距。这种差距限制了检索性能，增加了对检索能力增强和提示工程的依赖。\u003c/p\u003e\n\u003cp\u003e为了解决这些问题，检索器之前增加了一步对输入进行改写的过程，填补给定输入与检索需求之间的差距。这一步利用了LLM本身的能力对原始查询进行有针对性的改写。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9aQuery%20Enhancement-assets/image%201.png\" alt=\"image 1.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e比如，来自\u003ca href=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques_runnable_scripts/query_transformations.py\"\u003eRAG_Techniques\u003c/a\u003e 的这段prompt：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003equery_rewrite_template\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Original query: \u003c/span\u003e\u003cspan class=\"si\"\u003e{original_query}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Rewritten query:\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e使LLM可以更加系统地将用户的原始查询改写为更符合检索需求的形式。\u003c/p\u003e","title":"RAG工具箱：Query Enhancement"},{"content":"写在前面 2024年的国庆假期，第一次带父母出国，目的地是马来西亚。选择这里，主要是考虑到语言方便，也因为这里的食物丰富。作为新手，这样的目的地比较适合，尤其是对于福建闽南人，马来西亚有种特别的亲切感。\n行程大致是厦门飞槟城，之后乘火车前往怡保，再到吉隆坡，最后从吉隆坡回程。\n这次游记，我尝试用食物作为主线来串联整个行程。虽然这对我来说是个挑战，但希望通过这样的方式，记录下这次的旅行。\n海南没有海南面包和海南鸡 槟城位于马来西亚西北，分成槟岛和威省两部分，作为东南亚重要的贸易港口之一，槟城自18世纪以来便吸引了无数移民，马来人、华人、印度人和欧洲人的足迹在这里留下了不可磨灭的印记，因其多元文化和历史，槟城的首府乔治市（George Town）也被联合国教科文组织纳入人世界文化遗产地。\n第一站是【多春茶室】。尽管其老店名声在外，但为了避开人群，我们选择了位于旧关仔角钟楼附近的分店。清晨9点到达时，茶室内还未拥挤，扫码点餐的现代化模式也让整个过程更加方便。\n点餐是件有趣的过程。面包、抹酱、饮品，一步步选择下来，经典的烤海南面包加上浓郁的咖椰酱，搭配香浓的Kopi-O，让这平凡的早餐充满满足感。\n烤海南面包加上咖椰酱\n有趣的是，海南面包源自南洋，并非来自海南本土。早年海南人漂洋过海来到南洋，受雇于英国人家庭和西化的峇峇家庭做帮佣，学习了西方的烘焙技艺，结合本地口味，创造出了适合东南亚人的海南面包。这些经历使他们将西式的咖啡与面包融入本地，逐渐普及成为南洋的经典早餐文化。\n当然，当地人早餐常吃的生熟蛋也是值得一试的， 鸡蛋泡在热水中几分钟，蛋白微凝固但仍带液态，蛋黄保持流动感，吃时只需加入酱油和白胡椒调味，用勺子轻轻搅拌，也可以蘸面包吃。如果不习惯生鸡蛋，店家还贴心地提供了不同熟度的选择。\n在这家店，店主还提供了4RM一个椰浆饭供食客自取，经典的江鱼仔鸡蛋口味，对于第一次尝试椰浆饭的人而言，味道的确很奇特。sambal辣酱并没有想象中那么辣，带有一丝丝的甜味，米饭也是经过调味的。\n茶室不远处便是槟城的地标——旧关仔角钟楼。钟楼由著名华商张弼士捐资修建，以纪念维多利亚女王登基60周年。与钟楼相邻的康华丽斯堡则是另一个标志性景点，虽名为堡垒，实未参与战斗。\n旧关仔角钟楼\n乔治市的街道以交叉十字贯穿各个文化区域。从小印度区入至华人区，再到马来区，都是一幅幅独特的文化画卷。右边为殖民时期的建筑，左侧则是古朴的华人会馆；再往前走，便是色彩斑斓的印度庙宇了。恍惚间，仿佛还能看到那些早年南来的移民，提着皮箱穿过狭窄的街巷，寻找着属于自己的一方天地。\n吉宁甲必丹回教堂\n槟城的美不止于其自然风光，而在于文化的碰撞与融合。早期移民将家乡味道带入南洋，创造出诸如海南鸡饭这样的经典。\n伍秀泽海南鸡饭茶室\n午后，我们寻觅到【伍秀泽海南鸡饭茶室】。这家茶室外观简单得甚至有些不起眼，但店里却充满着人间烟火气息。菜单也十分简单：白切鸡、烧鸡、鸡杂和鸡油饭。\n海南鸡饭\n鸡肉鲜嫩多汁，配上油光闪亮的鸡油饭，再添一勺微辣的辣椒酱，与黄瓜片相伴，让人回味无穷。\n一杯槟城特有的冰豆蔻水，为这丰盛的一餐画上美丽的句号，而在槟城的每一次咀嚼，都是对过去的重温。\n○○○\n【多春茶室】\n位置：37, Bishop St, Georgetown, 10200 George Town, Penang, 马来西亚\n营业时间：08:30-16:30 （周四店休）\n【伍秀泽海南鸡饭茶室】\n位置：340, Lbh Chulia, George Town, 10200 George Town, Pulau Pinang, 马来西亚\n营业时间：10:00-17:00 （周三店休， 周末营业时间延后一小时）\n粿条和饮料 粿条，本是潮汕人乃至闽南人对于米制面条的称呼。粿条的形式多种多样，既可以用来做汤，也可以干拌或者炒制。其中，最为著名的当属炒粿条。\n炒粿条的名字源自福建话“Char Kway Teow”。虽然它发源于潮州，但在潮州并不流行像马来半岛那样使用重酱油、鱼露，或加入血蚶的风味。\n几代人的迁徙流传，使得最初的潮汕炒粿条逐渐演变成如今的槟城炒粿条。在今天的马来半岛，各地的茶餐室、路边摊甚至食阁中，随处可见这一道菜。当地华人适应了赤道气候，也逐渐偏爱浓烈的口味，将黑酱油和鱼露加入其中，使其味道与原先的潮汕风味逐渐拉开了距离。\n槟城的炒粿条以其丰富的本地特色而闻名，以至于“槟城炒粿条”如今已然成为马来西亚外的“炒粿条”代名词。也许正是槟城人所炒的粿条充满了独特风味，让其从冠以“潮州”的乡土印记中脱颖而出，成为一种具有浓厚本地色彩的代表。\nChar Kway Teow\n打车时，遇见了一位潮州籍司机，第四或第五代的移民，已在马来西亚土生土长。她流利地说着潮州话，对自己的潮州根源抱有一份朦胧的认同。谈起美食，她兴致勃勃地推荐我去汕头街夜市，说那里的炒粿条最为“正宗”。然而，当我提起“潮汕”这个词，她的神情里浮现出些许困惑，似乎在她的概念中，“潮汕”并非一个清晰的地理或文化印象。她坦言，自己从未踏足故乡，“潮州”这个名字虽熟悉，却难以在她的心中勾勒出具体的图景，甚至未曾意识到潮州与汕头竟是毗邻而居。这样的“故乡”存在于她的血脉之中，却并不真实地存在于她的认知地图上。她的故事让我感受到这种隔代传承的身份认同中所夹带的微妙“模糊感”。\n或许，对她而言，故乡更像是从家人口中传下的记忆碎片，随着时间的推移，早已淡化成一种混杂的情感寄托。而那份隔空相望的疏离感，则正悄然渗透在她对潮州味道的理解中。也许，正是这份若即若离的情感，使她乐于将汕头街夜市当作心中的潮州，以这里的夜市小吃作为记忆的重构——故乡的味道也随之重构、演变，成了她所熟悉而珍视的“正宗”。\n粿条的故事也延伸到了汕头街的夜市，本以为【新月宮茶室】的炒粿条会是首选，但隔壁大排长龙的【天皇鸡脚粿条汤】却吸引了目光，不得不改变行程。这个路边摊没有座位，摊主售卖的卤味种类繁多，拼盘中有软糯的卤鸡脚、大肠、豆制品和卤蛋。粿条汤也有多种选择——清汤、黑汤和干拌，其中黑汤便是在清汤中加入一勺深色卤汁。在潮汕地区，炸蒜油始终是不可缺少的调味料，在这里也不例外。那一小勺炸蒜油，更是粿条汤的点睛之笔。这家店的招牌卤鸡脚，口感软糯入口即化，只可惜调味有点淡了，不过沾上店家提供的秘制调料，便不是太大的问题。\n天皇鸡脚粿条汤\n清汤粿条汤\n黑汤粿条汤\n卤味拼盘\n由于是路边摊，若想坐下堂食，还得借隔壁糖水铺的座位，条件是点上一杯饮料。这种餐位共享的习俗，在马来西亚的食阁、茶餐室、美食中心里早已根深蒂固。其源头可追溯至华人下南洋之时，彼时许多小贩因摊位有限，便与邻近的饮品摊位相互协作——食客借用座位，得点上一杯饮料作“入场费”。这一传统一路延续，逐渐形成一种心照不宣的默契。在槟榔屿， 不少地方甚至明文规定需点饮料，否则每人需支付额外0.5令吉的座位费。\n要求点饮料的告示\n马来西亚位于赤道附近，气候湿热，常年高温，这样的环境下，人们对冰凉饮料的需求自然高涨。不仅是解渴之需，更是日常生活中不可或缺的清凉享受。而在这片多元文化交织的土地上，马来人、华人、印度人等多个族群共同生活，他们的饮食文化在交汇中相互融合，形成了丰富多样、独具本地特色的饮料种类。\n在马来西亚的茶室，即所谓的“Kopitiam”里，点一杯饮料并不像在一般咖啡馆那般简单。这里有一套独特的组合式命名法，仿佛饮品的“方言”。这种规则依循特定的次序，将饮品种类、奶的选择、糖的添加等信息融为一体。\n常见饮料命名组合方式\n1. 基础饮料\n• Kopi：咖啡\n• Teh：茶\n• Milo：美禄\n2. 奶的种类\n• O：不加奶，即“乌”，来自于福建话。\n• C：加入淡奶，Canned Milk。\n• 无标注：加炼乳\n3. 糖的量\n• Kosong：马来语，无糖\n4. 温度\n• Peng：福建话冰，也有使用Ais （源于马来语，更源头则是英语ice）\n在这套规则之外，马来西亚还有几款令人印象深刻的本地特饮。比如，拉茶（Teh Tarik），通过“拉”茶的技艺将其来回高抛，层层拉出细腻的泡沫。或是备受年轻人喜爱的美禄恐龙（Milo Dinosaur），在一杯美禄上再撒上一层厚厚的美禄粉，巧克力的浓郁香气更加突出。\nKopi、Milo、Barley（薏米水）\n还有马来西亚人解暑必备的煎蕊（Cendol），这是一道口感丰富、层次分明的传统甜品。绿色的煎蕊条由绿豆粉或糯米粉制成，并以斑兰叶汁染色，散发淡淡的香气，与椰浆、椰糖、刨冰相融合，形成一碗甜而不腻的冰凉饮品。偶尔还会加入红豆、玉米、仙草等配料，增添风味，口感层次丰富，是在炎热气候中最受欢迎的甜品之一。\n槟榔律驰名潮州煎蕊\n此外，马来西亚的传统饮品中，还有一种经常被误解的“薏米水”，即当地所谓的“Barley”。这里的“Barley”指的是大麦，并非中药所指的薏苡仁。虽说“薏米水”这个叫法并不准确，但大麦煮成的饮品清爽解渴，早已深入人心。大麦因其外形酷似薏米，在马来西亚被称作“洋薏米”，久而久之，“洋”字不见了，反倒留下了“薏米水”这一独特的名称。\n○○○\n【新月宮茶室】\n位置：86, Lebuh Kimberley, George Town, 10100 George Town, Pulau Pinang, 马来西亚\n营业时间：08:00-22:30\n【天皇鸡脚粿条汤】\n位置：78, Lebuh Kimberley, George Town, 10100 George Town, Pulau Pinang, 马来西亚\n营业时间：17:00-23:00\n【Penang Road Famous Teochew Chendul】\n位置：27-29, Lebuh Keng Kwee, George Town, 10100 George Town, Pulau Pinang, 马来西亚\n营业时间：09:00-18:30\n福建人和福建虾面 在槟城，当地华人群体中福建人占了绝大多数。\n于是乎在源头称之为闽南语的语言，到了南洋，因其主导性而被称之为“福建话”。闽南话在此地几乎畅行无阻，甚至连一些印度裔的小贩也能操一口流利的福建话。想起初来槟城时与一位Grab司机的对话，他祖籍泉州，但彼时彼刻我还不确定他是否也是华人，于是用英语交谈，直到父亲用闽南话试探他是否能听懂。司机会心一笑，便用福建话和我们无障碍交流起来。这种语言的亲切感，也许是因为共同的根源，使得陌生的异地瞬间拉近了距离。\n福建移民不仅带来了他们的方言，还携来了信仰与文化。在槟城早期，由于殖民政府对陆地按面积征税，许多福建移民为了避税，巧妙地选择在海上建房，形成一片片独特的“姓氏桥”，以“周姓桥”为最为闻名的代表。其桥头设有庙宇供奉天公，每年农历正月初八，居民会在此举行盛大的拜天公仪式。而这一信仰，始终没有变过。\n姓氏桥\n周氏桥头的庙宇\n除了语言文化信仰，还有一碗面。提到槟城的福建面，这一话题在马来西亚可谓是引起争议的话题。槟城的福建面其实就是福建虾面，而这道美食则是由早期移民福建人带入当地的美味，因售卖者多为说着福建话的福建人因而得名“福建面”，而在吉隆坡，新加坡，福建面则是另一种样貌——加了酱油膏的黑色炒面。\n关于福建面，还有这样一个民间传说：在日军占领槟榔屿期间，人民生活困苦，鱼虾都被迫献给日军，渔民们只好用剥下的虾头和虾壳来煮汤，便创制出了这道美食。虽说这应该只是个民间杂谈， 并无具体的考究，但也点明了这碗面的核心—由虾头虾壳熬制而成的高汤，仍未随时间而动摇。\n相较于上世纪舶来品的沙茶面，厦门的虾面才是南洋虾面的源头。虾面使用一种名为“狗虾”的野生小虾（因其特殊性，每年四月至七月都会断产），先去壳取肉再煮熟。狗虾的虾壳炒香后捶打成酱，再与猪骨慢火熬制四小时。虾面的主要原料在于虾，但其独特之处却在于汤底。\n在槟城，虾和猪骨依旧是虾面汤的灵魂，但增加了Sambal酱，也增添了更多的辣味和海鲜的浓郁。Sambal酱是一种用发酵的虾酱、干辣椒和新鲜辣椒制成的调料。除了黄面和豆芽外，槟城的虾面中总是会放入蕹菜，即空心菜。\n在落日洞边上的【渔人码头】，只因住在附近， 而选择了在此觅食，Grab司机倒也好奇我们是否收到了当地朋友的推荐，而选择了这么家似乎属于当地人私藏的食阁。食阁的好处便是能够在一顿饭中获得多种选择。而让我记忆深刻的，还是那碗福建虾面。香辣的汤底和浓厚的风味瞬间击中了味蕾。少了厦门虾面的那种甜，却多了一份来自热带的火辣。用开水汆烫的黄面、米粉、蕹菜，再加上肉片、水煮蛋和豆芽，浇上浓郁的特制虾汤，汤上放在一把勺子，勺内有少许Sambal辣酱，依个人喜好调节辣度。\n福建虾面\n我大胆猜测，这或许是早年的福建移民南下槟城后，长年适应当地湿热气候，逐渐放弃了家乡的清淡口味，而在日积月累的过程中调重了味道，以刺激食欲。延用了家乡的虾和面，又加上南洋风味的辣酱，最终成就了这道独一无二的本地福建虾面。\n渔人码头晚霞\n○○○\n【渔人码头】\n位置：92A, 1, Lebuh Sungai Pinang, 11600 Jelutong, Pulau Pinang, 马来西亚\n营业时间：08:30-00:00\n怡保白咖啡和芽菜鸡 怡保，这座因锡矿而兴起的城市，曾在十九世纪的喧嚣中展露繁华。那些年，来自南洋的劳工涌入，为这片土地注入了活力。然而，到了五十至六十年代，锡价下跌，需求萎缩，昔日的热闹逐渐消退。时光似乎停留在50年代，街头的店铺大多紧闭门扉，仿佛旧日的繁华仅残留在老旧的招牌上。这些招牌多以竹帘制成，简单的笔画勾勒出商号名称，透着一丝未曾褪色的昔日风韵。然而，在这破败的街巷间，一些无法离开或不舍离去的老人们，依旧守望着故乡的影子。\n怡保街头\n因只是将怡保作为槟城到KL的中转站，活动范围只局限在了火车站附近的旧厂街。旧厂街的街角，三五成群的华裔、印度裔老人围坐聊天，邻桌的茶室里，来自不同族群的人们在一片屋檐下共享时光。这里的人们彼此间的友好显而易见，彼此往来无碍。偶遇的Grab司机也会提到马来朋友，仿佛这些对立和冲突只是外来者的夸大，或者是政客用以拉拢选票的政治游戏。\n怡保街头\n怡保的特色小吃芽菜鸡，传言原先是芽菜与鸡一同盛放。岁月流转，食物也随之演变，如今鸡与芽菜分置盘中，简单而别具风味。尤以“红油”鸡丝河粉最为有名，自七十年代起，芽菜与清汤河粉成了标配，鸡丝河粉却少有提供。然而，品尝之际，才觉那极为粗壮的芽菜透着脆爽，这是只有怡保水土才能培育出的佳品。依山傍水的地理造就了这一独特风味，豆芽也因此而名声在外。\n芽菜、鸡和鸡丝河粉\n数家有名的芽菜鸡店都聚集在同一街角，行色匆匆，难以抉择。【安记芽菜鸡】店里的装潢古旧，时光似乎凝固在墙角，一旁的老式唱片机正播放着七八十年代的旋律，白板上仅写着几道招牌菜，似乎在坚守那份精致而不贪多的执着。简单的酱油胡椒调味衬托出芽菜的自然清甜，一碗鸡油饭，几块白斩鸡，轻尝一口，简简单单但并不普通。老顾客依旧按着旧习惯点餐，只是如今应顾的却是语言不通的外劳，偶尔还需老板亲自上前指点。\n安记芽菜鸡内的招牌\n跨过几条巷弄，映入眼帘的是另一番旧日风情。【南香茶餐室】，这间被视为白咖啡发源地的老茶室，依旧保留了传统的模样。白咖啡的味道仿佛凝结了时代的变迁，据说是早年一位咖啡小贩在炒咖啡豆时加入了焦糖，才成就了这款特有的香浓口味。如今这香气馥郁的白咖啡已成为怡保的名片，白咖啡品牌的迅速推广，让这种独特的风味远播四方。进店后点上一杯白咖啡，再来一盘蛋挞或鸡肉酥，仿佛能在这微苦中回味旧时光。\n南香茶餐室 白咖啡\n沿街行至二奶巷、三奶巷，这些名字带着几分遥远的传奇色彩。那些锡矿时期的华商为情妇安置的街巷，如今已是游客蜂拥而至的商业街，卖着各种小商品，留下的不过是一点点变了味的旧日记忆。\n○○○\n【安记芽菜鸡沙河粉】\n位置：No. 48/51, 53, Jalan Yau Tet Shin, Taman Jubilee, 30300 Ipoh, Perak, 马来西亚\n营业时间：10:00-21:00\n【南香茶餐室】\n位置：2, Jalan Bandar Timah, 30000 Ipoh, Perak, 马来西亚\n营业时间：06:30-16:30\n（未完）\n","permalink":"https://niraya666.github.io/travel/%E5%A4%A7%E9%A9%AC%E6%B8%B8%E8%AE%B02024/","summary":"\u003ch2 id=\"写在前面\"\u003e写在前面\u003c/h2\u003e\n\u003cp\u003e2024年的国庆假期，第一次带父母出国，目的地是马来西亚。选择这里，主要是考虑到语言方便，也因为这里的食物丰富。作为新手，这样的目的地比较适合，尤其是对于福建闽南人，马来西亚有种特别的亲切感。\u003c/p\u003e\n\u003cp\u003e行程大致是厦门飞槟城，之后乘火车前往怡保，再到吉隆坡，最后从吉隆坡回程。\u003c/p\u003e\n\u003cp\u003e这次游记，我尝试用食物作为主线来串联整个行程。虽然这对我来说是个挑战，但希望通过这样的方式，记录下这次的旅行。\u003c/p\u003e\n\u003ch2 id=\"海南没有海南面包和海南鸡\"\u003e海南没有海南面包和海南鸡\u003c/h2\u003e\n\u003cp\u003e槟城位于马来西亚西北，分成槟岛和威省两部分，作为东南亚重要的贸易港口之一，槟城自18世纪以来便吸引了无数移民，马来人、华人、印度人和欧洲人的足迹在这里留下了不可磨灭的印记，因其多元文化和历史，槟城的首府乔治市（George Town）也被联合国教科文组织纳入人世界文化遗产地。\u003c/p\u003e\n\u003cp\u003e第一站是【多春茶室】。尽管其老店名声在外，但为了避开人群，我们选择了位于旧关仔角钟楼附近的分店。清晨9点到达时，茶室内还未拥挤，扫码点餐的现代化模式也让整个过程更加方便。\u003c/p\u003e\n\u003cp\u003e点餐是件有趣的过程。面包、抹酱、饮品，一步步选择下来，经典的烤海南面包加上浓郁的咖椰酱，搭配香浓的Kopi-O，让这平凡的早餐充满满足感。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/6F734170-B982-4F62-934A-22FF8C6CCE86_1_105_c.jpeg\" alt=\"6F734170-B982-4F62-934A-22FF8C6CCE86_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e烤海南面包加上咖椰酱\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e有趣的是，海南面包源自南洋，并非来自海南本土。早年海南人漂洋过海来到南洋，受雇于英国人家庭和西化的峇峇家庭做帮佣，学习了西方的烘焙技艺，结合本地口味，创造出了适合东南亚人的海南面包。这些经历使他们将西式的咖啡与面包融入本地，逐渐普及成为南洋的经典早餐文化。\u003c/p\u003e\n\u003cp\u003e当然，当地人早餐常吃的生熟蛋也是值得一试的， 鸡蛋泡在热水中几分钟，蛋白微凝固但仍带液态，蛋黄保持流动感，吃时只需加入酱油和白胡椒调味，用勺子轻轻搅拌，也可以蘸面包吃。如果不习惯生鸡蛋，店家还贴心地提供了不同熟度的选择。\u003c/p\u003e\n\u003cp\u003e在这家店，店主还提供了4RM一个椰浆饭供食客自取，经典的江鱼仔鸡蛋口味，对于第一次尝试椰浆饭的人而言，味道的确很奇特。sambal辣酱并没有想象中那么辣，带有一丝丝的甜味，米饭也是经过调味的。\u003c/p\u003e\n\u003cp\u003e茶室不远处便是槟城的地标——旧关仔角钟楼。钟楼由著名华商张弼士捐资修建，以纪念维多利亚女王登基60周年。与钟楼相邻的康华丽斯堡则是另一个标志性景点，虽名为堡垒，实未参与战斗。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/F8C5900A-FE98-4DCF-8B04-E73A9CB9A30C_1_201_a.jpeg\" alt=\"F8C5900A-FE98-4DCF-8B04-E73A9CB9A30C_1_201_a.jpeg\"  /\u003e\n\n\u003cem\u003e旧关仔角钟楼\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e乔治市的街道以交叉十字贯穿各个文化区域。从小印度区入至华人区，再到马来区，都是一幅幅独特的文化画卷。右边为殖民时期的建筑，左侧则是古朴的华人会馆；再往前走，便是色彩斑斓的印度庙宇了。恍惚间，仿佛还能看到那些早年南来的移民，提着皮箱穿过狭窄的街巷，寻找着属于自己的一方天地。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/73FE20C9-AFC7-4F90-BCD9-6980BDE26F13_1_105_c.jpeg\" alt=\"73FE20C9-AFC7-4F90-BCD9-6980BDE26F13_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e吉宁甲必丹回教堂\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e槟城的美不止于其自然风光，而在于文化的碰撞与融合。早期移民将家乡味道带入南洋，创造出诸如海南鸡饭这样的经典。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/E399B1CB-F4F0-4639-86E7-A6F3F8C91BFC_1_201_a.jpg\" alt=\"E399B1CB-F4F0-4639-86E7-A6F3F8C91BFC_1_201_a.jpg\"  /\u003e\n\n\u003cem\u003e伍秀泽海南鸡饭茶室\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e午后，我们寻觅到【伍秀泽海南鸡饭茶室】。这家茶室外观简单得甚至有些不起眼，但店里却充满着人间烟火气息。菜单也十分简单：白切鸡、烧鸡、鸡杂和鸡油饭。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/48AD27F9-8A94-4558-A87E-334128902AB1_1_105_c.jpeg\" alt=\"48AD27F9-8A94-4558-A87E-334128902AB1_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e海南鸡饭\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e鸡肉鲜嫩多汁，配上油光闪亮的鸡油饭，再添一勺微辣的辣椒酱，与黄瓜片相伴，让人回味无穷。\u003c/p\u003e\n\u003cp\u003e一杯槟城特有的冰豆蔻水，为这丰盛的一餐画上美丽的句号，而在槟城的每一次咀嚼，都是对过去的重温。\u003c/p\u003e\n\u003cp\u003e○○○\u003c/p\u003e\n\u003cp\u003e【多春茶室】\u003c/p\u003e\n\u003cp\u003e位置：37, Bishop St, Georgetown, 10200 George Town, Penang, 马来西亚\u003c/p\u003e\n\u003cp\u003e营业时间：08:30-16:30 （周四店休）\u003c/p\u003e\n\u003cp\u003e【伍秀泽海南鸡饭茶室】\u003c/p\u003e\n\u003cp\u003e位置：340, Lbh Chulia, George Town, 10200 George Town, Pulau Pinang, 马来西亚\u003c/p\u003e\n\u003cp\u003e营业时间：10:00-17:00 （周三店休， 周末营业时间延后一小时）\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"粿条和饮料\"\u003e粿条和饮料\u003c/h2\u003e\n\u003cp\u003e粿条，本是潮汕人乃至闽南人对于米制面条的称呼。粿条的形式多种多样，既可以用来做汤，也可以干拌或者炒制。其中，最为著名的当属炒粿条。\u003c/p\u003e\n\u003cp\u003e炒粿条的名字源自福建话“Char Kway Teow”。虽然它发源于潮州，但在潮州并不流行像马来半岛那样使用重酱油、鱼露，或加入血蚶的风味。\u003c/p\u003e\n\u003cp\u003e几代人的迁徙流传，使得最初的潮汕炒粿条逐渐演变成如今的槟城炒粿条。在今天的马来半岛，各地的茶餐室、路边摊甚至食阁中，随处可见这一道菜。当地华人适应了赤道气候，也逐渐偏爱浓烈的口味，将黑酱油和鱼露加入其中，使其味道与原先的潮汕风味逐渐拉开了距离。\u003c/p\u003e\n\u003cp\u003e槟城的炒粿条以其丰富的本地特色而闻名，以至于“槟城炒粿条”如今已然成为马来西亚外的“炒粿条”代名词。也许正是槟城人所炒的粿条充满了独特风味，让其从冠以“潮州”的乡土印记中脱颖而出，成为一种具有浓厚本地色彩的代表。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/6762BC7A-C04B-4F8A-B9DC-3E5BA158D640_1_105_c.jpeg\" alt=\"6762BC7A-C04B-4F8A-B9DC-3E5BA158D640_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eChar Kway Teow\u003c/em\u003e\u003c/p\u003e","title":"食在大马"},{"content":"前言 国庆期间，在西马玩了快一个礼拜，期间体验了从北海到怡保的ETS、槟城的升旗山登山缆车，还有吉隆坡复杂多样的轨道交通系统。旅行中，我踩过一些坑，也解锁了一些便利的玩法。经过查阅不少攻略，加上实地探索和切身体会，我决定将这次旅途中的交通攻略整理成这篇文章，分享给大家。\n注：本文所包含的信息有效截止至2024年10月，同时主要讨论西马（马来半岛）的轨道交通；希望能对你的大马之旅有所帮助。\n大马铁路概述 马来西亚的铁路系统起源于19世纪末的英国殖民时期，当时的铁路主要是为了支持锡矿业和橡胶种植园的经济发展。第一条铁路于1885年建成，连接了太平和砵威港口（今称为瓜拉十八丁），而太平是当时霹雳州英国人的行政中枢。这条铁路的建设是为了便于将矿产资源从内陆运输到港口。此后，英国殖民政府开始加快在马来亚各地开辟铁路的步伐，主要目的是为了促进殖民地经济发展，例如吉隆坡至巴生的铁路于1886年建成通车，芙蓉至波德申的铁路于1891年建成通车，吉隆坡至万挠的铁路于1892年建成通车，安顺至打巴路的铁路于1893年建成通车。1895年，受英国保护的雪兰莪、森美兰、霹雳和彭亨四州组成马来联邦。当时马来亚不同州的铁路各自发展，没有一个统一的规划和管理机构，于是殖民政府当局在1901年成立了马来联邦铁路（FMSR），统合管理马来西亚半岛上各地包括马来联邦、马来属邦、海峡殖民地的铁路。这些初期建设的米轨铁路成为连接各主要城镇和经济区的重要交通工具。\n20世纪初，马来西亚的铁路网迅速扩展，最重要的两条干线为东海岸线和西海岸线。二战期间，铁路遭到日军的严重破坏，但战后得到了重建。1948年，马来联邦铁路被改组为马来亚铁路（MR）。1963年，马来亚、新加坡、砂拉越和沙巴组成了新的联邦国家——马来西亚（新加坡于1965年退出），马来亚铁路的官方名称也更改为马来语“Keretapi Tanah Melayu”（KTM）。\n现如今，马来西亚的铁路系统主要由两条干线组成：西海岸线和东海岸线。东海岸线（也称金马士－道北铁路）由马来亚铁道运营，起自金马士，途经吉兰丹、彭亨、森美兰，穿越马来半岛中部，全长528公里，最终到达泰国边境的道北。虽然名为东海岸线，但大部分路段穿行于内陆丛林地带，因此又被称为“丛林铁路”，吸引许多背包客体验。西海岸线则起自泰国边境的巴东勿刹，沿西海岸一直延伸至柔佛的南端，通过新柔长堤连接新加坡，全长950公里，途经包括吉隆坡在内的八个州属，是马来西亚铁路运输的重要交通大动脉。由国有企业马来亚铁道公司（KTMB）运营的铁路系统经过了多次改组和现代化，如今的铁路服务已经涵盖了从通勤列车到高速城际列车的多种交通模式。目前，马来西亚铁路系统不仅联通国内主要城市，还连接着泰国和新加坡，是东南亚地区重要的交通枢纽。\n来源\n铁路分类 KTM Komuter（城际通勤列车）\nKTM Komuter是由马来亚铁道公司（KTM）运营的通勤铁路服务，最初于1995年在巴生谷地区启用，后于2015年扩展至北部各州（如霹雳、槟城、吉打和玻璃市）。该服务连接了吉隆坡及其周边郊区，还服务于槟城和其他北部州属，总长度约456公里，包含芙蓉线、巴生港线等多条线路。KTM Komuter的列车是空调电力列车，并提供便捷的“停车换乘”设施，适合游客和通勤者方便进入市区，避免交通拥堵。KTM通勤铁路还为乘客提供多种票务选项，包括通过KTMB移动应用程序购买手机票（二维码）、Touch \u0026rsquo;n Go（仅限巴生谷地区）、自动售票机与车站售票柜台购票。\nETS（电动列车服务）\nETS（Electric Train Service）是由马来亚铁道公司运营的快速城际电力动车组服务，于西海岸线运行，连接金马士与巴东勿刹，全长950公里。ETS是马来西亚最快的米轨列车，速度可达每小时140公里，属于准高速铁路。列车分为Gold、Silver、Platinum 以及 Express 4种等级，分别对应不同的服务和票价，是前往怡保、槟城等城市的高效方式。\nKTM Intercity（城际列车）\nKTM Intercity是马来亚铁道运营的柴油牵引城际列车服务，覆盖马来半岛的南部地区、新加坡以及泰国。部分列车主要运行于东海岸线金马士至道北之间，并延伸至新加坡。2021年4月推出了柴油动车组（DMU）列车，取代了原有的传统列车，服务于东海岸的丹绒至吉利地段。DMU列车配备现代化设施，包括集群座椅、LED显示屏、电源插座、祈祷室、厕所、小餐馆及行李存储空间，并有无障碍设施，满足残障人士的需求。\n如何购买火车票 购买火车票的方式有多种，最为方便的是通过KTMB官方网站或手机应用程序进行在线购买。也可以在吉隆坡中环车站（KL Sentral）及其他主要车站直接购票，但热门线路的车票在高峰时期（如假日、周末）可能会售罄，建议提前预订。\n以网上购买ETS车票为例，需在KTMB官网注册账号，然后选择出发和到达车站、以及对应的ETS车次类型（如Gold、Platinum、Express等）。不同级别的车次在停靠站数量和服务上有所不同，例如Platinum和Express级别提供商务座，商务座采用1+2的座位排布，包含免费食物和饮料，价格相对更高；普通席则是2+2的座位排布。Gold车次则没有商务座位。选择座位时，需要注意座位方向，有些座位是与行驶方向相反的，如果不喜欢反向乘坐，记得选择“forward”座位。\nETS列车的车型也有所区别，Platinum和Express列车多为较新款，由中国中车制造，而Silver级别列车则采用较老款，速度较慢，停靠站较多。\nETS-Express列车（摄于Butterworth车站） ETS-Gold列车，采用旧款车型（摄于Ipoh车站）\n吉隆坡轨道交通 吉隆坡及其周边地区有完善的轨道交通系统，涵盖了多种轻轨和地铁线路。\nLRT（轻快铁）\nLRT是吉隆坡的轻轨系统，包含3号 LRT Ampang Line、4号LRT Sri Petaling Line和5号LRT Kelana Jaya Line。LRT贯穿吉隆坡市中心及重要商业区，是游客前往市区景点如双子塔、茨厂街的首选交通工具。在高峰期，列车会非常拥挤。\n5号线LRT，摄于Bangsar车站\n3号LRTAmpang Line（橙黄色）：占美清真寺，独立广场，ICC pudu等景点。\n4号LRT Sri Petaling Line（深红色）：占美清真寺，独立广场，ICC pudu等景点。\n5号LRT Kelana Jaya Line（粉色）：可以前往双子塔、KLCC水族馆等主要景点。\nMRT（地铁）\nMRT是吉隆坡的新兴地铁系统，分为Kajang线和Putrajaya线，覆盖了市区及巴生谷更广阔的区域。\n9 号线 MRT Kajang Line（深绿色） ：可前往国家博物馆，village park（知名的nasi lemak）等地；\n10 号线MRT Putrajaya Line（黄色）：可前往KLCC，海脚人餐厅等地。\nKL Monorail（单轨铁路）\n路线编号为8，而颜色则为绿色。\nKL Monorail是吉隆坡独特的单轨铁路，线路为黄绿色，专门服务于市中心，主要连接购物和娱乐区（如武吉免登、KL Sentral等）。这条单轨铁路全长8.6公里，共有11个车站，将吉隆坡中央车站交通枢纽与市中心的金三角地带连接起来。\n1996年，马来西亚政府批准兴建吉隆坡单轨列车的计划，1997年1月正式动工，然而，1997年亚洲金融风暴导致融资困难，工程被迫暂停。1998年，本国企业MTrans公司接手，并在2003年8月31日正式完工并通车。通车后，单轨列车因客流量不如预期及经营收入不足而在2007年被马来西亚国家基建公司（Prasarana）收购。\n单轨列车的内部设计独特，不同车厢有着不同的座椅布局，多为面向窗户的座位，能够让乘客更好地欣赏吉隆坡的城市景观。然而，列车的安全门常处于开启状态，车门开关也时常发生故障，显示出在1997年金融危机后及其经营不善下所留下的问题。不过，尽管存在运营上的挑战，KL Monorail仍然成为了吉隆坡的一张城市名片，尤其在武吉免登（Bukit Bintang）等热门地带，为游客提供了方便的出行选择，并成为Bukit Bintang的知名打卡地。始发于KL Sentral，途经如Bukit Bintang等购物和娱乐区。 独特设计的座位布局，摄于KL Sentral\nMonorail，摄于Bukit Bintang\nKLIA Ekspres（吉隆坡机场快线）\nKLIA Ekspres是巴生谷的第六条列车线路，连接吉隆坡中央车站（KL Sentral）与吉隆坡国际机场（KLIA 1 \u0026amp; KLIA 2）。全程仅需33分钟，每20分钟一班，票价为55马币。KLIA Ekspres路线编号为6，线路颜色为紫色。该服务的列车提供宽敞的座位和行李存放空间，乘坐马来西亚航空、国泰航空、峇迪航空的旅客还可以在吉隆坡中央车站进行市区预办登机和行李托运服务，方便乘客直接登机。\nKLIA Transit（吉隆坡机场支线）则是第七条线路，使用相同轨道，但中途停靠布城和其他站点，如粉红清真寺等著名景点。全程约40分钟，每15至30分钟一班。票价按站收费，最少6.5马币起。需要注意的是，KLIA Ekspres和KLIA Transit在KL Sentral的进站口不同。\nKLIA Ekspres 和Transit的进站口不同\n登山缆车 槟城的升旗山登山缆车（Penang Hill Funicular Train）是登上升旗山山顶的正常途径，除非你愿意爬3个小时上山。这条缆车线路始建于1923年，最初为两段式铁路，2010年经过现代化改造，合并为一段式系统并更换为封闭式空调车厢。缆车全长8.6公里，是世界上最陡的斜坡缆车之一，全程时间大约5到20分钟，视途中停靠情况而定。它将游客从山脚的Air Itam带到升旗山顶，可以俯瞰槟城市景以及对岸的Butterworth。\n升旗山曾是殖民时期的避暑胜地，缆车的建造也是为了方便上山交通。缆车采用钢缆拉动车厢，以适应陡峭的坡度而安全上下山。缆车的普通往返票价为30马币，此外还有VIP票价为80马币，VIP票无需排队，且可以优先上车，占据有利的拍照位置。然而在非暑假的工作日下午，通常不太需要排队，因此无需额外购买VIP票。\n升旗山，槟城 升旗山小火车（登山缆车啦）\n交通卡选择 Touch \u0026rsquo;n Go（TnG）卡 最常见的是Touch \u0026rsquo;n Go（TnG）卡，这是一种多功能交通卡，可以用于轻轨（LRT）、地铁（MRT）、单轨列车（Monorail）、KTM火车以及公交车等。此外，TnG卡还可在便利店和部分商店中使用。\nTouch \u0026rsquo;n Go 实体卡：可以在便利店、地铁站（需提前预约）或KL Sentral的Touch \u0026rsquo;n Go Hub购买。提前在淘宝上购买也是一个选择。\nTNG eWallet：是一款需要马来西亚或新加坡手机号注册的手机应用，可以作为电子支付钱包，但不能直接作为实体交通卡使用。\nMyCity Pass MyCity Pass是一种短期通行卡，适用于游客和本地居民，允许在指定时间内不限次数乘坐RapidKL管理的公共交通工具，包括LRT、MRT、单轨列车、BRT和RapidKL公交车。\nMyCity 1日通行证：马来西亚人需支付 RM6，外国游客为 RM10。\nMyCity 3日通行证：马来西亚人需支付 RM15，外国游客为 RM25。\n这些价格不包括Touch \u0026rsquo;n Go卡本身的费用，用户必须有一张Touch \u0026rsquo;n Go卡，并确保卡内有至少 RM5 的余额。MyCity Pass可以在LRT、MRT、单轨列车和BRT的客服中心，或一些公交枢纽（如Pasar Seni和KLCC）购买，也可以通过MyRapid PULSE应用程序在线购买。注意，此卡不适用于ERL（如KLIA Ekspres和KLIA Transit）及KTM Komuter。\nKL Travel Pass KL Travel Pass包含KLIA Ekspres的单程或往返票，并提供2天内无限次乘坐吉隆坡快捷通（RapidKL）旗下的LRT、MRT和单轨列车服务（不包括KTM火车）。\n单程机场快线 + 2天无限次乘坐LRT/MRT/单轨列车：RM80。\n往返机场快线 + 2天无限次乘坐LRT/MRT/单轨列车：RM125。\nKL Travel Pass可在KLIA（第一和第二航站楼）以及KL Sentral的KLIA Ekspres售票柜台购买，也可通过线上平台如KLOOK购买。此卡本质上是一张Touch \u0026rsquo;n Go卡，充值后还可以用于RapidKL公交车及部分零售商店的支付。机场快线车票的有效期为自购买日起一个月，2天的无限次乘车卡将在首次使用后连续两天内有效（非48小时）。\n说实话，体验下来，travelpass并不是一个太好的选择， 首先并没有优惠太多，如果想回本需要在2天内坐够30RM的地铁，而KL的地铁轻轨以1RM起步价， 平均一站增加0.1RM，如果住在市区很难有机会坐这么多站的地铁；\n总结 马来西亚的铁路交通承载着这片土地的历史，与其他交通方式相比，火车不仅能穿越城市与乡村，更能体验到当地的文化与风土人情。\n总体来说，马来西亚的轨道交通体系并不是太复杂（和日本相比）。无论是吉隆坡的地铁、轻轨，还是连通主要城市的城际列车，都是方便快捷的出行选择。尽管马来西亚的油价低廉、打车费用不高，但在城市内，高峰时段的交通拥堵是常见问题。当然，轨道交通仍然是值得考虑的选择。\n参考资料 wiki：马来西亚铁路运输\nwiki：马来亚铁道\nKTMB官网\nMyCity Pass\nklook: KL TravelPass\n","permalink":"https://niraya666.github.io/travel/%E5%A4%A7%E9%A9%AC%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A%E6%94%BB%E7%95%A5/","summary":"\u003ch2 id=\"前言\"\u003e\u003cstrong\u003e前言\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e国庆期间，在西马玩了快一个礼拜，期间体验了从北海到怡保的ETS、槟城的升旗山登山缆车，还有吉隆坡复杂多样的轨道交通系统。旅行中，我踩过一些坑，也解锁了一些便利的玩法。经过查阅不少攻略，加上实地探索和切身体会，我决定将这次旅途中的交通攻略整理成这篇文章，分享给大家。\u003c/p\u003e\n\u003cp\u003e注：本文所包含的信息有效截止至2024年10月，同时主要讨论西马（马来半岛）的轨道交通；希望能对你的大马之旅有所帮助。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"大马铁路概述\"\u003e\u003cstrong\u003e大马铁路概述\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e马来西亚的铁路系统起源于19世纪末的英国殖民时期，当时的铁路主要是为了支持锡矿业和橡胶种植园的经济发展。第一条铁路于1885年建成，连接了太平和砵威港口（今称为瓜拉十八丁），而太平是当时霹雳州英国人的行政中枢。这条铁路的建设是为了便于将矿产资源从内陆运输到港口。此后，英国殖民政府开始加快在马来亚各地开辟铁路的步伐，主要目的是为了促进殖民地经济发展，例如吉隆坡至巴生的铁路于1886年建成通车，芙蓉至波德申的铁路于1891年建成通车，吉隆坡至万挠的铁路于1892年建成通车，安顺至打巴路的铁路于1893年建成通车。1895年，受英国保护的雪兰莪、森美兰、霹雳和彭亨四州组成马来联邦。当时马来亚不同州的铁路各自发展，没有一个统一的规划和管理机构，于是殖民政府当局在1901年成立了马来联邦铁路（FMSR），统合管理马来西亚半岛上各地包括马来联邦、马来属邦、海峡殖民地的铁路。这些初期建设的米轨铁路成为连接各主要城镇和经济区的重要交通工具。\u003c/p\u003e\n\u003cp\u003e20世纪初，马来西亚的铁路网迅速扩展，最重要的两条干线为东海岸线和西海岸线。二战期间，铁路遭到日军的严重破坏，但战后得到了重建。1948年，马来联邦铁路被改组为马来亚铁路（MR）。1963年，马来亚、新加坡、砂拉越和沙巴组成了新的联邦国家——马来西亚（新加坡于1965年退出），马来亚铁路的官方名称也更改为马来语“Keretapi Tanah Melayu”（KTM）。\u003c/p\u003e\n\u003cp\u003e现如今，马来西亚的铁路系统主要由两条干线组成：西海岸线和东海岸线。东海岸线（也称金马士－道北铁路）由马来亚铁道运营，起自金马士，途经吉兰丹、彭亨、森美兰，穿越马来半岛中部，全长528公里，最终到达泰国边境的道北。虽然名为东海岸线，但大部分路段穿行于内陆丛林地带，因此又被称为“丛林铁路”，吸引许多背包客体验。西海岸线则起自泰国边境的巴东勿刹，沿西海岸一直延伸至柔佛的南端，通过新柔长堤连接新加坡，全长950公里，途经包括吉隆坡在内的八个州属，是马来西亚铁路运输的重要交通大动脉。由国有企业马来亚铁道公司（KTMB）运营的铁路系统经过了多次改组和现代化，如今的铁路服务已经涵盖了从通勤列车到高速城际列车的多种交通模式。目前，马来西亚铁路系统不仅联通国内主要城市，还连接着泰国和新加坡，是东南亚地区重要的交通枢纽。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/b90015596a676e51d6df9b98736e90da.jpg\" alt=\"b90015596a676e51d6df9b98736e90da.jpg\"  /\u003e\n\n\u003cem\u003e\u003ca href=\"https://www.pinterest.com/pin/ktm-the-620km-east-coast-rail-route-ecrr--834010424715426579/\"\u003e来源\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"铁路分类\"\u003e\u003cstrong\u003e铁路分类\u003c/strong\u003e\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eKTM Komuter（城际通勤列车）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eKTM Komuter是由马来亚铁道公司（KTM）运营的通勤铁路服务，最初于1995年在巴生谷地区启用，后于2015年扩展至北部各州（如霹雳、槟城、吉打和玻璃市）。该服务连接了吉隆坡及其周边郊区，还服务于槟城和其他北部州属，总长度约456公里，包含芙蓉线、巴生港线等多条线路。KTM Komuter的列车是空调电力列车，并提供便捷的“停车换乘”设施，适合游客和通勤者方便进入市区，避免交通拥堵。KTM通勤铁路还为乘客提供多种票务选项，包括通过KTMB移动应用程序购买手机票（二维码）、Touch \u0026rsquo;n Go（仅限巴生谷地区）、自动售票机与车站售票柜台购票。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eETS（电动列车服务）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eETS（Electric Train Service）是由马来亚铁道公司运营的快速城际电力动车组服务，于西海岸线运行，连接金马士与巴东勿刹，全长950公里。ETS是马来西亚最快的米轨列车，速度可达每小时140公里，属于准高速铁路。列车分为Gold、Silver、Platinum 以及 Express 4种等级，分别对应不同的服务和票价，是前往怡保、槟城等城市的高效方式。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eKTM Intercity（城际列车）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eKTM Intercity是马来亚铁道运营的柴油牵引城际列车服务，覆盖马来半岛的南部地区、新加坡以及泰国。部分列车主要运行于东海岸线金马士至道北之间，并延伸至新加坡。2021年4月推出了柴油动车组（DMU）列车，取代了原有的传统列车，服务于东海岸的丹绒至吉利地段。DMU列车配备现代化设施，包括集群座椅、LED显示屏、电源插座、祈祷室、厕所、小餐馆及行李存储空间，并有无障碍设施，满足残障人士的需求。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"如何购买火车票\"\u003e\u003cstrong\u003e如何购买火车票\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e购买火车票的方式有多种，最为方便的是通过KTMB官方网站或手机应用程序进行在线购买。也可以在吉隆坡中环车站（KL Sentral）及其他主要车站直接购票，但热门线路的车票在高峰时期（如假日、周末）可能会售罄，建议提前预订。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/%e6%88%aa%e5%b1%8f2024-10-13%20%e4%b8%8b%e5%8d%887.15.27.png\" alt=\"截屏2024-10-13 下午7.15.27.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/%e6%88%aa%e5%b1%8f2024-10-13%20%e4%b8%8b%e5%8d%887.16.04.png\" alt=\"截屏2024-10-13 下午7.16.04.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e以网上购买ETS车票为例，需在KTMB官网注册账号，然后选择出发和到达车站、以及对应的ETS车次类型（如Gold、Platinum、Express等）。不同级别的车次在停靠站数量和服务上有所不同，例如Platinum和Express级别提供商务座，商务座采用1+2的座位排布，包含免费食物和饮料，价格相对更高；普通席则是2+2的座位排布。Gold车次则没有商务座位。选择座位时，需要注意座位方向，有些座位是与行驶方向相反的，如果不喜欢反向乘坐，记得选择“forward”座位。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/%e6%88%aa%e5%b1%8f2024-10-13%20%e4%b8%8b%e5%8d%887.47.05.png\" alt=\"截屏2024-10-13 下午7.47.05.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eETS列车的车型也有所区别，Platinum和Express列车多为较新款，由中国中车制造，而Silver级别列车则采用较老款，速度较慢，停靠站较多。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/7F6C6773-CA59-499B-93C2-2825C2DAE157_1_105_c.jpeg\" alt=\"7F6C6773-CA59-499B-93C2-2825C2DAE157_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eETS-Express列车（摄于Butterworth车站）\u003c/em\u003e\n\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/B03BE15A-8BF1-4709-BC16-4C95696ED6FE_1_105_c.jpeg\" alt=\"B03BE15A-8BF1-4709-BC16-4C95696ED6FE_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eETS-Gold列车，采用旧款车型（摄于Ipoh车站）\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"吉隆坡轨道交通\"\u003e\u003cstrong\u003e吉隆坡轨道交通\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e吉隆坡及其周边地区有完善的轨道交通系统，涵盖了多种轻轨和地铁线路。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/%e6%88%aa%e5%b1%8f2024-10-14%20%e4%b8%8b%e5%8d%887.56.41.png\" alt=\"截屏2024-10-14 下午7.56.41.png\"  /\u003e\n\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLRT（轻快铁）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLRT是吉隆坡的轻轨系统，包含3号 LRT Ampang Line、4号LRT Sri Petaling Line和5号LRT Kelana Jaya Line。LRT贯穿吉隆坡市中心及重要商业区，是游客前往市区景点如双子塔、茨厂街的首选交通工具。在高峰期，列车会非常拥挤。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/107EF1BC-2124-4F8E-BCEA-7027EA7E3C54_1_105_c.jpeg\" alt=\"107EF1BC-2124-4F8E-BCEA-7027EA7E3C54_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e5号线LRT，摄于Bangsar车站\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e3号LRTAmpang Line（橙黄色）\u003c/strong\u003e：占美清真寺，独立广场，ICC pudu等景点。\u003c/p\u003e","title":"2024-大马轨道交通攻略"},{"content":"AI 新闻 OpenAI 发布o1-preview OpenAI于9月12日发布了其最新的大型语言模型o1-preview，该模型在推理能力上实现了重大突破。o1模型通过使用Chain of Thought技术，能够解决复杂的数学和编程问题，其表现超越了之前的GPT-4o模型。\no1模型在多个基准测试中表现出色，包括在美国国际数学邀请赛中取得接近全国前500名的成绩，并在科学问题的准确度上超过了人类博士水平。该模型通过强化学习增强了推理能力，能够逐步分析问题，拆解复杂步骤并自动修正错误。\nOpenAI还推出了o1-mini版本，虽然规模较小，但在推理任务上表现优异，且成本更低。o1-mini特别适合编程任务，预计未来将向所有ChatGPT用户开放。\n相关信息：\nsystem card\n官方介绍\nReverse engineering OpenAI’s o1\n去年 Jim Fan 对于 Q* 的预测\n相关GitHub仓库： Awesome LLM Strawberry (OpenAI o1)\n关于 Open AI o1 的 AMA\nMIT EI seminar, Hyung Won Chung from OpenAI. \u0026ldquo;Don\u0026rsquo;t teach. Incentivize.\u0026rdquo;\nQwen2.5系列模型 阿里云的Qwen团队近日发布了Qwen2.5系列大型语言模型。该系列包括多种尺寸的模型，从0.5B到72B参数不等，并提供基础和指令调优版本。Qwen2.5在多个方面取得了显著进步，包括更强的知识储备、编码和数学能力的提升，以及在指令跟随、长文本生成和结构化数据理解方面的改进。\nQwen2.5支持多达29种语言，能够处理长达128K的上下文，并生成最多8K的文本。该模型在多语言支持、角色扮演和聊天机器人条件设置方面表现出更高的弹性。\n此外，Qwen2.5还推出了专门用于编码和数学的模型，如Qwen2.5-Coder和Qwen2.5-Math，进一步增强了在这些领域的应用能力。Qwen2.5-Coder特别在代码生成、推理和修复方面表现出色，而Qwen2.5-Math则在数学推理能力上有显著提升。\n官方博客：Qwen2.5: 基础模型大派对！\nDeepSeek-V2.5 DeepSeek-AI 推出了其最新模型 DeepSeek-V2.5，成功融合了之前版本 DeepSeek-V2-Chat 和 DeepSeek-Coder-V2-Instruct 的功能。该模型集成了通用语言处理和编程能力。\nDeepSeek-V2.5 经过性能优化，特别在写作、任务执行和高级编程等方面表现出色，在 AlpacaEval 2.0 和 HumanEval Python 等基准测试中超越了前代产品。此外，模型还新增了函数调用功能，使其能够与外部工具交互，拓展了实时天气报告、翻译等应用场景。\n该模型采用了多头潜在注意力（MLA）等技术创新，显著提升了推理速度和资源利用效率。同时，DeepSeek-V2.5 在 Hugging Face 平台上开源，并提供商业化许可，允许企业将其用于自动化客服、内容生成等业务流程中。其性能表现已跻身主流排行榜，与 GPT-4-Turbo 和 LLaMA3-70B 等顶尖模型齐名。\nLlama3.2 Meta发布的Llama 3.2是该系列的重大升级，拥有从1B到90B参数的多种模型版本，适用于边缘计算设备和数据中心。其中，小型模型（1B和3B）专为移动设备和物联网优化，支持实时处理和高达128K的上下文窗口。而大规模的11B和90B模型则首次引入了视觉处理能力，能够处理文本和图像的结合应用，如图像生成描述、文档分析等。此外，Llama 3.2提供Llama Stack分发工具，简化了开发者的部署流程。这个版本进一步推动了Meta在开放源代码和多模态AI应用中的发展。 Llama 3.2: Revolutionizing edge AI and vision with open, customizable models\n值得关注的开源项目 g1: Using Llama-3.1 70b on Groq to create o1-like reasoning chains 项目旨在通过使用 Llama-3.1 70b 模型在 Groq 上创建类似 o1 的推理链，以提高大型语言模型（LLM）的推理能力。该项目通过提示策略，帮助 LLM 解决通常难以处理的逻辑问题。g1 通过动态的推理链展示 LLM 的“思考”过程，允许用户看到每个推理步骤的标题和内容。项目的目标是展示仅通过提示就能克服 LLM 逻辑问题的潜力，并为开源社区提供灵感，以开发新的策略来生成类似 o1 的推理。g1 适用于需要改进逻辑推理能力的应用场景，尤其是那些涉及复杂问题解决的领域。\n项目地址: GitHub - bklieger-groq/g1\nLocal File Organizer Local File Organizer 是一个开源项目，旨在通过人工智能技术帮助用户高效地管理本地文件，包括文本、图像和PDF文档。该工具利用 Google Gemma 2-2B 和 Llava v1.6 模型，能够智能扫描、重组和整理文件，确保用户的隐私安全。所有处理均在本地设备上完成，无需互联网连接，确保数据不被外泄。\n其 GitHub 页面：Local File Organizer。\nGOT-OCR2.0 该项目提供了一个强大的OCR模型，能够处理各种类型的光学字符，包括文本、数学公式、表格和图形等。GOT-OCR2.0的设计旨在提高OCR的准确性和效率，支持多种输入格式和输出类型，适用于学术研究和实际应用。\n更多信息请访问项目的GitHub页面：GOT-OCR2.0。\n模型： huggingface.co/ucaslcl/GOT-OCR2_0\nFish Speech Fish Speech 是一个全新的开源文本转语音（TTS）解决方案，专为有基础的开发者设计。该项目基于 700,000 小时的多语言音频数据进行训练，支持多种语言，包括英语、中文、德语、日语等。其特点在于高质量的语音合成和丰富的表达能力，能够接近人类的语言处理水平。Fish Speech 的优点在于其开放性和可扩展性，开发者可以根据自己的需求进行定制和优化。更多信息请访问项目的 GitHub 页面：Fish Speech GitHub。\n出海网站经验总结 这个开源项目是一个针对海外网站的笔记管理工具，旨在帮助开发者更高效地记录和管理与海外网站相关的信息。项目的特点包括简洁的界面、易于使用的笔记功能以及支持多种格式的内容输入。优点在于它能够帮助有基础的开发者快速整理和查找信息，提高工作效率。该项目适合希望提升信息管理能力的开发者。更多信息可以访问项目的GitHub地址：GitHub - PrinceSinghhub。\nAwesome-Dify-Workflow Awesome-Dify-Workflow 是一个开源项目，旨在分享一些实用的 Dify DSL 工作流程，适合开发者自用和学习。该项目的特点包括支持多任务并行和会话变量，所有工作流程均可免费使用，且易于导入和修改。它为有基础的开发者提供了一个直观的界面，方便他们快速构建和发布自己的工作流。通过使用 Dify，开发者可以专注于创新，而无需从头开始构建复杂的系统。更多信息和代码可以在 GitHub 上找到。\nitext2kg itext2kg 是一个开源项目，旨在利用大型语言模型构建增量知识图谱。该项目的特点包括灵活性和可扩展性，允许开发者在不断变化的数据环境中动态更新知识图谱。它支持多种数据源，并提供了友好的接口，方便开发者进行集成和扩展。该项目特别适合具备一定基础的开发者，能够帮助他们在知识图谱构建和管理方面提升效率和效果。更多信息请访问项目的 GitHub 页面。\nKotaemon Kotaemon是一个开源的基于RAG（Retrieval-Augmented Generation）技术的工具，旨在帮助开发者与文档进行交互和对话。该项目的特点包括易于使用的界面、灵活的文档处理能力以及支持多种本地和云端模型的集成。Kotaemon特别适合有一定基础的开发者，能够通过自定义和扩展功能来满足特定需求。其优点在于能够提升文档检索和问答的效率，适用于各种应用场景。更多信息请访问其GitHub页面：Kotaemon GitHub。\n影音记录 精选歌单 Live演出 09.17 Milflife Vas 09.21 can festival 舟山 电影 是枝裕和影展《比海更深》\n最近在追的一部韩综《思想验证区域》，非常推荐。\n官方提供的政治倾向测试：https://thecommunity.co.kr/home\n书\u0026amp;阅读摘录 读完了JD-万斯《乡下人的悲歌》\nThe Intelligence Age The story of progress will continue, and our children will be able to do things we can’t.\nHow did we get to the doorstep of the next leap in prosperity?\nIn three words: deep learning worked.\nIn 15 words: deep learning worked, got predictably better with scale, and we dedicated increasing resources to it.\n再见了，盛夏 大部分中产阶级的打工人在工作困顿之际难免反思到工作的目的究竟是什么，而得到的结论普遍为以下几种： 第一种，为钱工作。 第二种，为未来工作。 第三种，为了不工作。\n在我身边的绝大多数人目前也都遵循这一成功模板来构建属于自己的幸福生活，而它的终局只能是幻觉。\n在《规训与惩罚》中，福柯指出这种规训在社会中具有着普遍性，称之为微权力。这种规训微权力通过规范来控制个体的行为，让社会形成了一种全景敞视的监狱，个体在这种权力结构中被迫自我规训，成为合格的社会成员。\n因此，工作和生活保持平衡是一个狡猾说法，它的隐藏含义是将工作与生活置于平等的定位。而生活是一切，工作只是工作，后者是为前者服务的手段而已。\n老猴子：因为空有天赋，不思进取，小富即安，沉迷享乐。想安逸，又想名利；想快意江湖，又想成佛作祖，哪有这样的好事？身本多忧，怎可全求？\nCreativity Fundamentally Comes From Memorization a flash of inspiration connecting internalized concepts. …\nBy definition, you can\u0026rsquo;t even be certain of novelty without familiarity of existing works. Creativity comes to those who have internalized the patterns of their art \u0026ndash; they can see the connection or novelty because it\u0026rsquo;s all in their head.\nTherefore autonomy enables creativity, and a system helps achieve autonomy quicker.\nIt involved two steps:\nMemorizing the different classifications/patterns in a concept [1] Exposing myself to a breadth of cases so I could match the real-world example to one of the previously learned patterns But I maintain that such pattern recognition is just another form of memorization, though often a more subconscious one. This is learned from repeated exposure, and what I\u0026rsquo;m attempting to do when I try to see a \u0026ldquo;breadth\u0026rdquo; of cases after learning the framework and available patterns to use.\nOnce this heuristic is internalized, it becomes a new primitive and we focus our attention to higher-level problems.\nOnce you learn music theory, you\u0026rsquo;re able to violate it in ways that resonate. Avant-garde art requires knowing what the establishment is to push the boundaries as orthogonally as possible.\nIf creativity is born from autonomy in multiple areas, then one way to become more creative is to become autonomous in many areas!\nSo if you want to be creative, learn lots of things in-depth, and learn how to learn them fast. Embrace systems to achieve this, and use them to free you up from the basics \u0026ndash; so you can focus on the novel.\n","permalink":"https://niraya666.github.io/monthly/2024-09/","summary":"\u003ch2 id=\"ai-新闻\"\u003eAI 新闻\u003c/h2\u003e\n\u003ch3 id=\"openai-发布o1-preview\"\u003eOpenAI 发布o1-preview\u003c/h3\u003e\n\u003cp\u003eOpenAI于9月12日发布了其最新的大型语言模型o1-preview，该模型在推理能力上实现了重大突破。o1模型通过使用Chain of Thought技术，能够解决复杂的数学和编程问题，其表现超越了之前的GPT-4o模型。\u003c/p\u003e\n\u003cp\u003eo1模型在多个基准测试中表现出色，包括在美国国际数学邀请赛中取得接近全国前500名的成绩，并在科学问题的准确度上超过了人类博士水平。该模型通过强化学习增强了推理能力，能够逐步分析问题，拆解复杂步骤并自动修正错误。\u003c/p\u003e\n\u003cp\u003eOpenAI还推出了o1-mini版本，虽然规模较小，但在推理任务上表现优异，且成本更低。o1-mini特别适合编程任务，预计未来将向所有ChatGPT用户开放。\u003c/p\u003e\n\u003cp\u003e相关信息：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://cdn.openai.com/o1-system-card.pdf\"\u003esystem card\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://openai.com/index/introducing-openai-o1-preview/\"\u003e官方介绍\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.interconnects.ai/p/reverse-engineering-openai-o1\"\u003eReverse engineering OpenAI’s o1\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://x.com/dotey/status/1835395891483554105\"\u003e去年 Jim Fan 对于 Q* 的预测\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e相关GitHub仓库： \u003ca href=\"https://github.com/hijkzzz/Awesome-LLM-Strawberry\"\u003e\u003cstrong\u003eAwesome LLM Strawberry (OpenAI o1)\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://x.com/op7418/status/1835153295855567356\"\u003e关于 Open AI o1 的 AMA\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=kYWUEV_e2ss\"\u003eMIT EI seminar, Hyung Won Chung from OpenAI. \u0026ldquo;Don\u0026rsquo;t teach. Incentivize.\u0026rdquo;\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"qwen25系列模型\"\u003eQwen2.5系列模型\u003c/h3\u003e\n\u003cp\u003e阿里云的Qwen团队近日发布了Qwen2.5系列大型语言模型。该系列包括多种尺寸的模型，从0.5B到72B参数不等，并提供基础和指令调优版本。Qwen2.5在多个方面取得了显著进步，包括更强的知识储备、编码和数学能力的提升，以及在指令跟随、长文本生成和结构化数据理解方面的改进。\u003c/p\u003e\n\u003cp\u003eQwen2.5支持多达29种语言，能够处理长达128K的上下文，并生成最多8K的文本。该模型在多语言支持、角色扮演和聊天机器人条件设置方面表现出更高的弹性。\u003c/p\u003e\n\u003cp\u003e此外，Qwen2.5还推出了专门用于编码和数学的模型，如Qwen2.5-Coder和Qwen2.5-Math，进一步增强了在这些领域的应用能力。Qwen2.5-Coder特别在代码生成、推理和修复方面表现出色，而Qwen2.5-Math则在数学推理能力上有显著提升。\u003c/p\u003e\n\u003cp\u003e官方博客：\u003ca href=\"https://qwenlm.github.io/zh/blog/qwen2.5/\"\u003eQwen2.5: 基础模型大派对！\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"deepseek-v25\"\u003eDeepSeek-V2.5\u003c/h3\u003e\n\u003cp\u003eDeepSeek-AI 推出了其最新模型 DeepSeek-V2.5，成功融合了之前版本 DeepSeek-V2-Chat 和 DeepSeek-Coder-V2-Instruct 的功能。该模型集成了通用语言处理和编程能力。\u003c/p\u003e\n\u003cp\u003eDeepSeek-V2.5 经过性能优化，特别在写作、任务执行和高级编程等方面表现出色，在 AlpacaEval 2.0 和 HumanEval Python 等基准测试中超越了前代产品。此外，模型还新增了函数调用功能，使其能够与外部工具交互，拓展了实时天气报告、翻译等应用场景。\u003c/p\u003e\n\u003cp\u003e该模型采用了多头潜在注意力（MLA）等技术创新，显著提升了推理速度和资源利用效率。同时，DeepSeek-V2.5 在 Hugging Face 平台上开源，并提供商业化许可，允许企业将其用于自动化客服、内容生成等业务流程中。其性能表现已跻身主流排行榜，与 GPT-4-Turbo 和 LLaMA3-70B 等顶尖模型齐名。\u003c/p\u003e","title":"2024-09 月刊"},{"content":"第一次来到舟山，仅仅是因为Can-festival音乐节。\n上次错过了越位音乐节，这次看到Can-festival的阵容，根本不可能再错过。光是Day2的阵容：We Lost the Sea、Lost in Kiev、The Seven Mile Journey、声子虫，几乎全是我播放器里多年循环的乐队。更别提Day3，This Will Destroy You和World’s End Girlfriend等老牌乐队，还有年初遗憾未能见到的四月雨。\n与“普拉桑”的极限拉扯 出发前几天，台风“普拉桑”紧跟着前一个台风的步伐，直扑江浙沪，着实让我焦虑不已。临行的前一天，我每隔二十分钟就要刷新一次台风路径图，心里祈祷它能转向或加速离去。担心音乐节会取消，或是要在狂风暴雨中听音乐。不过，最终台风带来的破坏都留在了上海，而到了舟山，却意外地迎来了晴朗的天气。\n暴雨中的南浦大桥\n后摇和雨更配 其实，我一向讨厌下雨。但谁能想到，在海边，后摇与风雨竟如此契合。或许，这正是音乐的魔力。雨水肆意洒落，浸湿了镜片，视线模糊不清，分不清脸上的水滴究竟是从天而降，还是心底涌出的泪水。音乐与环境的交织，给了我一种难以言喻的感动。\n海边\n当安尼西亚说这首歌是献给孩子，愿这个世界没有伤害时，我的心突然被一种无法抑制的情感击中。或许我们无法阻止每一个悲剧的发生，但在音乐的陪伴下，我希望这个世界能逐渐变得更加温暖。愿这首《离别》不仅仅是一首哀歌。\nLost in Kiev 在合成器音色和处理上，确实惊艳到我了。不过，他们的贝斯手长的真的好像萨拉赫。\nLost in Kiev\nLITE 的音乐在雨中展现出了出乎意料的活力。即使雨水不停，现场的每个人都随着节奏跳动，那种难以言喻的现场氛围，仿佛雨水并不再是困扰，而是与音乐共舞的一部分。\n雨中的LITE\n声子虫的音墙依旧厚重，像一座无形的高山压迫着我。正如去年看他们演出时的一样， 特别喜欢他们开场时的创意—随着音乐的铺垫进行， 在屏幕上，一笔一画地把“聲子蟲” 三个字缓慢打出。\n缺了最后一笔的“聲子蟲”\n一直期待We Lost the Sea的现场演出，但就在那时，吉他却出了问题。声音时有时无，让本该震撼人心的演奏显得有些破碎。雨水不停地落在身上，心情也随之变得复杂。一点小小的遗憾，像是雨中一滴未曾融入地面的水珠，总让人感到有些不完整。\nWe Lost the Sea — by 群里的艺术家\nThe Seven Mile Journey — by 群里的艺术家\n年纪渐长，似乎每次出门前都有更多的顾虑。年轻时我总能义无反顾地冲向每一个音乐节，不管是风雨还是其他未知的挑战，都觉得无所谓。然而这次，在雨中，我的脑海里浮现出更多的“如果”：如果天气更好一些，如果能再勇敢一些……我无法完全放下这些顾虑，哪怕台上的音乐依旧动人，我却发现自己没有像以前那样尽兴了。也许成长的代价，就是学会在更多的限制和考量中寻找自己的平衡。\n遗憾，化作前行的力量 只看了Day2的演出后，考虑到气温骤降和大雨，再加上风大到船次取消，去普陀山的计划也被迫放弃，我决定提前撤离。没想到，海边的天气竟如此难以捉摸，Day3下午居然又转晴了。错过了傍晚的晚霞，也错过了在L-ISA环绕音响下的World’s End Girlfriend演出，实在是遗憾至极。然而，这些遗憾，正是让我期待下一次音乐节的动力。或许，正因为未尽的体验，才会让我对未来的演出更加期待。\n也感谢群里的小伙伴传来的照片和视频， 能让我第一时间见证这些美丽的瞬间。\n我错过的晚霞 — by 群里的艺术家\n四月雨和夕阳 — by 群里的艺术家\nThis Will Destroy You — by 群里的艺术家\nWEG — by 群里的艺术家\n当然，更要感谢罐总Can给大家带来的这次演出，愿大家都能圆梦。\n","permalink":"https://niraya666.github.io/travel/2024-can-festival-%E5%9C%A8%E6%B5%B7%E8%BE%B9-%E6%B7%B7%E6%9D%82%E7%9D%80%E9%9B%A8%E6%B0%B4%E5%92%8C%E6%B3%AA%E6%B0%B4-%E5%90%91%E5%90%8E%E6%91%87/","summary":"\u003cp\u003e第一次来到舟山，仅仅是因为Can-festival音乐节。\u003c/p\u003e\n\u003cp\u003e上次错过了越位音乐节，这次看到Can-festival的阵容，根本不可能再错过。光是Day2的阵容：We Lost the Sea、Lost in Kiev、The Seven Mile Journey、声子虫，几乎全是我播放器里多年循环的乐队。更别提Day3，This Will Destroy You和World’s End Girlfriend等老牌乐队，还有年初遗憾未能见到的四月雨。\u003c/p\u003e\n\u003ch2 id=\"与普拉桑的极限拉扯\"\u003e与“普拉桑”的极限拉扯\u003c/h2\u003e\n\u003cp\u003e出发前几天，台风“普拉桑”紧跟着前一个台风的步伐，直扑江浙沪，着实让我焦虑不已。临行的前一天，我每隔二十分钟就要刷新一次台风路径图，心里祈祷它能转向或加速离去。担心音乐节会取消，或是要在狂风暴雨中听音乐。不过，最终台风带来的破坏都留在了上海，而到了舟山，却意外地迎来了晴朗的天气。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/DE10C0BD-A093-45EE-B67F-421C767FE234.jpg\" alt=\"DE10C0BD-A093-45EE-B67F-421C767FE234.jpg\"  /\u003e\n\n\u003cem\u003e暴雨中的南浦大桥\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"后摇和雨更配\"\u003e后摇和雨更配\u003c/h2\u003e\n\u003cp\u003e其实，我一向讨厌下雨。但谁能想到，在海边，后摇与风雨竟如此契合。或许，这正是音乐的魔力。雨水肆意洒落，浸湿了镜片，视线模糊不清，分不清脸上的水滴究竟是从天而降，还是心底涌出的泪水。音乐与环境的交织，给了我一种难以言喻的感动。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/8E78A7AB-8588-4165-B71F-261E284C1E51_1_105_c.jpeg\" alt=\"8E78A7AB-8588-4165-B71F-261E284C1E51_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e海边\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e当安尼西亚说这首歌是献给孩子，愿这个世界没有伤害时，我的心突然被一种无法抑制的情感击中。或许我们无法阻止每一个悲剧的发生，但在音乐的陪伴下，我希望这个世界能逐渐变得更加温暖。愿这首《离别》不仅仅是一首哀歌。\u003c/p\u003e\n\u003cp\u003eLost in Kiev 在合成器音色和处理上，确实惊艳到我了。不过，他们的贝斯手长的真的好像萨拉赫。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/ABF036E9-FBF3-4B73-9B8F-6804C5FFBEBD_1_105_c.jpeg\" alt=\"ABF036E9-FBF3-4B73-9B8F-6804C5FFBEBD_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eLost in Kiev\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eLITE 的音乐在雨中展现出了出乎意料的活力。即使雨水不停，现场的每个人都随着节奏跳动，那种难以言喻的现场氛围，仿佛雨水并不再是困扰，而是与音乐共舞的一部分。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/CEF4C784-5E96-402F-A73B-AD3ED115EF1F_1_105_c.jpeg\" alt=\"CEF4C784-5E96-402F-A73B-AD3ED115EF1F_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e雨中的LITE\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e声子虫的音墙依旧厚重，像一座无形的高山压迫着我。正如去年看他们演出时的一样， 特别喜欢他们开场时的创意—随着音乐的铺垫进行， 在屏幕上，一笔一画地把“聲子蟲” 三个字缓慢打出。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/46590887-7972-4CEF-A4F7-A40BF24EDAE4_1_105_c.jpeg\" alt=\"46590887-7972-4CEF-A4F7-A40BF24EDAE4_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e缺了最后一笔的“聲子蟲”\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e一直期待We Lost the Sea的现场演出，但就在那时，吉他却出了问题。声音时有时无，让本该震撼人心的演奏显得有些破碎。雨水不停地落在身上，心情也随之变得复杂。一点小小的遗憾，像是雨中一滴未曾融入地面的水珠，总让人感到有些不完整。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/BEA4ACE5-B17A-48FB-9E95-EE2BFA987988_1_105_c.jpeg\" alt=\"BEA4ACE5-B17A-48FB-9E95-EE2BFA987988_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eWe Lost the Sea — by 群里的艺术家\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/1EF13608-9B3E-4159-90C3-034AAD648C5D_1_105_c.jpeg\" alt=\"1EF13608-9B3E-4159-90C3-034AAD648C5D_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eThe Seven Mile Journey — by 群里的艺术家\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e年纪渐长，似乎每次出门前都有更多的顾虑。年轻时我总能义无反顾地冲向每一个音乐节，不管是风雨还是其他未知的挑战，都觉得无所谓。然而这次，在雨中，我的脑海里浮现出更多的“如果”：如果天气更好一些，如果能再勇敢一些……我无法完全放下这些顾虑，哪怕台上的音乐依旧动人，我却发现自己没有像以前那样尽兴了。也许成长的代价，就是学会在更多的限制和考量中寻找自己的平衡。\u003c/p\u003e","title":"2024 Can-festival：在海边 混杂着雨水和泪水 向后摇"},{"content":"AI 新闻 Grok-2 Beta 版本 xAI公司发布了Grok-2和Grok-2 mini的测试版，现已向𝕏平台上的Premium和Premium+用户开放。Grok-2是一个先进的语言模型，在聊天、编码和推理方面表现出色，超过了Claude 3.5 Sonnet和GPT-4-Turbo等竞争对手。Grok-2 mini则在速度和答案质量之间取得平衡。此外，关于图像生成功能可能被滥用的问题也引起了关注。\n微软发布Phi-3.5系列模型 https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280\n涵盖了Phi-3.5-mini、Phi-3.5-MoE和Phi-3.5-vision三种模型。\nPhi-3.5-mini：拥有3.8亿参数，支持128K上下文长度，专注于多语言支持和推理能力。\nPhi-3.5-MoE：混合专家模型，总参数量为42亿，但每次激活仅使用6.6亿参数。\nPhi-3.5-vision：具备4.2亿参数，能够处理多帧图像理解和推理任务，在单图像基准测试中表现显著提升。\nCursor AI 最近大热的一款AI编程辅助工具。\n不仅支持多行代码的自动完成，还能够根据代码上下文进行智能生成和重写， 代码解释和调试帮助。\n2024年8月，Cursor AI 完成了一轮6000万美元的融资，使得公司的估值达到了4亿美元。\nGoogle发布Gemma 2 2B模型及其相关工具 Google发布了新一代轻量级开源大语言模型Gemma 2 2B，并推出了一系列相关工具，包括ShieldGemma和Gemma Scope。这些新工具旨在提升AI的安全性、透明度和性能。\nQwen2-VL https://qwenlm.github.io/zh/blog/qwen2-vl/\n通义千文团队开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API。\n72B在大部分指标上甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等。\nQwen2-VL 可理解20分钟以上长视频，并将其用于基于视频的问答等应用。\n值得关注的开源项目 Deep Live Cam real time face swap and one-click video deepfake with only a single image\nGithub: https://github.com/hacksider/Deep-Live-Cam\nmoffee moffee 是一个开源的幻灯片制作工具，可以将 Markdown 文档转换为简洁、专业的幻灯片。\n（等效于自动实现LLM生成markdown+vscode中Marp渲染）\nGitHub：github.com/BMPixel/moffee\n【三年面试五年模拟】算法工程师的求职面试秘籍 【三年面试五年模拟】算法工程师秘籍。涵盖AIGC、传统深度学习、自动驾驶、机器学习、计算机视觉、自然语言处理、具身智能、元宇宙、AGI、SLAM等AI行业面试笔试经验与干货知识。\nGitHub：https://github.com/WeThinkIn/Interview-for-Algorithm-Engineer\nAI-Scientist The AI Scientist项目旨在开发一个能够进行科学研究和发现新知识的全自动系统。该系统利用大型语言模型（LLMs）等基础模型，独立执行研究任务。与现有的需要大量人工监督或限制在特定任务上的模型不同，AI Scientist能够自动生成科学论文、进行实验设计和数据分析。\nGitHub - SakanaAI/AI-Scientist\nClapper Clapper 是一个开源的AI故事可视化工具，旨在通过交互式、迭代和直观的过程，让任何人都能使用AI创建视频，而无需外部工具、电影制作或AI工程技能。与传统的视频编辑器或使用3D场景作为输入的现代AI编辑器不同，Clapper通过高层次的抽象（如角色、地点、天气、时间段、风格等）来迭代故事。项目还在开发一个导演模式，用户可以通过全屏模式和AI助手互动来制作电影。\nGitHub - jbilcke-hf/clapper\nmindSearch MindSearch 是一个基于大型语言模型（LLM）的多代理框架，用于构建类似于 Perplexity.ai Pro 和 SearchGPT 的网络搜索引擎。该项目旨在通过模拟人类思维的方式，提供深度的 AI 搜索功能。其核心功能动态图构建， 可以将查询分解成多个子问题查询，并调用工具动态扩展图。\nGithub: https://github.com/InternLM/MindSearch\nSD变现宝：一键把comfyui工作流转换成小程序 旨在将ComfyUI工作流一键转换为小程序，包括“微信小程序”、“抖音小程序”、“微信内H5”和“微信外H5”，并支持支付变现功能。\nGitHub - zhulu111/ComfyUI_Bxb\n影音记录 精选歌单 Live演出 08.03 Sat. yinyin vas\n电影 《异形：夺命舰》\n书\u0026amp;阅读摘录 读完《数字化孤独：社交媒体时代的亲密关系》。\n写书真是个不挣钱的事情 但是写书，仍旧是跟写文章不同的事情。既然我们谈到深度阅读，我们就也需要考虑深度写作。我可以每天写切中当前新闻热点的文章，这也是一种写作。但是如果这些文章是日抛型的，一年后，甚至一个月后，大家看到就索然无味的话，那么这些写作就是应景的，浅表的。但是深度的写作，可以有更长时间的价值。别的不提，我前面提到的三篇爆款文，至少是有跨越一定时间的力量的。\n我想我应该给自己一点点压力，去写一些真正的书，真正有野心成为一个有体系，有结构的作品。也许写作方法还是边写边在公众号上发布，但是写的时候，就开始思考结构是什么，主题是什么，要解决什么样的问题。 这样才成为深度写作。虽然不太挣钱。\n李沐重返母校上海交大，从LLM聊到个人生涯，这里是演讲全文 短期来看，算力翻倍，价格可能会有 1.4 倍的提升。但是长期来看，当竞争变得越来越激烈，摩尔定律会发挥作用，就是说算力翻倍，价格不一定变。所以长期来看算力会变得越来越便宜。 … 很多时候，大家不要去想我现在能搞多大的模型，一年之后，这个模型会贬值。我想说，大模型不是特别有性价比的东西。你要想清楚，从长期来看，你的模型能带来什么价值，让你能够保值。\n所以，很有可能在未来一阵子，因为受限于内存大小和数据的尺寸，我觉得 100B 到 500B 会是未来主流的一个大势。\n音乐是一种表达，是一个人的交互，任何一个什么感觉你都可以通过音乐表达 … 现在你掌握了这个表达工具之后，我们未来很多人会用音乐这个形式来表达想法和情感。我觉得这个可能是影响力会特别大的，那它不是个技术问题，它可能还是一个商业问题。\n因为设计好的东西不一定满足我们的所有需求，可能满足了 80%，但没有满足对细节的需求，这时可以通过长文本，即输入很长的文字来解决\n在应用层面，AI 本质上是去辅助人类完成任务，给人类提供无限的人力资源。我将应用分成三类: 第一类就是文科白领。白领是用自然语言去跟人、跟世界打交道，包括写文章或者其他。 第二个是工科白领，目前 AI 想取代程序员还早得很。 最后一个是蓝领阶级，这反而是最难的，这里面唯一做的好的是自动驾驶。 对于文科白领的工作，AI 已经能完成简单任务，复杂任务需要继续努力。对于工科白领的工作，简单任务还需要努力，复杂任务存在困难。对于蓝领的工作，除了无人驾驶和特定场景（比如工厂，场景变化不大，也能采集大量数据），AI 连简单任务都做不了，完成复杂任务更难。\n现在在我看起来预训练是一个工程问题，后训练才是一个技术问题。 … 8B 和 70B 的情况也不一样，8B 上调的很多东西在 70B 上不一定成立。\n就是说没有真正的垂直模型，就算是一个很垂直领域的模型，它的通用能力也是不能差的。\n所以很多时候，评估是你最重要的事情，先把评估做好，再去做别的事情。\n从最基本的目标来说，去大公司，是为了升职加薪；读 PhD ，你要保证自己能毕业；而创业的目标是要能推出产品，要么上市，要么卖掉，这是每天都需要思考的。\n打工人的好处是，可以在一个相对简单的环境里学习各种从业知识，比如一个技术如何落地、产品怎么做出来、怎么设计、怎么运营、怎么管理。\n其次是干完被安排的任务后，晚上睡觉不用太担心其他，不会做噩梦。 …\n那么打工人的坏处是什么？坏处就是停留在打工人或者职业经理人的思维。\n创业：有「生死一瞬间」的刺激，也有「三小时醒一次」痛苦\n你要有一个强烈的动机，而强烈的动机要么来自很深沉、很底层的欲望，要么来自很深的恐惧。\n如果这个问题有学术价值，你可以考虑去读 PhD；如果这个问题有商业价值，你可以考虑去创业；如果以上两种属性都不够强烈，但至少有成长价值，那先做做打工人也未尝不可。\n你用导师或者上级的角度去总结自己：你每周干了哪些事情？为什么这些目标没达成？\n这是一个最好的时代，新的技术带来了很多新的机会，就算没有新一代技术出现，现有的技术对世界未来几年的影响都会非常大。 … 同时，这也是一个最坏的时代，在座的各位付出的努力要比上一代更多。上一代吃到了时代红利，而到了你们这一代，时代红利还是有的，只是需要付出更多努力。\n李沐：创业一年，人间三年！ 回过头想，如果AGI是达到普通人类水平，客户要的是专业人士的水平。 … 虽然当时我们内心对AGI充满敬畏，但感觉是避不开的。\n第四阶段的认知是，好的垂直模型通用能力也不能弱，例如reasoning，instruction following这些能力垂直上也是需要的。长远来看，通用和垂直模型都得朝着AGI去。只是垂直模型可以稍微偏科一点，专业课高分，通用课还行，所以研发成本稍微低一点，研发方式也会不一样点。\n我希望工作上有个非常厉害的虚拟助手能跟我一起发明新的东西。等我老了也想有很有意思的机器人陪着。我对于未来的预测是，生产工具越来越发达，一个人完成之前一个团队才能完成的事情，导致人类更加个体独立，大家都忙着追求自己的事情，从而更加孤独。\n创业之后才真正觉得团队的重要性。在大厂的时候，觉得自己是个螺丝钉，团队成员是螺丝，甚至团队也是个螺丝钉。但创业团队就是一辆车。车小点，但能跑，能载重，转弯灵活，各个角落都能去。\n36氪出海·日本｜Sparticle金峰：生成式AI创业，日本能够提供什么机会？ 在大语言模型时代，语言的隔阂正在被消减，跨语种的信息平权成为可能。\n“日本市场是有一定延时性的，在美国、国内激烈竞争的时候，我们做日韩台、做小语种，竞争更小。\n金峰观察到，“日本企业更讲究‘匠人精神’，不追求爆发式增长或者颠覆式创新。做垂直行业、聚焦核心场景，能找到客户、服务好客户，也能过得很好。对他们来说，风险企业反而比较奇怪。”\n在日本面临的语言文化差异也是巨大的。“中国公司的商业逻辑反而可能和美国很像，追求效率，但日本更复杂，人脉、商脉的建立更漫长。\n德鲁克最经典长文：你必须成为自己的CEO 我们生活的这个时代充满着前所未有的机会：如果你有雄心，又不乏智慧，那么不管你从何处起步，你都可以沿着自己所选择的道路登上事业的顶峰。\n但前提是，你必须成为自己的首席执行官，知道何时改变发展道路，并在可能长达50年的职业生涯中不断努力、干出实绩。\n我的优势是什么？ …\n要发现你的优势，唯一的方法就是回馈分析法。 每当你做重要决定或采取重要行动时，写下你期望发生的事情。9或12个月以后，将实际结果与你的期望做对比。\n…\n我是如何做事的？\n首先要清楚，你是阅读型还是倾听型。\n其次，要了解一个人的做事方式，还需要弄清一点，即要知道这个人的学习方式。\n有些人在实干中学习，而另外一些人则通过听自己说话来学习。\n不过，此时他们应该弄清楚以下三个问题的答案：\n我的优势是什么？\n我的做事方式是怎样的？\n还有，我的价值观是什么？\n电商的美国游戏，靠卷赢不了 Temu 的加速崛起更加凸显了行业老大 Amazon 的问题：它便宜不下来。\n在美国的电商游戏里，想要参与就一定要解决物流的问题。但美国需要解决的物流问题和中国完全不是一个。 … 但在美国情况却相反：从中国到美国的物流成本甚至低于美国境内的物流成本。\n这么做显然是不可行的。「郊区大 House」的生活图景在几十年后最终把美国割裂成了两个地理空间：一半是中产阶级和富人，另一半是中下层中产和贫民。而这个几十年城市规划错误的持续积累，把它的苦果也喂给了 Temu 在内的所有需要配送的服务者。\n因为为期几十年的「去工业化」不只是让一批工厂消失，也带走了工厂的上下游、不再培养熟练工人，它失去了大规模生产的能力。而苦果被这个国家的所有人一同咽下。在 Temu 之前，沃尔玛、亚马逊的大半商品已经来自中国。\n失去工业的美国，却有 96% 的工业空间都被仓库租走了。这是个仓库的国家，密密麻麻的大型仓库、个人仓库出现在乡村、郊区、港口、城市之间。美国有全球最大规模的仓库，而且还在不断增加。从 2007 年的 14,600 个仓库增长到了 2023 年大约 22,000 个。即便如此，美国依然面临仓库短缺的问题。\n原本生产一切的美国产业先被转移到欧洲、日本；然后是韩国、四小龙；现在是中国，我们成为了新的世界工厂。未来还有一部分可能会去到越南、印度、印尼\u0026hellip;\u0026hellip;而美国的工厂变成仓库、中产社区衰退成 Hood、工人变成朝不保夕的穷人。 接住了工业的地方会源源不断的生产，商品漂洋过海和再次回到一切的起点与曾经的生产者汇合。经济对人的抛弃结了痂，成了电商企业报表中一条别别扭扭的、需要解释的成本细项。\nThe LLM Triangle Principles to Architect “LLM-Native apps are 10% sophisticated model, and 90% experimenting data-driven engineering work.”\nBuilding production-ready LLM applications requires careful engineering practices. When users cannot interact directly with the LLM, the prompt must be meticulously composed to cover all nuances, as iterative user feedback may be unavailable.\nCurrently, agents (without implicit guidance) are not very good at planning complex processes and usually skip essential steps.\nThe truth is that we don’t always need the largest model; it depends on the task.\n智变时代 / 全面理解机器智能与生成式 AI 加速的新工业革命 “最重要的不是它解决的具体问题，而是广泛意义的通用性在增加”\n大语言模型不是数据库，而是推理引擎！\n接下来，AI 要做的事情就是降低企业在知识工作者上的支出，让大家购买或者是租用能自己工作的 SaaS，从而提高在软件市场的支出。\n新形态的 AI 公司将会是 AI 自动化管理的供应商，只需要少量的人类，可以支持更大规模的客户 AI 劳动力概念的新型态软件，将吃掉传统 SaaS 的市场空间，让他们的利润减少，当然也会同时减少人类的工作机会，但给了 AI 初创企业新的市场机会，来挑战传统软件公司\n现代经济学中有一个简单的定义：当商品、服务或资源的供应或容量增加导致其消费增加时，就会发生诱导需求（Induced demand）。 我们现在使用搜索引擎的成本几乎为零，代价是需要看广告 随着能源、智能和灵活性的成本趋近于零，整个经济将变得更大，结构上更加盈利。科技公司将有巨大的机会销售他们即插即用的劳动力和智能，本质上是将现在分散且不一致的人类劳动力产品化、集中化、按需化和作为服务给软件化，即 SaaS 化。\n在智能时代，将劳动力支出转化为研发支出将改变公司的成本结构，虽然这需要更多的前期投资，但能降低生产和服务的边际成本。\n但是从长远看，人类构建的知识有个明显的天花板，它们的发展会被限制 … 我们无法用符号的逻辑来穷尽这种复杂；相反，我们应该只内建能够找到并捕获这些复杂性的元方法 … 哲学家维特根斯坦（Johann Wittgenstein）很早就表达了对这种近似性的思考：按学科分类做“知识图谱”的方法根本不可能穷尽所有的知识，事物之间总有些相似性是模糊的、不明确的、难以用语言来形容的。 … “理解”就是知道如何将这些符号转换成向量，以及这些向量的元素应该如何相互作用来预测下一个符号的向量。知识体现在你使用的向量及其元素之间的相互作用上，而不是符号规则。 概念作用的权重构成了模型的知识！\n在许多事情都被自动化的情况下，什么是值得学习的呢？\n首先，学会提问，未来的教育应侧重于如何提出有价值的问题，而不仅仅是回答问题。好的思考才能带来好的问题，批判性思维和好奇心是思考的前提，也是人类文明能够向前的动力。\n其次，学会广泛而深入地思考 —— 尽可能多地吸收各种知识和思维模式，重点培养结构化思维。调将复杂的问题分解成更小、更易管理的部分，并通过逻辑和有条理的步骤来解决这些部分，从而达到解决整体问题的目的。\n要成为能指挥 AI 帮我们干活的超级个体，就像 Sam Altman 在 WTF 上比喻的那样：“如果每个人都有一家由一万个能力极强的虚拟 AI 员工组成的公司，世界将会怎样？” 要成为能调动 AI 干活的人，而不是被 AI 替代的人。 … 除了前面提到的三个内在能力提升之外，更好的情绪管理和沟通能力，会强化建立社会连接的效率，人类的服务会变得更加昂贵；\n","permalink":"https://niraya666.github.io/monthly/2024-08/","summary":"\u003ch1 id=\"ai-新闻\"\u003eAI 新闻\u003c/h1\u003e\n\u003ch3 id=\"grok-2-beta-版本\"\u003eGrok-2 Beta 版本\u003c/h3\u003e\n\u003cp\u003exAI公司发布了Grok-2和Grok-2 mini的测试版，现已向𝕏平台上的Premium和Premium+用户开放。Grok-2是一个先进的语言模型，在聊天、编码和推理方面表现出色，超过了Claude 3.5 Sonnet和GPT-4-Turbo等竞争对手。Grok-2 mini则在速度和答案质量之间取得平衡。此外，关于图像生成功能可能被滥用的问题也引起了关注。\u003c/p\u003e\n\u003ch3 id=\"微软发布phi-35系列模型\"\u003e微软发布Phi-3.5系列模型\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280\"\u003ehttps://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e涵盖了Phi-3.5-mini、Phi-3.5-MoE和Phi-3.5-vision三种模型。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePhi-3.5-mini\u003c/strong\u003e：拥有3.8亿参数，支持128K上下文长度，专注于多语言支持和推理能力。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePhi-3.5-MoE\u003c/strong\u003e：混合专家模型，总参数量为42亿，但每次激活仅使用6.6亿参数。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePhi-3.5-vision\u003c/strong\u003e：具备4.2亿参数，能够处理多帧图像理解和推理任务，在单图像基准测试中表现显著提升。\u003c/p\u003e\n\u003ch3 id=\"cursor-ai\"\u003eCursor AI\u003c/h3\u003e\n\u003cp\u003e最近大热的一款AI编程辅助工具。\u003c/p\u003e\n\u003cp\u003e不仅支持多行代码的自动完成，还能够根据代码上下文进行智能生成和重写， 代码解释和调试帮助。\u003c/p\u003e\n\u003cp\u003e2024年8月，Cursor AI 完成了一轮6000万美元的融资，使得公司的估值达到了4亿美元。\u003c/p\u003e\n\u003ch3 id=\"google发布gemma-2-2b模型及其相关工具\"\u003e\u003cstrong\u003eGoogle发布Gemma 2 2B模型及其相关工具\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eGoogle发布了新一代轻量级开源大语言模型Gemma 2 2B，并推出了一系列相关工具，包括ShieldGemma和Gemma Scope。这些新工具旨在提升AI的安全性、透明度和性能。\u003c/p\u003e\n\u003ch3 id=\"qwen2-vl\"\u003e\u003cstrong\u003eQwen2-VL\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://qwenlm.github.io/zh/blog/qwen2-vl/\"\u003ehttps://qwenlm.github.io/zh/blog/qwen2-vl/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e通义千文团队开源了 Qwen2-VL-2B 和 Qwen2-VL-7B，并发布了 Qwen2-VL-72B 的 API。\u003c/p\u003e\n\u003cp\u003e72B在大部分指标上甚至超过了 GPT-4o 和 Claude3.5-Sonnet 等。\u003c/p\u003e\n\u003cp\u003eQwen2-VL 可理解20分钟以上长视频，并将其用于基于视频的问答等应用。\u003c/p\u003e\n\u003ch1 id=\"值得关注的开源项目\"\u003e值得关注的开源项目\u003c/h1\u003e\n\u003ch3 id=\"deep-live-cam\"\u003eDeep Live Cam\u003c/h3\u003e\n\u003cp\u003ereal time face swap and one-click video deepfake with only a single image\u003c/p\u003e\n\u003cp\u003eGithub: \u003ca href=\"https://github.com/hacksider/Deep-Live-Cam\"\u003ehttps://github.com/hacksider/Deep-Live-Cam\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"moffee\"\u003emoffee\u003c/h3\u003e\n\u003cp\u003emoffee 是一个开源的幻灯片制作工具，可以将 Markdown 文档转换为简洁、专业的幻灯片。\u003c/p\u003e","title":"2024-08 月刊"},{"content":"在使用大型语言模型（LLM）时，我们常常面临一个挑战：如何从模型输出中准确提取自己所需的信息。例如，当我们希望模型输出 JSON 格式的数据时，由于模型生成的内容并不总是稳定，可能需要额外编写大量的正则表达式来匹配并提取其中的有效信息。然而，由于 LLM 的能力，导致其输出结构并不永远可靠。\n现阶段， 让LLM按要求生成特定格式文本的主要方法有几种种：\n微调：使模型的输出遵循特定格式\nOpenAI Json-mode/Structured Outputs/function-calling: 这些功能允许模型生成更严格、结构化的输出，但受限于openAI平台。\n格式约束：在decoding阶段进行约束，限制模型的输出，\nPrompt Engineering： 最简单的办法，但不稳定。\n多阶段prompting： 通过多个步骤的提示逐步引导模型生成所需的格式。\n本文将聚焦在Structured Outputs， 受限编码， 和prompt-engineering的角度，探讨它们在生成特定格式文本中的应用和效果。\nJson Mode 仅特定模型和平台支持\n以openAI 为例， 在openai.chat.completions.create 参数中增加response_format={\u0026quot;type\u0026quot;:\u0026quot;json_object\u0026quot;} 即可（具体参见：response_format ）。\n需要在prompt中要求输出json格式\n不能保证完全按要求的格式结构输出\n但非100%成功率，存在一些需要额外检测和适当处理的edge case。\nHandling edge cases 根据OpenAI官方文档提供的处理方案 https://platform.openai.com/docs/guides/structured-outputs/json-mode we_did_not_specify_stop_tokens = True try: response = client.chat.completions.create( model=\u0026#34;gpt-3.5-turbo-0125\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant designed to output JSON.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Who won the world series in 2020? Please respond in the format {winner: ...}\u0026#34;} ], response_format={\u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34;} ) # Check if the conversation was too long for the context window, resulting in incomplete JSON if response.choices[0].message.finish_reason == \u0026#34;length\u0026#34;: # your code should handle this error case pass # Check if the OpenAI safety system refused the request and generated a refusal instead if response.choices[0].message[0].get(\u0026#34;refusal\u0026#34;): # your code should handle this error case # In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing print(response.choices[0].message[0][\u0026#34;refusal\u0026#34;]) # Check if the model\u0026#39;s output included restricted content, so the generation of JSON was halted and may be partial if response.choices[0].message.finish_reason == \u0026#34;content_filter\u0026#34;: # your code should handle this error case pass if response.choices[0].message.finish_reason == \u0026#34;stop\u0026#34;: # In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a \u0026#34;stop token\u0026#34; if we_did_not_specify_stop_tokens: # If you didn\u0026#39;t specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object # This is guaranteed to parse successfully and should now contain \u0026#34;{\u0026#34;winner\u0026#34;: \u0026#34;Los Angeles Dodgers\u0026#34;}\u0026#34; print(response.choices[0].message.content) else: # Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately pass except Exception as e: # Your code should handle errors here, for example a network error calling the API print(e) 使用pydantic的方案 使用pydantic的方案 from pydantic import BaseModel, EmailStr, ValidationError # 定义你期望的 JSON 数据模型 class UserModel(BaseModel): name: str age: int email: EmailStr # 检查 JSON 是否符合模型的函数 def validate_json(json_str): try: # 将输入的 JSON 字符串转换为 UserModel 实例 user = UserModel.parse_raw(json_str) # 如果验证通过，返回字典 return user.dict() except ValidationError as ve: print(f\u0026#34;JSON validation error: {ve.json()}\u0026#34;) return None # 示例用法 json_str = \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;}\u0026#39; validated_json = validate_json(json_str) if validated_json is not None: print(\u0026#34;JSON is valid and conforms to the schema:\u0026#34;) print(validated_json) else: print(\u0026#34;JSON is invalid.\u0026#34;) Json-Mode 更多是对于输出json的格式进行检查(即Json格式的有效性)\nStructured Outputs 在模型升级到2024-08-06版本后， OpenAI增加了Structured Outputs功能。\n其旨在保证模型始终生成符合所提供的JSON Schema的响应，而无需处理edge case，以及json内容错误。\n其优势在于：\n无需验证或重试格式不正确的响应\n可通过编程检测模型是否安全生成（在API返回结果中增加了\u0026quot;refusal”字段）\n无需复杂的prompt用于限制输出结构\n相较于Json-Mode， 虽然两者都确保生成有效的JSON，但只有Structured Outputs能确保输出json-schema的一致性。因此理论上无需额外的json-schema检查的代码步骤。\n如何使用 以官方文档所提供的`Structured data extraction`为例：\nfrom pydantic import BaseModel from openai import OpenAI client = OpenAI() class ResearchPaperExtraction(BaseModel): title: str authors: list[str] abstract: str keywords: list[str] completion = client.beta.chat.completions.parse( model=\u0026#34;gpt-4o-2024-08-06\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;...\u0026#34;} ], response_format=ResearchPaperExtraction, ) research_paper = completion.choices[0].message.parsed output { \u0026#34;title\u0026#34;: \u0026#34;Application of Quantum Algorithms in Interstellar Navigation: A New Frontier\u0026#34;, \u0026#34;authors\u0026#34;: [ \u0026#34;Dr. Stella Voyager\u0026#34;, \u0026#34;Dr. Nova Star\u0026#34;, \u0026#34;Dr. Lyra Hunter\u0026#34; ], \u0026#34;abstract\u0026#34;: \u0026#34;This paper investigates the utilization of quantum algorithms to improve interstellar navigation systems. By leveraging quantum superposition and entanglement, our proposed navigation system can calculate optimal travel paths through space-time anomalies more efficiently than classical methods. Experimental simulations suggest a significant reduction in travel time and fuel consumption for interstellar missions.\u0026#34;, \u0026#34;keywords\u0026#34;: [ \u0026#34;Quantum algorithms\u0026#34;, \u0026#34;interstellar navigation\u0026#34;, \u0026#34;space-time anomalies\u0026#34;, \u0026#34;quantum superposition\u0026#34;, \u0026#34;quantum entanglement\u0026#34;, \u0026#34;space travel\u0026#34; ] } 和在LLM输出的结果中通过`pydantic`进行格式检查类似，只是直接将格式要求传给了模型，减少了额外工作。 等价于\n... \u0026#34;response_format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;json_schema\u0026#34;, \u0026#34;json_schema\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;research_paper_extraction\u0026#34;, \u0026#34;schema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;authors\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;abstract\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;keywords\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } }, \u0026#34;required\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;authors\u0026#34;, \u0026#34;abstract\u0026#34;, \u0026#34;keywords\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false }, \u0026#34;strict\u0026#34;: true } } 此外， 文档中还提供了COT的Structured Outputs例子\nChain of thought\nfrom pydantic import BaseModel from openai import OpenAI client = OpenAI() class Step(BaseModel): explanation: str output: str class MathReasoning(BaseModel): steps: list[Step] final_answer: str completion = client.beta.chat.completions.parse( model=\u0026#34;gpt-4o-2024-08-06\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful math tutor. Guide the user through the solution step by step.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;how can I solve 8x + 7 = -23\u0026#34;} ], response_format=MathReasoning, ) math_reasoning = completion.choices[0].message.parsed output:\n{ \u0026#34;steps\u0026#34;: [ { \u0026#34;explanation\u0026#34;: \u0026#34;Start with the equation 8x + 7 = -23.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;8x + 7 = -23\u0026#34; }, { \u0026#34;explanation\u0026#34;: \u0026#34;Subtract 7 from both sides to isolate the term with the variable.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;8x = -23 - 7\u0026#34; }, { \u0026#34;explanation\u0026#34;: \u0026#34;Simplify the right side of the equation.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;8x = -30\u0026#34; }, { \u0026#34;explanation\u0026#34;: \u0026#34;Divide both sides by 8 to solve for x.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;x = -30 / 8\u0026#34; }, { \u0026#34;explanation\u0026#34;: \u0026#34;Simplify the fraction.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;x = -15 / 4\u0026#34; } ], \u0026#34;final_answer\u0026#34;: \u0026#34;x = -15 / 4\u0026#34; } 相比于之前普通版本的COT：\n在测试阶段，程序化检查中间Chain的中间过程变得更加简单\n更优美的提取最终结果\n限制分析过程中的每一步格式，以提高正确率？\nOpenAI API中提供了两种形式的Structured Outputs\nFunction Calling\n使用 json_schema 响应格式\n在function calling （tool-using）时， 在定义tools时需增加\u0026quot;strict\u0026quot;: true 字段，如：\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_delivery_date\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the delivery date for a customer\u0026#39;s order. Call this whenever you need to know the delivery date, for example when a customer asks \u0026#39;Where is my package\u0026#39;\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;order_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The customer\u0026#39;s order ID.\u0026#34;, }, }, \u0026#34;required\u0026#34;: [\u0026#34;order_id\u0026#34;], \u0026#34;additionalProperties\u0026#34;: False, }, }, \u0026#34;strict\u0026#34;: True, } ] 注意：\u0026quot;strict\u0026quot;: true,会产生一定的延迟（10s以下）\nStructured Outputs功能引入了一个新的字段refusal，用于在模型拒绝执行请求时提供反馈。当用户生成的输入时，模型可能会因为安全原因拒绝履行请求，此时API响应会包含一个名为“refusal”的新字段来指示模型拒绝了请求。当API响应中出现refusal字段时，开发者可以程序化地检测到模型生成了拒绝而不是符合模式的输出。\nConstrained Decoding What Is Constrained Decoding constrained decoding is a technique that manipulates a generative model\u0026rsquo;s token generation process to constrain its next-token predictions to only tokens that do not violate the required output structure.\n仅对严格需要生成的输出部分，从模型的下一个Token概率分布中的兼容Token集进行采样。\n简单来说，对于LLM的输出过程，模型会根据之前的词语序列，计算出下一个词可能出现的概率，并根据这个概率生成下一个词。这一过程会不断重复，直到生成完整的句子或段落（或遇到[EOS]标记）。在普通的生成过程中，模型的概率空间是所有可能的Token。\n相对而言，在引导生成（Guided Generation）过程中，模型的输出会受到额外的引导或约束，以确保生成的内容更加符合某些特定的要求或目标。这种引导可以通过调整概率分布、加入上下文信息、或施加额外规则来实现。\n具体来说， 对于给定的输入序列 $\\tilde{S}_t$ 和模型参数 $\\theta$，\n模型首先计算出一个未加约束的概率分布 $\\alpha$，表示下一个Token $\\tilde{s}_{t+1}$ 的可能性 ，\n$$ \\alpha = \\text{LM}(\\tilde{S}_t, \\theta) $$\n为了引入约束或限制，构造一个布尔掩码 $m$，这个掩码将限制下一个Token的选择范围。掩码函数 $m(\\tilde{S}_t)$将原始分布中的部分不符合约束条件的Token概率置为0，形成一个新的受限分布 $\\tilde{\\alpha}$：\n$$ \\tilde{\\alpha} = m(\\tilde{S}_t) \\odot \\alpha $$\n即在原始分布$\\alpha$ 上应用掩码 $m$，只保留那些满足约束条件的Token的概率，其他Token的概率被置为0。\n接下来，模型根据受限分布 $\\tilde{\\alpha}$ 进行采样，选择下一个Token $\\tilde{s}_{t+1}$\n$$ \\tilde{s}_{t+1} \\sim \\text{Categorical}(\\tilde{\\alpha}) $$\n这个步骤意味着模型只会从符合约束条件的Token集合中选择下一个Token，确保生成的输出遵循预定的规则或格式。\n以上内容出自 outlines的paper：Efficient Guided Generation for Large Language Model 具体内容可自行查看。\n以常见的Json格式约束为例（Fast JSON Decoding for Local LLMs with Compressed Finite State Machine），\n有多种框架可以利用本地模型实现约束解码， 如： outlines，SGLang等。\nvllm+outlines 实现结构化输出 依赖环境：\n! pip install vllm ! pip install openai ! pip install instructor 注意：需要使用到GPU，建议使用colab\n启动vllm模型服务，选择适合GPU显存大小的模型：\n#! vllm serve Qwen/Qwen1.5-4B-Chat --max_model_len=10000 ! vllm serve Qwen/Qwen1.5-1.8B --max_model_len=10000 引导生成json结构：\nfrom openai import OpenAI # Set OpenAI\u0026#39;s API key and API base to use vLLM\u0026#39;s API server. openai_api_key = \u0026#34;EMPTY\u0026#34; openai_api_base = \u0026#34;http://localhost:8000/v1\u0026#34; api_client = OpenAI(api_key=\u0026#39;YOUR_API_KEY\u0026#39;, base_url=\u0026#39;http://localhost:8000/v1\u0026#39;) model_name = api_client.models.list().data[0].id json_schema = { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;thought\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, \u0026#34;answer\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;thought\u0026#34;, \u0026#34;answer\u0026#34;] } query = \u0026#34;What is the capital of China?\u0026#34; # Feed the random question into the existing query system_prompt = \u0026#34;Respond only with a json object containing the following fields and nothing else: thought, answer.\u0026#34; completion = api_client.chat.completions.create( model=model_name, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_prompt}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query} ], extra_body={ \u0026#34;stop_token_ids\u0026#34;: [128009], \u0026#34;response_format\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34;}, \u0026#34;guided_json\u0026#34;: json_schema } ) 输出：\nprint(completion.choices[0].message.content.strip()) # {\u0026#34;thought\u0026#34;: \u0026#34;What is the capital of China?\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;Beijing\u0026#34;} 另一个例子， 引导生成符合结构的《Harry Potter》角色属性：\n定义角色属性结构\nfrom openai import OpenAI from pydantic import BaseModel, Field import base64 import instructor import time from enum import Enum from typing import List from glob import glob class Property(BaseModel): key: str = Field(description=\u0026#34;Must be snake case\u0026#34;) value: str class House(Enum): Griffindor = \u0026#34;gryffindor\u0026#34; Hufflepuff = \u0026#34;hufflepuff\u0026#34; Ravenclaw = \u0026#34;ravenclaw\u0026#34; Slytherin = \u0026#34;slytherin\u0026#34; Targaryen = \u0026#34;targaryen\u0026#34; # This schema is what guides generation class Character(BaseModel): name: str age: int house: House properties: List[Property] api_client = OpenAI(api_key=\u0026#39;YOUR_API_KEY\u0026#39;, base_url=\u0026#39;http://localhost:8000/v1\u0026#39;) client = instructor.from_openai(api_client, mode=instructor.Mode.MD_JSON) model_name = api_client.models.list().data[0].id system_prompt = \u0026#34;\u0026#34;\u0026#34;\\ You are a highly creative assistant tasked with generating fictional characters for a fantasy novel. Each character belongs to a specific house, and their properties should reflect their house\u0026#39;s traits. However, ensure that the characters are entirely original and do not directly reference or resemble any characters, events, or settings from the Harry Potter series. Avoid using names, events, or any elements that would make the character too similar to existing Harry Potter content. The goal is to create unique and novel characters that can stand on their own in a fantasy world. \u0026#34;\u0026#34;\u0026#34; def run(name): try: response = client.chat.completions.create( model=model_name, response_model=Character, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: system_prompt }, { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: name } ], temperature=0.1, # Adjust temperature for creativity ) return response except Exception as e: print(f\u0026#34;An error occurred: {e}\u0026#34;) return None # Example usage character_name = \u0026#34;Voldemort\u0026#34; character = run(character_name) if character: print(character) else: print(\u0026#34;Failed to generate character.\u0026#34;) # name=\u0026#39;Voldemort\u0026#39; age=50 house=\u0026lt;House.Slytherin: \u0026#39;slytherin\u0026#39;\u0026gt; properties=[Property(key=\u0026#39;power\u0026#39;, value=\u0026#39;dark magic\u0026#39;), Property(key=\u0026#39;reputation\u0026#39;, value=\u0026#39;evil\u0026#39;), Property(key=\u0026#39;history\u0026#39;, value=\u0026#39;murdered his parents and became the Dark Lord\u0026#39;)] Prompt-based Methods 根据奥卡姆剃刀法制，“如无必要，勿增实体”， 或许使用prompt也可以实现， 避免引入不必要的复杂性。\n使用Prompt Engineering 要求LLM输出符合格式要求的json（或者其他）时， 可能可以用的一些小Tips：\n明确的指令和提示，并提供具体的例子: 往往LLM的few-shot learning 的能力还行， 模仿个简单的json格式基本不在话下。\n使用占位符： 为具体的json元素标注数据类型和格式，便于LLM理解。这一步骤类似于给json定义一个schema。如\n{ \u0026#34;user\u0026#34;: \u0026#34;示例用户\u0026#34;, \u0026#34;id\u0026#34;: 12345, \u0026#34;items\u0026#34;: [ {\u0026#34;item_name\u0026#34;: \u0026#34;示例物品1\u0026#34;, \u0026#34;quantity\u0026#34;: 2}, {\u0026#34;item_name\u0026#34;: \u0026#34;示例物品2\u0026#34;, \u0026#34;quantity\u0026#34;: 1} ] } 而不是一句“将结果输出成json格式”\n调整温度： 采用较低的温度设置\n分步骤生成： 对于复杂的JSON结构，可以要求模型逐步生成每个部分，然后将它们组合起来； 或先生成JSON的高层结构，然后再深入生成每个子部分。\n让LLM自己检查：类似“三段式翻译” 之类的prompt， LLM首先生成第一版json，而后LLM对LLM进行检查，并提出修改建议， 最后基于建议和初版输出生成最终版结果。\n格式有效性检查： 使用如Pydantic等工具进行验证和解析，如果模型生成了不正确的JSON，可以通过循环过程直到输出有效的JSON为止；或使用一些第三方库修复json, 如json_repair。\n这里推荐一个开源项目， TypeChat， 一个由微软开发，用于让LLM输出类型定义的结果。TypeChat 自动处理生成提示、验证 LLM 输出是否符合定义的类型，并在必要时修复不符合的输出。\n其核心是通过prompt的方式，要求LLM按特定要求输出。\n以官方提供的Calendar例子为例：\n其实现功能即，将自然语言转换成json格式， 用于后续的代办事项的工具使用。\n如输入：\n📅\u0026gt; I need to get my tires changed from 12:00 to 2:00 pm on Friday March 15, 2024 则会输出：\n{ \u0026#34;actions\u0026#34;: [ { \u0026#34;actionType\u0026#34;: \u0026#34;add event\u0026#34;, \u0026#34;event\u0026#34;: { \u0026#34;day\u0026#34;: \u0026#34;Friday March 15, 2024\u0026#34;, \u0026#34;timeRange\u0026#34;: { \u0026#34;startTime\u0026#34;: \u0026#34;12:00 pm\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2:00 pm\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;get my tires changed\u0026#34; } } ] } 首先需要事先定义具体且复杂的schema，如 schema.py\nfrom typing_extensions import Literal, NotRequired, TypedDict, Annotated, Doc class UnknownAction(TypedDict): \u0026#34;\u0026#34;\u0026#34; if the user types text that can not easily be understood as a calendar action, this action is used \u0026#34;\u0026#34;\u0026#34; actionType: Literal[\u0026#34;Unknown\u0026#34;] text: Annotated[str, Doc(\u0026#34;text typed by the user that the system did not understand\u0026#34;)] class EventTimeRange(TypedDict, total=False): startTime: str endTime: str duration: str class Event(TypedDict): day: Annotated[str, Doc(\u0026#34;date (example: March 22, 2024) or relative date (example: after EventReference)\u0026#34;)] timeRange: EventTimeRange description: str location: NotRequired[str] participants: NotRequired[Annotated[list[str], Doc(\u0026#34;a list of people or named groups like \u0026#39;team\u0026#39;\u0026#34;)]] class EventReference(TypedDict, total=False): \u0026#34;\u0026#34;\u0026#34; properties used by the requester in referring to an event these properties are only specified if given directly by the requester \u0026#34;\u0026#34;\u0026#34; day: Annotated[str, Doc(\u0026#34;date (example: March 22, 2024) or relative date (example: after EventReference)\u0026#34;)] dayRange: Annotated[str, Doc(\u0026#34;(examples: this month, this week, in the next two days)\u0026#34;)] timeRange: EventTimeRange description: str location: str participants: list[str] class FindEventsAction(TypedDict): actionType: Literal[\u0026#34;find events\u0026#34;] eventReference: Annotated[EventReference, Doc(\u0026#34;one or more event properties to use to search for matching events\u0026#34;)] class ChangeDescriptionAction(TypedDict): actionType: Literal[\u0026#34;change description\u0026#34;] eventReference: NotRequired[Annotated[EventReference, Doc(\u0026#34;event to be changed\u0026#34;)]] description: Annotated[str, Doc(\u0026#34;new description for the event\u0026#34;)] class ChangeTimeRangeAction(TypedDict): actionType: Literal[\u0026#34;change time range\u0026#34;] eventReference: NotRequired[Annotated[EventReference, Doc(\u0026#34;event to be changed\u0026#34;)]] timeRange: Annotated[EventTimeRange, Doc(\u0026#34;new time range for the event\u0026#34;)] class AddParticipantsAction(TypedDict): actionType: Literal[\u0026#34;add participants\u0026#34;] eventReference: NotRequired[ Annotated[EventReference, Doc(\u0026#34;event to be augmented; if not specified assume last event discussed\u0026#34;)] ] participants: NotRequired[Annotated[list[str], \u0026#34;new participants (one or more)\u0026#34;]] class RemoveEventAction(TypedDict): actionType: Literal[\u0026#34;remove event\u0026#34;] eventReference: EventReference class AddEventAction(TypedDict): actionType: Literal[\u0026#34;add event\u0026#34;] event: Event Actions = ( AddEventAction | RemoveEventAction | AddParticipantsAction | ChangeTimeRangeAction | ChangeDescriptionAction | FindEventsAction | UnknownAction ) class CalendarActions(TypedDict): actions: list[Actions] 这些类表示各种日历操作，每个操作类都有一个actionType字段，确定操作类型：\n• FindEventsAction：查找事件，包含一个eventReference字段，用于指定要查找的事件属性。\n• ChangeDescriptionAction：更改事件描述，包含eventReference（可选）和新的description字段。\n• ChangeTimeRangeAction：更改事件的时间范围，包含eventReference（可选）和新的timeRange字段。\n• AddParticipantsAction：为事件添加参与者，包含eventReference（可选）和参与者列表participants（可选）。\n• RemoveEventAction：删除事件，包含一个eventReference字段。\n• AddEventAction：添加新事件，包含一个event字段。\n在TypeChat的核心 prompt如下：\nprompt = f\u0026#34;\u0026#34;\u0026#34; You are a service that translates user requests into JSON objects of type \u0026#34;{self._type_name}\u0026#34; according to the following TypeScript definitions: ``` {self._schema_str} ``` The following is a user request: \u0026#39;\u0026#39;\u0026#39; {intent} \u0026#39;\u0026#39;\u0026#39; The following is the user request translated into a JSON object with 2 spaces of indentation and no properties with the value undefined: \u0026#34;\u0026#34;\u0026#34; 以上面的例子，最终生成的prompt：\n\u0026#34;\u0026#34;\u0026#34; You are a service that translates user requests into JSON objects of type \u0026#34;CalendarActions\u0026#34; according to the following TypeScript definitions: ``` interface CalendarActions { actions: Array\u0026lt; | AddEventAction | RemoveEventAction | AddParticipantsAction | ChangeTimeRangeAction | ChangeDescriptionAction | FindEventsAction | UnknownAction \u0026gt;; } // if the user types text that can not easily be understood as a calendar action, this action is used interface UnknownAction { actionType: \u0026#34;Unknown\u0026#34;; // text typed by the user that the system did not understand text: string; } interface FindEventsAction { actionType: \u0026#34;find events\u0026#34;; // one or more event properties to use to search for matching events eventReference: EventReference; } // properties used by the requester in referring to an event // these properties are only specified if given directly by the requester interface EventReference { // date (example: March 22, 2024) or relative date (example: after EventReference) day?: string; // (examples: this month, this week, in the next two days) dayRange?: string; timeRange?: EventTimeRange; description?: string; location?: string; participants?: string[]; } interface EventTimeRange { startTime?: string; endTime?: string; duration?: string; } interface ChangeDescriptionAction { actionType: \u0026#34;change description\u0026#34;; // event to be changed eventReference?: EventReference; // new description for the event description: string; } interface ChangeTimeRangeAction { actionType: \u0026#34;change time range\u0026#34;; // event to be changed eventReference?: EventReference; // new time range for the event timeRange: EventTimeRange; } interface AddParticipantsAction { actionType: \u0026#34;add participants\u0026#34;; // event to be augmented; if not specified assume last event discussed eventReference?: EventReference; // new participants (one or more) participants?: string[]; } interface RemoveEventAction { actionType: \u0026#34;remove event\u0026#34;; eventReference: EventReference; } interface AddEventAction { actionType: \u0026#34;add event\u0026#34;; event: Event; } interface Event { // date (example: March 22, 2024) or relative date (example: after EventReference) day: string; timeRange: EventTimeRange; description: string; location?: string; // a list of people or named groups like \u0026#39;team\u0026#39; participants?: string[]; } ``` The following is a user request: I need to get my tires changed from 12:00 to 2:00 pm on Friday March 15, 2024 The following is the user request translated into a JSON object with 2 spaces of indentation and no properties with the value undefined: \u0026#34;\u0026#34;\u0026#34; 参考 A Guide to Structured Generation Using Constrained Decoding\n在 🤗 Transformers 中使用约束波束搜索引导文本生成\nStructured Outputs\nIntroduction to Structured Outputs\nFunction calling\nStructured Outputs for Multi-Agent Systems\nAdd guided decoding for OpenAI API server\noutlines: Serve with vLLM\n如何正确约束输出格式，Deepmind最新：让LLM精准生成任何格式的文本，COLM2024\nCoalescence: making LLM inference 5x faster\nEfficient Guided Generation for Large Language Models\nFast JSON Decoding for Local LLMs with Compressed Finite State Machine\n通过Typechat控制LLM的输出\n大模型 JSON 格式化输出小技巧\nFormatron\nOutlines 〰️\nSGLang\nTypeChat\n","permalink":"https://niraya666.github.io/posts/llm-%E8%BE%93%E5%87%BA%E9%99%90%E5%88%B6structured-outputs%E5%8F%97%E9%99%90%E7%BC%96%E7%A0%81%E5%92%8C%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B/","summary":"\u003cp\u003e在使用大型语言模型（LLM）时，我们常常面临一个挑战：如何从模型输出中准确提取自己所需的信息。例如，当我们希望模型输出 JSON 格式的数据时，由于模型生成的内容并不总是稳定，可能需要额外编写大量的正则表达式来匹配并提取其中的有效信息。然而，由于 LLM 的能力，导致其输出结构并不永远可靠。\u003c/p\u003e\n\u003cp\u003e现阶段， 让LLM按要求生成特定格式文本的主要方法有几种种：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e微调：使模型的输出遵循特定格式\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAI Json-mode/Structured Outputs/function-calling:\u003c/strong\u003e 这些功能允许模型生成更严格、结构化的输出，但受限于openAI平台。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e格式约束\u003c/strong\u003e：在decoding阶段进行约束，限制模型的输出，\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePrompt Engineering\u003c/strong\u003e： 最简单的办法，但不稳定。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e多阶段prompting： 通过多个步骤的提示逐步引导模型生成所需的格式。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e本文将聚焦在Structured Outputs， 受限编码， 和prompt-engineering的角度，探讨它们在生成特定格式文本中的应用和效果。\u003c/p\u003e\n\u003ch2 id=\"json-mode\"\u003eJson Mode\u003c/h2\u003e\n\u003cp\u003e仅特定模型和平台支持\u003c/p\u003e\n\u003cp\u003e以openAI 为例， 在\u003ccode\u003eopenai.chat.completions.create\u003c/code\u003e 参数中增加\u003ccode\u003eresponse_format={\u0026quot;type\u0026quot;:\u0026quot;json_object\u0026quot;}\u003c/code\u003e 即可（具体参见：\u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format\"\u003eresponse_format\u003c/a\u003e ）。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e需要在prompt中要求输出json格式\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e不能保证完全按要求的格式结构输出\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e但非100%成功率，存在一些需要额外检测和适当处理的edge case。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdetails\u003e\n  \u003csummary\u003eHandling edge cases\u003c/summary\u003e\n  \u003cdetails\u003e\n    \u003csummary\u003e根据OpenAI官方文档提供的处理方案\u003c/summary\u003e\n    https://platform.openai.com/docs/guides/structured-outputs/json-mode\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"n\"\u003ewe_did_not_specify_stop_tokens\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecompletions\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;gpt-3.5-turbo-0125\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;You are a helpful assistant designed to output JSON.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Who won the world series in 2020? Please respond in the format {winner: ...}\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"p\"\u003e],\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"n\"\u003eresponse_format\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;json_object\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"c1\"\u003e# Check if the conversation was too long for the context window, resulting in incomplete JSON \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efinish_reason\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;length\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# your code should handle this error case\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"c1\"\u003e# Check if the OpenAI safety system refused the request and generated a refusal instead\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;refusal\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# your code should handle this error case\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e][\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;refusal\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"c1\"\u003e# Check if the model\u0026#39;s output included restricted content, so the generation of JSON was halted and may be partial\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efinish_reason\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content_filter\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# your code should handle this error case\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efinish_reason\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;stop\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a \u0026#34;stop token\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ewe_did_not_specify_stop_tokens\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"c1\"\u003e# If you didn\u0026#39;t specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"c1\"\u003e# This is guaranteed to parse successfully and should now contain  \u0026#34;{\u0026#34;winner\u0026#34;: \u0026#34;Los Angeles Dodgers\u0026#34;}\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"c1\"\u003e# Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"k\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"c1\"\u003e# Your code should handle errors here, for example a network error calling the API\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e  \u003c/details\u003e\n  \u003cdetails\u003e\n    \u003csummary\u003e使用pydantic的方案\u003c/summary\u003e\n    使用pydantic的方案\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003epydantic\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eBaseModel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eEmailStr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eValidationError\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"c1\"\u003e# 定义你期望的 JSON 数据模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eUserModel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eBaseModel\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"n\"\u003eage\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"n\"\u003eemail\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eEmailStr\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"c1\"\u003e# 检查 JSON 是否符合模型的函数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003evalidate_json\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ejson_str\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# 将输入的 JSON 字符串转换为 UserModel 实例\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"n\"\u003euser\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eUserModel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparse_raw\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ejson_str\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# 如果验证通过，返回字典\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"n\"\u003eValidationError\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eve\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;JSON validation error: \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eve\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"c1\"\u003e# 示例用法\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"n\"\u003ejson_str\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;}\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"n\"\u003evalidated_json\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalidate_json\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ejson_str\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003evalidated_json\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;JSON is valid and conforms to the schema:\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalidated_json\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;JSON is invalid.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e  \u003c/details\u003e\n\u003c/details\u003e\n\u003cp\u003eJson-Mode 更多是对于输出json的格式进行检查(即Json格式的有效性)\u003c/p\u003e","title":"LLM 输出限制：Structured Outputs、受限编码和提示词工程"},{"content":"摘要 本文探讨了“黑暗之魂”系列游戏在当代日本社会中的意义定位。我认为，这款游戏可以被看作是当今文化潮流的象征，就像有人可能会把杰克·凯鲁亚克的《达摩流浪者》视为60年代反文化的象征一样。我主张，“黑暗之魂”系列通过寓言的方式表达了人们在一个日常行为和社会本身的意义都变得极不稳定的时代中生活的焦虑。游戏采用碎片化的叙事方式，融合了佛教的形而上学思想，并以描绘一个曾经辉煌的王国在最后时刻挣扎的阴森哥特式美学为背景。这种对当代社会焦虑的表达，与日本的后现代主义话语密切相关。通过将这些游戏视为文本、环境和游戏系统之间的反馈回路，我把这些游戏的主要概念主题与大泽真幸提出的“后虚构时代”概念，以及东浩纪对御宅族的定义联系起来。\n前言 在人文学科这个领域，我们认为艺术和文学不仅仅是艺术鉴赏。当我们阅读夏目漱石1或三岛由纪夫2的作品时，教授和学生不仅仅像粉丝那样陶醉于他们优美的文笔，而是作为评论家，从中挑出深刻的思想。通过这种方式阅读足够多的作品，我们会发现一种更宏大的东西：一个跨越时代和文化的思想洪流，在这个洪流中，生活的神秘、美丽、恐怖和悲剧汇聚成我们人类经验的漩涡和流动。文学是指南，就像老话说的那样，是通向美好生活的指南：它是一面镜子，反映我们的位置，也是我们漫步时的地图。简而言之，伟大的作品让我们变得更强大。它们让我们清晰地面对生活的矛盾，并有勇气承受其重压。我们学习文学是为了找到人类经验中的共性，并将其内化。这至少是人文学科的理想和创立原则。\n但如果这是正确的，我们能说这种深刻的体验只是文学独有的吗？书面文字是否有某种独特的力量，能够以其他媒介无法企及的方式激发反思？最终，文学只是一个个故事，但故事并不局限于一种媒介。故事是跨媒介的。它们可以口头讲述，可以印刷，可以通过舞蹈、图像，甚至可能通过尚未构想的方式来讲述。这是因为故事首先存在于人类心灵的深层结构中，不同的媒介让这些结构的不同方面显现出来。故事是人类生活的核心，它们赋予我们的社会和个人生活以连贯性和意义。\n故事是个人身份的基石，人类将经验转化为故事的能力可能与我们感知时间的能力有关。毕竟，故事是什么？它是以有意义的方式组织和编辑时间。简而言之，通过研究故事，特别是那些在社会想象中占据重要地位的故事，我们可以进入人类心灵的最深处。当我们接近这一神秘领域时，我们更接近自己，也许会找到一些我们遗忘的，对我们至关重要的东西，它像火药桶上的火花一样在我们脚下点燃，在这场大火中我们的世界观重生。\n新的媒介在大学里常常面临艰难，因为每种媒介都必须越过高文化裁决者的障碍3。这在二十世纪初的电影和摄影上是如此，在今天的漫画、电视和视频游戏上也是如此。但如果我们仅因为这些作品属于错误的媒介而忽视了沟口健二4的作品，人文学科会变得多么贫乏？偏重研究一种媒介而忽视其他媒介有什么好处？这种偏见是最糟糕的，我们都因这种武断的判断而变得贫乏。我们切断了自己探索那些激发我们深层结构的新视角的机会。故事在新的媒介中同样存在，此外，对那些在这些新媒介中成长的人来说，发现其中的故事是引人入胜、动人且深具意义的。视频游戏中的故事体验与其他媒介有着根本的不同，正是这种不同使得研究它们变得如此重要。它反映了一种新的世界观和价值观。我们不再生活在一个技术只是工具的世界里；技术是我们生活的环境。电影和电视是工业时代的本土媒介，而视频游戏是数字时代的本土媒介。视频游戏不是小众的。这个行业在全球产生的收入大大超过了电影和印刷品。这本身就使得研究它们成为紧迫的事情。\n话虽如此，我们必须认识到，视频游戏是一种新兴的媒介。就像早期的电影只是基本现象的再现——火车向屏幕疾驰或马在慢动作中奔跑——视频游戏的媒介中无疑有着尚未开发的巨大潜力。此外，我们还不能像理解文学那样全面理解这一媒介。我们还没有足够的时间去理解视频游戏在文化中可能扮演的角色以及它们对塑造我们的价值观和性格的影响。我不会在这里主张创建一个新的经典目录。现在还为时过早，而且流行经典化的过程早已在进行中。本文研究的游戏《黑暗之魂》系列已经被这样经典化了。它享有一种地位，其他同类游戏以它为标准，而那些共享其设计原则的游戏被称为“魂类游戏（Soul-like）”，这可能类似于许多文学或电影作品被描述为“卡夫卡式的”5。然而，我不会试图将这款游戏作为一部伟大作品来评估，以便将研究这一系列游戏制度化。我认为这些游戏被经典化不仅仅是因为它们“有趣”，而是因为它们直接处理了当代日本及其年轻一代面临的问题、焦虑和挑战。这是我们将要问的问题：这些游戏中反映了什么，使其在跨文化范围内以及在粉丝和评论家中产生如此深刻的共鸣？\n我将在这里主要讨论日本，但我相信在研究这款游戏时得出的教训具有全球意义。目前，这些意义将保持原样。本质上，为了故事而玩游戏是一件矛盾的事情。游戏的内部组件，作为一个具象化的代码系统和作为不同类型叙事（文本、视觉、听觉）的载体，对玩家提出了一系列相互竞争的需求。这种矛盾在《黑暗之魂》中和几乎所有其他游戏中一样没有解决，但该系列在这些相互竞争的元素之间达到了罕见的一致性，可能直接反映了日常生活中数字世界的体验。我希望我在这里的分析，即使只是初步和不完整的，也能成为反思社会走向、新的价值结构的出现以及文学解释或在这种情况下的游戏解释可以对这些问题有所启示的机会。\n第一章：引言 本文旨在探讨“黑暗之魂”系列游戏与当代日本社会状况之间的关系。我认为，“黑暗之魂”系列通过寓言形式表达了在一个日常行为意义甚至社会本身都变得不稳定的时代生活的焦虑。这一表达通过破碎的叙事方式实现，这种方式结合了佛教形而上学，并以描绘一个昔日伟大王国最后挣扎的阴暗哥特风格为背景。这种寓言中的焦虑与日本后现代性讨论密切相关，特别是现代性项目失败和进步叙事腐败的观点。在我的论文中，我认为这种讨论在“黑暗之魂”中通过模拟经典英雄之旅得以体现，但最终通过剥夺玩家的英雄成就感并用深深的模糊和不确定性取而代之，颠覆了这一旅程。\n“黑暗之魂”系列是一款第三人称动作RPG，其核心机制围绕探索一个破败的世界展开。该系列由著名日本游戏公司FromSoftware在总裁宫崎英高的指导下开发。第一款游戏于2011年9月发布，第三款于2016年4月发布。这是一款难度极高的游戏，设定在一个末世奇幻世界，要求玩家克服许多复杂的运动任务。游戏的一大特点是其缺乏具体的叙事阐述。然而，有一些概念性主题将这些游戏联系在一起。游戏中总是有一个被“不死诅咒”摧毁的王国，并且总是有神秘人物指示玩家杀死旧神并使用他们的灵魂来“连接火焰”。这个过程在第一款游戏中被解读为一个世界复兴过程，但在第二和第三款游戏中被揭示为不可避免的衰败螺旋。\n全球销量数百万，三部作品在国外几乎普遍获得好评，赢得了众多“年度游戏”称号和大量其他行业奖项。它对当代游戏设计产生了深远的影响，许多采用类似设计的游戏经常被称为“魂类”或“魂风格”。这已成为一种陈词滥调，类似于带有超现实或存在主义色彩的文学作品被描述为“卡夫卡式”。我认为其地位的原因并非游戏的“有趣”，而是它有一些关于我们今天所处世界的强有力表达，这些表达与那些对现代性的后现代批评观点一致的人产生共鸣。\n本文的核心论点是，“黑暗之魂”系列体现了具有后现代意识的生活体验。这个术语“后现代意识”是我用来总结后现代思想的一个总称。然而，我特别关注日本分支，引用了社会学家大泽真幸6、文化评论家东浩纪7、文学学者田中真纪子8和小说家村上春树9的著作。从弗朗索瓦·利奥塔10（Francois Lyotard）开始——他首次提出“后现代”一词——后现代思想的不同分支的核心主张是，曾经用来指导社会的“大叙事”已经无可救药地破裂并且无法修复。大泽和其他思想家拾起了这条思路，并将其应用到具体的日本背景。他们认为，日本战后时期的大叙事的破裂在年轻一代中引发了虚无主义行为，这种行为表现为从政治原因和社区参与转向过度消费主义和一种表现为御宅族形象的媒体崇拜。在一些极端情况下，这种御宅族主义可能导致病态行为，如奥姆真理教11的恐怖主义，但需要注意的是，这些思想家将这种暴力行为视为御宅族行为连续体的最外端，而不是在类别上有所不同。从大泽和东浩纪等后现代思想家那里，我将后现代意识提炼为两个基本特征：1. 拒绝或无法接受宏大叙事和意识形态作为意义的来源 2. 对“他者”的日益焦虑和敌意。在这种情况下，他者指的是主体与他者之间的经典哲学二元关系；换句话说，就是自我与世界上所有其他自我之间的关系。大叙事的破裂削弱了依赖这些叙事来维系和稳定的社区纽带和同胞感。这种意识是“黑暗之魂”系列设计和叙事的主要当代文化元素。\n然而，后现代意识在日本确实广泛传播，这并不是显而易见的。出于这个原因，我在第一章中追溯了大叙事在日本历史中逐渐瓦解的确切方式，从战后时期开始一直到现在。这使得我们能够看到后现代意识的发展及其在日本社会和“黑暗之魂”系列中的表达。没有这一步骤，就不可能识别贯穿这个系列的后现代意识。\n为了挖掘日本后现代意识的文化根源，我采用了大泽的文化时代精神理论，将战后时期分为三个阶段：意识形态时代（1945 - 1972），虚构时代（1972 - 1995）和后虚构时代（1995 - 现在）。大泽在其作品中提出的基本观点是，意识形态的死亡——本质上是他对“宏大叙事”的术语——刺激了新的形式，这些形式最终使他们脱离了日本战后社会的意识形态基础。从1972年的联合赤军事件12开始，激进的左翼政治让位于非政治的消费享乐主义。这导致了围绕显著消费形式尤其是时尚的亚文化的繁荣。然而，对一些人来说，对更深层意义的渴望仍然存在，这最终催生了新时代的邪教，即奥姆真理教，它们同样从传统宗教和动漫中汲取力量来满足这种需求。\n奥姆真理教的重要性在于他们既标志着传统信仰和意识形态的弱化，也标志着对任何被他们视为“他者”的事物的日益不容忍。他们还标志着一种对远在天边但又近在咫尺的敌对他者的偏执。例如，他们认为日本政府被全球犹太共济会阴谋控制，而他们自己的莲花团体则充满了双重间谍。他们在1995年对这个被认为是犹太人的他者的恐怖袭击，具有讽刺意味的是，传播了这种对奥姆真理教妖魔的偏执，这些妖魔潜伏在学校、企业甚至政府中。这种怪异他者的概念在“黑暗之魂”中得到了有力的表达，因为它向玩家展示了一个充满敌对个体和群体的世界——这些实体是玩家角色的他者——他们的目标和实践是难以理解的。这也通过一种系统得到了加强，即玩家始终连接到互联网，并且开放给其他玩家入侵。\n东浩纪采纳了大泽的时代精神，并提出了一种“动物化”理论。他指出，由于后现代时代缺乏大叙事，人们失去了理想和抱负。因此，他们退缩到一个以收集符号为中心的世界，这些符号被添加到一个巨大的符号数据库中并被重新利用。 他的主要证据是，当今的御宅族不再对叙事感兴趣。相反，他们只是享受识别、重新混合和扩展数据库的组件。他的预测是，御宅族的活动——他们无休止地收集符号——是大文化的先锋，后续几代人将在自己的领域中变得越来越像御宅族。东浩纪所指出的这一流行文化趋势在“黑暗之魂”中体现为深深的模糊和稀疏的叙事。\n那么“黑暗之魂”究竟如何通过游戏玩法表达后现代意识呢？我认为它通过为我们提供一个任务和一个王国来模拟传统的英雄之旅，但剥夺了我们理解这个任务意义的任何连贯方式。游戏提供了一个视觉细节丰富的世界，但完全缺乏具体的阐述，将玩家置于考古学家和法医科学家的角色中。然而，从未有任何可靠的解释；只有更多的线索和片段，有些是矛盾的。这被称为“环境叙事”，它创造了游戏与后现代思想之间最关键的接触点；即它对主叙事的拒绝。有“拯救王国”的建议，但仅仅是建议，主要是通过类型的惯例传达的，而不是叙事。你所做事情的意义及其服务的目的从未完全清楚。\n重要的是要记住，虽然可以从传统的文学解释视角来解释“黑暗之魂”系列的叙事，但当试图将游戏机制解释为寓意时，会遇到许多实际和理论问题。出于这个原因，我在第二章中对当代游戏研究的批判理论进行了文献综述。利用Espen Aarseth13和其他奠定该领域基础的思想家的论点，我认为需要结合“第一人称”现象学方法和更传统的文学分析方法，才能正确评估视频游戏中潜在的潜在意义。这样做是为了建立一种分析方式，通过与游戏系统的斗争，让玩家体验到我上面概述的后现代意识的两个基本特征。我在第三章中将这种现象学方法付诸实践，探索了一种特别有效的文本叙事、环境叙事和游戏玩法的融合，捕捉到游戏作为一个统一体验的本质。\n除了后现代主义，还有另一股影响游戏的力量，那就是佛教末法概念14。这两种影响力在有趣的方式上重叠，并产生了惊人的相似之处。末法在镰仓时代是一个极具影响力的概念，正如我们的时代一样，那是一个深刻的社会转型时期。末法指的是佛教教义失去其效力，人们无法再依靠自己的努力来获得启示。随之而来的是僧侣和寺庙变得不可否认的腐败，人们因此无法离开轮回，被困在一系列日益退化的转世中。我在第四章中分析了末法的哲学核心，并展示了它如何与大泽和东浩纪的后现代话语一致。此外，我认为，只要“黑暗之魂”系列表达了后现代意识，它主要是通过采用这个佛教末世论概念来表达的。后现代思想和末法都表达了一种恐惧，即事情只会变得更糟，人类的努力无法改变结果。\n在第五章中，我回到现象学和文学分析的混合方法，展示末法概念以及佛教形而上学的混合如何作为构建游戏虚拟世界的核心要素，尽管游戏的美学显然源于中世纪的欧洲奇幻调色板。这些佛教元素共同揭示了游戏世界被困在一个日益退化的轮回中，玩家的任务基本上是一种徒劳的行为，只会推动这个痛苦的循环。我认为佛教形而上学的融合最终表达了东浩纪的结论，即后现代世界及其死去的叙事和怪异他者无可避免地导致社会整体的“动物化”。尽管“黑暗之魂”是一款美学显然源于中世纪欧洲奇幻风格的游戏，但它充满了末法的逻辑，因此游戏深受异教影响，通过采用和改造佛教形而上学，在阴暗的骑士在破败的城堡中冒险的表面下表达了一种后现代情感。\n衰败叙事 过去几十年，日本几乎成了停滞的代名词。这在很大程度上归因于日本在1970年代到1980年代经历的“奇迹”，以及他们令人震惊的衰落，自那以后，国家似乎一直处于无所事事的状态。在美国，当《纽约时报》或《华尔街日报》等出版物提到日本时，往往很少有积极的报道，而是集中在一小部分话题：人口老龄化，年轻人自杀；货币贬值，索尼、丰田和东芝等企业巨头被更精简的竞争对手击败；核燃料仍在福岛渗入地下水，但无人承担责任，等等，每一个预言都比前一个更严峻。在西方，日本作为一个正在衰败的帝国的叙述已经流传已久。这种衰败的印象如此强烈，似乎只有惯性才能维持一切运作，以至于人们忘记了日本仍然是世界上最富有、最富裕和技术最先进的国家之一，仍然领先于许多西欧国家。这种停滞的叙述也存在于日本。正如现任首相安倍晋三在2012年选举中的口号所示：“日本を取り戻す”（取回日本）。从谁那里取回？取回到哪里？凭借他的“安倍经济学”15议程中的“三支箭”，他将从日本自己的“萎缩心态”中拯救日本（Sharp）。如果说1980年代的代表是那些狂热工作、渴望企业晋升的上班族，那么近年来这种原型被更为无力的角色所取代：草食男子16（sōshoku danshi）、尼特族17（NEET，指不在职或不在学的年轻人）、隐蔽青年18（hikikomori，指社会隐士）和其他贬损的称呼。诚然，日本的经济和文化问题与其他任何国家一样真实存在，但这种叙述的力量已经超越了现实，成为一种独立的存在。\n这种“萎缩”的心态和随之而来的文化表达，我将称之为后现代意识。这种意识必须与正统的后现代哲学区分开来，因为哲学本身只是对围绕日本的整体衰败叙述的众多情感反应之一。\n后现代意识的核心是一种源于对人类社会已经达到不可否认的衰退点的感知——无论是有意识还是无意识的——的焦虑。鉴于上面概述的围绕日本经济和文化衰退的广泛文化讨论，后现代意识在日本文化心理中占据了中心地位。因此，这些源自后现代意识的停滞叙述在日本的流行媒体中也找到了强大而多样的表达。虽然有许多例子可以引用，但游戏《黑暗之魂》及其所有续集，是这种“萎缩”心态的卓越典范。要理解它如何以这种方式运作，我们不仅要看叙述，还要看《黑暗之魂》相对于同类游戏的存在。《黑暗之魂》属于一种被称为角色扮演游戏（RPG）的游戏类型。RPG通常是一个开放式的幻想世界模拟，玩家可以探索领土、收集宝物和击败怪物。通常，玩家创建一个角色——一个在游戏世界中的化身——并以此角色进行任务，其最终目标通常涉及拯救某物（王国、存在平面、公主等）免于毁灭。另一个RPG的基本特征是成长概念。尽管有许多设计模式和隐喻用于模拟成长过程，通常玩家的化身通过击败怪物获得经验值，并通过积累这些点数“升级”。通过升级过程，玩家解锁能力，他们的化身变得更强大。总的来说，RPG往往是许多文化神话中常见的英雄之旅模式的模拟：年轻、未成型的人必须离开家园的安全区域，面对未知的危险，克服它们，培养他们潜在的能力，并以强大和完全实现的人的身份返回，无论是字面上还是象征性地。\n在其基本设计中，《黑暗之魂》作为RPG是相当传统的。然而，它有许多叙述和游戏设计选择，颠覆了这个几十年旧的类型的许多传统。作为一个叙述，它可以被解读为一种政治和文化的寓言，++直接反映了生活在一个意识形态叙述失去吸引力和连贯性的衰败文明中的经验++。它也是一种新兴新类型的前沿：后末日幻想。在这款游戏中，王国被占领，英雄们都死了或堕落了，没有人可以拯救，而你存在的最大威胁是那些与你在废墟中一起生存的人。社区是不可能的。基本的本体状态是无休止的失败，而“英雄”只是注定要忍受它。甚至死亡也不能免除角色的负担，而你被赋予的任务的意义和目标是模糊和不明确的。叙述几乎不存在。当游戏开始时，玩家被简单地放在一个被怪物和不死者占领的废弃王国中，没有明确的理由说明为何事情会如此，或者他们应该怎么做。仅有的叙述存在于零碎的文本中，只提供诱人但最终不完整或矛盾的关于世界的信息。所有这些叙述元素都对已建立的类型惯例提出了严重挑战，因此将其与衰退的讨论联系起来。\n在游戏设计方面，《黑暗之魂》有两个突出的特点。第一个是游戏被有意设计得异常困难。这实际上是其吸引力的主要来源之一。该系列在西方的广告标语是“准备去死”（Prepare To Die），而在日本，该系列被认为是しにがみ（死亡之神）的主要例子，暗示玩家在与远比玩家创建的化身强大的巨大、恐怖的怪物斗争时必然会经历无数次死亡。另一个特点是，尽管游戏主要设计为单人体验——也是RPG类型的一个既定惯例——玩家始终连接到互联网。这被视频游戏评论家广泛赞誉为该类型的重要创新。在实践中，这创造了一种紧张的局势，玩家总是暴露在其他玩家的“入侵”之中，这些玩家会试图杀死该玩家。这种互动中内置了一个复杂的风险/回报系统，入侵者通过杀死其他玩家可以获得很多，但如果失败则会失去很多。\n实际上，始终连接到互联网使得游戏世界充满了偏执和危险。其他玩家是玩家在游戏中可能面对的最危险和不可预测的怪物。入侵玩家经常会潜伏在一个不合适的时机突然袭击，玩家无法自卫，从而逆转他们不幸的受害者辛苦获得的进展。我们将详细探讨叙述和游戏系统，但这一系列游戏的基本概况引出了一个问题：在一个没有希望拯救王国，并且异常困难常常到不公平地步的游戏中，怎么会有如此大的吸引力？但事实上，这款游戏在日本、欧洲和北美都极具吸引力。这也指出了游戏的设计和叙述如何与在无聊时代生活的年轻一代的经验产生共鸣。\n日本文化领域的停滞叙述需要进一步澄清。我认为《黑暗之魂》可以被解读为对当代日本状况的政治和文化寓言。这是一种后现代的英雄之旅的颠倒：一种被污染的神话碎片的模拟。它直接反映了许多年轻日本人今天试图在庞大、失修的机构中航行时所经历的无聊和徒劳的感觉，这些机构至多对他们的存在漠不关心，至多直接敌对。然而，这种寓言不仅仅是关于生活在1980年代“奇迹”之后的经济问题。它同样涉及生活在一个国家、政治和文化叙述已经失去了大部分意义和连贯性的社会中的社会和心理成本，这些叙述曾经作为一种心理基石——为个人和社区身份提供稳定。\n催化这一恶化的事件起源于1995年，那一年发生了神户地震和沙林毒气袭击。在西方媒体中，关于这些事件的文化或政治意义的讨论通常不多，但在许多日本知识分子中，这些事件似乎标志着现代性的决定性破裂。在著名小说家村上春树的书《地下：东京毒气袭击和日本心理》中，对沙林毒气袭击幸存者的一系列采访，并得出结论，这一事件的后果不可逆转地揭示了日本社会的所有矛盾、缺陷和内在不稳定性：\n“神户地震和1995年1月和3月的东京毒气袭击是日本战后历史上两起最严重的悲剧。毫不夸张地说，这些事件前后的日本意识发生了显著变化。这两次灾难将作为我们生活中的两个里程碑，永远铭刻在我们的心灵中。如此灾难性的事件接连发生是令人震惊的，也是巧合的。然而，当日本的“泡沫经济”破裂，标志着那些泛滥成灾的时代结束时，这些事件带来了对日本国家根本根源的批判性审视。就像这些事件一直在等待伏击我们。两者共有的是压倒性的暴力元素：一个是不可避免的自然灾难，另一个是可以避免的人为灾难。”（村上，237页）\n通过村上等许多知识分子的工作，1995年已成为一个单一且不可改变的地位：通往破碎和功能失调时代的门户。这里分析的主要理论家——东浩纪、大泽真幸和田中基子——都将1995年视为进入反乌托邦现状的起点。这些思想家的语调本身就有一种微妙的后末日色彩，仿佛我们生活在一个无法拯救的被污染时代。这也是衰败讨论的一部分。这些思想家有一种确定的末世论信念，将他们与这一讨论联系起来。\n无论如何，这些灾难比任何经济灾难都更可能是现任首相希望根除的“萎缩”心态的原因。尽管已经过去几十年，1995年的事件作为焦虑和无助感的触发点，在《黑暗之魂》的寓言中找到了源头。经济的持续低迷、老龄化人口、年轻一代的就业前景减少以及福岛等灾难，只是加剧了这些灾难在1995年最初引发的负面情绪。福岛等事件只帮助在日本意识中创造了一种遗产。近年来，右翼极端民族主义政治的重新出现和尼特族及隐蔽青年的社会退缩，形成了对这一相同的文化病态的极端和相反的情感反应。右翼试图通过与其真实和感知的对手（韩国、中国、朝鲜）进行公开的对抗来复兴“大和”帝国的活力，试图通过历史修正主义抹去日本帝国的暴力和道德可憎之处，并试图通过修改宪法（第9条）恢复战争作为国家主权权利。而尼特族和隐蔽青年则将自己沉溺于网络和其他形式的私人数字享乐中。他们没有叙述可供复兴，没有事业可供加入，甚至没有可以参与的兴趣小组。与他人沟通实在是太困难了，在他们看来，也太危险了。\n我的观点不是《黑暗之魂》系列是这些问题的根源，甚至不是它们的贡献，而是它是对在公众意识中循环的深层社会焦虑，特别是对视频游戏作为熟悉媒介的年轻一代的一种特别有力的表达。《黑暗之魂》系列的持续流行，作为一种社会腐化的象征，反映了人们在日常生活中的感觉。但这些焦虑到底是什么，它们如何在《黑暗之魂》中表现出来？另一种问这个问题的方法是，后现代意识的基本要素是什么？定义1995年后时代的有两个主要元素。1. 对构建战后秩序的政治和社会叙述的拒绝；2. 一种对他者的弥漫性焦虑，侵蚀了对社会过程的信任和信心。他者的概念是指自我与世界上遇到的所有其他自我之间的二元关系的一半。这一概念的延伸是文化他者，即一种感觉某些人与自己相似，另一些人则是异类。日本社会通常被认为具有非常同质的国家认同感，彼此之间有相同的文化、种族和相似的价值观。这反过来又创造了强烈的信任和社区感，但这种亲近的逆向效应是，外来者被认为比在美国、英国或法国等文化多样性更大的国家中更为陌生。大泽真幸的论点是，奥姆真理教的恐怖活动将潜在威胁引入了公众。这种恐惧是一种诡异的他者的恐惧，这种他者会突然出现，以一种深刻的方式扰乱你的生活，有时甚至是致命的。社会结构中的这种破裂从未真正修复，并继续受到挑战，直到现在。关于诡异他者的偏执是定义《黑暗之魂》的氛围和叙述的主要特征之一，也是其游戏机制，尤其是其在线组件。\n要理解《黑暗之魂》如何作为一种描述后现代意识的寓言运作，我们必须追踪战后日本社会状况的衰退轨迹。这根本上是一个现代进步叙述——本质上是从西方继承的——首先在1960年代末引发了激烈的意识形态斗争，然后在1980年代变成一种无政治的享乐主义，最后，最终在五个正义的宗教狂热者用雨伞尖端戳破装满沙林毒气的袋子的时刻，被一种微妙的虚无主义彻底摧毁。为此，我们将研究社会学家大泽真幸的工作。大泽花费了相当多的职业生涯试图回答奥姆为什么会做他们所做的事情。在他的书《虚构时代的终结》中，他认为答案涉及意识形态本身的致命缺陷：一种危险而空洞的思维模式，导致在一个自我设计的世界中越来越高的疏离感。\n理性时代 借用社会学家見田宗介19（Mita Munesuke）的观点，大泽真幸将战后时代分为两个不同的阶段。第一个阶段他称为“理想的时代（理想の時代，risō no jidai）”，第二个阶段他称为“虚构的时代（虚構の時代，kyokō no jidai）”。理想的时代始于20世纪60年代，结束于1972年，虚构的时代从1972年开始，一直持续到1995年。大泽将“理想”和“虚构”描述为“可能世界（可能の世界 kanōsei no sekai）”，因为它们都反映了我们对世界的感知以及我们认为世界应该如何。然而，尽管这两个概念在根本层面上是相关的，但它们在几个重要方面是不同的。大泽认为，理想与现实世界密切相关，而虚构则完全脱离现实世界：\n理想是一个在未来有望或期待在现实中实现的可能世界。因此，理想必须存在于现实的因果延伸之上。从这个意义上讲，理想不是一个纯粹的可能世界，而是广义现实世界的一个方面。相反，虚构是一个可以在现实中实现与否无关的可能世界，因此是纯粹的反现实。（大泽，41页）\n理想は、未来において現実に着地することが予期（期待）されているような可能世界である。だから、理想は、現実の因果的な延長上になくてならない。その意味では、理想は、純粋な可能世界ではなく、むしろ広義の現実世界の一局面である。それにたいして、虚構は現実への着地ということについてさしあたって無関連ありうる可能世界であり、それゆえ純粋な反現実である。(Osawa, 41)\n暂且不讨论虚构如何将我们与现实疏离，日本的理想时代很大程度上受到冷战阴影的笼罩。当亚洲大部分地区处于苏联的保护伞下时，日本则在美国的庇护下，因而在意识形态上致力于民主和经济自由主义。在20世纪60年代，日本在这些条件下几乎在各个方面都得到了繁荣：科学、基础设施、经济、文化等。大泽将20世纪60年代称为理想的黄金时代，并指出家用电器是赋予大众理想的物质形式：\n理想时代在大众层面的黄金期是20世纪60年代。当然，这与日本的高速增长期相吻合。在此期间，例如，受到国民绝大多数人广泛需求的家电产品赋予了大众理想物质的表达……经济增长和科学技术的进步并不是因为理想而被接受的，相反，正因为理想作为可能的现实在广泛（全球范围内）被信奉，经济才能够增长，科学和技术才能进步。（大泽，44页）\n理想の時代の、大衆的な水準で黄金期は、1960年代である。もちろん、これは、高度成長期にあたる。この時期、たとえば、国民の圧倒的な大多数によって広範に欲求された家電製品が、大衆的理想に物質的な表現を与えた\u0026hellip;経済成長や科学・技術の進歩があったから理想が抱かれたのではなく、逆に、理想が可能的現実として広く（世界規模で）信憑されたがゆえに、経済が成長することができ、また科学や技術進歩していると感受されたのである。(Osawa, 44)\n乐观和对更美好（物质富足）未来的渴望是这一时代的指路明灯，这种乐观本身就反映了对美国模式的意识形态拥抱，推动了日本从一个被毁灭的国家向一个繁荣的国家的转变。大泽将这种发展归因于对可能世界的执着。在他的著作《拥抱失败》中，约翰·多尔20（John Dower）通过分析战后立即涌现的泛泛女孩21现象，生动地展示了这一拥抱的一个具体表现——这些年轻女性在占领期间主动接近并与美军士兵交往：\n在那段饥饿和匮乏的岁月里，美国人的物质舒适简直令人瞠目结舌。使美国“伟大”的是它如此富有；而对许多人来说，使“民主”具有吸引力的是它显然是通往繁荣之路。在普通人中，没有哪个群体像泛泛女孩那样公然利用征服者的物质财富。她们从美国军用商店获得物品，而在那些贫困的日子里，这些商店真的就像是魔法之地的宝库：不仅装满了基本食品，还有酒和香烟、糖果和美味佳肴，以及性感而颓废的女性物品如口红和尼龙袜。（多尔，136页）\nIn those years of acute hunger and scarcity, the material comforts of the Americans was simply staggering to behold. What made America “great” was that it was so rich; and, for many, what made “democracy” appealing was that it apparently was the way to become prosperous. Among ordinary people, no group tapped the material treasures of conquerors as blatantly as the panpan. They were the recipients of goods from the U.S. military exchange posts that in those impoverished days truly seemed like treasure houses from a magic land: crammed not only with basic foodstuffs, but with liquor and cigarettes, sweets and delicacies, voluptuously decadent feminine things such as lipstick and nylon stockings. (Dower, 136)\n到20世纪60年代后期，美国占领军所展示的意识形态愿景在很大程度上以非常具体的方式兑现了其承诺，普通日本人开始以便利设施、消费电子产品、便利和所有现代化工业国家的装饰形式享受经济发展的成果。大泽认为，这是意识形态在社会中最顺畅地运作的时候。尽管存在意识形态冲突，人们完全接受了作为他们理想“可能世界”的意识形态，并为之努力，无论他们是马克思主义者还是资本主义者。然而，需要注意的是，尽管20世纪60年代有所增长，仍然有许多日本人处于贫困之中。使20世纪60年代成为理想时代的不是相对的繁荣本身，而是对这种繁荣的承诺，以及对战后秩序确实朝着这个方向发展的普遍信念。普通日本人顽强地坚持这种愿景。这是当时的流行意识形态：汽车、收音机、流行音乐、方便获取的食品以及所有在战后立即投降的绝望、痛苦和毁灭中缺失的舒适和物质享受的承诺。\n然而，大泽认为，这种对意识形态的拥抱最终导致了意识形态向他称之为“虚构”的转变。这与意识形态结构本身的内在不稳定性有关，而不一定与日本的意识形态建制的具体情况有关。大泽以最广泛的术语谈论意识形态，但他将驱动意识形态的动机归结为他从佛教借来的一个原则：“貧病争（ひんびょうそう）”22。这个术语可以粗略翻译为“世间的烦恼”，它们被识别为贫穷、疾病和争斗。他主要基于意识形态与虚构的区别，意识形态是面向与貧病争斗争的。换句话说，大多数意识形态都承诺从世间的烦恼中解救出来。这种救赎总是必然在未来，但对其实现的信念保持稳定，并促使在现实世界中采取具体行动。这在大多数世界宗教，特别是忏悔宗教中是如此，但在像资本主义（自由市场=繁荣和自由）和马克思主义（废除财产/阶级=救赎不平等）等世俗概念中也是如此。然而，在理想和貧病争之间的关系中存在一个悖论：\n理想时代是一个社会整体上被疏离到理想中的时代。在这样的时代，最大的痛苦是从理想中被永久疏离。换句话说，在意识形态关系中，人们经历了双重疏离。貧病争代表了一种不可逃避的条件，在这种条件下，实现和接触我们的理想变得极其困难，我们不可避免地永远与之疏离。这种疏离引起的痛苦只能通过对我们理想的重新信仰来解决，只要我们认为其实现的可能性足够高。这只有在我们已经被疏离到意识形态中的情况下才有可能。（大泽，45页）\n理想の時代とは、社会が全体として理想へと疎外されている時代である。このような時代において、最大の不幸とは、理想から永続的に疎外されること、つまり理想との関係において二重に疎外されることである。「貧病争」は、理想への到達・接近が著しく困難になりうるような－理想からの永続的な疎外を余儀なくするような－宿命的な環境条件を代表している。不幸は、－すでに理想への疎外によって条件づけられている以上は－、到達の蓋然性が十分に高い現実性として理想を回復することによってしか、解消されない。(Osawa, 45)\n大泽指出，在苦难和救赎承诺之间存在一个相互强化的解释循环。貧病争，按定义，使人们与各自意识形态的救赎承诺疏离，但这种痛苦只能通过复兴理想来消除。++苦难往往不会导致信仰者失去信仰，而是激励他们按照自己已经信奉的叙事来解释自己的苦难。++使苦难有意义是一种常用于应对我们生活中悲剧性事件的应对机制。矛盾的是，这意味着即使我们离意识形态所承诺的可能世界极其遥远，貧病争也能加强意识形态信仰。当我们对可能世界的信仰非常强烈时，我们会在这种意识形态中“疏离”，只通过其中提供的狭隘解释视角来看待世界。这反映了走向教条主义和正统的驱动力，在这种状态下，貧病争仅作为意识形态引擎的燃料。然而，这个循环的关键部分在某种程度上超出了它；我们必须继续相信可能世界是可能的。大泽的虚构时代描绘了一种信念崩溃的普遍现象，但这种崩溃不应与某种信仰的丧失混淆，例如虔诚的天主教徒变成无神论者或坚定的资本家转变成马克思主义革命者。这只是将一种意识形态换成另一种意识形态。基本结构和取向保持不变。从意识形态到虚构的转变代表了一种新的疏离，一步远离了貧病争和意识形态之间的循环。这是一步远离世界本身。\n大泽举了许多例子，说明在20世纪80年代，日本的意识形态地图如何逐渐脱离大规模叙事（资本主义对抗共产主义），并催生了新人类（新人類 しんじんるい）23、御宅族（オタク）24以及最终的奥姆真理教（Aum Shinrikyō）和1994-1995年的沙林毒气袭击。然而，需要注意的是，在所有这些案例中，无论是伊斯兰教还是马克思主义，意识形态都与当前世界的实际问题密切交织。投射的可能世界正是通过这种对比才可以理解的。正是在与貧病争的这种重要关系中，意识形态与虚构分道扬镳。**对于大泽来说，从意识形态到虚构的转变是由20世纪60年代政治活动后的深刻失望所催化的。**虽然大泽认为20世纪60年代是意识形态运作最顺畅的时期，但这并不意味着那个时代没有意识形态冲突。相反，就政治而言，20世纪60年代末的日本充满了社会动荡。20世纪60年代意识形态运作顺畅的概念并不意味着政治和谐，而是指个人能够完全地、彻底地投身于构成其特定意识形态立场的叙事。\n20世纪60年代标志着许多学生运动，反映了当时日本普遍的政治激活。这些运动大多针对日本政府支持越南战争等问题提出抗议，但间接上也形成了对美国影响日本外交政策的一种抵制。反帝国主义和反美情绪主要由日本共产党刺激。它们创建了全学联合（Zengakuren）25，这是一个在大学生中创造了广泛的左翼文化的组织（羽根，465页）。最终，这些运动演变成了与警察的暴力冲突，抗议者和警察都受到了伤害，有时甚至死亡：\n20世纪60年代中期，东京的一些大学开始出现反对学费上涨的学生示威。总的来说，1968年有115所大学面临学生抗议、示威和骚乱。大规模罢工，设置路障，占领重要建筑。教授们经常被激进学生审讯，被逼供认罪行。（羽根，465页）\n暴力的出现也加剧了最初和平政治议程的混乱。这些抗议由许多不同议程的左翼团体拼凑而成，其中一些比其他更加激进。没有具体或连贯政治需求的暴力最终导致这些学生运动消亡，留下了持久的政治失望感。田中元子（Motoko Tanaka）26在大泽的时代背景基础上，认为安保和全共斗运动的失败是结束理想时代的一个主要因素。全共斗运动27是一个特别持久的事件，形成了一种由其他学生起义效仿的围攻战模式。这场斗争始于东京大学，当时医学生要求废除实习制度。管理层的压迫态度引发了一场全校范围的争议。1968年7月，学生们成立了全共斗，即全学共斗会议的简称。随着斗争的持续，全共斗逐渐激进，开始质疑日本社会的基础。他们占领了整个大学六个月，最终被警察驱逐。田中写道：\n安保和全共斗等政治运动的失败突显了即使在占领结束后，美国对日本政治的持续影响。经历了这些政治运动失败的年轻男性，有意或无意地吸收了应对美国力量的困难和失败的创伤。（田中，46页）\n大泽本人指出，1972年的联合赤军事件（連合赤軍事件）28是从理想时代转向虚构时代的主要转折点。联合赤军是几个激进马克思主义团体合并，意图暴力推翻日本政府。赤军是其中一个宣战日本政府并开始一系列恐怖袭击的团体，包括劫持飞机和袭击特拉维夫的罗德机场，导致26人死亡，80人受伤。政府对该团体进行镇压，逮捕了大多数成员，迫使其余成员与其他团体合并，最终隐藏起来。这个新合并的激进左翼在长野山区建立了一个秘密基地，进行军事训练。在那里发生了对自己队伍的清洗：14名成员因被认为缺乏革命热情而被残忍杀害，有的被绞死，有的被刀割。警察发现了他们的藏身之处并进行逮捕，剩余成员劫持人质， 将自己封锁在一座山间小屋中。这场持续一整天的围攻是日本第一次现场直播的马拉松式电视广播，持续了10小时40分钟。此事件导致了左翼激进政治的急剧衰退。\n当然，意识形态热情的衰退不仅限于左派，也存在于右派。在另一部相关的作品《不可能性的时代》中，大泽将三岛由纪夫未遂的政变视为另一个普遍意识形态崩溃的象征（大泽，13页）。在所有这些案例中，这些意识形态失败的高可见度都表明意识形态本身的某种无力感，其最激进的支持者的过激行为被转化为电视观众眼中的电视节目，令大多数人觉得这些不是高尚革命者的反抗行为，而是堕落的社会病态者的过激行为。取代这种意识形态热情的是大多数人只是享受经济繁荣的成果。\n虚构时代 大泽的虚构时代从1972年延续到1995年，但或许最能捕捉大泽对虚构与意识形态定义的隐喻无疑是“泡沫”这一概念。日本的1980年代通常被称为“泡沫时代”，意指经济的快速崛起及其随后的崩溃。这也传达了一种隔离感。1980年代对日本来说是一个极其繁荣和平的时期，“泡沫”这个比喻传达了普通日本人被困在一种虚假的安全感中的感觉，他们在从战争创伤中完全复苏的社会中享受着温暖、超现代的光芒。然而，这一时期本应被视为对现状充满乐观和信心的时代，却存在一种根本的模糊性。繁荣带来了物质上的丰富，但不一定带来个人或精神上的丰富。大泽认为，如前所述，虚构试图在世界之外重构意识形态，反映出对这种潜在世界在现实中实现的前景的极大失望。简言之，政治和革命活动被放弃了，但对理想的追求并没有消失。\n我们需要认识到，这两者之间没有明确的界限。相反，这种区分反映了控制现实感知和社会活动的态度连续体。我们越接近虚构标记的光谱，就越疏远现实，并且通常会完全拒绝当前的现实状态。此外，越接近虚构光谱，我们就越倾向于完全拒绝既定秩序，并放弃调整或改变它的计划。大泽认为，取而代之的是一种末世论修辞，即世界变得无法拯救，理想在不清除一切的情况下完全无法实现。他将虚构定义为在虚构中保存和扩展的意识形态冲动（大泽, 56）。\n这可以从字面上理解为流行媒体如漫画和动漫作为探索和满足意识形态冲动的空间，但也可以比喻为在这一时期开始繁荣的新兴消费主义和新兴宗教的神秘宇宙论。\n大泽用来体现虚构的三个象征是迪斯尼乐园的建设、涩谷作为东京主要娱乐区取代新宿的发展以及新人类的出现，这些新人类是这个新的“虚构”时代的本地居民（大泽, 46-49）。“新人类”一词可以翻译为“新人”或“新种族”。这个词首次出现在1985年，当时《朝日新闻》刊登了一系列对日本年轻人的采访，标题为《新人类的旗手们》（Herbig）。它通常指的是在1970年代及以后的成长的人群。1970年代和1980年代成长的一代人没有经历过战后重建的艰辛。相反，他们在一个富裕、有影响力、技术先进的国家中成长，这个国家鼓励他们把自己打造成富裕社会的一员。与1950年代和1960年代的年轻人截然不同的是，新人类对左翼政治毫无兴趣，事实上，几乎没有任何政治倾向，因此他们被称为“反应无代”（Herbig）。这种差异不仅仅是文化上的，更是身体上的。之前的几代人经历过粮食短缺，而新一代人从小就能定期过度享受。1965年至1985年间，日本男性的平均身高增加了7厘米，平均体重增加了5公斤，而女性的平均身高增加了6厘米，平均体重增加了1.4公斤。代替意识形态或政治抱负的是，这一代人更感兴趣的是在公司超级结构中的个人进步，或者是个人议程，关注如何设计新的生活方式。如果这些生活方式提出了挑战上一代权威的新价值观，那么可供新人类使用的财富和富裕让他们可以摆脱这些旧的社会结构的重压。例如，许多年轻人在这段时间里有能力搬出家庭独立生活，导致了另一个绰号的出现：“独身贵族”（Herbig）。\n这里讨论的每一项——迪斯尼乐园、涩谷和新人类——都反映了某种与定义过去的意识形态斗争的疏离、隔离或冷漠。迪斯尼乐园在这里是一个很好的隐喻，因为它不仅象征着日本泡沫经济的巨大成功，也象征了一种从世界中物理上的隔离感。迪斯尼乐园被精心设计，一旦顾客进入，他们就看不到其边界以外的东西。这是为了让顾客（尤其是儿童）能够完全沉浸在一个自主的幻想世界中，而不受世俗的干扰。从这个意义上说，迪斯尼乐园作为一种消费体验，完全是以购买从日常生活中暂时逃脱的机会为基础的。将政治或明确的意识形态活动引入迪斯尼乐园将是对其作为幻想空间的自主性的侵犯。但这种政治与休闲之间的绝对分离，在一个脱离普通生活的空间中具体呈现出来，本身就是一种意识形态。这是意识形态与虚构之间的界限开始变得清晰的地方。\n然而，东京的城市中心本身正在经历一种由房地产开发商主导的“迪士尼化”，这些开发商在政府的基本不加监管的情况下进行开发。最终使涩谷成为东京首要娱乐区的新代表，是该地区完全转变为一个主要的消费空间：\n“涩谷的成功在很大程度上归功于西武集团的投资。通过这些投资，整个区域被开发成一个时尚的‘超现实’空间——换句话说，一个充满符号差异的前沿空间——适应了消费社会。”\n“渋谷が成功したのは、とりわけ西武系資本の投入によって、街の全体が、消費会社に適合した、「ハイパーリアル」でオシャレな－つまり先進的な記号的差異に満ちた－空間へと仕立てあげられたからである。要するに、渋谷は虚构の时代に适合した盛り场だったのだ。”（大泽, 47）\n我们可以通过当代对日本的描绘来佐证大泽的论点。涩谷著名的十字路口——经常在延时摄影中显示，其日常通勤者像迅速变化的人群一样脉动——不仅是东京的一个有力象征，也是日本作为一个超现代国家的象征。此外，涩谷因此转变为一个象征日本未来的标志，不仅对日本人而言，也是对整个世界而言，作为未来本身的象征。\n大泽使用“超现实”（ハイパーリアル，hyper-real）一词指的是一种新的城市沉浸感。这是一种建立在对消费主义无疑庆祝基础上的超高清未来愿景。他没有详细解释这个词的确切含义，但他的用法无疑是借鉴了让·鲍德里亚在其著作《模拟与拟像》中的概念。在他的书中，鲍德里亚将当代社会描绘为一个不再包含任何现实参照的巨大符号狂欢。一个符号的意义只能是其他符号在不断变化的符号系列中的另一个符号。在这种框架下，符号获得了自己的自主权，并对我们的现实感知产生了强大的影响。通过博尔赫斯的《科学的准确性》作为寓言，他断言地图作为表征取代了领土，最终成为需要创建新地图的领土本身（Baudrillard, 1）29。没有参照闭合系统之外的任何事物的符号，他称之为拟像。在鲍德里亚的观点中，现代社会与后现代社会的区别在于，现代社会专注于通过更高水平的工业化进行生产，而后现代社会则围绕着生产“模拟”来组织，他指的是电视、互联网、电影、广告、虚拟现实等。\n这种新的媒体生态本质上是一个创造拟像的引擎。例如，真人秀的核心是其主体的非脚本化互动，但实际上是高度编排、制作和编辑的，使这些互动符合我们对戏剧的期望。这种新的“现实”视野因此被消费者接受为解释自己和自己经历的新模式。在鲍德里亚的模拟社会中，身份是通过这种图像的挪用来构建的，从模拟中派生的模型决定了个体如何看待自己以及如何与他人关系。通过延伸，一个超现实空间是一个设计用于交叉并反映这种模拟世界的空间。它利用生活在“模拟”中的各种交织的拟像系统来构建其美学。\n迪斯尼乐园是超现实空间的最佳例子，因为它代表了将地图（构成迪斯尼的符号矩阵）变为实际领土的真诚尝试，在这种情况下，地图无疑是优先的。这在涩谷并不明显，但当我们开始思考将这一地区转变为一个旨在促进各种消费的空间的基本动机时——情趣酒店、夜总会、购物中心、百货商店、娱乐中心——显然，这个世界离不开几十年来在现代广告和品牌建设中构建的符号语言。广告本身就是一个完全致力于通过符号的精心融合来改变产品认知的领域。这些符号被融合到产品的形象中，以便以特定方式阅读产品。通过这种方式，全新的设备或服务被转化为绝对必要品，奢侈品不仅仅授予地位，而是不可逆地与特定的“生活方式”身份混淆。与美国的大型购物中心相似，涩谷是广告、营销和品牌的物理化表现。它试图将城市空间密封在这个不断变化的新符号领土中。在这个空间内，他断言： “真实是从微型化单元、矩阵、存储器和指令模型中生产出来的——通过这些，它可以无限次地复制。它不再需要理性，因为它不再需要与某种理想或负面的事物进行比较。它只不过是操作性的。事实上，因为它不再被想象所包围，它根本不再是真实的。它是一个超现实的产品，一个没有大气层的超空间中的辐射综合体。”（Baudrillard, 2）\n在这种新密封的图像领土中进化并被其服务的新一代就是新人类： “虚构时代的年轻文化最密切相关的是1980年代前半期的新人类，然后是后半期的御宅族。新人类没有被任何意识形态或概念吸引，而是在他们的消费、时尚和兴趣层面上通过‘符号的戏耍’来表现自己的身份。这是一种态度，表达了他们从支配意识形态时代的理想重压下解放出来。”（Osawa, 48）\n在1972年至1995年虚构时代中出现的各种年轻文化中，“符号的戏耍”本身成为了理想的来源。这几十年间有无数的亚文化和时尚趋势——大多源自涩谷和原宿的街头文化——完美地展示了这种“戏耍”。这些众多亚文化的共同点是对生活方式的热爱，这种生活方式主要由美学定义。不同类型的コギャル（kogyaru）亚文化提供了一个生动且特别字面的例子，展示了新人类一代人所拥抱的狂热符号混搭。\nコギャル（高中女生）时尚大约始于1990年代，但其实践和美学可以追溯到早期的亚文化现象。“ギャル”（Gyaru，女孩，源自英语的“gal”）这个称号自1970年代末以来一直用于指代年轻女性的前卫、冒险风格，并在当代用作后缀，表示ギャル连续体内的细分。最初，コギャル风格是一种对校服风格的改编——西装外套、裙子和长袜——被改造得更性感，并混合了大胆的元素，如染发、夸张的化妆，偶尔还有整形手术。随着时间的推移，コギャル风格变得更加多样化和极端，产生了无数不同的变种。“コギャル”这个词与少年犯罪甚至卖淫联系在一起，并且成为老一代人批评的目标。批评者认为这些年轻女性是寄生在父母身上的自恋者，破坏了传统价值观和女性身份。文化人类学家劳拉·米勒在一篇讨论1990年代年轻人时尚的文章中使用“无国籍”（むこくせき，mukokuseki）一词来解释コギャル美学背后的文化逻辑。她使用这个词来说明日本时尚潮流不是由模仿国外特定趋势驱动的，而是希望将尽可能多的吸引符号混合到一个单一的整体中：\n“无国籍的审美刻意唤起某个地方或某个历史时期的图像——越南农民裤子、美洲原住民的流苏和珠子、1960年代的佩斯利图案——但将这些与匡威跑鞋或日本木屐、棒球帽或拉斯塔法里编织贝雷帽结合或并置\u0026hellip;\u0026hellip;日本年轻人的时尚中发现的超历史和文化融合并不仅仅是美国文化帝国主义的证据。仔细观察会发现，在许多情况下，被挪用的物品是有意“误用”或扭曲的。例如，一些コギャル玩弄廉价的夏威夷意象，使用假塑料花环、兰花、koa种子项链和波利尼西亚印花纱笼，但并不是为了看起来像真正的夏威夷人。那些采用这些装饰的人有时被称为“地方女孩”或“纱笼女儿”\u0026hellip;\u0026hellip;コギャル和其他人夸大了他们外表的人工性。结果在真诚、嘲讽和媚俗之间徘徊，使人难以分辨他们是否或在多大程度上是在开玩笑。”（Miller, 83-84）\n这种审美的共同特点是将常见或熟悉的物品与不寻常的物品搭配，创造出高度异国情调，有时甚至是故意怪异的新背景：一件鲜红色的格子纹和服，8英寸高的厚底鞋和一大片亮粉色的头发。这些符号不指向西方，甚至不指向江户时代，它们只属于コギャル时尚文化中的符号网络。像涩谷和原宿这样的超现实城市空间不仅是购买服装的地方，也是展示它们的空间。\n这种通过时尚和个人品味定义自己找到意义的新人类一代，通过拥抱这种新的超现代性的果实来表达自己。他们不是通过政治观点表达自己，而是通过显著的消费形式来创造身份。从这个意义上说，他们完全生活在日本的泡沫中。然而，新人类仍然是一个广泛的类别，反映了广泛的信仰和社会活动。对于大泽来说，完全体现虚构时代主要方面的亚文化是御宅族（Otaku）。御宅族起源于新人类一代，但发展成了一个独特的群体：\n“御宅族指的是一种从新人类中发展出来的青年文化。如果新人类的行为特征在于他们从过去时代的执着中解放出来，那么相反，御宅族的行为特征在于新的执着。**御宅族是这样一群人：他们对一个特定领域——动画、电子游戏、电脑、偶像歌手等——表现出过度的、几乎不合理的热情。**我将这种不合理的热情定义为‘意义’（这里我们可以认为是该领域的社会必要性，或像艺术一样，它本身具有的价值）和御宅族对该兴趣领域收集的信息密度之间的明显失衡。信息的密度压倒了意义。例如，在大多数情况下，如动画的典型例子中，故事[物语]只是虚构[虚构]的积累。因此，对于御宅族来说，他们钟爱的虚构世界似乎比普通现实更重要。”\n“オタクは、この新人類から分化・発展してきた若者の類型である。新人類が、過去の時代を支配していた執着からの解放の身振りによって特徴付けられるのだとすれば、逆にオタクは、新たな拘泥の身振りによって特徴付けられる。オタクとは、かつてだったら興味として片付けられるような一見瑣末なさまざまな領域—アニメーション、テレビゲーム、コンピュータ、アイドル歌手等々—に、不合理なまでに過剰に熱狂的に耽溺する人々である。「不合理なまで過剰」というのは、その領域の社会的な必要とか（芸术のような）それ自身として有すると考えられている価値といったような「意味」の大きさと、その領域に関してオタクが集积する「信息」の浓度との间に、バランスが失われているように见えるということ、「意义」の大きさを「信息」の浓度が压倒しているように见えるということ、である。たとえばアニメーションの場合が典型であるように、多くの场、物语＝虚构の集合である。それゆえ、オタクにとっては、まるで、通常の现実よりも、彼らが爱する虚构の世界の方がより重要なものになっているように见えるのである。”（Osawa, 48）\n大泽在这里提出的御宅族概念非常熟悉，但他从他们对流行媒体的热情中提取了更大的意义，而这常常被忽视。如果说红军联盟成员完全投身于马克思主义革命理想的热情，那么御宅族的特征在于对他们的细分兴趣的痴迷。这些信仰在结构和总体方向上的区别在于，马克思主义革命旨在改变周围的条件以服务于其愿景，而御宅族对周围条件基本无动于衷。他们只是希望更加深入地沉浸在自己喜欢的电视系列中，或者换句话说，更深入地进入特定的符号宇宙。大泽在上面提出的意义（いみ）与信息（じょうほう）之间的平衡，意味着对符号本身的奢华享受逐渐超过了它们的内容。虚构中所代表的理想逐渐在虚构本身中得到保留，达到理想无法在虚构之外生存的地步。这本质上构成了大泽所说的意识形态与虚构之间的关键区别。然而，单独指出御宅族是日本社会结构中的某种异常是错误的。大泽试图指出的是，御宅族对虚构的倾向在整个文化中普遍存在，或许只是通过程度上有所区分。\n御宅族与超现实 大泽在这里讲述的故事可以看作是宏大叙事逐渐消亡及人们为应对这种变化所作的努力。这些叙事曾是赋予私人和公共行动意义并为某些社区活动提供稳定的社会结构。超现实和“新人类”代表了一种意识形态的畸变，将激进分子的能量重新导向个人主义的新意识形态。\n自20世纪70年代以来，宏大叙事的死亡，或者说意义的死亡，一直是许多有影响力的哲学家和学者作品中的共同主题。这个概念最早由法国文学批评家Francois Lyotard30提出，后来成为后现代哲学的主要标志之一。“宏大叙事”一词通常指的是18世纪欧洲出现的各种社会建构，这些建构是为了将欧洲各王国的封建拼凑整合为统一的实体，形成具有单一身份的大型民族国家。知识上，宏大叙事体现在人性和理性的观念中，革命叙事如马克思主义中，经济上则表现为生产至上的国家存在理由。大泽的论点基本上是宏大叙事在日本如何消亡的具体案例。他的论点坚定地位于后现代哲学的传统中，讨论的是时代精神中的意识形态和虚构的死亡过程。\n从这个角度看，“新人类”亚文化和御宅族的出现只是对这些叙事死亡的情感反应。文化批评家东浩纪在他的《御宅族：日本的数据库动物》中，描述了御宅族作为后现代世界中努力保持理智的不安幸存者的形象：\n“御宅族无论走到哪里，都随身携带大量的书籍、杂志、同人志和塞满大纸袋的碎片，像寄居蟹一样，”因为如果他们不随身携带“自我的壳”——即他们的群体归属幻想——他们就无法在精神上保持稳定……现代性受宏大叙事的支配。相反，在后现代性中，宏大叙事崩溃，社会整体的凝聚力迅速减弱。在日本，这种减弱在1970年代加速了，当时高速经济增长和“政治季节”结束，日本经历了石油危机和联合赤军事件。从这个角度看，我们可以将御宅族用垃圾亚文化材料构建“自我壳”的行为模式视为填补宏大叙事消失所造成空虚的行为模式。（东浩纪, 27）\n这种“寄居蟹”形象，与穿戴各种奇异饰品和配件的コギャル（日本街头少女）并无二致，有时她们甚至被这些饰品贴满。消费主义达到极致是一种虚构：一种对世界不感兴趣的意识形态，仅关注符号。这是一种沉浸于超现实的狂喜。御宅族只是消费主义的一个极端代表。这种消费主义类似于意识形态热情，但应用于某些小众兴趣。大泽和东浩纪告诉我们，无论我们是否消费漫画，我们都是在宏大叙事的遗骸中为寻求我们的身份的寄居蟹。从这个角度看，像涩谷这样充满光辉和魅力的超现实空间实际上是一个精神的深渊。\n虚构时代的时代精神仅仅指出晚期资本主义社会中滋生一种几乎宗教态度的消费者的趋势，他们对自己的爱好、喜爱的电视节目、音乐类型等充满热情。西方的民主、工业化国家（暂且包括日本）正是由于其巨大的财富和高度发达的基础设施，使我们能够过上与貧病争大多隔绝的生活；饥饿不存在，现代医学可以治愈许多疾病，洪水和地震的影响通过技术进步大大减弱等。这反过来使我们与现实疏远，同时将我们进一步沉浸在一个与自然世界完全不同的日益技术化的环境中。\n城市一直提供某种程度的与贫病死的疏离。然而，基础设施中表现出的先进技术越来越使我们能够从未曾可能的方式中消除死亡、疾病和暴力。超现实就像虚拟现实的类比原型；一种存在的层面，甚至隔绝了城市本身的更严酷环境，专注于消费和机械化的满足。这就是涩谷与迪斯尼乐园的联系所在，使这些超现实空间与东京臭名昭著的山谷（さんや）31地区或大阪的釜崎（釜ヶ崎，かまがさき）32区不同，在那里贫困的体验是痛苦可见的。城市提供的疏离使自然世界的侵入或突发死亡的可能性变得更加创伤。这就是为什么沙林毒气攻击和1995年神户地震对国家心理产生如此深远影响的原因。这两个事件对大泽有特别重要的意义，因为它们扰乱了城市的超现实环境，结束了虚构时代。\n进入超现实本质上是现代化西方的特权，可以说，创造超现实空间的欲望在大泽定义的意识形态最纯粹形式中反映出来：消除贫病死的斗争。讽刺的是，这削弱了我们与意识形态的关系，将贫病死推向模糊的边缘。然而，超现实从未完全隔离我们；我们始终容易遭受灾难、受伤或突如其来的死亡，破坏我们对世界安全或稳定的看法。超现实只是掩盖了我们对这种灾难可能性的意识，直到它以灾难性的事件出现在我们的生活中。\n70年代日本右翼和左翼意识形态活动的明显失败——红军的暴力、三岛由纪夫的自杀、学生运动的混乱和政治无能，甚至远处苏联的解体——导致了结构日本战后社会的宏大叙事的普遍崩溃。**从这个角度看，大泽的虚构时代是由意识形态时代向虚构时代过渡的双重运动推动的：超现实中的疏离和政治幻灭。**在这种背景下，新人类对经典意识形态斗争不感兴趣或不信服。他们转向内心的虚构和超现实的享乐空间，作为新理想的空间。对于1980年代的御宅族，他们实际上献身于虚构，作为新理想可以蓬勃发展的空间。超现实是鼓励和增强向虚构冲动的空间。像涩谷这样的空间由开发商建造，作为对消费符号的激情献身的自然基质。正如田中元子（Motoko Tanaka）敏锐指出：\n“……现代日本[已经]标准化和媒体化，有着精心维护的分销管理系统。在日本，消费者需求可以立即和机械地满足，无需他者的介入。以前只能通过社会交流获得的欲望对象，如食物和性释放，现在可以很容易地获得，或几乎不需要交流。”（田中, 57）\n当然，这不仅适用于日本，还适用于整个后工业化西方。现代性的成果导致了许多基本需求自动、即时，甚至在我们意识之外得到满足的系统。根本上，超现实代表了一种对虚拟现实的渴望。新宿和迪斯尼乐园就像原型；模拟形式的虚拟空间。但是，虚拟的渴望究竟是什么？斯洛文尼亚哲学家斯拉沃热·齐泽克（Slavoj Zizek）对这个术语提供了一个有趣的解释：\n“[虚拟现实]提供了被剥夺其实质的现实……就像脱咖啡因咖啡闻起来和尝起来像真正的咖啡但不是实物……一切都被允许，你可以享受一切——只要它被剥夺了使其危险的实质。”（齐泽克, 38）\n这种对虚拟的概念，即享受无后果的乐趣或去除物体本质的欲望，是我们迄今为止探讨的超现实和虚构概念的核心。向虚构的运动反映了一种沉浸在符号的吸引世界中的愿望，在那里理想可以不受阻碍地生存，超现实的城市空间同样反映了一种欲望，即将城市中任何不愉快或危险的因素剥离。然而，超现实倾向于那些不仅使我们与超现实之外的现实疏离，而且间接地与他者疏离的系统。这导致了对现实的共同感知的削弱。那些在70年代末和80年代开发的超现实空间充当了当代虚拟空间扩散的原型。虚拟现实和超现实仍然保持各自的领域，但技术的整体趋势指向政府和私营部门积极努力将数字实用性与物理空间整合（智能手机与城市的互动是一个很好的例子；例如Uber，甚至约会服务如Tinder）。超现实中反映的虚拟渴望与从70年代末到现在的视频游戏的加速增长和流行密切相关。然而，如果这些空间使某种行为或某种消费关系得以实现，大泽会将其定义为转向虚构的行为，但它们并不一定决定这种行为。大泽关于御宅族的论点揭示了沉浸在超现实中的欲望如何反映出一种快乐的享乐主义，但也可以反映出对世界的深层次怨恨，迅速转变为病态的虚无主义。\n虚构的负面逆转 超现实世界的主要问题在于，虽然它充满了符号和物质上的享受，但却缺乏意义。对当代整体情况常见的情感反应是讽刺、玩世不恭和刻意的冷漠。另一种常见反应是制定复杂且高度综合的新世纪信仰体系。这些信仰体系鼓励信徒完全服从一种彻底另类的世界观，通常看起来像科幻小说或幻想作品一样刻意。在这个意义上，它是虚构时代最有力的象征。这些信仰体系通常强调世界的虚假和无意义。因此，围绕新世纪信仰体系组织的邪教往往核心是一个末世论叙事，要么寻求逃避当前形式的世界，要么寻求其毁灭。在社会学家岛园嗣的研究中，指出新世纪邪教成员的两个关键特点：一是成员主要来自相对富裕或中产阶级背景；二是加入邪教的主要原因是世界的虚假、虚幻或无意义 (田中, 48)。这两个特点似乎概括了问题的核心。尽管这些人生活舒适，物质上得到满足，但这种满足并不能填补当代生活中某种根本性的意义空虚。\n对于大泽来说，虚构时代（1972-1995）的主要象征是奥姆真理教。他认为这个组织是御宅族的极端表现。这在字面上是部分正确的。众所周知，麻原彰晃和他的追随者是漫画和动漫的狂热爱好者，这些虚构的内容直接融入了他们的综合信仰体系中，包括诺斯替基督教、吠陀神秘主义、阴谋论和超自然伪科学。一个小例子是“宇宙清洁器”，这是一个大型空调单元，麻原声称可以净化外界毒素的空气（大泽, 50）。这个名字直接取自流行动漫《宇宙战舰大和号》，其叙事围绕人类试图修复被敌对外星人辐射污染的地球。此外，该邪教还通过身体和精神上远离外界，建立了一个名为莲花村的公社，作为他们的新乌托邦时代的诺亚方舟，并称之为香巴拉，这是从藏传佛教中借用的天堂概念。通过隐退，他们得以完全封闭在自己设计的世界中。莲花村是他们尝试创建的超真实空间，在这里他们可以用新的虚构符号包裹自己。全面探讨他们所有的宗教实践在这里是不可能的，但简而言之，像“宇宙清洁器”这样的遗物、麻原的秘传布道、能量转移仪式如夏克提巴特，以及电击疗法和在黑暗房间里度过数天的极端宗教训练，都是他们沉浸在自己的超真实中的一部分。\n麻原彰晃将许多阴谋论作为他们神话的一部分。他们认为日本政府是一个由恶意的全球犹太阴谋（共济会）操纵的傀儡，实际上控制了世界。他们认为美国政府在共济会的影响下开发了一种地震机器，导致了1995年的神户地震。在他们看来，世界是不可救药的，受恶意力量的控制。他们甚至认为自己的公社莲花村里充满了共济会间谍，这导致了一个秘密内圈的形成，最终负责毒气袭击。在奥姆未能在1990年获得政治职位后，大泽指出，麻原在该团体内部刊物中的言辞变得越来越末日论和对日本政府的敌意。甚至有证据表明，在1995年之前有几次失败的毒气袭击（奥尔森, 413 - 416）。未能当选相当于外界对其虚构的拒绝，这对他们来说是不可容忍的侮辱，促使进一步的神秘启示和更多的阴谋论，以巩固他们的信仰。\n奥姆还标志着日本社会状况与《黑暗之魂》系列之间的第一个接触点。在每个游戏中，总是存在着在已坍塌王国废墟中仍在运作的敌对、神秘的亡灵邪教。这在第三部作品中特别明显，几乎整个游戏过程中都可以看到亡灵邪教的证据。这些无法理解的信仰实践者代表了有组织社会的最后残余，他们一见到玩家就完全致力于消灭你。正如奥姆的一些成员甚至对其实践和目标也感到模糊一样，《黑暗之魂》中的亡灵邪教的实践和目标也是如此。只有他们作为玩家威胁的存在是明确的。这重现了许多人对奥姆的恐惧和焦虑，他们只是一个威胁和神秘的实体。\n大泽认为，像奥姆真理教这样的新宗教与基督教等旧的千禧年传统不同之处在于，他们通过退缩故意脱离与“贫病争”的斗争。佐证岛园的论点，他指出这种退缩的主要动机是“生活的空虚”（生活の空虚）（大泽, 35）。他将新信仰与旧信仰进行对比。例如，基督教在《圣经》中包含一个末世论叙事，涉及物质与神圣最终完美结合（《启示录》实际上是麻原的主要灵感来源之一）。然而，大多数形式的基督教在通过慈善等努力积极参与与“贫病争”的斗争。换句话说，他们不通过退缩来拒绝现实的严酷，而是积极努力通过各种可能的途径消除它。这使得它们在经典意义上是意识形态的。令人惊讶的是，许多在日常生活中与贫困和疾病作斗争的人往往是宗教信徒。如果我们看一下像天主教会这样的机构，大多数教区居民来自第三世界，如南美洲、非洲甚至贫困水平较高的亚洲。相比之下，属于虚构时代的宗教如奥姆真理教明确拒绝世界，通常渴望世界的终结或其他形式的毁灭。在我们的文化中，人民圣殿教（乔恩斯顿大屠杀）33和天堂之门34是两个虚构取向而非意识形态取向的宗教的好例子。\n岛园对加入像奥姆真理教这样的团体的动机的研究似乎表明，现代经验中出现的某种缺失需要被填补。这是从意识形态转向虚构的根源，在面对当代的模糊、混乱和意识形态的空虚时对意义的真诚渴望。虚构就像是在这混乱中的生命线，但一个人越深入这领域，病态行为的潜力就越高。如果沉迷于自己最喜欢的系列神话的御宅族是对虚构和超真实的“积极”异化，那么实行恐怖主义的末日邪教则是无法容忍其虚构界限之外世界的“消极”异化。奥姆对其自创虚构的痴迷，以及他们完全退隐至莲花村——他们自己设计的超真实空间的尝试——使他们无法接受任何与其虚构相矛盾的存在。这是对虚构的依恋导致对世界虚无主义拒绝的关键点。在后现代性的背景下——即宏大叙事不再起作用的世界——现实无法维持这种虚构中潜在的理想，往往会导致认为现实本身某种程度上是被污染的结论。\n第二章：受苦之魔力圈 未实现的道路 文学解释充满了模棱两可，因为一个文本可以有无数种解读。这一点在电子游戏中更为显著，因为其“游戏性”（或游戏特质）和叙事结构之间存在根本性的张力。传统的文学解释依赖于文本的某种稳定性，以便提取出一种“解读”。在《了不起的盖茨比》中，读者只能通过一个“路径”来进行叙事。威尔逊总是射杀盖茨比，而这一叙事事件的描述对所有读者来说都是相同的。虽然可能有许多解释方式——例如，我们可以将其解读为对资本主义意识形态如何使无产阶级自相残杀的马克思主义批判——但这种解释只有在基于菲茨杰拉德的原始作品《了不起的盖茨比》的基础上才有效。\n如果我们将电子游戏，特别是RPG（角色扮演游戏）类型，视为一种文本，它们更像是一个选择你自己的冒险小说，其中玩家的自主性决定了事件的进程。从叙事角度来看，这种思维方式有助于分析《黑暗之魂》，因为它的叙事结构大致是线性的。它有一个开始，并有几个决定结局的叙事选择，而这些点之间有限的叙事内容是由“作者”（设计师）设定的。然而，即便在这里也存在问题，因为游戏作为美学艺术品并不主要在于通过叙事来前进，推动玩家想知道接下来会发生什么。游戏的体验在于游戏本身，这种游戏形式可能与叙事或其中所出现的虚构世界有松散甚至完全没有实质性的联系。例如，想象一下，你只能在完成一场5公里跑步比赛且时间在六分钟以内的情况下从第九章继续到第十章。在文学中，这些活动之间没有联系，但在电子游戏中，“跑步比赛”的体验非常重要，并直接影响我们对叙事的解读。在像《黑暗之魂》这样的游戏中，完成游戏大约需要40个小时，而阅读、观看过场动画或听对话的时间可能只有30分钟。\n大多数电子游戏是困难且繁琐的，即使对那些熟悉这一媒介的人来说也是如此。对于那些没有所需数字素养和基本操作游戏设备技能的人来说，它们简直是不可能的。正是这种繁琐和挑战性，导致电子游戏学者Espen Aarseth称电子游戏为“需耗费精力的文学”（ergodic literature），即“需要付出巨大努力才能让玩家穿越游戏文本”（Aarseth，第2页）。“ergodic”这个词来源于希腊词语ergon和hodos，分别意为“工作”和“路径”。这个概念出现在Aarseth的开创性作品《Cybertext: Perspectives on Ergodic Literature》中，成为这一新领域的核心理论工具。这个概念的核心是玩家在游戏中拥有的自主性，这种自主性是书籍读者或电影观众所不具备的：\n读者，即使强烈地参与到叙事的展开中，也无能为力。就像足球比赛的观众，他可以推测、推断、外推，甚至大声咒骂，但他不是比赛的参与者。就像火车上的乘客，他可以研究和解释不断变化的风景，可以随意休息眼睛，甚至拉下紧急刹车并下车，但他不能自由地改变轨道方向。他无法拥有玩家的影响乐趣：“让我们看看我这样做会发生什么。”读者的乐趣是偷窥的乐趣。安全但无能为力。（Aarseth，第2页）\n然而，除了“影响的乐趣”——即通过游戏系统选择自己的“路径”的乐趣之外，还存在其负面反转的未探索可能性。在另一篇题为“定义现实”（Define Real）的文章中，他写道：\n\u0026hellip;玩家意识到他们体验的部分性质，众多策略和未走的路径，以及游戏中可能存在他们永远不会遇到的谜团，超出他们技能水平的解决方案\u0026hellip;你可能永远不会知道你的选择的确切结果；也就是说，你到底错过了什么（Aarseth，第3页）。\n在文学中，没有只有在特定条件下才会显现的秘密章节。当我们从头到尾阅读《了不起的盖茨比》时，我们确信已经读完了整部作品。当然，多次阅读会揭示多种解释，但我们从不会为是否仍有大量未发现内容而感到焦虑。从形式上、游戏设计角度以及文本/电影的角度来看，《黑暗之魂》是一款推动这种感觉的游戏，有时这种感觉令人不安、疲惫和疏远。应对这种感觉是游戏作为一个偶然性系统运作的关键方面。在对第一款《黑暗之魂》游戏的分析中，Daniel Vella将这种焦虑称为“需耗费精力的讽刺”（ergodic irony）（Vella，第16页），以表示我们的自主性如何可以削弱我们对游戏“完成”或“掌握”的驱动力。\n除了“需耗费精力的讽刺”中潜在的焦虑之外，还有由游戏行为产生的不稳定心理模式的问题。尽管被称为“游戏”，但在这个领域仍有相当大的争论，即电子游戏是否应该被视为游戏。这并不难理解，因为电子游戏通常同时既是一个故事又是一个具有规则和程序的游戏。当我们踏上篮球场时，我们进入了一个特定的心理框架；所有的行动都是通过一个基本的目标来理解的，即在一个内化的任意限制（例如，不携带球、不击打其他球员等）下将球投入篮筐。\n实际上，正是这种框架使人成为篮球运动员。这正是荷兰社会学家Johan Huizinga所说的“游戏的魔法圈”（magic circle of play）（Huizinga，第32页）。它是指游戏划定了一个空间，在这个空间内只能采用某种心理框架。篮球场是一个“魔法圈”；它允许篮球运动的发生，但只有在篮球游戏本身的心理框架下才能使用。如果做其他事情，就会排除游戏的可能性。篮球场也可以用作高中舞会的场地，此时“圈”的魔法处于休眠状态。小说也是一种魔法圈，因为它是以某种心理框架为前提而创作的。当我们阅读小说时，通常会暂停许多其他阅读方式，以便欣赏故事。文学评论家、历史学家和文学爱好者不会以相同的方式阅读小说，尽管他们可能是同一个人。然而，所有这些框架都是允许的，并且不会削弱作品的基本存在。\n此外，阅读小说的行为通常并不是围绕着完成一个任意目标进行的，而阅读的“规则”并不是基于封闭的一套理性公理。电子游戏会在你无法理解或处理游戏作为一个规则和程序系统时让叙事停滞不前。电子游戏，特别是RPG类型，要求玩家在最基本的层面上通过多种心理模式进行转换。玩家必须随时准备成为电影观众、读者、统计数据的解释者以及复杂运动任务的执行者，所有这些都在几分钟之内完成。\n从这个意义上讲，魔法圈的概念开始崩溃，因为很明显，电子游戏是一个可以同时存在多个圈并要求不同心理框架的空间，而不会削弱电子游戏作为互动艺术品的功能。更好的理解电子游戏的方式是将其视为模拟，其中传统意义上的游戏要么被直接纳入，要么可以在设计师意图之外自发地出现。此外，尽管电子游戏通常将虚构作为体验的核心方面，但严格来说，它们与虚构并不完全相同。引用游戏学者Jesper Juul的话，电子游戏是“半真实的”：\n电子游戏是现实的，因为它们由玩家实际互动的真实规则组成，赢得或输掉游戏是一个真实事件。然而，通过杀死一条龙赢得游戏时，龙并不是真实的龙，而是虚构的。玩电子游戏因此是与真实规则互动，同时想象一个虚构世界，电子游戏是规则和虚构世界的结合。（Juul，第1页）\n然而，在“真实”与“虚构”龙的问题上，还有另一个重要的观点。Espen Aarseth认为，像《魔戒》这样的幻想作品中的龙和像《无尽的任务》（Sony Online Entertainment，1999）这样的幻想RPG游戏中的龙具有截然不同的本体论性质。即使是同一条龙，如《霍比特人》小说中的Smaug和《霍比特人》电子游戏改编中的Smaug，也是如此。区别在于虚构与模拟之间的差异。在题为“门与感知：游戏中的虚构与模拟”（Doors and Perceptions: Fiction vs Simulation in Games）的文章中，Aarseth写道：\n一条龙显然是虚构的，但另一条是模拟的。一条龙是供阅读或在电视或电影屏幕上观看的，另一条龙是供玩耍的。一条龙完全由符号构成，另一条由符号和动态模型构成，这个动态模型将指定其行为并响应我们的输入。正是这种模型行为使其与虚构不同，因为我们可以比了解虚构更亲密地了解模拟。虚构很少甚至从来都不是个人化的，而模拟通过体验可以成为个人化的。模拟允许我们测试其极限、理解因果关系、制定策略并产生变化，这些在虚构中被明确拒绝，但在现实中却非常相似。（Aarseth，第35页）\n然而，如果我们将游戏视为模拟，它们到底在模拟什么呢？如果模拟的是一些现实世界的活动，如驾驶飞机或汽车，这个问题的答案就很简单。在电子游戏行业本身，“模拟器”一词指的是一种试图在世界中再现实际、有时是非常平凡的体验的游戏。模拟可以是低保真或高保真的，但它们的参照物是显而易见的。\n以虚构为导向的游戏，本质上是将那种虚构作为一个数字世界的模拟。当我们考虑电子游戏与传统虚构中的既定类型的关系时，这一点最为明显。科幻电子游戏《光晕：战斗进化》（Bungie，2001）似乎直接借鉴了几部科幻作品，并将它们拼凑在一起。游戏发生的巨大的环形太空栖息地（即标题中提到的“光晕”）让人联想到Larry Niven的《环形世界》系列，而主角Master Chief穿着的“Mjolnir Powered Assault Armor”似乎暗示了John Steakley的《装甲》一书，该书的情节也围绕着穿着动力外骨骼与外星威胁作战的士兵展开。可以几乎无限地以这种方式继续下去，将光晕神话中的某些方面孤立出来，并试图从科幻经典作品中挖掘出影响。这种拼凑是类型小说的基本特征。不同的是，当这些元素被重新构建为一个虚拟的“现实”时。\n设计师不像作家那样，必须回答一些困难的问题，例如，等离子步枪的感觉和声音如何？它的射速有多快？它能穿透坦克的外壳吗？作家可以简单地依靠读者自己想象这些体验细节。以这种方式，《光晕》就像是对构成该类型的符号集合的模拟：模拟模拟物。然而，电子游戏并不是由影响其美学的虚构类型定义的，而是由游戏玩法的惯例定义的。《光晕》的虚构可能源自科幻，但它首先是一款第一人称射击游戏。因此，作为一种体验，它与《孤岛惊魂：原始杀戮》（Ubisoft Montreal，2016）——一款发生在人类原始的穴居人过去的“射击”游戏（标枪、弓箭等）——相比，具有更多的共同点，而不是与《幽浮》这款科幻游戏进行比较，尽管《幽浮》也涉及人类与外星人之间的冲突，但它是一款策略游戏，其原则更类似于国际象棋或Risk这样的棋盘游戏。\n然而，电子游戏不仅仅模拟作为美学集合的类型，或甚至是作为与外星人战斗的太空陆战队员的幻想，尽管它们在这些方面做得相当令人信服。它们还模拟人类存在的模式，或者换句话说，它们模拟文化原型。它们几乎是作为这些原型在古今虚构作品中表现的逻辑延伸来模拟的。\n在其作品《千面英雄》中，美国神话学家Joseph Campbell首次提出了“英雄之旅”的概念，这是一种在不同文化和时代中可见的叙事模式。他描述了基本模式如下：\n一个英雄从日常世界出发，进入一个充满超自然奇迹的区域：在那里遇到令人惊叹的力量，并赢得决定性的胜利：英雄从这个神秘的冒险中回来，拥有了为同伴带来恩惠的力量。（Campbell，第23页）\n正是这种模式的认知使我们能够看到《星球大战》中的卢克·天行者几乎与《伊利亚特》中的奥德修斯扮演了完全相同的文化功能。RPG类型作为一个整体几乎完全围绕这种模式在叙事和游戏设计中进行结构化。在这些游戏中，几乎总是有一个任务，一次跨越许多土地的旅程，一个成长的过程（例如，升级，解锁能力和获得强大的物品），以及一个英雄式的结局，一个王国，或者宇宙本身得救。《黑暗之魂》与其RPG前辈不同之处在于一个关键的类别：英雄式的结局。英雄的方面不仅被模糊和故意含糊不清，而且甚至没有提供一个真正的结局，像卢克自我实现为绝地武士并推翻邪恶帝国的那种方式。这是使《黑暗之魂》作为作品独特的关键方面，因为它模拟了英雄之旅，同时又颠覆了构成其核心价值观的基本结构。\n电子游戏之现象学 如果电子游戏是一种对虚构类型和文学/文化原型的模拟，而不是简单的虚构再现，那么我们该如何解释它们的意义呢？游戏的高度开放性和“能动性”特质（ergodic）在试图对某一作品进行严谨理解时带来了挑战。这一问题在游戏学者中广泛讨论，但一种广泛应用的解决方案是采用现象学方法，即Edmund Husserl和Martin Heidegger等思想家所提倡的在研究游戏体验的基本特征时所用的方法。简而言之，现象学是指通过经验调查现实，而不是单纯依赖外部的、理性的（主要是隐含的）公理体系来证明我们的知识。\n从根本上说，游戏的现象学是一种“第一人称”的分析方法，它不会忽视学者在与游戏这种物质系统互动时所产生的变化和解释框架。这种方法并不排除其他传统的文学解释方法，也不排除社会学、人类学或历史等其他研究模式，而是将这些模式纳入研究者的心理整体中。这样，评论家可以关注游戏过程中相对于具体文本和电影元素所出现的多种心理模式。毕竟，电子游戏不仅仅是叙事，也不仅仅是一组抽象的程序和规则。它们是两者的结合。玩家通常将这些元素作为一个整体来体验，仅仅关注叙事或游戏玩法中的任何一个都会忽略一些至关重要的方面。只有通过严格的自我反思过程，游戏评论家才能提出关于这种全新媒介中出现的意义类型的连贯论点。\n之前，我提到过Johan Huizinga对游戏的定义，即“魔力圈”，其中划定了一个需要特定心理框架的空间，才能使游戏存在。由于游戏在每一刻都引发多种心理模式，电子游戏理论家Dominic Arsenault和Bernard Perron借用了Huizinga的概念，并将魔力圈重新定义为“魔法循环”（magic cycle）（Arsenault，Perron 109）。这一术语指的是玩家与游戏系统之间的反馈循环，玩家通过探测系统来推进游戏，系统则产生新的游戏状态，激发玩家对游戏世界的不同理解，促使玩家采取进一步行动。在《黑暗之魂》系列中，玩家探索世界以获取材料，这些材料可以用来升级武器，使他们能够打败更强大的敌人，从而探索更多的世界，获得更多的武器和材料，依此类推。在这个过程中，他们将看到、阅读和体验到改变他们对自己活动看法并激发新行动的事物。\nArsenault和Perron的魔法循环概念在游戏研究中广泛传播，但游戏设计师早已意识到反馈循环对其媒介的重要性。作为《光环》的首席设计师，Jaime Griesmer曾在一次采访中说：\n在《光环1》中，大概有30秒的乐趣反复出现。如果你能找到30秒的乐趣，你基本上就能把它延伸成整个游戏。（Kietzmann，web）\n由于《光环》在游戏历史中的重要地位，这句话已成为商业领域的设计信条。然而，Griesmer在几年后的另一次采访中对此进行了重要补充：\n在Vidoc（视频纪录片）中被删掉的那句后半部分我谈到了如何在不同的环境中，以不同的武器、不同的载具、对抗不同的敌人、不同组合的敌人，有时对抗彼此战斗的敌人来进行那30秒的乐趣。《光环》的每30秒都不会重复；任务不断改变你所处的环境。（Kietzmann）\n这揭示了Arsenault和Perron意识到但其术语未能充分表达的魔法循环的重要方面。游戏玩法不是一个循环，而是一个不断扩展的螺旋，叙事和游戏元素交织在一起，创造出对游戏世界作为虚构空间和系统的不断演变的理解。\n在下一章中，我将对我在《黑暗之魂III》中遇到的一个隐藏区域进行现象学描述，该区域是在玩了大约30小时后发现的。角色和地点的名字将同时给出英文本地化和日文原版。这些本地化版本相对忠实但可能与原版有显著偏差。这是因为本地化过程优先考虑的是游戏的市场性和销售，而不是文本的忠实度。在原版日文和本地化版本之间有显著差异的地方，我将提供自己的翻译。这将在文本引用中注明。我将采用现象学方法来传达在魔法循环中发现的变化心理模式，这些模式不仅是整个系列的特征，而且可以在游戏的任何特定部分中浓缩。尽管从叙事角度来看，描述中会有许多看似晦涩的部分，但这种晦涩是故意构建在体验中的。只有当我们检查魔法循环的轮廓时，才能正确地将这些散落在这段短暂但强有力的体验中的叙事片段置于上下文中。这里提供的《黑暗之魂》系列的解释将展示叙事框架如何与游戏元素交织在一起，创造出必须整体分析的连贯体验。\n第三章：末法、死亡叙事与可怖他者 审判者古达，失败的英雄古达 在洛斯里克城堡的地下墓穴深处，有一座早已被遗弃的陵墓。这个房间曾经看起来像是一个礼拜的地方。房间中央矗立着一个巨大的石质圣杯，仿佛一个洗礼池。在它下面，一个身穿16世纪马西米连式板甲的骑士盘腿而坐，呈莲花座姿势。他在冥想吗？不，他早已死去，但人们不禁想知道他是否已经摆脱了这个充满幻觉、欲望和轮回的短暂世界。或许他在这个被遗弃世界的凉爽而孤寂的角落里，达到了那著名的般涅槃境界。\n凝视着他平静的身影，我不禁想起浮世绘版画中那幽默的拟态（見立て）。我回想起铃木春信（1725-1770）的一幅作品，画中一位身穿鲜红长袍的花魁在一片巨大的叶子上航行，这是对菩提达摩从印度抵达日本的幽默指涉（詹金斯，第100页；附图1）。江户时代的有识之士和富裕阶层会为这样一个将高僧与吉原的女孩混同在一起的形象感到兴奋。我们的骑士——安详地在这个洗礼池下休息的形象，不正如同释迦牟尼在菩提树下成道的拟态吗？这是开发者对玩家的巧妙玩笑吗？我们无从得知，但没有理由停留在无法解答的谜题上。当游戏本能占据上风时，这个房间就变成了一个需要搜刮宝物的地方。\n房间后部有一个箱子。我打开它，发现了一片龙鳞（顿时一阵快感涌上心头；我现在有足够的鳞片来升级一件特殊武器了），但箱子后面的墙壁有些奇怪。它的颜色与周围环境不匹配，看起来过于黑暗，仿佛这个部分的图像纹理有些瑕疵。这种不协调感令人无法忽视，而且地板上还刻着其他玩家留下的信息。我不需要特意去读它们就能感觉到这面墙后面有什么东西。我用剑击打墙壁，墙壁消失了——不过是个幻象。取而代之的是一个洞穴的入口。向洞穴迈出一步，屏幕上闪现出粗体文字：无主墓地（無縁墓地）。我感到一阵兴奋。寒意通过我的身体，汗毛竖起。我发现了一个秘密区域。带着忐忑，我穿过狭窄的通道，但走了几码后，我发现自己竟然在户外。头顶上没有天花板，只有无尽的黑暗，仿佛洞穴的屋顶高达数百英尺。我在这狭窄的峡谷中穿行，斩杀可怕的生物，它们扭曲的身躯以恐怖的速度前行，每个都残缺不全，陷入疯狂。\n然而这里有些东西让人感到不对劲，与这些畸形怪物的恐怖无关。某种潜移默化的既视感开始渗入我的脑海。当我穿过峡谷时，一条临崖的小路出现了。既视感愈发强烈。这里有更多的怪物，我很快将它们击败，但当我转过弯时，一座巨大的石门出现在视野中，那挥之不去的感觉变成了顿悟。我以前来过这里；这是通往传火祭祀场（火継ぎの祭祀場）的路，与我在游戏开始时的地方一模一样。这是正常游戏世界的黑暗翻版吗？是游戏虚构世界的一部分吗？我们还在叙事空间内吗？还是这是开发者在这庞大作品中隐藏的彩蛋：一种打破第四面墙的暗示，让玩家知道毕竟这只是一款游戏？\n但这个地方带来的感觉更像是恐惧，而不是内部笑话。上面的世界，即使在这秘密区域之外，已经是破碎和折磨的了，但这里更黑暗，更可怕，更堕落，完全没有希望。这无疑是一个回收利用的区域，使用了相同的数字艺术资产，但以某种方式，它与我在这些游戏中体验的世界并不矛盾。我是被困在时间循环中吗？另一个维度？平行世界？穿过石门的拱门，我可以看到院子里蹲着的审判者古达（灰の審判者グンダ）的身影。与游戏开始时的灰烬墓地（灰の墓場）一模一样（附图2）。我怀着忐忑的心情迈过大门，他抬起巨大的身躯，是我的三倍高，拿起他的巨大长柄战斧冲向我，跃起数十英尺，然后让他的全部重量砸在石头上。我勉强躲开。Boss战开始了，他的生命条出现在屏幕底部，但这次显示的名字不同。这次是：英雄古达（英雄グンダ）（附图3）。他的外貌也变了。他的眼睛闪着红光，动作更凶猛狂野，攻击模式更变化多端，难以预测，毫不留情。\n与第一次相比，这场战斗明显更加黑暗和残暴。难度也高了一个数量级。经过大约一分钟的奔跑，努力在他追击我时找准方位，我误判了他的攻击模式，他用长柄战斧的残酷重量击碎了我的角色。屏幕上出现了“你死了”的信息。我的角色在离我惨死地最近的篝火处复活。我感觉到这将是一场艰难的战斗。《黑暗之魂》系列中的Boss战很难，通常需要八九次死亡才能克服挑战。它们的难点在于识别Boss的攻击模式，让玩家能够系统地躲避攻击和利用空隙。耐心、观察和计划会得到回报，急躁和贪婪则会受到严厉惩罚。需要一种禅意的超然态度。我在精神上准备好要死很多次，直到掌握古达的攻击模式。\n我不想浪费时间击败古达和我之间的所有怪物。那太麻烦了。我冲过那些恐怖和畸形怪物，举着盾牌像人形冲撞车一样，只杀掉那些无法绕过的怪物。探索模式的兴奋感已经消失。我对我在哪里、为什么在这里、在这里意味着什么不再感兴趣。我完全专注于击败古达这个任务。这个空间曾经是恐惧和奇迹的地方，现在只是一个需要尽快穿越的地方。回到门前，现在入口处有一层薄雾在旋转。这是一个屏障，一个只有我能通过的封闭区域，其他怪物无法跟随。它是一个亲密的空间。薄雾对我来说意味着院子现在与游戏世界的其余部分隔离开来，在那之后，只有古达的挑战在等待。当古达被击败时，薄雾会消失。穿过薄雾，战斗开始，我在一系列时机不当的闪避后被迅速肢解。这开始了一个死亡、复活和冲回院子的循环。\n起初，我在无尽的死亡中保持冷静。每次死亡只是一次试探性的任务，以进一步了解我的对手。每次尝试，我都能更好地识别他的动作，更一致地躲避攻击，在不受惩罚的情况下偷袭他。我每次都离击败他更近一点，但在大约第20次死亡后，挫败感开始显现。每次失误都引发咒骂和愤怒的嘟囔。我的挫败感让我玩得更糟，进展开始倒退。在这苦难的最低谷，一个来自另一个世界的恶意玩家入侵了我的游戏，在我与凶残的亡灵攻击犬搏斗时，从背后刺杀了我。这导致我失去了一个珍贵的东西，一个人形余烬（残り火），用于在死亡后恢复角色的全部潜力。挫败感不断积累。\n最终，古达的攻击过于狂野，方向变化多端，节奏既慢又快。我对这个系列很有经验，期待它会很难，并且享受挑战，但我遇到了瓶颈。我感到一丝受挫的骄傲，但我拿起笔记本电脑，向更聪明的玩家寻求策略来解决我的问题。看了几个视频后，我对古达的攻击模式有了新的见解：我应该使用的武器，应该等待的时机，攻击的时机，躲避的时机，以及无数微小的细节都会对我有帮助。战斗仍然困难，死亡不止，但我现在清楚地看到了前进的道路。这只是一个我是否有足够技能去执行的问题。\n第十次尝试，带着这些新的策略，我感受到了。这就是那次。必须是那次。带着坚定的决心，我全力以赴，每次闪避都准确无误，等待合适的时机挥剑，抵制住过度攻击的冲动。在削减了他的生命条并躲避无数疯狂的攻击后，古达被击倒在地。我看到屏幕上出现提示，表示可以执行特殊的终结一击。我按下正确的按钮，我的角色用剑刺穿了对手的身体。他发出雷鸣般的野兽般的吼叫，他的身体在一片灰烬中消失。屏幕上闪现出胜利的信息：“火的继承者被消灭了。”一切都结束了，我胜利了。我长舒一口气，全身充满了满足感和兴奋的强烈混合。但我到底得到了什么？这场战斗残酷、令人沮丧、让人意志消沉，但最终，击败Boss的快乐只来自于我学会了他的攻击模式。而且我还需要借助互联网的指导。这不是一次完全的失败，但也不是一次完全的胜利。我感觉到在这次经历之外还有更深的奥秘，但现在还不是我发现它们的时候。我关闭了游戏机，休息眼睛，思考。那次经历是什么？它意味着什么？\n破碎的叙事 上面的现象学描述展示了在游戏过程中不断变化的心理模式。这些模式从寻宝到敬畏的探索，从文本分析到视觉解读，从理论思考到实际操作，从挫折到狂喜，以及这两者之间的一切。游戏的整体流程在与世界的具体挑战进行身体上的互动和对所见所闻产生的好奇之间不断切换。总体而言，游戏系统的推动和拉动作用使玩家始终处于悬念之中，不仅因为环境的挑战，还因为游戏中的真相从未以连贯的方式揭示。游戏的呈现极其丰富和暗示性，但总是留给玩家更多的问题而不是答案。要理解这一系列游戏如何作为一个整体运作，有必要勾勒出基本的游戏机制与反复出现的叙事主题的关系，从而赋予游戏整体体验一种统一的逻辑。\nDark Souls系列在RPG类型的其他游戏中独树一帜，因为它几乎没有向玩家提供任何叙事说明。此外，它也没有给玩家一种续集之间有任何关系的确定感。将该系列联系在一起并使其被认作续集的是游戏设计的连续性、有限叙事中的概念主题以及某些情感特征，这些都是该系列的标志。从这个意义上讲，该系列非常符合东浩纪的数据库理论，因为传统意义上的叙事几乎完全不存在。现有的叙事说明——无论是电影“过场动画”、对话还是沿途找到的文字碎片——都以一种抽象和神话创造的方式运作，玩家很容易忽略。对游戏世界的明确理解最终并不是享受游戏的必要部分，尽管对某些玩家来说可能很有吸引力，因此被简化为体验的外围方面。\n每款游戏都发生在一个不同名字的王国中，这些王国处于后末日的破败状态。这种破败状态的原因不明，但总是与“不死的诅咒”有关，这种诅咒使居民无法永久死亡（他们不断转生），最终变成疯狂和怪物化的样子。这被称为“空心者”（hollow），而玩家在游戏中创造的化身也受到这种诅咒。这种诅咒以黑暗标记（ダークリング）表示，一个框住黑暗虚空的闪烁火环（见附图4）。系列中暗示，只有通过猎杀他人以收集“灵魂”，才能抵抗空心化的影响并最终避免发疯。在每款游戏中，玩家的任务是“链接火焰”（火を継ぐ），即复活死寂世界的过程。这涉及到击败五个强大的实体——在黄金时代的堕落王国的始祖和领主——并收集他们的灵魂，以便用作催化这种重燃的燃料。一旦玩家将这些伟大的灵魂吸收到自己体内，他们也必须牺牲自己作为新纪元火焰的燃料。\n乍一看，将所有三款Dark Souls游戏联系在一起的概念主题似乎只是对无数西方文明神话的另一种演绎，这些神话围绕年轻一代为了振兴传统而做出牺牲。然而，Dark Souls系列通过对其有效性的细微质疑并削弱个人能动性的概念，复杂化了这种典型的英雄公式。Dark Souls系列展示了这个循环不仅是社会通过代际生存的必要过程，而是一种对自然秩序的绝望斗争，因而是一个收益递减的循环。在第三款游戏的结尾，重燃过程已经退化到世界无法修复的地步。玩家的任务不过是在一系列逐渐退化的现实循环中的最后阶段。\n灵魂的概念是叙事和游戏玩法中的另一个重要概念主题。在每个游戏世界中，只有少数几个角色尚未变成空心者，你可以与他们交易物品。你通过杀死空心者和遍布世界的怪物获得的灵魂是你用来与这些角色交易的货币。这些角色可以成为你战斗中的盟友，但在游戏过程中，许多角色最终会变成空心者并攻击你。从叙事的角度来看，这在那些仍保持理智的人之间营造了一种不信任的氛围，因为他们可能只是因为他们是掠食者。这一概念也从叙事中渗透到游戏中。游戏内置的在线功能意味着几乎在任何时候，其他玩家都可以侵入你的世界，杀死你并夺取你身上的灵魂供自己使用。这反映了村上春树和大泽的诡异他者概念。你的敌人本质上与你相似；因此，在Dark Souls的世界里，无论是从叙事还是游戏角度，你都不能真正信任任何人。\n灵魂也是你角色变得更强大的主要方式。当你获得灵魂时，可以用它们来增强和升级你的角色和你拥有的武器。这对于探索世界更危险的部分至关重要。然而，当你变得更强时，角色的升级成本呈指数增长，而收益却是递增的。这需要进一步探索世界的危险区域以寻找更强大的灵魂。与灵魂任务相关的重要游戏机制在玩家探索世界时会产生一种持续的紧张和危险感。当你的化身被杀死时，所有未使用的累积灵魂都会掉落在他们死去的地方，但化身本身会在最近的“篝火”（见附图5）处复活。这些篝火是标志着你进度的安全港湾。它们之间间隔极远，必须由玩家“点燃”才能使用。玩家可以在篝火处休息而不必担心攻击，甚至可以用它们传送到其他已经点燃的篝火。通过篝火，玩家得以探索世界。当玩家的化身在篝火处重生时，他们有一次机会回到死去的地方并找回他们的灵魂，灵魂以一个绿色的旋转粒子球体悬浮在地面上。如果玩家在找回这些灵魂之前再次死亡，灵魂将永远丢失。因此，玩家在篝火之间的危险旅程中总是面临巨大的风险。除了作为安全港湾外，篝火还是玩家补充化身健康和物资的地方。然而，这可能是双刃剑。当敌人被杀死时，他们会无限期地保持死亡状态，但每当玩家在篝火处休息时，他们会复活。在这些破碎的诅咒王国中，没有“免费的午餐”。所有进展都是艰难获得的，并充满风险。\n环境叙事 尽管在《黑暗之魂》系列中故事情节被弱化，但它并非完全不存在。理解游戏世界的神话主要通过解释性的循环，这种循环由丰富的视觉信息和散布在世界各地的物品附加说明文字驱动。整个系列传达的主要氛围显然是后末日的，第三部作品更是将这种美学推向极致。在名为洛斯里克高墙的区域，玩家探索一座完全荒废的梦魇般的哥特式城堡。灰白皮肤、眼窝空洞的可怜人跪在地上，双手合十祈祷，崇拜着巨龙的尸体和似乎正在变成细长树木的人类尸体。看起来这里被某种邪教占据。这些人通常无视玩家，但他们由士兵看守，这些士兵一看到玩家就会恐怖地尖叫，召唤同伴来迎战。这里的基调无疑是后末日的，但没有任何解释。\n这种叙事风格通常被评论家和设计师称为环境叙事。在游戏的任何时刻，玩家在《黑暗之魂》系列中不断自问的中心问题是：这里发生了什么？自然，解释会有所不同，但发现过程是激发《黑暗之魂》游戏魅力循环的一个重要部分。在2010年的游戏开发者大会上，Arkane Studios的设计师Harvey Smith称赞了这种叙事风格的重要性，认为这是视频游戏作为一种媒介所独有的机会：\n“……环境叙事是一种主动的叙事方式。瑞士心理学家Jean Piaget表明，游戏、发现和互动是学习的关键。这种主动的学习方式创造了参与感，进而产生投入感。学生和玩家都会带着自己的经验，因此解释的行为具有个人意义……这一概念背后的理论是封闭法则。作为人类，我们有一种天生的需要，将视觉元素分类并纳入更大的框架。为此，我们得出结论。Scott McCloud在《理解漫画》中将这一概念应用于视觉叙事：‘重要的是在面板之间发生了什么。’” (Harvey)\n因此，在上述现象学描述中，我们在遇到古达时看到的略微打开的石棺，通过人类连接符号点的欲望联系在一起。Harvey认为，这导致了一种深刻的沉浸感，使一款精心制作的游戏充满了自发的突现意义。尽管这种方法在许多游戏中很常见，但《黑暗之魂》系列将这一概念推向极致，常被认为是这种世界设计的高水准标杆。\n玩家在《黑暗之魂》中通过文字、视觉和动作建立的联系跨越了大量的时间和续作。在第二部游戏中，玩家来到了一座显然是战场的城堡。巨人般、树状的尸体到处都是，这些尸体像树一样长出了根，完全占据了整个城堡，打破了厚重的石墙和护墙。最终，玩家会找到一部电梯，将他们带到城堡的下层，他们将在那里第一次与一个仍然活着的巨人boss战斗，尽管这个巨人像针垫一样被各种剑和矛刺穿。一旦boss战开始，屏幕底部会出现一个显示boss角色“生命值”的条。玩家对boss造成的每次打击都会让生命条减少，向玩家传达他们在战斗中的进展信息。虽然这个条的主要功能是信息性的，但它也执行了一个微妙的叙事功能，通过命名对手来反映其叙事意义。在这种情况下，boss的名字出现在生命条上方，称为最后的巨人，反映了它的叙事重要性。尽管在此之前看到无数与这个boss角色相似的尸体，这是唯一提供任何当前情况信息的地方。显然，这个巨人是城堡内部某次冲突的最后幸存者。一旦这个巨人被击败，它的灵魂会自动添加到玩家的库存中。附加在灵魂上的说明文字如下：\n巨人の森の地下に縛られていた／生き残りの巨人のソウル／王国を蹂躙しつくした巨人たちの长は、／名もしれぬ英雄によって倒されたという／长は砦の地下に繋がれ、封じられたが、それは何もかもが滅び去ったあとだった\n被困在巨人森林地下的巨人灵魂/这个王国的摧毁者之一/一个无名英雄打败了这个巨人首领/他被绑在堡垒地下，被封印，但一切都已经被毁。 (FromSoftware, Dark Souls II; 我的翻译)\n这里有很多信息，但它提出的问题远比回答的多。我们知道巨人是入侵者，这解释了城堡的状态和无数的尸体。我们知道这个特定的巨人是巨人中的一个领导者，我们也知道他被打败并被俘虏，这就是我们找到他现在这种状态的原因。但他们为什么要入侵？这发生在多久以前？玩家知道了发生了什么，但并没有更多信息。这种不断的神秘感是玩家如何解释他们在游戏世界中看到、做和体验的一个重要方面。随着他们继续在游戏中前进，其他带有说明文字的物品将为整体画面添加更多信息，但有时是矛盾的。每次披露都会带来新的隐藏。这些巨人的谜团在我们在第三部游戏的开始遇到其中一个尸体时更加复杂。洛斯里克（第三部）和德兰尼格（第二部）之间是否存在连续性，还是这仅仅是一个“彩蛋”，是对系列长期粉丝的一个小惊喜？这是不可能知道的。\n这种叙事风格催生了无数的论坛帖子和YouTube视频，致力于研究分散的说明文字和环境信息之间的联系，并以此为证据提供关于世界的连贯解释。这些视频中的一些非常受欢迎，获得了数百万的观看。YouTuber VaatiVidya制作了无数视频，基于游戏内的证据创建了关于游戏背景故事的理论。然而，他不仅止步于此，还通过挖掘游戏代码并检索通常在游戏的正常商业版本中无法访问的信息片段（音频文件、缺失的场景等）“揭示”叙事内容。这需要特殊的工具和代码知识来发掘这些片段，指向了一种完全不同的消费虚构作品的模式。VaatiVidya的许多视频经常超过一百万次观看，他通过对游戏神话的个人解释成功地将其职业化。这仅仅是因为游戏本身提供的信息非常少。在游戏设计师在游戏内提供大量虚构历史的情况下，例如流行的西方RPG《上古卷轴V：天际》（Bethesda Softworks，2011），对游戏神话的理论研究几乎没有意义。显然，这些理论在粉丝中引起了激烈的争论，因为游戏中内置的证据实在太少，无法得出定论。\n有趣的是，虽然在英语世界中存在数百甚至数千个这样的视频，但在日语网络区域几乎不存在这种内容。YouTube上有成千上万来自日本内容创作者的关于《黑暗之魂》的视频，但他们很少处理从叙事片段中解构神话。显然，这种做法在日本的系列粉丝中不太普遍，他们似乎更喜欢制作分析游戏系统的内容（教程、指南）或制作实况视频。这似乎证实了东浩纪的观点，即新一代御宅族对重建作品中潜在的大叙事不感兴趣。对于他们来说，《黑暗之魂》丰富的情感特征数据库已经足够了。\n第四章: 后现代意识与末法意识 末法 正如前一章所述，“点燃火焰”的命令是驱动《黑暗之魂》系列的核心主题；它构成了推动玩家前进并赋予游戏进展意义的主要目标。点燃火焰带有许多传统西方神话的意味，集中在神灵和英雄展开宏大的任务，以恢复王国或领域的正当状态。在许多方面，RPG类型可以被视为围绕这一设想的一系列模拟游戏。从表面上看，《黑暗之魂》似乎没有什么不同，只是氛围更加暗黑，表现更加抽象。然而，当我们审视这一概念在整个系列中的发展时，很明显在《黑暗之魂》中，点燃火焰实际上是走向消亡的下坡路。点燃火焰——就像古老谚语“传递火炬”——代表了社会及其结构价值观的延续。因此，它大致类似于意识形态在我们社会中所扮演的角色，即通过构建我们对世界的体验的宏大叙事将价值观从一代传递到另一代。《黑暗之魂》的基本结构质疑这一过程的价值，暗示其中存在深刻的腐败，并且一代又一代试图维持微弱火焰的努力不过是人类虚荣心的表现。正是在这种点燃火焰的隐喻中，我们看到了通过佛教形而上学反映的大泽和东浩纪对日本文化景观的后现代评估。\n末法（まっぽう）的概念是佛教早期的一种古老的末世论概念，但直到镰仓时代（1185-1333）才在日本受到广泛关注。据推测，末法是在印度受到希腊人入侵期间发展起来的，但直到6世纪传入中国后才具体出现：\n“当这种思想传入中国时，在南北朝和隋朝建立期间渗透进了大众意识。重要的是574年周武帝对佛教的迫害。”（Bloom，第145页）\n末法思想，像其他类型的启示性思想一样，往往在过渡、冲突和不确定的背景下出现。从南北朝到隋朝的过渡标志着从长期的政治混乱到统一的中国帝国的转变。镰仓时代也反映了一个政治混乱的时期，武士家族篡夺了京都名义上的中央政府。在这些时期，末法概念在日本佛教机构的神学话语中变得弥散，产生了许多独特的日本佛教形式，包括净土宗和日莲宗35——这些佛教形式至今仍然流行。这个词，可以字面翻译为“法的终结”，指的是佛陀教义中包含的真理随着时间的推移会失去其相关性，直到它们在世界上不再发挥任何功能或目的。\n总的来说，根据这种模式，佛教有三个时代。佛陀去世后最初的500年被称为正法时代（しょうほう）。在这个时代，佛陀的教义按照应有的方式被实践，佛陀教义的精髓对所有人开放。接下来的500年被称为像法时代（ぞうほう），尽管佛陀的教义以各种形式和仪式形式实施，但它们已经失去了法的精髓。像法时代之后的10,000年被称为末法，在这个时代，形式和精髓都已经衰退，通过圣贤或个人努力实现觉悟的可能性完全为零。在这个时代，佛教本身最终会被遗忘，痛苦和纷争将持续不减。\n从基本视觉层面来看，《黑暗之魂》是末世氛围的。此外，每次重新点燃火焰时都会逐渐消逝的火焰主题以及法的传递越来越弱的启示性观念，像一场漫长的电话游戏，直到信息完全丢失，都有着强烈的概念平行。这甚至与大泽的三种意识形态、虚构和后虚构（动物）时代之间存在平行。大泽的论点基于这样一个观点：宏大叙事变得越来越弱和腐败，直到没有什么能够支撑它们。历史上有趣的平行现象将后现代主义和末法思想以意想不到的方式联系在一起。\n尽管在镰仓时期末法一词广泛流传，但解释却相当多样。然而，这个词在日本宗教生活中的起源通常与一个名为《末法灯要记》的文献的出现有关。这个文献通常归因于天台宗的创始人最澄（767-822），但专家对此争论不休。许多人认为这只是一个净土宗僧人的伪作。不容置疑的是，它对当时主要佛教思想家的明显影响，包括法然、荣西、道元、亲鸾和日莲。比起宗教文献，《末法灯要记》决定性地具有政治色彩。最澄在其中为忽视传统佛教戒律的僧侣的做法辩护，反对政府的批评。宗教学者Michele Marra解释了支持最澄论点的逻辑：\n“……政府在批评中忘记了这些僧侣生活在末法时代，因此，他们不能适用为生活在真理理想时期的僧侣制定的规则。由于在末法时代只有口头教义存在，而实践不存在，觉悟不可达成，戒律也随之消失，因此，坚持僧侣违反戒律是毫无意义的。不存在的东西如何被破坏？同样的原因，戒律也不能被遵守。”（Marra，第287页）\n然而，《末法灯要记》在反驳中并没有止步于此：\n“《末法灯要记》暗示那些努力遵守戒律的僧侣，因此屈从于政府要求的僧侣，表现出对佛教的形式态度，忘记了其宗教的人性意义。因此，他们被比作摧毁国家的蝗虫……”（288页）\n该论著的最终结论是，那些“名义上的僧侣”，即至少表面上扮演角色的僧侣，应受到敬畏和尊重，因为他们代表了在这种情况下可能达到的最高精神成就。因此，他们应该被视为珍贵的，就像在一个没有银和金的世界里，铜被视为珍贵一样。东浩纪对“动物主义”的默认支持在某种程度上与最澄的论点有相似之处。在一个意义无法存活的时代，除了成为一个御宅族并将自己困在只有你渴望的情感特点的毛绒世界中，还有什么其他选择呢？暗示中，依附于旧式意识形态信仰的人是错觉的，甚至是危险的。这是涵盖在后现代主义范畴下的哲学广泛持有的信念。\n尽管《末法灯要记》在日本主要佛教思想家中广泛流传，但在关于末法的形而上学状况下应该做些什么的解释却十分多样。然而，绑定这些不同解释的往往是对天台宗佛教的拒绝。事实上，法然、道元、荣西、日莲和亲鸾都曾接受过天台宗僧侣的训练 36，后来他们得出结论认为该机构已经失去了对真正法的把握。这最终导致每个人都与这个强大的机构决裂，创立了自己的传统。天台宗自最澄时代以来一直是平安朝廷支持的官方宗派。在藤原氏等强大支持者以及其他重要贵族家族的支持下，它兴盛并成为日本的主要佛教形式。其与平安贵族的关系如此紧密，以至于它几乎成为一种类似于拜占庭皇帝与早期东正教会关系的国教。这里可以画出的另一个平行现象是，天台宗试图通过将其在中国遇到的许多异端佛教传统综合为一种大师解释的方式来塑造其信仰：\n“日本天台宗的第一个特征是倡导一种综合佛教，即所谓的‘一大圆满教’的理想……所有佛陀的教义最终没有矛盾，可以统一为一个综合和完美的系统。”（Hazama，第102页）\n从这个角度来看，接受末法概念的传统拒绝了天台宗的中央权威。Thomas Kasulis写到亲鸾对此的特殊反应：\n“对亲鸾来说，平安世界观的权威机构已经不再相关。在比叡山，天台宗的中心，亲鸾年轻时曾在那里学习和修行，他们全面教授所有形式的知识，无论是显教还是密教。每个僧侣都可以利用各种修行方法，突破魔罗的幻觉军队，直面现实。\n整个企业得到了平安贵族的社会、经济、文化和政治支持。然而，尽管在山上有那么多的见识和实践，镰仓日本在山谷中的世界似乎正在崩溃。政府是军事政变和几十年公开战争的结果；疾病和自然灾害广泛发生；个人为自己的解放而内省，但发现自己被内部冲突分心和困扰，这些冲突与外部世界的情况一样真实和令人沮丧。在这种情况下，圣贤之道，天台（和真言）的承诺，即与现实和自身和谐相处，只会加剧人们对真正发生了什么以及如何应对的不知所措感。这种对世界和自我的深刻不可理解和失控的感觉就是末法意识的本质。”（Kasulis，第20页）\nKasulis所说的末法意识是一种认为现有秩序无法拯救的感觉。除了Kasulis，许多学者还将末法思想的出现与异常数量的饥荒、自然灾害、军事冲突和政治斗争联系起来。然而，有理由相信镰仓时代在这些事件的实际数量上并不与日本历史的其他时代不同，但在宗教思想家感知和解释这些事件的方式上可能存在质的差异。不可否认的是，镰仓时代标志着日本文化和政治中心的巨大转变。这是从京都贵族向镰仓和东部地区的地方武士家族的转变。动荡的政治秩序无疑对天台宗观点和实践的合法性构成了挑战。从这个角度来看，末法意识是一种情感反应，回应了一个结构社会的叙事似乎无法拯救的时代。向过去的激进转变变得必要。\n毫无疑问，在我们自己的时代，我们也生活在同样动荡和过渡的时期。国家权力在全球经济体系面前已经削弱，战争已变成在国际政治边缘运作的分裂组织之间的冲突。公司从民主结构中夺取权力，不再需要在国家层面运作。即使是苹果，这个美国企业精神的象征，也在中国制造其产品，在库比蒂诺设有办公室，并在爱尔兰注册以避美国税率。在我们自己的国家，特朗普总统的政治口号“让美国再次伟大”反映了对事情正在分崩离析并需要拯救的恐惧。数字技术以深刻模棱两可的方式改变了人类互动的基本体验。如果我们接受宗教学者的总体分析，即末法意识在动荡和过渡时期出现，那么东浩纪和大泽作品中所倡导的后现代主义似乎也反映了一种新的末法意识。后现代思想主要围绕真理之死和叙事之死展开，当以这种方式看待时，它似乎相当启示性。这是对工业和国家现代时代成长的回应，转而进入信息时代和新兴全球秩序的动荡模糊中。\n以类似的方式，《黑暗之魂》系列属于这种来自生活在这种变动中的个体的末法意识情感反应的范畴。《黑暗之魂》系列的显著之处在于它在一个西方外壳和西方媒介中传达了一种传统佛教的启示性思维。然而，末法和后现代意识形态之间有一个重要区别：在镰仓时代对末法做出回应的人们是真正有兴趣通过重新概念化指导其信仰的超验价值来追求一个解决方案。对亲鸾来说，或许是最具后现代色彩的末法思想家之一，人们只有通过放弃自己的自主力量并服从于阿弥陀佛愿力的慈爱，才能逃避不可避免的退化。对日莲来说，他的解决方案是崇敬《法华经》并念诵题目。这些本质上是以不可否认的衰退为背景的精神解决方案。相比之下，后现代思想缺乏信仰成分，简化为对以往价值结构的简单否定。这成为其自身倒置的教条；无叙事的教条，无真理的教条。《黑暗之魂》传达了末法的符号学和传统，但它倡导的是一种更符合后现代思想的更为阴暗的观点。没有对抗末法形而上学轨迹的阴暗预后的超验原则。在这方面，《黑暗之魂》是一部充满日本过去，无论是现代还是古代的感性作品，但植根于对日本当今轨迹的阴郁思考。\n第一火焰：欲望之源 通过末法意识和后现代主义的解释框架，我们可以将连接火焰这一核心主题解读为跨代社会叙事的恶化，以及日本本土末世论范式的重构。虽然游戏采用了西方美学，但其设计和美学却充满了佛教形而上学影响下的独特日本感性。西方的外在形式也反映了日本战后社会和现代日本的整体情况。尽管日本国家的形成在很大程度上是由本土精英自主完成的，但毫无疑问，日本的法律、教育、政治甚至文化机构的核心构造是一系列源自19世纪和20世纪欧洲社会的概念嫁接。自1868年成为现代国家以来，日本一直是组合（組み合わせ）的杰出典范，即通过巧妙地组合现有元素创造出新的事物。\n明治政府在构建政府时，既借鉴了德国的法律体系，也吸取了英国的政治经济哲学。二战后，日本战败后麦克阿瑟推动的宪法改革进一步加剧了日本国家的拼凑性质。尽管这部宪法在公众中得到了广泛接受（尽管近来有修改的尝试，但仍在继续维护），但它仍然是少数西方人撰写的外来文件。这份文件构成了战后日本叙事的基础——这个永远和平、永远繁荣、永远服务于全球进步秩序的国家——仍然是日本国家的核心。但这种外来性并没有抹去日本的文化或历史。宪法只是提供了一个框架，真正的核心是数百年积累的文化，这些文化构成了日常生活的基础。现代日本人的生活与西方同行非常相似，但充满了源自悠久历史的感性和行为，这些历史与民主或经济自由主义没有必然联系。如果东浩纪和大泽真幸所描绘的是日本社会叙事的崩塌，那么这种崩塌主要表现为那些从欧洲继承的进步、自由和平等观念的瓦解。如果这些叙事正在衰退，那么这种衰退自然会被表现为一个崩塌的西方王国。这就像日本的西方性如死皮般剥落。如何更好地表现这一点呢？通过一幅充满幽灵和变异尸群的哥特式大教堂画面。\n尽管《黑暗之魂》系列偏重于环境叙事，但并不意味着它完全没有传统的叙事展现。然而，这种展现风格更接近于前现代的神话诗意传统，而非现代的奇幻作品。这两者的区别大致可以概括为理性和直觉原则在叙事中的区别。主要由于J.R.R.托尔金的《魔戒》三部曲的影响，大多数现代奇幻作品试图创造一个全新的世界，这个世界有着完整的虚构历史，充满了虚构的种族、语言，以及一套部分透明的公理来支配作品的内部现实。当一个作品不仅仅是一本书，而是一个跨越多个媒介和数十个单独作品的系列时，这可能会变得相当复杂。通常，这些元虚构信息的主体是以一种可以通过因果关系的理性原则来解释的方式构建和呈现的。这允许粉丝基于系列中的“证据”对他们喜欢的作品进行“理性”论证（历史、社会、物理、生物等）。当创作者创作的新内容不符合先前确立的虚构宇宙公理时，粉丝们甚至会批评他们。例如，在《哈利波特》系列中，咒语只能用魔杖施放。如果在一个《哈利波特》的衍生作品中，一个角色不用魔杖就施放了咒语，这将与先前确立的公理逻辑不一致。为了解决这个矛盾，需要建立一些额外的公理来包含先前的公理（即，这个角色可以不用魔杖施放咒语，因为他们拥有独角兽的血液，这赋予他们对魔法的更深连接）。这些公理不断演变以适应新作品，但也为了在世界构建过程中保持完整性，并避免粉丝的责难。\n相比之下，前现代的神话诗意作品几乎不尝试用理性公理来支撑其虚构现实。它们的功能是象征性或情感性的，而非字面的，因此从现代角度来看，它们似乎充满了矛盾。为什么宙斯可以变成公牛或天鹅？为什么神明可以受伤或被杀害而他们却是神？在传统神话中，发生的事情就是发生了；没有对底层公理的诉求。《黑暗之魂》几乎没有提供理性原则，也几乎没有提供全面的历史，因此其叙事风格更像传统神话。称之为完全相同可能有些牵强，但它顽固拒绝现代奇幻及其庞大元叙事的理性主义是其美学的关键方面。唯一的反驳是电子游戏本身是理性、数学的系统，装饰着叙事。因此，在这种媒介中总是存在一种奇怪的游戏与叙事之间的冲突。《黑暗之魂》通过将其机制概念化为游戏世界困扰的表达来避开这个问题，但保持细节故意模糊，以避免对元虚构进行正式和严格的理解。\n三部游戏的开头都有一个简短的电影，让玩家对他们进入的世界有一个模糊但高度暗示的框架。我们将分别考察每个游戏，作为美学演变的序列，但我们将详细分析第一款游戏，追踪其完整的叙事弧。这是因为对第二和第三款游戏中的选择的理解依赖于第一款游戏中确立的美学原则。在我们开始分析第一款游戏的介绍电影之前，重要的是要注意，尽管《黑暗之魂》是一款日本游戏，但原版中没有日语配音。换句话说，游戏中的英语配音不是本地化的结果，而是开发者为营造某种氛围而做出的美学选择。不懂英语的日本玩家必须依靠字幕来理解游戏中的对话，这强化了沉浸在异域世界中的感觉。还值得一提的是，配音演员都带有英国口音，这反映了奇幻类型跨作品和媒介长期以来的传统。这只是《黑暗之魂》西方外衣总体情感数据库的一个部分。英文音频和日文文本大致相当，但有时在重要方面有所不同。在以下引用中，我将保留原文英文音频：\n古い時代／世界はまだわかたれず、霧に覆われ／灰色の岩と大樹と、朽ちぬ古竜ばかりがあった／だが、いつかはじめての火がおこり／火と共に差异がもたらされた／熱と冷たさと／生と死と／そして光と闇と／そして、闇より生まれた幾匹かが／火に惹かれ、王のソウルを見出した／最初の死者、ニト／イザリスの魔女と、混沌の娘たち／太陽の光の王グウィンと、彼の騎士たち／そして誰もしらぬ小人／それらは王の力を得、古竜に戦いを挑んだ／グウィンの雷が、岩のウロコを貫き／魔女の炎は嵐となり／死の瘴气がニトによって解き放たれた／そしてウロコのない白竜、シースの裏切りにより／遂に古竜は敗れた。／火の時代の始まり／だが、やがて火是将灭，暗闇だけが残る／今や、火はまさに消えかけ／人の世には届かず、夜ばかり続き／人の中に、呪われたダークリングが现れはじめていた\n在古老的时代，世界尚未形成，被雾笼罩／一个灰色岩石、大树和永恒巨龙的土地／但随后有了火焰，随着火焰的出现，差异也随之而来／热与冷／生与死，以及当然，光与暗／然后从黑暗中，他们来了，并在火焰中找到了王之灵魂／最初的死者尼特／伊扎里斯的女巫及其混沌之女们／太阳之光之王格温及其忠实的骑士们／还有那不为人知的矮子／凭借王者的力量，他们向龙发起挑战／格温强大的雷霆撕裂了它们的石鳞／女巫编织了巨大的火焰风暴／尼特释放出死亡和疾病的瘴气／而无鳞白龙席斯背叛了自己的同类，龙族不复存在／于是火焰的时代开始了／但很快火焰将熄灭，只剩下黑暗／即使现在只剩下余烬，人类看不到光，只看到无尽的黑夜／在活人中出现了被诅咒的黑暗印记。是的，的确如此。黑暗印记标记了亡者。在这个土地上，亡者被驱赶到北方，被关在等待世界末日的地方\u0026hellip;（Farz; 视频片段）\n这一简短片段的音乐显然属于恐怖或悬疑范畴。开头是极简主义的；情绪化的弦乐在背景中幽幽作响，竖琴添加了不和谐的点缀，使画面充满了神秘和不安。火焰被描述为生命的起源和现实基本二元性的起源，例如生与死（0:30）。在许多西方神话中，火或光通常用来代表神圣的创造源泉。这在许多现代奇幻作品中也适用，因此“光”在许多文化和神话中都具有强烈的积极意义。然而，这种隐含的情感滤镜将火焰呈现为某种险恶和危险的东西。在观众遇到这火焰之前，镜头将我们带入地球的深处。在那里，火焰像地狱般的舌头在一个巨大的洞穴张开的嘴巴中燃烧。无论是视觉效果还是音乐，都没有什么鼓励我们将火焰视为神圣的生命源泉。它更像是可憎的生命源泉。火焰出现时低音提琴和铜管的引入表明它既可怕又强大（0:50）。整个片段在不和谐程度上接近挽歌风格，特别是弦乐部分。尼特的出现伴随着图瓦喉音歌手的引入，他们在背景中凶恶地低吟，进一步加剧了恐惧感（1:15）。这些图瓦歌手在介绍剩下的三位神祇时继续吟唱：伊扎里斯的女巫，格温大人，以及隐秘的矮子（1:15-1:46）。这些喉音歌手虽然是微妙的补充，但为传统的西方管弦乐增添了“东方”的神秘感，突出了故事中的神秘色彩。\n这种投射在火焰上的恐惧滤镜与佛教形而上学和隐喻语言一致。在巴利语经典中，《火焰经》（Adittapariyana Sutra），也称为《火焰讲经》，使用火焰作为隐喻来代表由欲望引起的苦（dukkha）。传统认为佛陀向火崇拜的苦行者讲授这部经文，以便将他们转化为他的精神实践：\n比丘们，一切皆燃烧。什么在燃烧？比丘们，眼在燃烧，可见形式在燃烧，视觉意识在燃烧，视觉印象在燃烧，任何因视觉印象而产生的感受，愉快的、痛苦的、既不愉快也不痛苦的，都在燃烧。燃烧着什么？燃烧着欲火，燃烧着恨火，燃烧着愚昧火；我说它燃烧着生、老、死，燃烧着悲伤、哀悼、痛苦、忧愁和绝望。（Thanisarro）\n在这段经文中，火与我们对感知的执着所带来的痛苦混为一谈，同时也是现实基本二元性的一个方面，如生/死和快乐/痛苦。这与《黑暗之魂》神话中火作为差异起源的概念相平行。当我们在电影中看到地球深处的翻滚火焰时，它并没有呈现为神圣的创造之光，而是作为混沌的、原始的欲望引擎。从这种角度来看，恐惧的情感滤镜与佛教隐喻语言密切吻合。此外，随着这种火焰的出现，令人恐惧、可怜的僵尸般的剪影从黑暗中浮现。他们愚蠢地摇摇晃晃地走向这火焰，身体姿态笨拙，暗示着粗糙和未形成的心理。他们没有圣经传统中高贵原始人的尊严。这些蹒跚而行的可怜虫将被火焰的力量转变为如格温大人那样的崇高个体，但这种英雄转变有些污秽和怪异。\n格温大人的美学显然让人想起宙斯、奥丁、沃坦或因陀罗般的神祇，因为他是一个统治光明和掌握雷霆的众神之王。然而，我们对他的第一印象却是一个疲惫的老人（1:34）。火焰在他面前燃烧，但他的眼睛浑浊，他的头发狂野、凌乱，看起来像稻草一样干燥，仿佛被长时间暴露在热中所损坏。他似乎没有传统神话中父亲形象的活力或智慧。此外，他身后的骑士军团与典型的西方骑士形象几乎没有共同之处。没有高贵、勇敢或个人英雄主义的符号。这些骑士是无脸、诡异的存在，过于纤细和高挑，具有昆虫般的特质，因为他们的角盔略微让人联想到钳子。他们在一个巨大的洞穴檐下整齐地站成无尽的队列，静如雕像。这些被火焰转化的可怜虫——即被欲望转化的——成为了一个小型的神祇团。\n有一个父神（格温），一个混沌母亲/生育神（伊扎里斯的女巫），和一个冥界神（尼特）。然而，“隐秘的矮子”的存在打破了这个神祇团的连贯性。他被称为这些王者之一，但他却不参与斗争。他那扭曲的小小身影贪婪地蹲在火焰旁边——就像佛教神话中的饿鬼（APPX. 图6）——对这些伟大神祇的阴谋漠不关心，专注于他手中那小小的火焰在他烧焦的手掌间跳动（1:40-1:52）。这一形象进一步加深了火焰吸引可怜生物的嫉妒，即使它对其持有者造成伤害的印象。确实，尽管伊扎里斯的女巫拥有更为尊贵的形象，她的手看起来也像矮子一样烧焦了（1:28）。其他神祇也是如此，每个人都在他们面前握着一团火焰，凝视着它。这是火焰时代的真正起源，那些摇摇晃晃的实体以王者自居的盲目欲望。\n许多西方神话和奇幻的经典特征在这里得到应用，但它们由与火焰相关的恐惧滤镜定义。即使在火焰时代介绍时，这种滤镜也没有消失。我们看到一个美丽的王国，但光是暗红色的，暗示一个已经在黄昏的时代。当与龙族的冲突中使用低音提琴、合唱团和铜管乐器时，表示战争的能量和恐惧，伴随和平王国的音乐则回到了开头部分的悬念和预感模式。总体而言，火焰时代的特点是静止、惰性和预感。这部介绍电影的整体重点不在于世界的起源，而在于其当前的衰退状态及其看似污秽的起源。抛开佛教形而上学，这部介绍电影牢牢确立了系列的末世氛围。亡者的诅咒，通过黑暗印记表现，也与火焰联系在一起：一个环绕空洞深渊的燃烧圆圈。这与早期的原始火焰展示相平行。它也是一个在广阔黑暗中的微弱火焰。通过这个标记，火焰本身与亡者的诅咒联系在一起。在佛教形而上学中，燃烧的欲望火焰引发了黑暗印记，这是驱动轮回的能量。在这一点上，黑暗印记在游戏的符号学与电影中遇到的世界神话概述之间创建了一个有趣的接触点。\n如前所述，那些受到这种诅咒困扰的人无法死亡。如果他们被杀，他们只是转世。连续的转世导致角色“空心化”，意味着他们失去了理智。在游戏中，这是通过视觉表现的。那些变得空心的人变成了可怕的僵尸般的实体，与介绍电影中的蹒跚求火者非常相似。这也符合佛教形而上学，因为逃离无尽的轮回单调是该传统的主要目标。佛教中的最高精神成就是达到涅槃，这也依赖于与火相关的隐喻。这个词的词源字面意思是“熄灭”，就像吹灭蜡烛一样。末法意识叠加在这个概念之上的是人类制度中的某种精神退化，使人们无法熄灭保持他们在越来越堕落的轮回中的欲望之火。人们无法达到涅槃所允许的真正死亡。这与亡者的诅咒类似，通过一个无法熄灭的火圈标志。这让你一次又一次地复活，不论你被亡者、变异体还是统治这个世界的怪物杀死多少次。这个世界的一切都被这个诅咒的影响所驱动。\n这一大约三分钟的序列是玩家用来理解系列第一款游戏前十几个小时的主要叙事框架。比起叙事内容本身，这部简短电影建立的整体基调和氛围简洁地总结了游戏的全部内容。虽然介绍中出现的角色在第二款游戏中不会回归，但他们在第三款游戏中会以一种间接且诱人的方式被提及。因此，这部电影作为魔法循环的温床，当玩家渴望“连点成线”时，这个循环就会出现。下一章将回到现象学角度，讲述这个开场电影如何在玩家的脑海中保留并在完成《黑暗之魂》第一款游戏的大约40小时内发生变化。我们不会以这种方式看待第一或第三款游戏，但会追踪第一款游戏中建立的叙事和游戏主题的发展。这将表明，连接火焰的主要任务是一个无意义的活动，导致“火之时代”越来越堕落的轮回。\n第五章：成为英雄的意义是什么？ 葛温，薪王 终于，在穿过一个充满恶臭的变异食人族棚户区，跋涉过浓密的毒沼，击败一个会喷吐熔岩的半女巫半蜘蛛的恐怖存在之后，我在地下大教堂塔楼的深处找到了钟。一个愤世嫉俗、心碎的骑士告诉我关于这个钟的信息，但没有解释我为何要寻找它。他说几乎不可能找到，还嘲笑了我，但并未说明其用途。此时，我已在游戏中花费了二十小时，找到它几乎是偶然。我一直在这个可怕的荒原中游荡，无数次死亡，但获得了武器，通过吸收强大的灵魂来增强自己的力量。钟旁有一个杠杆，我必须去调查一下。果然，它不仅仅是装饰；当我操作我的角色站在它前面时，屏幕上出现了一个消息框。我可以拉动它，于是我照做了。这个杠杆可拉动这一事实几乎是一种命令。钟声响起，进入了一个过场动画。在远处，一个无脸巨人，身体像被火烧焦了一样斑驳的灰黑色，听到了钟声，拉动了一条巨大的链条。这使得一座似乎建在悬崖侧面的堡垒的大门升起。我不确定这是在哪里发生的，也不明白为什么，但显然世界的一部分对我开放了。我可以继续穿过大教堂，深入被遗忘的深处，但我被一种强烈的欲望所驱使，想回到地表，回到传火祭祀场（火継ぎの祭祀場），这个破碎世界中唯一的避难所。\n回到那里，我注意到有些东西变了。整个神殿的石墙上回荡着一种奇怪的咔哒声。顺着声音，我走到了篝火旁边的破旧礼拜堂般的建筑里。在那里，我发现一个巨大的蛇状生物从深处冒了出来。原本礼拜堂里的石地板现在变成了深渊，这条蛇从这黑暗的空洞中探出头来，身体的其余部分延伸到下面的深处。它全身黑色，眼睛巨大、鼓胀，橙色的眼睛有蛇一样的瞳孔，上唇上挂着奇怪的肉囊，类似胡须。他有一个鹰钩鼻和巨大的牙齿，块状，从血红色的牙龈中凸出，不停地咬合——这是咔哒声的来源。从这个角度看，他几乎像一匹马。他的外表确实非常怪异，但他并不敌对。当我接近这个生物时，出现了一个对话选项。我按下控制器上的X键开始了互动，他用一种完全与外表不符的老者声音说：\nおお、お主か／目覚ましの鐘を鳴らしたのは／わしは世界の蛇、王の探索者フラムト／大王グウィンの親友じゃ／目覚しの鐘を鳴らした、不死人の勇者よ／お主に、不死の使命を伝えたい／よいかな？\n“啊，是你吗？你是那个敲响觉醒之钟的人吗？我是始祖蛇，寻王者弗拉姆特，大王葛温的亲密朋友。敲响觉醒之钟的不死勇者，我想告诉你你的命运。你愿意接受启示吗？”\n我的头有点晕。敲钟的目的是为了见这个奇怪的蛇？我真的要相信他是我在开场动画中看到的那个人的朋友吗？这些钟是某种测试吗？尽管有这些疑问，这却是游戏中很长一段时间以来第一个明确的叙事发展。我很好奇，也没有理由忽略他。我接受了：\nよかろう。ならば伝えよう／不死人の勇者よ／お主の使命は…大王グウィンを継ぐことじゃ／かの王を継ぎ、再び火を熾し、闇をはらい、不死の徴をはらうことじゃ／そのためには、まず王都アノール・ロンドで王の器を手に入れねばならぬ\n“很好。那么我很高兴与你分享。不死勇者。你的命运是…继承大王葛温。为了重新点燃火焰，驱散黑暗，解除不死的诅咒。为此，你必须前往亚诺尔隆德，获得王之器。”\n这是我第一次直接接受一个具体任务的明确指示。没有真正拒绝的理由，从某种意义上说，也无法拒绝。我可以按自己的方式继续探索这个世界，但到了某个时候，我知道我会耗尽探索的地方。毕竟这是一个游戏，它是有限的。只要我有意愿玩这个游戏——这基本上意味着探索更多的世界，收集更多的物品，并通过吸收灵魂来增强力量——最终我会找到他所寻找的这个器皿。在没有其他事情可做的情况下，真的有什么选择吗？\n弗拉姆特建立了一个关于继承和从世界瘟疫中救赎的叙事，但这个叙事本身就非常暧昧。在经过更多艰苦的试炼，前往游戏世界的遥远区域后，我获得了这个形状巨大的圣杯。我再次带着它来到蛇面前，他带我到地底深处的一个房间。他称之为传火祭祀场，并告诉我它是为继承大王葛温的继承者准备的。这个房间只是一个平台，坐落在一对巨大的石门前。他指示我将器皿放在门前，并告诉我作为“王的继承者”的职责。他告诉我，我必须用与葛温的灵魂同等力量的大灵魂填满王器。他列举了这些灵魂：墓王尼特，伊扎里斯的女巫，新伦多的四王，他们继承了葛温的灵魂碎片，以及白龙希斯。除了四王之外，这些“王”都被称为火之时代的英雄和始祖——那个没有不死诅咒的黄金时代。我感觉如果我用这些灵魂填满器皿，门就会打开。我按照他的指示，杀死了尼特，伊扎里斯的女巫和白龙。他们没有向我透露任何信息。当我遇到他们时，他们只是攻击我。最终，在击败新伦多的四王并获得其灵魂后，另一个与弗拉姆特一模一样的蛇出现了，用完全相同的声音提出了一个反提案：\n…ようこそ、不死の勇者よ／我は、世界の蛇、闇撫でのカアス／貴公ら人を導き、真実を伝える者だ／では、我は隠さず真実を語ろう／かつて火のはじまり、貴公ら人の先祖は／古い王たちの後に、四つ目のソウルを見出した／闇のソウルだ／貴公らの人の先祖は、闇のソウルを得て、火の後を待った／やがて火は消え、闇ばかりが残る／さすれば、貴公ら人、闇の時代だ／…だが／王グウィンは、闇を恐れた／火の終わりを恐れ、闇の者たる人を恐れ／人の间から生まれるであろう、闇の王を恐れ／世界の理を恐れた／だから奴は、火を継ぎ、自らの息子たちに、火を率い、縛らせた／貴公ら人が、すべて忘れ、呆け、闇の王が生まれぬように／…我は世界の蛇／正しい時代を、王を探すもの／だがもう一人の蛇、フラムトは、理を忘れ、王グウィンの友に堕した／よいか、不死の勇者よ／我カアスが、貴公に、正しい使命を伝えよう／理に返して火を継ぎ、今や消えかけの王グウィンを杀し／そして、四人目の王となり、闇の时代をもたらすのだ\n“我将毫不留情地分享真相/在火焰的开始，古代王者发现了三种灵魂/但你的祖先发现了第四种独特的灵魂/黑暗灵魂/你的祖先夺取了黑暗灵魂并等待火焰熄灭/很快，火焰消失，只剩下黑暗/于是开始了人类的时代，即黑暗时代/然而…葛温大王在黑暗面前颤抖/他紧紧抓住他的火焰时代，极度恐惧人类/以及将来会在他们中诞生的黑暗王者/葛温大王违背自然法则/通过牺牲自己来延续火焰，并命令他的子女引导人类/葛温模糊了你的过去，以防止黑暗王的诞生/我是始祖蛇/我寻求纠正过去的错误，发现我们真正的王/但另一条蛇弗拉姆特失去了理智，成为了葛温大王的朋友/不死的勇者，我们站在十字路口/只有我知道你的命运真相/你必须摧毁那正在消逝的葛温大王，他依附于火焰，违抗自然/并成为第四位王者，从而开启黑暗时代！”\n卡斯提供的关于这个世界的历史和形而上学基础的简略草图并不清楚。人类时代是黑暗时代？当他说我的祖先时，他指的是开场动画中的那个不起眼的小矮人吗？小矮人是“人类”，而其他人不是吗？“黑暗灵魂”究竟是什么？这些任务发布者，弗拉姆特和卡斯，他们的外表怪异而阴险，似乎有着完全不透明的动机。重新链接火焰——就像寻王者弗拉姆特希望的那样——以迎来没有不死者的另一个火焰时代，这难道不是一件好事吗？从幻想题材的普遍惯例来看，黑暗时代似乎是需要避免的东西。任何倡导黑暗时代的人，肯定是“坏人”，但我对这个世界以及发生了什么和当前正在发生的事情了解太少。这不是有可能吗？卡斯所说的是真的？葛温，紧紧抓住火焰，拒绝了世界的自然循环，带来了一场灾难？在游戏的上下文中，无法最终验证他的陈述是否真实。无论我相信哪条蛇，我的任务实际路径似乎都没有改变。此时，这只是我个人的解释问题。我已经用必要的灵魂填满了王器，我打算打开通往传火祭祀场的门。这是游戏的逻辑。这也是探索的逻辑。我只是必须知道一旦我执行这些任务会发生什么。我想看看游戏会向我揭示什么。\n我回到传火祭祀场，将最后一个大灵魂释放到王器中，厚重的石门在我面前打开，露出一个纯白的空洞，中间有一个墨黑的斑点。这似乎是一条通道。走过白色空洞，朝着黑色斑点走去，鬼魅般的白色骑士在我面前行进并消失。它们是某些过去事件的残影吗？空洞让位于一个巨大的塔楼的痛苦景象，在昏暗而病态的黄色天空中耸立。我无法确定这是晨光还是余晖。感觉就像我进入了一个停滞的状态。当我走过一些烧焦的柱子时，游戏提示我进入了“The Kiln of the First Flame”（最初的火的炉）。地板完全覆盖着灰白的灰烬，就像在沙丘中行走。有某种爆炸吗？\n在与穿着黑色盔甲的骑士战斗后（它是黑色的因为它像其他一切一样被烧焦了吗？），我在一个充满灰烬和中央有一个闪烁篝火的房间里与葛温大王对峙。然而，当我与他对峙时，他有些不对劲。在开场动画中，他似乎是父神的典型形象，但现在他看起来像个畸形的怪物。他已经空洞了。挥舞着一把燃烧的剑，他的眼睛是黑洞，肉体像木炭一样烧黑了。他一直在这里，守护着世界的最后一块燃烧的余烬，紧紧抓住火焰时代的遗迹。然而，这让他变成了一个无意识的怪物。他的头衔也变了。当我进入房间并看到他的生命条出现在屏幕底部，象征着另一个Boss战时，他的名字变成了薪の王（Lord of Cinder）。看来他作为太阳神的显赫地位已经沦为仅仅是一个燃烧的余烬。薪的意思是什么？我应该把它理解为火种吗？这就是寻王者弗拉姆特所说的继承葛温的意思吗？我必须杀死这个国王，继承他的称号，成为薪之王吗？我将自己变成火种来燃烧这个破碎的世界吗？这个国王的意义，他的意图，以及我在做什么都非常不清楚。\n然而，清晰的是一种悲剧感。与其他二十五个Boss战的音乐通常是轰鸣、黑暗和充满活力的，象征着挑战的严峻、恐怖和刺激不同，这段音乐孤独而悲伤。一架孤独的钢琴在小调中演奏着悲伤的曲调，我与这位曾经伟大的王者的疯狂躯壳搏斗。这种感觉奇异地感人但也神秘。音乐似乎暗示着这种冲突是不可避免的，是一种悲剧。但这是为什么呢？我不是来驱散黑暗并复兴火焰时代的吗？它似乎暗示所有自称为王者和王国保护者的命运是多么可悲。\n在我击败葛温大王后，我走到炉的中央，那里有篝火。当你接近时，会出现一个消息框，允许你按控制器上的X键在世界中启动一个动作。但这次出现的不是通常我接近未点燃的篝火时出现的消息（篝火を灯す），而是一个新选项：火を継ぐ。在这个时候，我可以链接火焰，接着又开始了一个过场动画。我的角色伸手进入火焰，火焰慢慢蔓延到他们的身体，吞噬他们并蔓延到整个炉子，然后爆发成一片火海。竖琴开始演奏，一个孤独的女人的声音开始唱歌，字幕开始滚动。大概这意味着一个新的火焰时代已经开始，但在此之后没有进一步的叙述或图像播放。音乐并没有暗示或确认任何胜利的感觉，而是更多的悲伤和神秘，仿佛整个40小时的游戏过程只是一个很久以前的神话的演绎。\n在玩家击败葛温之后，游戏还提供了另一个选择。这完全不明显，但我偶然从观看其他玩家的视频中了解到。如果玩家从炉中徘徊并返回传火祭祀场，会开始另一个过场动画，弗拉姆特和卡斯与众多其他蛇一起出现，宣告玩家的角色为新黑暗之王。他们从一开始就串通好了吗？如果是这样，为什么？为什么一条蛇敦促我链接火焰，而另一条蛇试图劝阻我，如果最后他们都在玩家角色面前一起鞠躬？没有答案，任何选择的意义都非常模糊。链接火焰是忠于大多数RPG试图模拟的典型英雄身份。然而，蛇形任务发布者的缺乏答案、氛围和模棱两可的动机与我拯救王国的感觉相悖。如果有的话，感觉我被操纵成了一个牺牲的羔羊，一个火种之王。\n在随后的两款游戏中，链接火焰的主题最终将被揭示为不那么英雄的，但这无法从第一款游戏中具体看出。这只是通过游戏世界的符号学和游戏行为所感受到的。要汲取的主要思想是，葛温紧紧抓住生命之火，拒绝让它熄灭，并沦为一个无意识、不死的怪物。在佛教的隐喻语言中，必须放弃燃烧的欲望之火，以逃离无尽轮回的痛苦。无尽地活着就是在轮回的业火中燃烧和燃烧。\n名为“自我”的炙热余烬 我们不会深入探讨第二款游戏，但简要回顾一下“连接火焰”这一主题的发展还是值得的。第一款游戏中的所有概念主题在这里也都存在：有一个不死的诅咒，一个象征它的黑暗标志，篝火，一个获取强大灵魂的命令，连接火焰并接替早已成灰的国王，但仅此而已。这是一个新的王国，带有新的谜团。在这款游戏中，玩家必须坐在一个被称为“渴望王座”（渇望の玉座）的王座上来连接火焰，而这个王座无疑是一个巨大的窑炉，这与第一款游戏有着间接的联系。这种连接火焰和坐在渴望王座之间的直接联系，使火焰作为佛教欲望隐喻的关系更加具体化，这种欲望引发了苦难并推动了轮回的循环。它还表明了欲望如何赋予生命以动力。与第一款游戏不同的是，玩家没有选择是否坐在王座上的权利。一旦击败最终Boss，一段旁白会开始，角色的化身坐在王座上，窑炉的门将角色封闭在内：\n火を継ぐ者、呪いをその身に引き受ける者…／貴方が火を継げば、再びソウルは育ち、同じことが繰り返される／それを望むも、拒むも…貴方が決めることです／王たるものよ、王座へ／その先は、貴方にしか見えないのです “你，连接火焰的你，你，承受诅咒的你…/一旦火焰被连接，灵魂将重新繁荣，这一切将再次上演/这是你选择接受或拒绝的事情/伟大的君主，请坐上你的王座/前方的道路只有你能看见。”\n到第二款游戏结束时，我们完全无法感觉到第一款游戏的事件是否与其续集有任何联系，但叙述者的讲述清楚地表明，“连接火焰”只是循环的一部分。值得注意的是，游戏的创意控制从FromSoftware的首席设计师宫崎英高（Hidetaka Miyazaki）转移到了年轻的下属Tomohiro Shibuya。因此，游戏在基调和风格上有许多有趣的变化也就不足为奇了。这种差异在第三款游戏中最为明显，因为导演控制权又回到了宫崎英高，第一款游戏的联系也更加明显。无论如何，在接受日本著名游戏出版物Famitsu采访时，Shibuya表示，第二款游戏确实与第一款游戏有联系，但在叙事上并不完全如此：\n“我不能完全回答这个问题，但两个设定确实是有联系的，”他说。“然而，就故事而言，我不会说有直接的联系。游戏设定在同一个星球的不同部分——换句话说，如果第一款游戏设定在北极，这款游戏就设定在南极；是那种对比。‘时间’是我们在创作时牢记的一个核心关键词。我现在不能给出具体细节，所以希望你能用你的想象力来理解。”\nShibuya对时间的强调在游戏中通过一个流浪的魔法师Straid of Olaphis的角色得到了呼应，他通过出售魔法来帮助你：\nこの地には、かの国以前にも／多くの国が存在しておったらしい／国は栄え、しかしいずれ衰えていく／あたかも火の燃えあがり、消えるが如く／繰り返し繰り返し／しかしのォ、消えかけた火はやがて蘇るのよ／そして再び国が栄える、その姿を変えながら全ては呪いよ！／ヒェヒェヒェ！／オヌシの如き呪われ人こそが、火を引き受けるのよ ヒェヒェヒェ… “在这个地方，在那个国家之前，/有许多国家曾经存在/国家繁荣，但终将衰落/就像火焰燃烧，最终熄灭/一遍又一遍/但消逝的火焰最终会复活/然后新的国家会再度繁荣，换上新的面貌/这一切都是诅咒！嘿嘿嘿！/像你这样的被诅咒之人，将承受火焰。嘿嘿嘿…”\n这种长时间周期的主题，王国在不同形式中兴衰，与佛教的末世观美学相比基督教的末世观更为相符。在基督教的末世中，耶稣会回来进行最后的审判，将人类带入永恒的天堂。这里的重点是永恒和对世间问题的最终解决。在高度异质的佛教传统中，类似的叙事也存在，但它们总是基于劫（kalpic cycles）的理念，即整个现实旋转的千禧年的缓慢转变。\n除了欲望和时间，游戏还更强调了空虚过程。这在角色Lucatiel of Mirrah身上得到了最清楚的体现。Lucatiel是一个你遇到的流浪骑士，她会偶尔在你的任务中帮助你。随着你的进展，你会在世界的不同地点遇到她，但在每次相遇中，她都会因不死的诅咒而失去更多的记忆。这让她感到恐惧，她最终向你坦白：\n私は、失うことを恐れている／記憶を、私自身を／もし、お前を殺せば／この呪いが解けると言われたら…／私はためらうことなく、剣を抜くのだろう／私は、生き続けたい 私であり続けたいのだ／そのためなら…何を犠牲にしても…／何と醜く、浅ましいことか… “失去让我无比恐惧。失去记忆，失去自我。/如果有人告诉我，杀了你就能解除这个诅咒…/我会毫不犹豫地拔剑。/我不想死，我想存在。/为了这一点，我愿意牺牲任何东西，任何东西。/这让我感到羞耻，但这是事实。有时，我觉得自己沉迷于这个叫做‘自我’的小东西。/但即便如此，我还是迫切想要保护它。/我这样感觉错了吗？如果你是我，你也会这样做吧？也许我们从出生起就被诅咒了。”\n最终，接近游戏末尾时，Lucatiel变得空虚并会攻击玩家的化身。Lucatiel的感受和不死诅咒可以与佛教的无我（Anatta）教义联系起来。这一理念是构成佛教实践的主要形而上学原则之一。该术语指的是人类内部没有构成自我的永久的、根本的物质。对这一教义的确切解释在各宗派、神学家和学者之间存在争议，但这一理念在佛教传统中的核心地位相当于苦（dukkha）和无常（anicca）。总的来说，无我教义断言构成“自我”的只是意识中的现象流，这只是自我的错觉。对这种所谓的“自我”的执着是一种主要的苦难来源，因此是宗教实践中需要克服的主要障碍之一。作为一个角色，Lucatiel体现了当我们拼命抓住我们的存在，抓住所谓的“自我”时产生的痛苦。这赋予黑暗标志之火一种略有不同的意义。它标志着玩家的化身被困在轮回的循环中，永无止境地燃烧。\n薪王，火之继承者 虽然整个《黑暗之魂》系列贯穿着许多佛教主题，但在第三部游戏的结尾，末法的联系变得尤为明显。在第三部游戏中，系列核心概念中的隐喻成熟并凝聚成一种对旧制度无效努力的毁灭性批判。这不仅体现了佛教末法的思想，还展示了一种荒凉的后现代视角，揭示了旨在推动世界前进的传统和循环。在第三部游戏中，末法意识与后现代主义相融合，形成了一种双重视角。相应地，第三部游戏的开场动画比第一部更加末日化：\nそうさね、そこはロスリック／火を継いだ、薪の王たちの故郷が、流れ着く場所さね／だから巡礼者たちは北に向かい／そして、予言の意味を知るのさ／「火は陰り」／「王たちに玉座なし」／継ぎ火が絶えるとき、鐘が響き渡り／古い薪の王たちが、棺より呼び起こされるだろう／深みの聖者、エルドリッチ／ファランの不死队、深淵の监视者たち／そして、罪の都の孤独な王／巨人のヨーム／けれどね／きっと王たちは、玉座を捨てるだろう／そして、火の无き灰たちがやってくる／名もなく、薪にもなれなんだ、呪われた不死／けれど、だからこそ／灰はのこり火を求めるのさね\n\u0026ldquo;Yes, indeed / It is called Lothric / where the transitory lands of the Lords of Cinder converge / In venturing north, the pilgrims discovered the truth of the old words: / \u0026lsquo;The fire fades and the lords go without thrones\u0026rsquo; / When the link of the fire is threatened, the bell tolls / unearthing the old Lords of Cinders from their graves\u0026hellip;Aldrich, Saint of the Deep / Farron\u0026rsquo;s Undead Legion the Abyss Watchers / The reclusive lord of the Profaned Capital, Yhorm the Giant / Only, in truth / the Lords will abandon their thrones / And the Unkindled will rise / Nameless, accursed Undead, unfit even to be cinder / And so it is, that ash seeketh embers.\u0026rdquo;\n与第一部游戏不同，这里没有起源故事，没有时间黎明时的原始场景，没有新旧神之间的战斗，只有一个正在沉入沙土中的破败王国。可怕的“朝圣者”背上绑着巨石，倒在沙地里死去，这里的神已经耗尽腐化，被钟声重新唤醒为薪之王。父神完全缺席，但他在上一部游戏中的头衔依然存在。正如葛温大人在最终BOSS战中被称为薪之王，现在这个称号变成了复数形式。几个王被点名，这也起到了预示作用。这些是需要被打败的敌人，他们拥有巨大的灵魂，将被用来重新点燃火焰。确实，当你击败这些命名实体时，不再像第一部游戏那样在屏幕上出现“YOU DEFEATED”（你打败了）字样，而是在打败一个次要BOSS时会出现“HEIR OF FIRE DESTROYED”（火的继承者被摧毁），最终击败这些命名实体时，则会出现“LORD OF CINDER FALLEN”（薪之王陨落）。\n在第一部游戏中，当玩家第一次遇到葛温大人时，他的生命条上方出现的名字在英文本地化中被称为薪之王，而在日文原版中为薪の王。这里有一个概念问题需要解决，因为游戏将日文术语“takigi”与英文术语“cinder”联系起来，虽然这些术语相关，但它们在重要方面有所不同，复杂了点燃火焰的隐喻。很容易将这一差异视为英文本地化团队对细微差别的草率忽略，但第三部游戏将这一差异加以规范。开场动画中的英文配音和击败BOSS后出现的新信息是游戏日文原版的一部分，因此必须将“cinder”和“takigi”两个词一起解释。\n“Cinder”可以科学地指火山岩，或者在更松散的意义上指炭或煤的余烬。在游戏背景下，“cinder”显然是指后者，因为它支持“点燃火焰”这一核心隐喻。当火焰熄灭时，木炭余烬仍在白灰下燃烧。可以通过揭开这些余烬并在其上放置新的引火物来重新点燃火焰。余烬在火熄灭后可以长时间保持燃烧，并已知会引发森林火灾和其他事故。当我们将其与第一部游戏的事件联系起来时，“cinder”这一隐喻似乎有非常明确的关联。在第一部游戏中，作为“被选中的不死者”，你的任务是找到死去世界灰烬下仍在发光的余烬（四个伟大的灵魂），并用它们重新点燃世界的火焰。另一方面，“takigi”意为火焰的燃料，但特指通过窑炉加工成木炭的木材。\n在这里也许可以找到与日本自身对木炭依赖的历史共鸣。木炭作为燃料的依赖是日本前现代历史上砍伐森林的主要原因。在江户时期，木材的过度依赖达到了危机点，最终导致严重的经济和政治问题。这促使幕府制定了复杂的林业政策和新的造林技术。木炭作为燃料是前现代日本社会的一个重要组成部分，因为它燃烧得更有效率，更容易控制，并且产生更高温的火焰。要制作木炭，必须将木材放在窑中，通过燃烧普通木材来加热窑。这意味着用来制作这种燃料的普通木材消耗成倍增加。木炭的巨大效用是可以获得的回报，因为未经处理的木材燃烧温度不足以冶炼钢铁或其他重要金属。例如，制造东大寺大佛像需要16.3万立方英尺的木炭。因此，木炭的生产具有重要的文明意义。然而，文明越发展，这种生产越无法维持。最终，它会留下荒芜的废地，短缺会给依赖这种燃料的社会带来严重问题。聚集燃料以保持我们这个时代的火焰愈加明亮会耗尽世界的活力，导致灾难的观点在《黑暗之魂》中显然存在。这与我们自己的时代密切相关。这种叙述在当代环境问题（如全球变暖或石油峰值）中很常见，也带有末日色彩。在日本的情况下，能源消耗的双刃性质在福岛事件后关于核能的左右翼激烈辩论中有所体现。\n第二个赋予“takigi”这个术语特殊意义的观点是，为使其成为燃料，必须通过火进行处理。每个游戏就像一个过程，玩家的化身像木炭一样被准备好，通过玩家必须面对的考验，以及点燃人类心灵的欲望之火的过程。从叙述角度看（即你的角色作为“被诅咒的不死者”渴望灵魂），也是如此，从游戏设计的角度看也是如此。实际上，为使游戏作为作品取得成功，它必须通过设计成功激发玩家继续玩的欲望之火。\n只有通过不断发展玩家化身的内在火焰，化身才准备好用作新时代的燃料。将“takigi”解读为余烬，即隐藏在死去时代灰烬下的热余烬，以及经过特殊处理以为新时代创造更强大火焰的木炭，对玩家在整个系列中的行为提供了一种有趣的解释。在每个游戏中，玩家的任务是通过收集隐藏在死去世界灰烬下的余烬（腐化的旧王灵魂），用自己作为特别准备的燃料重新点燃世界。\n在第一部游戏中，我们首次见到葛温大人时，他被介绍为“太阳的王”（太陽の王），但在我们遇到他那令人厌恶的空心状态时，他的头衔已变为薪之王（薪の王）。作为火焰时代的主要创始人，很容易将他代表为火焰的余烬，过去的影子，必须被利用来重新点燃世界。然而，我们也可以将他不愿放弃火焰以及他在巨大窑炉中的长期隔离，视为一种自我枯竭。他的存在在几个世纪里在窑中被准备好，作为新火的催化剂。蛇要求你继承他的王位，如果你这样做，那正是发生的事情。\n你的考验使你准备好作为木炭，作为你收集的所有引火物的催化剂，即你积累的所有灵魂。然而，准备木炭总是意味着为更强大的火焰牺牲大量材料。当我们将“takigi”一词的双重含义解读为余烬（过去）/木炭（未来）时，就出现了一个循环，但这是一个回报递减的循环。每次重新链接火焰都会消耗越来越少的资源，因此每次王国的转世（新名字和新王）都只是一个退化的复制品。如果在第一和第二部游戏中这只是暗示，那么在第三部游戏中这被巩固了。也许第一部游戏仍然可以按照传统西方神话来解读，即通过新的血液复兴旧制度。然而，第三部游戏的结局只能通过东浩纪的后现代主义和末法来理解，即不断减少的循环。\n这种对游戏的解读在我们比较第一部游戏与第三部游戏的最终BOSS战时更加复杂。一旦玩家收集了所有薪之王的头颅并将其放在他们的宝座上，又一个通往最初的火之炉（最初の火の炉）的路径打开了。这是第一部游戏中与葛温对峙的地方，但这个区域看起来更像是千百个过去时代的噩梦废墟，而不是巨大的窑炉。在高高的天空中，一个燃烧的圆圈框住了黑暗的深渊，像是一个地狱般的日食。它向地平线以外的某个地方倾泻下火流。你周围是巨大的城堡，堆积在一起，形成一个庞大的破碎堆，螺旋状地高耸入云。它们似乎是所有前世“王国”化身的废墟：Lothric、Londor、Drangleic，以及无数无名的时代。然而，这次炉中等待的是谁？不像你击败的那些王，它没有名字，只是“薪之王的化身”（王たちの化身）。这里的本地化实际上是与日文原版有很大不同。更合适的翻译应该是“诸王的化身”或“国王的化身”。“化身”一词（keshin）一般意为化身，但在日本佛教传统中也特指佛陀的化现。\n最后的BOSS战与玩家在这个世界中创造的化身并无不同。它也是一个与世界没有任何有意义关系的化身，只是某种外在意志的载体。它只是那些连接和重新连接火焰以维持时代衰退火焰的所有英雄的象征，但却是一个没有实质的怪物。它只是一个空的盔甲，一个连接火焰仪式的傀儡。这通过它拥有的能力和攻击模式得到了加强。游戏中的其他BOSS完全不同于玩家角色。它们的力量、能力和美学完全不同，但诸王的化身使用的技术和能力模仿你的。它的攻击模式反映了玩家在游戏中发展角色时可以采取的不同路径：巫师、敏捷的盗贼、坚定的骑士、混乱的火焰法师等。从这个意义上说，它是“迷宫讽刺”的终极体现，因为它将所有未走的路径、所有可能成为英雄的路径混杂成一个空洞的怪物。\n当这个无面实体、牺牲的英雄的集合体被击败时，你可以像过去的游戏一样选择连接火焰。第一部游戏只有两个不同的结局，现在有四个，每个结局比上一个更加末日。这里没有“好”结局。第一个结局是玩家按照指示连接火焰。就像第一部游戏一样，玩家的化身伸手进入篝火，他的手点燃了。但这次火焰没有蔓延到他的全身，导致一场巨大的火灾，而只是包裹在化身上，像死灰中的余烬一样微弱地闪烁。似乎每次重新链接火焰都会产生越来越弱的火焰，在这一系列的最终篇章中，仪式本身似乎无效。当火焰包裹住玩家的化身时，他沮丧地坐在篝火旁休息。这个火焰甚至没有力量消耗链接它的人。这个结局明显与第一部游戏中看似英雄的“好”结局形成鲜明对比，巩固了游戏与末法意识的联系。这个时代已经太过退化，无法让这个英雄仪式有任何目的或意义。在第三部游戏中，我们已经远远超过了像法（外表的时代），因为仪式甚至看起来都不像。\n其他三个结局在本质上没有不同。在第二个结局中，玩家可以召唤开火者（firekeeper）——在游戏开始时遇到的角色——到火炉中，让她熄灭火焰。在第三个结局中，通过某种晦涩难懂的仪式，玩家将这个黄昏时代固定下来，成为不死之王。这个结局特别难以达成，因为它要求你完成一系列复杂的秘密任务，几乎不可能在没有借助互联网的情况下自己发现。第四个结局特别神秘。它开始时与第二个结局一样，但当开火者去熄灭火焰时，你可以击打她。如果你这样做，一段过场动画会开始，角色的化身将开火者击倒在地，把脚放在她的头上，从她的手中取出燃烧的余烬。当这一切发生时，一直在三部游戏开场动画中担任旁白的老妇人说道：\n名も無く、薪にもなれなんだ、呪われた不死／けれど、だからこそ／灰は残り火を求めるのさね。 \u0026ldquo;名不见经传的被诅咒的不死者，不配称为薪／因此灰烬才会寻求火星\u0026rdquo;\n这句话重复了开场的一句话，其中角色被称为不配称为薪的被诅咒的不死者。这也反映了游戏核心主题的进一步退化，因为你在第一部游戏中被称为“被选中的不死者”。在这种不可理喻的残忍行为之后，你的化身将偷来的余烬举向倾泻的火流，仿佛在献祭。旁白的言辞似乎在暗示，正是因为玩家角色作为“灰烬”的不配，才使他们渴望“火星”。\n正如这一媒介和系列创作者的本性，没有一个结局是标准结局。这与游戏中的后现代意识相吻合，或许也适用于游戏中的普遍情况，因为多结局是这一媒介的常态。最终分析中，无法从游戏本身的背景中得出具体的叙述解释。每个结局都只是暗示性的，而不是决定性的。它的意义几乎完全取决于我们对其少量具体文本元素及其情感特征的感知：它的外观、声音、感觉等。第四个结局中化身对开火者的残忍行为意味着什么？并不清楚。通过手势和整体语气显然表明这些角色之间正在发生某种侵犯行为。在四个结局中，你的重连火焰之旅和消除不死诅咒的目标没有一个结局暗示这一目标已经实现。也许这个游戏世界最具后现代特色的就是这些“选择”及其完全不透明的意义。当然，无法选择任何一个结局作为标准结局，即使我们能做到，其意义也是不可解读的。这个系列唯一具体的方面是一系列情感特征、主题和游戏设计元素，使其可识别为一个系列。这个星座指向未来的无望、过去的罪恶和生活在现在的痛苦。出现的模式是，旨在复兴世界的英雄仪式被揭示为不过是一个空洞的循环，绝望的灵魂紧握着即将熄灭的余烬，进一步混淆了现实的结构。赋予这一仪式最初意义的宏大叙述是如此模糊，以至于已无法被察觉。\n结论：黑暗之魂与后现代意识 在这篇文章中，我论证了《黑暗之魂》系列，特别是第三部作品，是当代日本社会后现代意识的文化反映。大泽和东浩纪等思想家提出的后现代反叙事、极右翼意识形态的复兴（“日本的重振”），以及极端社会退隐现象（“蛰居族”和“草食男子”）等现象，都反映了一种由内部恐怖活动（如奥姆真理教）、出生率下降、福利系统恶化以及基础设施不稳定（如神户、福岛）引发的深层次焦虑。当我们所依赖的基础，无论是物质上的还是社会上的，似乎有消失的风险时，这种后现代意识便会产生。《黑暗之魂》系列也是这种焦虑的体现，它可能是当代焦虑最有力的文化表达。它的巨大成功和作为有史以来最伟大游戏系列之一的地位，尽管其不遵循行业最佳实践，似乎表明其美学和设计触动了众多游戏玩家的共同感受。\n那么，我们如何总结日本的后现代意识，这在许多方面似乎是正在整个后工业化西方蔓延的一种意识？可以说，后现代意识只是一种悲观情绪。这种情绪让人感觉我们的抱负将毫无意义，现代化的项目已经失败，个人采取有意义步骤去实现理想的观念，更不用说整个人类，是一个悲哀的笑话。这是对当代生活在某种程度上不如过去，甚至可能很可悲的恐惧。事实上，社会在某处进步的观念现在被视为一种极不时尚的想法。批判现代化抱负为危险的天真，已经成为几乎所有哲学或文学批评的固定部分。除了努力支持我的解释外，我还希望——即使只是初步地——表明后现代意识本身也是循环的。它只是社会在向陌生的新时期过渡时表现出的不稳定症状。这种悲观情绪，或者说是世界末日的情绪，就像当新社会组织模式在旧秩序下挣扎着出现时的成长痛苦。然而，我还想指出，我所标记为后现代意识的这种悲观情绪，仅仅是一种情绪。它的现实取决于我们在多大程度上愿意沉溺于它。\n有鉴于此，我认真对待大泽和东浩纪对当今日本社会心理或精神问题的诊断。无论现实如何，人们的感觉都能驱使他们走向自毁行为。这对整个社会来说可能会有非常严重的后果，即使这种想法仅仅被少数人接受。奥姆真理教的行动是由一个小团体实施的，但它们仍然设法在日本社会的核心造成了损害。然而，我拒绝后现代的预后——特别是东浩纪的预后——认为流行文化中传统叙事的转变是人们普遍放弃理想的证据，我们注定要退缩到技术享乐主义中，所有人都成为自己癖好的奴隶。\n这只是某些人正在走的一条道路，甚至这一点也是有争议的。与漫画和轻小说等其他流行媒介相比，电子游戏的核心在于要求解决一系列问题。无论这种需求与日常生活多么人为和遥远，它依然是愿意经历困难并克服挑战的表现。在一个极其基本的层面上，愿意经历困难去解决特定问题或克服特定挑战，支撑着所有人类的努力。叙述中的绝望与完成游戏所需的耐心和技巧之间，有一种奇怪的脱节。\n在电子游戏中，《黑暗之魂》系列代表了一组特别具有挑战性的问题。它提出了一个问题：如果没有对你努力的确认，为什么要经历这样一系列艰难的挑战？答案只能是过程本身是值得的，游戏本身的行为是其精神上的滋养来源。然而，玩游戏比单纯地享受乐趣要复杂得多。正如 Aarseth 等人指出的，ergos，也就是说“工作”，是电子游戏媒介的本质。《黑暗之魂》当然是一款将 ergos 和 ludos 紧密结合在一起的游戏。当然，人们玩《黑暗之魂》不是为了工作，以谋生（除了一些创业的 YouTuber）。然而，说玩《黑暗之魂》只是为了乐趣，这抹去了关于游戏体验的某些绝对关键的东西。《黑暗之魂》中所谓的“乐趣”是那种与爬山、跑马拉松或任何需要长时间持续努力的活动联系在一起的：痛苦、困难，只在结束时短暂满足，然后人们又心甘情愿地回到同样的挑战中。我们不称这些活动为乐趣，而是说它们令人满意，这正是克服《黑暗之魂》中的挑战的感觉。\n社会学家 Roger Caillois 认为，游戏是培养文化反射和习惯的熔炉。如果是这样的话，玩像《黑暗之魂》这样的游戏会培养出什么样的文化反射呢？这个问题的答案可能超出了本文的范围，但可以肯定的是，《黑暗之魂》鼓励我们接受挑战，以便我们可以获得与地狱般的壮丽景象同样令人惊叹的回报。这可能是推动玩家前进的最重要的回报：获得某种美丽景象的回报。尽管其黑暗和怪诞，每一部《黑暗之魂》系列作品在视觉呈现上都独具美感。\n《黑暗之魂》是一款极其复杂和综合性的游戏，它汲取了日本的现在和过去，但这一切都是为了表达对当人们开始质疑支撑其社会的主要观念时，自我实现、进步和社区可能性的深切焦虑。游戏玩法、叙事和美学的复杂交织共同形成了一种强烈的后现代意识表达，在这种意识中，角色在一个衰败的世界里独自面对充满敌意的他者，处于一个他们无法理解的情境中。游戏中包含的佛教形而上学进一步暗示，通过进行祭祀和复兴的仪式来纠正这种情况的追求，最终是毫无意义甚至腐败的。在这层意义上，《黑暗之魂》直接触及了后现代主义的巨大恐惧：无论我们如何努力避免湮灭，最终，这可能超出了我们的掌控。\n然而，游戏中也反映出一种尼采式的自我超越承诺——通过激烈的斗争实现自我超越。我认真对待后现代的诊断，即传统社会叙事的恶化造成了令人不安的自毁行为模式，但我不同意他们的预后。我也不同意他们关于历史独特性甚至传统信仰、信条和叙事不能更新或演变以满足文化需求的说法。我认为日本以前也经历过这样的情况。后现代意识可能只是一种精神压力——尽管是严重的——人们在社会经历重大结构转变和旧结构的叙事失去连贯性时经历的。它注定会在新的形式定型时循环回到一种乐观的情绪中。事物很少长时间保持不变——无论好坏——事实上，无论我们身处何地，没有人真正知道接下来会发生什么。这就是未来的运作方式。它是未知的，只有意识形态的教条主义才试图将其固定在某个位置，不变得像过去一样。我们必须有勇气对接下来发生的事情保持开放态度。\n夏目漱石。日本近代文学巨匠，被誉为“日本现代文学之父”，其作品探讨了人与社会、个人内心世界等深刻主题，代表作有《我是猫》、《心》、《明暗》等。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n三岛由纪夫。日本著名作家、剧作家和政治活动家，以其充满美学与哲学思考的作品著称，代表作有《金阁寺》、《春雪》、《海与毒药》等。他的作品往往探讨人性、荣誉、死亡等主题。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n或翻译成“新的媒介在大学里常常遇到困难，因为每一种媒介都必须通过那些把关“高雅文化”的权威人士的审查。”\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n沟口健二（1898-1956）是日本著名电影导演，以精湛的拍摄技巧和深刻的社会洞察力闻名。其代表作包括《雨月物语》和《山椒大夫》，他擅长于通过细腻的叙事手法展现人性的复杂。沟口被视为日本电影史上的大师之一，对后世电影影响深远。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n指代一种源自捷克作家弗朗茨·卡夫卡（Franz Kafka）作品风格的情境，通常描述一种充满荒诞、绝望和无助感的氛围。在这种情境中，个体往往陷入与无情且不可理解的官僚体系或命运的斗争中，感受到无力改变自身处境的痛苦和迷茫感。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n大泽真幸 Masachi Ōsawa（1958-）是日本当代著名的社会学家、哲学家和评论家，专注于社会理论、文化批评和历史哲学。研究焦点包括现代日本社会的结构性问题及后现代思想在日本的应用。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n东浩纪 Hiroki Azuma（1971-），日本文化评论家和哲学家，专注于后现代主义、信息社会理论及日本流行文化的研究。他的代表作《动物化的后现代》探讨了当代日本动漫、游戏等亚文化现象，并提出了“数据库消费”等独特概念。东浩纪在日本思想界和文化评论领域具有重要影响力，尤其在分析现代社会和文化变化方面贡献卓著。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n田中真纪子（Makiko Tanaka），日本文学学者，研究方向包括现代日本文学及文化，她的研究涉及后现代思想对日本文学的影响。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n村上春树（Murakami Haruki）是日本著名小说家，以其独特的风格和对梦幻与现实交织的描绘而闻名。他的代表作包括《挪威的森林》、《海边的卡夫卡》和《1Q84》，这些作品探讨了孤独、爱情、以及人类存在的意义。村上春树的作品广受国际读者喜爱，并对全球文学产生了深远影响。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n弗朗索瓦·利奥塔 Francois Lyotard（1924-1998），法国哲学家，以其对后现代主义的研究著称。他在1979年出版的《后现代状况》一书中首次提出了“后现代”一词，主张大叙事（如宗教、政治意识形态）的破裂和小叙事的兴起。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n奥姆真理教 日本的极端宗教团体，创立于1984年，因其在1995年东京地铁沙林毒气袭击事件中的恐怖行为而广为人知。该教派结合了多种宗教和神秘主义信仰，曾声称能够提供终极真理。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n联合赤军事件（日本语：連合赤軍事件）是指1971年至1972年间，由日本极左激进组织联合赤军（連合赤軍）发动的一系列暴力事件。联合赤军由“赤军派”和“京滨安保共斗”两派极端左翼组织合并而成，其成员信奉极端左翼思想，试图通过暴力推翻政府。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEspen Aarseth 挪威学者，游戏研究领域的开创者之一，以其对电子游戏叙事和游戏性分析的贡献著称。著有《Cybertext: Perspectives on Ergodic Literature》（《赛博文本：互动文学视角》），提出了“互动文学”的概念，强调玩家在游戏中的互动和参与对于理解游戏文本的重要性。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n佛教末法概念 佛教教义中的一个概念，指佛法逐渐衰微的时代，教义的力量减弱，人们无法通过修行获得解脱。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n“安倍经济学”是指日本首相安倍晋三（Shinzō Abe）自2012年开始推行的一系列经济政策。这些政策包括所谓的“三支箭”策略：大胆的货币政策、灵活的财政政策和促进投资的增长战略。安倍经济学的目标是通过刺激经济增长、提高通货膨胀率来摆脱长期的通货紧缩和经济停滞状态。这些政策在日本和国际上都引起了广泛关注和讨论，但也引发了对其效果和可持续性的质疑。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n草食男子（日语：草食(そうしょく)系男子(けいだんし)，sōshoku danshi）是日本的一种社会现象，指那些对事业、恋爱和结婚不感兴趣，生活态度温和、不争、不抢的男性。这个词最早由日本作家深泽真纪在2006年提出，反映了日本社会中一些年轻男性的生活态度和行为模式的变化。这些男性通常不追求传统意义上的男性气概和社会成就，反而更注重个人兴趣和生活质量。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n尼特族 （NEET，Not in Education, Employment, or Training）是指那些既不在学、也不在职、也不接受职业培训的年轻人。这个词源自英国，但在日本也广泛使用。尼特族的出现反映了现代社会中青年失业率高、就业压力大以及社会结构变化等问题。这一群体在日本引起了社会关注，因为他们的经济依赖性和社会疏离感对家庭和社会结构带来了挑战。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n隐蔽青年 （日语：引(ひ)き籠(こも)り，hikikomori）是指那些因社会压力、精神疾病或其他原因而长期闭门不出、与社会隔绝的年轻人。这种现象在日本尤为突出，被认为是社会压力、竞争激烈以及家庭结构变化等多重因素共同作用的结果。隐蔽青年通常在家中长时间不出门，不上学、不工作，依赖父母或家庭成员的支持。这个现象不仅影响到个人的心理健康和生活质量，也对家庭和社会产生了深远的影响。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n見田宗介 是日本著名社会学家，曾任东京大学名誉教授，是日本社会学界的重要人物之一。他的研究领域广泛，涵盖现代社会理论、文化社会学以及日本社会的独特发展轨迹。峰崎的代表作《现代社会的理论》（『現代社会の理論』）对日本社会现代性的转型和结构做了深刻分析，被认为是社会学研究的重要文献。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n约翰·多尔 是一位美国历史学家和作家，以研究日本历史和美日关系闻名。他的著作《拥抱失败：日本在美国的阴影下》（Embracing Defeat: Japan in the Wake of World War II）通过详细的历史分析和丰富的文献资料，探讨了二战后日本社会的变化和重建过程。多尔的研究不仅涉及政治和经济层面，还深入探讨了社会文化的转变，特别是日本人对美国模式的接受和适应。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n泛泛女孩（panpan girls）是指二战后日本被美国占领时期，一些日本女性与驻日美军士兵交往的现象。这些女性通常通过与美军士兵建立关系，获得他们带来的物资和经济利益。泛泛女孩现象反映了战后日本社会的贫困与物质匮乏，以及人们对物质富裕和更好生活的渴望。二战后，日本遭受了巨大的物质和经济损失，许多人生活在极度贫困之中。占领日本的美军士兵带来了大量的物资和财富，使得美军的生活条件远优于普通日本人的生活水平。泛泛女孩们通过与美军士兵的交往，获得了食品、酒类、香烟、糖果、化妆品等在当时极为稀缺的物品。泛泛女孩现象不仅仅是一个物质交换的过程，它还反映了当时日本社会对美国文化和生活方式的向往。对许多日本人来说，美国的富裕和舒适的生活条件成为他们追求的目标，这种对美国模式的向往也在一定程度上推动了战后日本的现代化和经济复苏。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n貧病争 ひんびょうそう是一个源自佛教的概念，指的是“世间的烦恼”，具体包括贫穷、疾病和争斗。这三个方面代表了人类在世俗生活中不可避免的痛苦和困扰。以下是对每个方面的解释: 贫穷（ひん），指物质上的匮乏和经济上的贫困，贫穷会导致人们为了生计而不断奋斗，成为生活中的一大烦恼；疾病（びょう），指身体上的疾病和健康问题，疾病的存在使得人们不断追求健康和医治；争斗（そう），指人际之间的冲突和社会中的斗争，这包括个人之间的争执、家庭内部的冲突、社会中的暴力和战争等，争斗使得人们生活在不安和紧张的环境中。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n新人类（しんじんるい）是20世纪80年代日本的一代年轻人，他们在思想、行为和文化上与之前的世代有显著的不同。这一群体的出现标志着日本社会的深刻转型，影响了当时的社会文化和经济发展。通过对新人类的研究，可以更好地理解日本意识形态在20世纪80年代的变化和发展。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n御宅族 通常用于描述对某一特定兴趣（如动漫、漫画、电子游戏等）有着极端热情和深入了解的人群。这个词汇最早出现在20世纪80年代，最初带有一定的负面意味，指那些沉迷于兴趣爱好、常常在家里闭门不出、不太参与社会活动的年轻人。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n全学联合（日语：全学連，Zengakuren）是“全日本学生自治会总联合”（全日本学生自治会総連合）的简称，成立于1948年，是日本历史上重要的学生运动组织。全学联合主要由日本各大学的学生自治会组成，代表学生在政治、社会问题上发声。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n田中元子 是一位研究日本战后政治运动的学者，特别关注20世纪60年代的学生运动，如安保运动（Anpo）和全共斗运动（Zenkyoto）。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n全共斗运动 （全学共闘会議，Zenkyōtō）是20世纪60年代末至70年代初，日本大学生和左翼组织发起的激进学生运动，旨在反对政府的教育政策、经济措施以及与美国的安保条约。这个运动的特点是学生自发组织、没有固定的领导层，通过激烈的抗议和占领校园来表达政治诉求。全共斗的行动在日本社会引起了广泛关注，成为那个时代反抗精神的象征。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n联合赤军事件（Rengō Akagun jiken）是指1972年日本联合赤军组织进行的一系列激进和暴力事件。联合赤军是由极左翼的日本学生运动组织合并而成的激进组织，他们在长野县浅间山庄劫持了人质，导致与警方的对峙长达10天。最终，日本警方采取了强攻行动，事件导致人质获救、部分赤军成员被捕或死亡，引发了日本社会对极端左翼运动的广泛批判。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n通过博尔赫斯的《科学的准确性》作为寓言，他断言地图作为表征取代了领土，最终成为需要创建新地图的领土本身： 来自于阿根廷作家豪尔赫·路易斯·博尔赫斯（Jorge Luis Borges）的一篇短篇小说《精确的地图》（也译作《科学的准确性》）。在这个故事中，博尔赫斯描绘了一个极端的情况：地图变得如此精确，以至于它与现实世界（领土）完全重叠，最后，地图的细节变得如此复杂，以至于它不再是现实世界的象征，而是成为了一个替代现实的东西。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n弗朗索瓦·李奥塔（François Lyotard，1924-1998）是法国著名的哲学家和文学批评家，以其在后现代主义领域的开创性研究而闻名。他的代表性著作《后现代状况》（La Condition Postmoderne，1979年）对20世纪末的思想产生了深远影响。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n山谷是东京台东区和荒川区附近的一个地区，以前被认为是日工、无家可归者和社会底层人员聚集的地方。在这个地区，贫困和社会问题较为突出。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n釜崎是大阪市西成区的一个地区，类似于东京的山谷，这里也有大量的日工和无家可归者。如今，这个区域的正式地名为”爱邻地区”（あいりんちく，Airin Chiku），但釜崎的称呼仍然流行。这一地区长期以来是社会底层人员的聚集地，经济落后，社会问题严重。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n人民圣殿教是美国的一个新宗教团体，由吉姆·琼斯（Jim Jones）于1950年代创立。该教团以其强烈的社会平等和共产主义色彩著称，特别强调种族平等与社会正义。然而，这一组织在1978年因乔恩斯镇大屠杀（Jonestown Massacre）而闻名。当时，吉姆·琼斯带领900多名信徒在南美圭亚那的乔恩斯镇集体自杀，成为现代宗教历史上最惨烈的悲剧之一。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n天堂之门是一个美国的新宗教团体，由马歇尔·阿普尔怀特（Marshall Applewhite）和邦妮·内托尔斯（Bonnie Nettles）于1970年代创立。这个教团以其独特的末世论思想和对外星生命的信仰而著称。教义核心是认为地球将被净化，信徒的灵魂需要通过离开身体（即自杀）以进入一艘外星飞船，从而到达更高的存在层次。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n**净土宗：**净土宗是日本佛教的一大宗派，起源于中国的净土思想，传入日本后由法然（1133-1212）创立。净土宗的核心教义是通过专一称念“南无阿弥陀佛”来获得往生西方极乐净土的保证。净土宗的实践重点在于简化修行，通过称念佛号（念仏），普通人也能获得解脱，而不必追求复杂的禅修或其他高深的佛教修行。**日莲宗：**日莲宗由日莲（1222-1282）在日本创立，主张《法华经》是释迦牟尼佛教法的究竟表达，因此对末法时代的唯一正法是称念“南无妙法莲华经”。日莲宗带有强烈的社会关怀意识，认为末法时代的混乱与个人、社会的正法失落有关，因此通过弘扬法华经，信众不仅能拯救自己，还能影响社会，带来和平与繁荣。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n法然（ほうねん，1133–1212）：日本净土宗的创始人，法然从天台宗的修行体系中转向单一的念佛法门，提倡通过专念“南无阿弥陀佛”来获得往生净土；道元（どうげん，1200–1253）：日本曹洞宗的创始人，他从天台宗的基础出发，前往中国学习禅宗，并将其带回日本。他的教义强调只管打坐这一直接的禅修实践；荣西（えいさい，1141–1215）：日本临济宗禅宗的创始人，他同样来自天台宗的传统，后来前往中国，学习禅宗并带回日本。他在日本还推广了茶道，被称为日本禅茶文化的先驱；日莲（にちれん，1222–1282）：日莲宗的创始人，日莲主张《法华经》是佛法的究竟表达，强调通过唱念“南无妙法莲华经”来获得救度，并积极参与社会、政治事务，呼吁社会改革；亲鸾（しんらん，1173–1263）：净土真宗的创始人，法然的弟子，他进一步发展了法然的念佛思想，主张即使是最普通的人通过称念阿弥陀佛也能获得救度。他强调阿弥陀佛的救度是不依赖个人修行的。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://niraya666.github.io/essay/%E5%90%91%E4%B8%8B%E8%9E%BA%E6%97%8B%E9%BB%91%E6%9A%97%E4%B9%8B%E9%AD%82%E7%B3%BB%E5%88%97%E6%B8%B8%E6%88%8F%E4%B8%AD%E7%9A%84%E5%90%8E%E7%8E%B0%E4%BB%A3%E6%84%8F%E8%AF%86%E4%B8%8E%E4%BD%9B%E6%95%99%E5%BD%A2%E8%80%8C%E4%B8%8A%E5%AD%A6/","summary":"\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cp\u003e本文探讨了“黑暗之魂”系列游戏在当代日本社会中的意义定位。我认为，这款游戏可以被看作是当今文化潮流的象征，就像有人可能会把杰克·凯鲁亚克的《达摩流浪者》视为60年代反文化的象征一样。\u003cstrong\u003e我主张，“黑暗之魂”系列通过寓言的方式表达了人们在一个日常行为和社会本身的意义都变得极不稳定的时代中生活的焦虑\u003c/strong\u003e。游戏采用碎片化的叙事方式，融合了佛教的形而上学思想，并以描绘一个曾经辉煌的王国在最后时刻挣扎的阴森哥特式美学为背景。这种对当代社会焦虑的表达，与日本的后现代主义话语密切相关。通过将这些游戏视为文本、环境和游戏系统之间的反馈回路，我把这些游戏的主要概念主题与大泽真幸提出的“后虚构时代”概念，以及东浩纪对御宅族的定义联系起来。\u003c/p\u003e\n\u003ch1 id=\"前言\"\u003e前言\u003c/h1\u003e\n\u003cp\u003e在人文学科这个领域，我们认为艺术和文学不仅仅是艺术鉴赏。当我们阅读夏目漱石\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e或三岛由纪夫\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e的作品时，教授和学生不仅仅像粉丝那样陶醉于他们优美的文笔，而是作为评论家，从中挑出深刻的思想。通过这种方式阅读足够多的作品，我们会发现一种更宏大的东西：\u003cstrong\u003e一个跨越时代和文化的思想洪流，在这个洪流中，生活的神秘、美丽、恐怖和悲剧汇聚成我们人类经验的漩涡和流动\u003c/strong\u003e。文学是指南，就像老话说的那样，是通向美好生活的指南：它是一面镜子，反映我们的位置，也是我们漫步时的地图。简而言之，伟大的作品让我们变得更强大。它们让我们清晰地面对生活的矛盾，并有勇气承受其重压。我们学习文学是为了找到人类经验中的共性，并将其内化。这至少是人文学科的理想和创立原则。\u003c/p\u003e\n\u003cp\u003e但如果这是正确的，我们能说这种深刻的体验只是文学独有的吗？书面文字是否有某种独特的力量，能够以其他媒介无法企及的方式激发反思？最终，文学只是一个个故事，但故事并不局限于一种媒介。故事是跨媒介的。它们可以口头讲述，可以印刷，可以通过舞蹈、图像，甚至可能通过尚未构想的方式来讲述。这是因为故事首先存在于人类心灵的深层结构中，不同的媒介让这些结构的不同方面显现出来。\u003cstrong\u003e故事是人类生活的核心，它们赋予我们的社会和个人生活以连贯性和意义。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e故事是个人身份的基石，人类将经验转化为故事的能力可能与我们感知时间的能力有关。\u003cstrong\u003e毕竟，故事是什么？它是以有意义的方式组织和编辑时间\u003c/strong\u003e。简而言之，通过研究故事，特别是那些在社会想象中占据重要地位的故事，我们可以进入人类心灵的最深处。当我们接近这一神秘领域时，我们更接近自己，也许会找到一些我们遗忘的，对我们至关重要的东西，它像火药桶上的火花一样在我们脚下点燃，在这场大火中我们的世界观重生。\u003c/p\u003e\n\u003cp\u003e新的媒介在大学里常常面临艰难，因为每种媒介都必须越过高文化裁决者的障碍\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。这在二十世纪初的电影和摄影上是如此，在今天的漫画、电视和视频游戏上也是如此。但如果我们仅因为这些作品属于错误的媒介而忽视了沟口健二\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e的作品，人文学科会变得多么贫乏？偏重研究一种媒介而忽视其他媒介有什么好处？这种偏见是最糟糕的，我们都因这种武断的判断而变得贫乏。我们切断了自己探索那些激发我们深层结构的新视角的机会。故事在新的媒介中同样存在，此外，对那些在这些新媒介中成长的人来说，发现其中的故事是引人入胜、动人且深具意义的。视频游戏中的故事体验与其他媒介有着根本的不同，正是这种不同使得研究它们变得如此重要。它反映了一种新的世界观和价值观。我们不再生活在一个技术只是工具的世界里；技术是我们生活的环境。电影和电视是工业时代的本土媒介，而视频游戏是数字时代的本土媒介。视频游戏不是小众的。这个行业在全球产生的收入大大超过了电影和印刷品。这本身就使得研究它们成为紧迫的事情。\u003c/p\u003e\n\u003cp\u003e话虽如此，我们必须认识到，视频游戏是一种新兴的媒介。就像早期的电影只是基本现象的再现——火车向屏幕疾驰或马在慢动作中奔跑——视频游戏的媒介中无疑有着尚未开发的巨大潜力。此外，我们还不能像理解文学那样全面理解这一媒介。我们还没有足够的时间去理解视频游戏在文化中可能扮演的角色以及它们对塑造我们的价值观和性格的影响。我不会在这里主张创建一个新的经典目录。现在还为时过早，而且流行经典化的过程早已在进行中。本文研究的游戏《黑暗之魂》系列已经被这样经典化了。它享有一种地位，其他同类游戏以它为标准，而那些共享其设计原则的游戏被称为“魂类游戏（Soul-like）”，这可能类似于许多文学或电影作品被描述为“卡夫卡式的”\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e。然而，我不会试图将这款游戏作为一部伟大作品来评估，以便将研究这一系列游戏制度化。我认为这些游戏被经典化不仅仅是因为它们“有趣”，而是因为它们直接处理了当代日本及其年轻一代面临的问题、焦虑和挑战。这是我们将要问的问题：\u003cstrong\u003e这些游戏中反映了什么，使其在跨文化范围内以及在粉丝和评论家中产生如此深刻的共鸣？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e我将在这里主要讨论日本，但我相信在研究这款游戏时得出的教训具有全球意义。目前，这些意义将保持原样。本质上，为了故事而玩游戏是一件矛盾的事情。游戏的内部组件，作为一个具象化的代码系统和作为不同类型叙事（文本、视觉、听觉）的载体，对玩家提出了一系列相互竞争的需求。这种矛盾在《黑暗之魂》中和几乎所有其他游戏中一样没有解决，但该系列在这些相互竞争的元素之间达到了罕见的一致性，可能直接反映了日常生活中数字世界的体验。我希望我在这里的分析，即使只是初步和不完整的，也能成为反思社会走向、新的价值结构的出现以及文学解释或在这种情况下的游戏解释可以对这些问题有所启示的机会。\u003c/p\u003e\n\u003ch1 id=\"第一章引言\"\u003e\u003cstrong\u003e第一章：引言\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003e本文旨在探讨“黑暗之魂”系列游戏与当代日本社会状况之间的关系。我认为，“黑暗之魂”系列通过寓言形式表达了在一个日常行为意义甚至社会本身都变得不稳定的时代生活的焦虑。这一表达通过破碎的叙事方式实现，这种方式结合了佛教形而上学，并以描绘一个昔日伟大王国最后挣扎的阴暗哥特风格为背景。这种寓言中的焦虑与日本后现代性讨论密切相关，特别是现代性项目失败和进步叙事腐败的观点。在我的论文中，我认为这种讨论在“黑暗之魂”中通过模拟经典英雄之旅得以体现，但最终通过剥夺玩家的英雄成就感并用深深的模糊和不确定性取而代之，颠覆了这一旅程。\u003c/p\u003e\n\u003cp\u003e“黑暗之魂”系列是一款第三人称动作RPG，其核心机制围绕探索一个破败的世界展开。该系列由著名日本游戏公司FromSoftware在总裁宫崎英高的指导下开发。第一款游戏于2011年9月发布，第三款于2016年4月发布。这是一款难度极高的游戏，设定在一个末世奇幻世界，要求玩家克服许多复杂的运动任务。游戏的一大特点是其缺乏具体的叙事阐述。然而，有一些概念性主题将这些游戏联系在一起。游戏中总是有一个被“不死诅咒”摧毁的王国，并且总是有神秘人物指示玩家杀死旧神并使用他们的灵魂来“连接火焰”。这个过程在第一款游戏中被解读为一个世界复兴过程，但在第二和第三款游戏中被揭示为不可避免的衰败螺旋。\u003c/p\u003e\n\u003cp\u003e全球销量数百万，三部作品在国外几乎普遍获得好评，赢得了众多“年度游戏”称号和大量其他行业奖项。它对当代游戏设计产生了深远的影响，许多采用类似设计的游戏经常被称为“魂类”或“魂风格”。这已成为一种陈词滥调，类似于带有超现实或存在主义色彩的文学作品被描述为“卡夫卡式”。我认为其地位的原因并非游戏的“有趣”，而是它有一些关于我们今天所处世界的强有力表达，这些表达与那些对现代性的后现代批评观点一致的人产生共鸣。\u003c/p\u003e\n\u003cp\u003e本文的核心论点是，“黑暗之魂”系列体现了具有后现代意识的生活体验。这个术语“后现代意识”是我用来总结后现代思想的一个总称。然而，我特别关注日本分支，引用了社会学家大泽真幸\u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e、文化评论家东浩纪\u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e、文学学者田中真纪子\u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e和小说家村上春树\u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e的著作。\u003cstrong\u003e从弗朗索瓦·利奥塔\u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e（Francois Lyotard）开始——他首次提出“后现代”一词——后现代思想的不同分支的核心主张是，曾经用来指导社会的“大叙事”已经无可救药地破裂并且无法修复\u003c/strong\u003e。大泽和其他思想家拾起了这条思路，并将其应用到具体的日本背景。他们认为，日本战后时期的大叙事的破裂在年轻一代中引发了虚无主义行为，这种行为\u003cu\u003e表现为从政治原因和社区参与转向过度消费主义和一种表现为御宅族形象的媒体崇拜\u003c/u\u003e。在一些极端情况下，这种御宅族主义可能导致病态行为，如奥姆真理教\u003csup id=\"fnref:11\"\u003e\u003ca href=\"#fn:11\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e11\u003c/a\u003e\u003c/sup\u003e的恐怖主义，但需要注意的是，这些思想家将这种暴力行为视为御宅族行为连续体的最外端，而不是在类别上有所不同。从大泽和东浩纪等后现代思想家那里，\u003cstrong\u003e我将后现代意识提炼为两个基本特征：1. 拒绝或无法接受宏大叙事和意识形态作为意义的来源 2. 对“他者”的日益焦虑和敌意\u003c/strong\u003e。在这种情况下，他者指的是主体与他者之间的经典哲学二元关系；换句话说，就是自我与世界上所有其他自我之间的关系。\u003cu\u003e大叙事的破裂削弱了依赖这些叙事来维系和稳定的社区纽带和同胞感\u003c/u\u003e。这种意识是“黑暗之魂”系列设计和叙事的主要当代文化元素。\u003c/p\u003e\n\u003cp\u003e然而，后现代意识在日本确实广泛传播，这并不是显而易见的。出于这个原因，我在第一章中追溯了大叙事在日本历史中逐渐瓦解的确切方式，从战后时期开始一直到现在。这使得我们能够看到后现代意识的发展及其在日本社会和“黑暗之魂”系列中的表达。没有这一步骤，就不可能识别贯穿这个系列的后现代意识。\u003c/p\u003e\n\u003cp\u003e为了挖掘日本后现代意识的文化根源，我采用了大泽的文化时代精神理论，\u003cstrong\u003e将战后时期分为三个阶段：意识形态时代（1945 - 1972），虚构时代（1972 - 1995）和后虚构时代（1995 - 现在）\u003c/strong\u003e。大泽在其作品中提出的基本观点是，意识形态的死亡——本质上是他对“宏大叙事”的术语——刺激了新的形式，这些形式最终使他们脱离了日本战后社会的意识形态基础。从1972年的联合赤军事件\u003csup id=\"fnref:12\"\u003e\u003ca href=\"#fn:12\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e12\u003c/a\u003e\u003c/sup\u003e开始，激进的左翼政治让位于非政治的消费享乐主义。这导致了围绕显著消费形式尤其是时尚的亚文化的繁荣。然而，对一些人来说，对更深层意义的渴望仍然存在，这最终催生了新时代的邪教，即奥姆真理教，它们同样从传统宗教和动漫中汲取力量来满足这种需求。\u003c/p\u003e\n\u003cp\u003e奥姆真理教的重要性在于他们既标志着传统信仰和意识形态的弱化，也标志着对任何被他们视为“他者”的事物的日益不容忍。他们还标志着一种对远在天边但又近在咫尺的敌对他者的偏执。例如，他们认为日本政府被全球犹太共济会阴谋控制，而他们自己的莲花团体则充满了双重间谍。他们在1995年对这个被认为是犹太人的他者的恐怖袭击，具有讽刺意味的是，传播了这种对奥姆真理教妖魔的偏执，这些妖魔潜伏在学校、企业甚至政府中。这种怪异他者的概念在“黑暗之魂”中得到了有力的表达，因为它向玩家展示了一个充满敌对个体和群体的世界——这些实体是玩家角色的他者——他们的目标和实践是难以理解的。这也通过一种系统得到了加强，即玩家始终连接到互联网，并且开放给其他玩家入侵。\u003c/p\u003e\n\u003cp\u003e东浩纪采纳了大泽的时代精神，并提出了一种“动物化”理论。他指出，由于后现代时代缺乏大叙事，人们失去了理想和抱负。\u003cstrong\u003e因此，他们退缩到一个以收集符号为中心的世界，这些符号被添加到一个巨大的符号数据库中并被重新利用\u003c/strong\u003e。 他的主要证据是，当今的御宅族不再对叙事感兴趣。相反，他们只是享受识别、重新混合和扩展数据库的组件。他的预测是，御宅族的活动——他们无休止地收集符号——是大文化的先锋，后续几代人将在自己的领域中变得越来越像御宅族。东浩纪所指出的这一流行文化趋势在“黑暗之魂”中体现为深深的模糊和稀疏的叙事。\u003c/p\u003e\n\u003cp\u003e那么“黑暗之魂”究竟如何通过游戏玩法表达后现代意识呢？我认为它通过为我们提供一个任务和一个王国来模拟传统的英雄之旅，但剥夺了我们理解这个任务意义的任何连贯方式。游戏提供了一个视觉细节丰富的世界，但完全缺乏具体的阐述，将玩家置于考古学家和法医科学家的角色中。然而，从未有任何可靠的解释；只有更多的线索和片段，有些是矛盾的。这被称为“环境叙事”，它创造了游戏与后现代思想之间最关键的接触点；即它对主叙事的拒绝。有“拯救王国”的建议，但仅仅是建议，主要是通过类型的惯例传达的，而不是叙事。你所做事情的意义及其服务的目的从未完全清楚。\u003c/p\u003e\n\u003cp\u003e重要的是要记住，虽然可以从传统的文学解释视角来解释“黑暗之魂”系列的叙事，但当试图将游戏机制解释为寓意时，会遇到许多实际和理论问题。出于这个原因，我在第二章中对当代游戏研究的批判理论进行了文献综述。利用Espen Aarseth\u003csup id=\"fnref:13\"\u003e\u003ca href=\"#fn:13\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e13\u003c/a\u003e\u003c/sup\u003e和其他奠定该领域基础的思想家的论点，我认为需要结合“第一人称”现象学方法和更传统的文学分析方法，才能正确评估视频游戏中潜在的潜在意义。这样做是为了建立一种分析方式，通过与游戏系统的斗争，让玩家体验到我上面概述的后现代意识的两个基本特征。我在第三章中将这种现象学方法付诸实践，探索了一种特别有效的文本叙事、环境叙事和游戏玩法的融合，捕捉到游戏作为一个统一体验的本质。\u003c/p\u003e\n\u003cp\u003e除了后现代主义，还有另一股影响游戏的力量，那就是佛教末法概念\u003csup id=\"fnref:14\"\u003e\u003ca href=\"#fn:14\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e14\u003c/a\u003e\u003c/sup\u003e。这两种影响力在有趣的方式上重叠，并产生了惊人的相似之处。末法在镰仓时代是一个极具影响力的概念，正如我们的时代一样，那是一个深刻的社会转型时期。末法指的是佛教教义失去其效力，人们无法再依靠自己的努力来获得启示。\u003cstrong\u003e随之而来的是僧侣和寺庙变得不可否认的腐败，人们因此无法离开轮回，被困在一系列日益退化的转世中\u003c/strong\u003e。我在第四章中分析了末法的哲学核心，并展示了它如何与大泽和东浩纪的后现代话语一致。此外，我认为，只要“黑暗之魂”系列表达了后现代意识，它主要是通过采用这个佛教末世论概念来表达的。\u003cstrong\u003e后现代思想和末法都表达了一种恐惧，即事情只会变得更糟，人类的努力无法改变结果\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e在第五章中，我回到现象学和文学分析的混合方法，展示末法概念以及佛教形而上学的混合如何作为构建游戏虚拟世界的核心要素，尽管游戏的美学显然源于中世纪的欧洲奇幻调色板。这些佛教元素共同揭示了游戏世界被困在一个日益退化的轮回中，玩家的任务基本上是一种徒劳的行为，只会推动这个痛苦的循环。我认为佛教形而上学的融合最终表达了东浩纪的结论，即后现代世界及其死去的叙事和怪异他者无可避免地导致社会整体的“动物化”。尽管“黑暗之魂”是一款美学显然源于中世纪欧洲奇幻风格的游戏，但它充满了末法的逻辑，因此游戏深受异教影响，通过采用和改造佛教形而上学，在阴暗的骑士在破败的城堡中冒险的表面下表达了一种后现代情感。\u003c/p\u003e\n\u003ch2 id=\"衰败叙事\"\u003e\u003cstrong\u003e衰败叙事\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e过去几十年，日本几乎成了停滞的代名词。这在很大程度上归因于日本在1970年代到1980年代经历的“奇迹”，以及他们令人震惊的衰落，自那以后，国家似乎一直处于无所事事的状态。在美国，当《纽约时报》或《华尔街日报》等出版物提到日本时，往往很少有积极的报道，而是集中在一小部分话题：人口老龄化，年轻人自杀；货币贬值，索尼、丰田和东芝等企业巨头被更精简的竞争对手击败；核燃料仍在福岛渗入地下水，但无人承担责任，等等，每一个预言都比前一个更严峻。在西方，日本作为一个正在衰败的帝国的叙述已经流传已久。这种衰败的印象如此强烈，似乎只有惯性才能维持一切运作，以至于人们忘记了日本仍然是世界上最富有、最富裕和技术最先进的国家之一，仍然领先于许多西欧国家。这种停滞的叙述也存在于日本。正如现任首相安倍晋三在2012年选举中的口号所示：“日本を取り戻す”（取回日本）。从谁那里取回？取回到哪里？凭借他的“安倍经济学”\u003csup id=\"fnref:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e议程中的“三支箭”，他将从日本自己的“萎缩心态”中拯救日本（Sharp）。\u003cstrong\u003e如果说1980年代的代表是那些狂热工作、渴望企业晋升的上班族，那么近年来这种原型被更为无力的角色所取代：草食男子\u003c/strong\u003e\u003csup id=\"fnref:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e16\u003c/a\u003e\u003c/sup\u003e\u003cstrong\u003e（sōshoku danshi）、尼特族\u003c/strong\u003e\u003csup id=\"fnref:17\"\u003e\u003ca href=\"#fn:17\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e17\u003c/a\u003e\u003c/sup\u003e\u003cstrong\u003e（NEET，指不在职或不在学的年轻人）、隐蔽青年\u003c/strong\u003e\u003csup id=\"fnref:18\"\u003e\u003ca href=\"#fn:18\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e18\u003c/a\u003e\u003c/sup\u003e\u003cstrong\u003e（hikikomori，指社会隐士）和其他贬损的称呼\u003c/strong\u003e。诚然，日本的经济和文化问题与其他任何国家一样真实存在，但这种叙述的力量已经超越了现实，成为一种独立的存在。\u003c/p\u003e\n\u003cp\u003e这种“萎缩”的心态和随之而来的文化表达，我将称之为后现代意识。这种意识必须与正统的后现代哲学区分开来，因为哲学本身只是对围绕日本的整体衰败叙述的众多情感反应之一。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e后现代意识的核心是一种源于对人类社会已经达到不可否认的衰退点的感知——无论是有意识还是无意识的——的焦虑\u003c/strong\u003e。鉴于上面概述的围绕日本经济和文化衰退的广泛文化讨论，后现代意识在日本文化心理中占据了中心地位。因此，这些源自后现代意识的停滞叙述在日本的流行媒体中也找到了强大而多样的表达。虽然有许多例子可以引用，但游戏《黑暗之魂》及其所有续集，是这种“萎缩”心态的卓越典范。要理解它如何以这种方式运作，我们不仅要看叙述，还要看《黑暗之魂》相对于同类游戏的存在。《黑暗之魂》属于一种被称为角色扮演游戏（RPG）的游戏类型。RPG通常是一个开放式的幻想世界模拟，玩家可以探索领土、收集宝物和击败怪物。通常，玩家创建一个角色——一个在游戏世界中的化身——并以此角色进行任务，其最终目标通常涉及拯救某物（王国、存在平面、公主等）免于毁灭。另一个RPG的基本特征是成长概念。尽管有许多设计模式和隐喻用于模拟成长过程，通常玩家的化身通过击败怪物获得经验值，并通过积累这些点数“升级”。通过升级过程，玩家解锁能力，他们的化身变得更强大。总的来说，RPG往往是许多文化神话中常见的英雄之旅模式的模拟：年轻、未成型的人必须离开家园的安全区域，面对未知的危险，克服它们，培养他们潜在的能力，并以强大和完全实现的人的身份返回，无论是字面上还是象征性地。\u003c/p\u003e\n\u003cp\u003e在其基本设计中，《黑暗之魂》作为RPG是相当传统的。然而，它有许多叙述和游戏设计选择，颠覆了这个几十年旧的类型的许多传统。作为一个叙述，它可以被解读为一种政治和文化的寓言，++直接反映了生活在一个意识形态叙述失去吸引力和连贯性的衰败文明中的经验++。它也是一种新兴新类型的前沿：后末日幻想。\u003cstrong\u003e在这款游戏中，王国被占领，英雄们都死了或堕落了，没有人可以拯救，而你存在的最大威胁是那些与你在废墟中一起生存的人\u003c/strong\u003e。社区是不可能的。基本的本体状态是无休止的失败，而“英雄”只是注定要忍受它。甚至死亡也不能免除角色的负担，而你被赋予的任务的意义和目标是模糊和不明确的。叙述几乎不存在。当游戏开始时，玩家被简单地放在一个被怪物和不死者占领的废弃王国中，没有明确的理由说明为何事情会如此，或者他们应该怎么做。仅有的叙述存在于零碎的文本中，只提供诱人但最终不完整或矛盾的关于世界的信息。所有这些叙述元素都对已建立的类型惯例提出了严重挑战，因此将其与衰退的讨论联系起来。\u003c/p\u003e\n\u003cp\u003e在游戏设计方面，《黑暗之魂》有两个突出的特点。第一个是游戏被有意设计得异常困难。这实际上是其吸引力的主要来源之一。该系列在西方的广告标语是“准备去死”（Prepare To Die），而在日本，该系列被认为是しにがみ（死亡之神）的主要例子，暗示玩家在与远比玩家创建的化身强大的巨大、恐怖的怪物斗争时必然会经历无数次死亡。另一个特点是，尽管游戏主要设计为单人体验——也是RPG类型的一个既定惯例——玩家始终连接到互联网。这被视频游戏评论家广泛赞誉为该类型的重要创新。在实践中，这创造了一种紧张的局势，玩家总是暴露在其他玩家的“入侵”之中，这些玩家会试图杀死该玩家。这种互动中内置了一个复杂的风险/回报系统，入侵者通过杀死其他玩家可以获得很多，但如果失败则会失去很多。\u003c/p\u003e\n\u003cp\u003e实际上，始终连接到互联网使得游戏世界充满了偏执和危险。其他玩家是玩家在游戏中可能面对的最危险和不可预测的怪物。入侵玩家经常会潜伏在一个不合适的时机突然袭击，玩家无法自卫，从而逆转他们不幸的受害者辛苦获得的进展。我们将详细探讨叙述和游戏系统，但这一系列游戏的基本概况引出了一个问题：在一个没有希望拯救王国，并且异常困难常常到不公平地步的游戏中，怎么会有如此大的吸引力？但事实上，这款游戏在日本、欧洲和北美都极具吸引力。\u003cstrong\u003e这也指出了游戏的设计和叙述如何与在无聊时代生活的年轻一代的经验产生共鸣\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e日本文化领域的停滞叙述需要进一步澄清。我认为《黑暗之魂》可以被解读为对当代日本状况的政治和文化寓言。这是一种后现代的英雄之旅的颠倒：一种被污染的神话碎片的模拟。\u003cstrong\u003e它直接反映了许多年轻日本人今天试图在庞大、失修的机构中航行时所经历的无聊和徒劳的感觉，这些机构至多对他们的存在漠不关心，至多直接敌对。然而，这种寓言不仅仅是关于生活在1980年代“奇迹”之后的经济问题。它同样涉及生活在一个国家、政治和文化叙述已经失去了大部分意义和连贯性的社会中的社会和心理成本，这些叙述曾经作为一种心理基石——为个人和社区身份提供稳定\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e催化这一恶化的事件起源于1995年，那一年发生了神户地震和沙林毒气袭击。在西方媒体中，关于这些事件的文化或政治意义的讨论通常不多，但在许多日本知识分子中，这些事件似乎标志着现代性的决定性破裂。在著名小说家村上春树的书《地下：东京毒气袭击和日本心理》中，对沙林毒气袭击幸存者的一系列采访，并得出结论，这一事件的后果不可逆转地揭示了日本社会的所有矛盾、缺陷和内在不稳定性：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“神户地震和1995年1月和3月的东京毒气袭击是日本战后历史上两起最严重的悲剧。毫不夸张地说，这些事件前后的日本意识发生了显著变化。这两次灾难将作为我们生活中的两个里程碑，永远铭刻在我们的心灵中。如此灾难性的事件接连发生是令人震惊的，也是巧合的。然而，当日本的“泡沫经济”破裂，标志着那些泛滥成灾的时代结束时，这些事件带来了对日本国家根本根源的批判性审视。就像这些事件一直在等待伏击我们。两者共有的是压倒性的暴力元素：\u003cstrong\u003e一个是不可避免的自然灾难，另一个是可以避免的人为灾难。\u003c/strong\u003e”（村上，237页）\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e通过村上等许多知识分子的工作，1995年已成为一个单一且不可改变的地位：\u003cstrong\u003e通往破碎和功能失调时代的门户\u003c/strong\u003e。这里分析的主要理论家——东浩纪、大泽真幸和田中基子——都\u003cstrong\u003e将1995年视为进入反乌托邦现状的起点\u003c/strong\u003e。这些思想家的语调本身就有一种微妙的后末日色彩，仿佛我们生活在一个无法拯救的被污染时代。这也是衰败讨论的一部分。这些思想家有一种确定的末世论信念，将他们与这一讨论联系起来。\u003c/p\u003e\n\u003cp\u003e无论如何，这些灾难比任何经济灾难都更可能是现任首相希望根除的“萎缩”心态的原因。尽管已经过去几十年，1995年的事件作为焦虑和无助感的触发点，在《黑暗之魂》的寓言中找到了源头。经济的持续低迷、老龄化人口、年轻一代的就业前景减少以及福岛等灾难，只是加剧了这些灾难在1995年最初引发的负面情绪。福岛等事件只帮助在日本意识中创造了一种遗产。近年来，右翼极端民族主义政治的重新出现和尼特族及隐蔽青年的社会退缩，形成了对这一相同的文化病态的极端和相反的情感反应。右翼试图通过与其真实和感知的对手（韩国、中国、朝鲜）进行公开的对抗来复兴“大和”帝国的活力，试图通过历史修正主义抹去日本帝国的暴力和道德可憎之处，并试图通过修改宪法（第9条）恢复战争作为国家主权权利。而尼特族和隐蔽青年则将自己沉溺于网络和其他形式的私人数字享乐中。他们没有叙述可供复兴，没有事业可供加入，甚至没有可以参与的兴趣小组。\u003cstrong\u003e与他人沟通实在是太困难了，在他们看来，也太危险了。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e我的观点不是《黑暗之魂》系列是这些问题的根源，甚至不是它们的贡献，而是它是对在公众意识中循环的深层社会焦虑，特别是对视频游戏作为熟悉媒介的年轻一代的一种特别有力的表达。《黑暗之魂》系列的持续流行，作为一种社会腐化的象征，反映了人们在日常生活中的感觉。但这些焦虑到底是什么，它们如何在《黑暗之魂》中表现出来？另一种问这个问题的方法是，后现代意识的基本要素是什么？定义1995年后时代的有两个主要元素。\u003cstrong\u003e1. 对构建战后秩序的政治和社会叙述的拒绝；2. 一种对他者的弥漫性焦虑，侵蚀了对社会过程的信任和信心\u003c/strong\u003e。他者的概念是指自我与世界上遇到的所有其他自我之间的二元关系的一半。这一概念的延伸是文化他者，即一种感觉某些人与自己相似，另一些人则是异类。日本社会通常被认为具有非常同质的国家认同感，彼此之间有相同的文化、种族和相似的价值观。这反过来又创造了强烈的信任和社区感，但这种亲近的逆向效应是，外来者被认为比在美国、英国或法国等文化多样性更大的国家中更为陌生。大泽真幸的论点是，奥姆真理教的恐怖活动将潜在威胁引入了公众。这种恐惧是一种诡异的他者的恐惧，这种他者会突然出现，以一种深刻的方式扰乱你的生活，有时甚至是致命的。社会结构中的这种破裂从未真正修复，并继续受到挑战，直到现在。关于诡异他者的偏执是定义《黑暗之魂》的氛围和叙述的主要特征之一，也是其游戏机制，尤其是其在线组件。\u003c/p\u003e\n\u003cp\u003e要理解《黑暗之魂》如何作为一种描述后现代意识的寓言运作，我们必须追踪战后日本社会状况的衰退轨迹。\u003cstrong\u003e这根本上是一个现代进步叙述——本质上是从西方继承的——首先在1960年代末引发了激烈的意识形态斗争，然后在1980年代变成一种无政治的享乐主义，最后，最终在五个正义的宗教狂热者用雨伞尖端戳破装满沙林毒气的袋子的时刻，被一种微妙的虚无主义彻底摧毁\u003c/strong\u003e。为此，我们将研究社会学家大泽真幸的工作。大泽花费了相当多的职业生涯试图回答奥姆为什么会做他们所做的事情。\u003cstrong\u003e在他的书《虚构时代的终结》中，他认为答案涉及意识形态本身的致命缺陷：一种危险而空洞的思维模式，导致在一个自我设计的世界中越来越高的疏离感。\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"理性时代\"\u003e理性时代\u003c/h2\u003e\n\u003cp\u003e借用社会学家見田宗介\u003csup id=\"fnref:19\"\u003e\u003ca href=\"#fn:19\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e19\u003c/a\u003e\u003c/sup\u003e（Mita Munesuke）的观点，大泽真幸将战后时代分为两个不同的阶段。第一个阶段他称为“理想的时代（理想の時代，risō no jidai）”，第二个阶段他称为“虚构的时代（虚構の時代，kyokō no jidai）”。理想的时代始于20世纪60年代，结束于1972年，虚构的时代从1972年开始，一直持续到1995年。大泽将“理想”和“虚构”描述为“可能世界（可能の世界 kanōsei no sekai）”，因为它们都反映了我们对世界的感知以及我们认为世界应该如何。然而，尽管这两个概念在根本层面上是相关的，但它们在几个重要方面是不同的。\u003cstrong\u003e大泽认为，理想与现实世界密切相关，而虚构则完全脱离现实世界\u003c/strong\u003e：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e理想是一个在未来有望或期待在现实中实现的可能世界。因此，理想必须存在于现实的因果延伸之上。从这个意义上讲，理想不是一个纯粹的可能世界，而是广义现实世界的一个方面。相反，虚构是一个可以在现实中实现与否无关的可能世界，因此是纯粹的反现实。（大泽，41页）\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e理想は、未来において現実に着地することが予期（期待）されているような可能世界である。だから、理想は、現実の因果的な延長上になくてならない。その意味では、理想は、純粋な可能世界ではなく、むしろ広義の現実世界の一局面である。それにたいして、虚構は現実への着地ということについてさしあたって無関連ありうる可能世界であり、それゆえ純粋な反現実である。(Osawa, 41)\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e暂且不讨论虚构如何将我们与现实疏离，日本的理想时代很大程度上受到冷战阴影的笼罩。当亚洲大部分地区处于苏联的保护伞下时，日本则在美国的庇护下，因而在意识形态上致力于民主和经济自由主义。在20世纪60年代，日本在这些条件下几乎在各个方面都得到了繁荣：科学、基础设施、经济、文化等。大泽将20世纪60年代称为理想的黄金时代，并指出家用电器是赋予大众理想的物质形式：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e理想时代在大众层面的黄金期是20世纪60年代。当然，这与日本的高速增长期相吻合。在此期间，例如，受到国民绝大多数人广泛需求的家电产品赋予了大众理想物质的表达……经济增长和科学技术的进步并不是因为理想而被接受的，相反，正因为理想作为可能的现实在广泛（全球范围内）被信奉，经济才能够增长，科学和技术才能进步。（大泽，44页）\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e理想の時代の、大衆的な水準で黄金期は、1960年代である。もちろん、これは、高度成長期にあたる。この時期、たとえば、国民の圧倒的な大多数によって広範に欲求された家電製品が、大衆的理想に物質的な表現を与えた\u0026hellip;経済成長や科学・技術の進歩があったから理想が抱かれたのではなく、逆に、理想が可能的現実として広く（世界規模で）信憑されたがゆえに、経済が成長することができ、また科学や技術進歩していると感受されたのである。(Osawa, 44)\u003c/strong\u003e\u003c/p\u003e","title":"翻译｜向下螺旋：《黑暗之魂》系列游戏中的后现代意识与佛教形而上学"},{"content":"AI 新闻 Llama3.1 发布 博客文章： https://ai.meta.com/blog/meta-llama-3-1/\n技术报告：https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\n405B，70B，和8B3个版本 在个别指标上，405B超过了GPT-4o Mistral Large 2 https://mistral.ai/news/mistral-large-2407/\n卷起来了\nOpenAI 发布 GPT-4o mini 更低的API价格，并取代chatGPT中的GPT-3.5 Turbo，成为免费使用的模型\nSAM-2 发布 https://ai.meta.com/blog/segment-anything-2/\nGitHub：https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file\n与SAM相比， 同时支持视频和图像的分割，更强大的能力\n快手可灵更新 https://klingai.kuaishou.com/\n值得关注的开源项目 knowledge_graph_maker 采用LLM生成knowledge graph\nGitHub： https://github.com/rahulnyk/knowledge_graph_maker\nQwen2-Audio 接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本\nGitHub: github.com/QwenLM/Qwen2-Audio\nLLM Twin Course An End-to-End Framework for Production-Ready LLM \u0026amp; RAG Systems by Building Your LLM Twin\nGithub：github.com/decodingml/llm-twin-course\nDocker-Proxy 自建Docker镜像加速服务，基于官方Docker Registry 一键部署Docker、K8s、Quay、Ghcr、Mcr等镜像加速\\管理服务。支持部署到Render\\Koyeb\nGitHub: github.com/dqzboy/Docker-Proxy\nMediaGo 支持流媒体下载、视频下载、m3u8 文件下载以及 B 站视频下载\nGitHub：github.com/caorushizi/mediago\nPPTX2MD a pptx to markdown converter\nGitHub: https://github.com/ssine/pptx2md\ngptpdf Using VLLM (like GPT-4o) to parse PDF into markdown.\nGitHub: https://sgithub.com/CosmosShadow/gptpdf\n我的足迹 去了趟WAIC-2024\n全场唯一能够看得上的Agent应用（可实际上手体验的），给亚马逊点个赞；亮点：通过RAG作为agent工作记忆，以实现正向迭代优化。\n明年不想去了；\n一些思考点：\n行业似乎处于一种降温的态势，大家和去年相比， 更加理性也务实了，但同质化也很严重； RAG以及所谓垂域“大模型”应用的难点仍然在数据源头上：如何避免garbage in garbage out 的窘境，这目前仍没有太好的方案； 各行各业都需要一些效率提升工具， AI是值得做的，但远没有到取代人这一步，用论坛上的句话说叫：“外行觉得（AI）内行，内行觉得（AI）外行”； AI时代对于下一代，或者是新人的培养是个问题， 答案对他们来说来的太快太轻松，以至于对他们的思维方式的培养和判断能力的提升是有害的； 整理了一些感兴趣的论坛， 用wishper + LLM三段式润色后， 把代码和文稿开源在了：WAIC-Media-Intelligence\n影音记录 精选歌单 Live演出 07.23 Tues. The Yusssef Dayes 万代南梦宫\n书\u0026amp;阅读摘录 本月完成： 《如何避免孤独终老》\nThe GraphRAG Manifesto: Adding Knowledge to GenAI https://neo4j.com/blog/graphrag-manifesto/\n使用 GraphRAG 的一种常见模式如下：\n进行向量或关键字搜索以找到初始节点集。 遍历图谱以获取相关节点的信息。 可选地，使用基于图的排序算法（如 PageRank）对文档进行重新排序。 与仅使用向量的 RAG 相比，GraphRAG 主要有三个方面的优势：\n更高的准确性和更完整的答案（运行时/生产 受益） 创建知识图后，构建和维护 RAG 应用程序更加容易（开发时间 受益） 提供更好的可解释性、可追溯性和访问控制（治理 受益） 你需要的不是智能体，而是一个适合 AI 的工作流 https://baoyu.io/blog/ai/you-dont-need-agent-but-ai-suitable-workflow\n不要将 AI 的解决方案局限在人类现有的解决方案上 不必完全依赖 AI 做决策，而是让 AI 辅助做决策或者做简单的决策 回归问题本质，AI 只是解决问题的工具\nYC 创业者的必读袖珍指南（中英文对照） https://x.com/HiTw93/status/1814985639240307141\nLaunch now - 立即启动 Build something people want - 打造人们需要的东西 Do things that don\u0026rsquo;t scale - 做一些无法扩展的事情 Find the 90 / 10 solution - 找到90%问题与10%解决方案的平衡 Find 10-100 customers who love your product - 找到10到100个热爱你产品的客户 All startups are badly broken at some point - 所有初创公司在某个时刻都会遇到严重问题 Write code - talk to users - 编写代码 - 与用户交流 \u0026ldquo;It’s not your money\u0026rdquo; - “这不是你的钱” Growth is the result of a great product not the precursor - 成长是优秀产品的自然结果，而非先决条件 Don’t scale your team/product until you have built something people want - 在你打造出人们需要的东西之前，不要扩展你的团队或产品 Valuation is not equal to success or even probability of success - 估值不等于成功，甚至不等于成功的可能性 Avoid long negotiated deals with big customers if you can - 如果可以的话，避免与大客户进行长时间的谈判 Avoid big company corporate development queries - they will only waste time - 避免与大公司进行企业开发咨询 - 他们只会浪费时间 Avoid conferences unless they are the best way to get customers - 除非参加展会是获取客户的最佳方式，否则避免参加 Pre-product market fit - do things that don’t scale: remain small/nimble - 在产品市场契合之前 - 做一些无法扩展的事情：保持小规模和灵活性 Startups can only solve one problem well at any given time - 初创公司在任何时候只能很好地解决一个问题 Founder relationships matter more than you think - 创始人之间的关系比你想象的更重要 Sometimes you need to fire your customers (they might be killing you) - 有时你需要“开除”你的客户（他们可能会毁了你） Ignore your competitors, you will more likely die of suicide than murder - 忽略你的竞争对手，你更可能死于自杀而非他杀 Most companies don\u0026rsquo;t die because they run out of money - 大多数公司并非因为资金耗尽而倒闭 Be nice! Or at least don’t be a jerk - 要友好！或者至少不要成为一个混蛋 Get sleep and exercise - take care of yourself - 睡觉和锻炼 - 照顾好自己 揭秘DeepSeek:一个更极致的中国技术理想主义故事 https://mp.weixin.qq.com/s/r9zZaEgqAa_lml_fOEZmjg\n而另一方面，国产大模型之前很少涉足架构层面的创新，也是因为很少有人主动去击破那样一种成见：美国更擅长从0-1的技术创新，而中国更擅长从1-10的应用创新。\n我们已经习惯摩尔定律从天而降，躺在家里18个月就会出来更好的硬件和软件。Scaling Law也在被如此对待。\n「暗涌」：听说DeepSeek对于卡和人的调集非常灵活。\n梁文锋：我们每个人对于卡和人的调动是不设上限的。如果有想法，每个人随时可以调用训练集群的卡无需审批。同时因为不存在层级和跨部门，也可以灵活调用所有人，只要对方也有兴趣。\n「暗涌」：一种松散的管理方式也取决于你们筛选到了一批强热爱驱动的人。听说你们很擅长从细节招人， 可以让一些非传统评价指标里优秀的人被选出来。\n梁文锋：我们选人的标准一直都是热爱和好奇心，所以很多人会有一些奇特的经历，很有意思。很多人对做研究的渴望，远超对钱的在意。\n翻译|工作自动化对政治行为的影响 https://mp.weixin.qq.com/s/Il3GrwbdVczK-ACwbyE4LA\n常规工人（routine workers）是最近技术变革的主要输家，并且倾向于支持民粹主义政党\n这些研究有一个共同发现——那些因技术变革而在经济上失利的人，通常会对政治现状产生反感，特别是向着民粹主义激进右翼倾斜。 技术变革还创造了一个不那么引人注目、但数量更多且在政治上高度相关的技术受益者群体。\n","permalink":"https://niraya666.github.io/monthly/2024-07/","summary":"\u003ch1 id=\"ai-新闻\"\u003eAI 新闻\u003c/h1\u003e\n\u003ch3 id=\"llama31-发布\"\u003eLlama3.1 发布\u003c/h3\u003e\n\u003cp\u003e博客文章： \u003ca href=\"https://ai.meta.com/blog/meta-llama-3-1/\"\u003ehttps://ai.meta.com/blog/meta-llama-3-1/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e技术报告：\u003ca href=\"https://ai.meta.com/research/publications/the-llama-3-herd-of-models/\"\u003ehttps://ai.meta.com/research/publications/the-llama-3-herd-of-models/\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e405B，70B，和8B3个版本\u003c/li\u003e\n\u003cli\u003e在个别指标上，405B超过了GPT-4o\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://scontent-msp1-1.xx.fbcdn.net/v/t39.2365-6/451735590_1030734788570365_1093008500142144333_n.png?_nc_cat=100\u0026amp;ccb=1-7\u0026amp;_nc_sid=e280be\u0026amp;_nc_ohc=CJNPYAljD8MQ7kNvgE9-8ZQ\u0026amp;_nc_ht=scontent-msp1-1.xx\u0026amp;oh=00_AYAOhN3NhE0dscHn8iazBrc3JmSU5XM9gJ6gjmkqOuUZ3w\u0026amp;oe=66C00DBE\" alt=\"https://scontent-msp1-1.xx.fbcdn.net/v/t39.2365-6/451735590_1030734788570365_1093008500142144333_n.png?_nc_cat=100\u0026amp;ccb=1-7\u0026amp;_nc_sid=e280be\u0026amp;_nc_ohc=CJNPYAljD8MQ7kNvgE9-8ZQ\u0026amp;_nc_ht=scontent-msp1-1.xx\u0026amp;oh=00_AYAOhN3NhE0dscHn8iazBrc3JmSU5XM9gJ6gjmkqOuUZ3w\u0026amp;oe=66C00DBE\"  /\u003e\n\u003c/p\u003e\n\u003ch3 id=\"mistral-large-2\"\u003eMistral Large 2\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://mistral.ai/news/mistral-large-2407/\"\u003ehttps://mistral.ai/news/mistral-large-2407/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-multiple.png\" alt=\"https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-multiple.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e卷起来了\u003c/p\u003e\n\u003ch3 id=\"openai-发布-gpt-4o-mini\"\u003eOpenAI 发布 GPT-4o mini\u003c/h3\u003e\n\u003cp\u003e更低的API价格，并取代chatGPT中的GPT-3.5 Turbo，成为免费使用的模型\u003c/p\u003e\n\u003ch3 id=\"sam-2-发布\"\u003eSAM-2 发布\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://ai.meta.com/blog/segment-anything-2/\"\u003ehttps://ai.meta.com/blog/segment-anything-2/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGitHub：\u003ca href=\"https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file\"\u003ehttps://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e与SAM相比， 同时支持视频和图像的分割，更强大的能力\u003c/p\u003e\n\u003ch3 id=\"快手可灵更新\"\u003e快手可灵更新\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://klingai.kuaishou.com/\"\u003ehttps://klingai.kuaishou.com/\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"值得关注的开源项目\"\u003e值得关注的开源项目\u003c/h1\u003e\n\u003ch3 id=\"knowledge_graph_maker\"\u003eknowledge_graph_maker\u003c/h3\u003e\n\u003cp\u003e采用LLM生成knowledge graph\u003c/p\u003e\n\u003cp\u003eGitHub： \u003ca href=\"https://github.com/rahulnyk/knowledge_graph_maker\"\u003ehttps://github.com/rahulnyk/knowledge_graph_maker\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"qwen2-audio\"\u003eQwen2-Audio\u003c/h3\u003e\n\u003cp\u003e接受各种音频信号输入，并根据语音指令执行音频分析或直接响应文本\u003c/p\u003e\n\u003cp\u003eGitHub: \u003ca href=\"http://github.com/QwenLM/Qwen2-Audio\"\u003egithub.com/QwenLM/Qwen2-Audio\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"llm-twin-course\"\u003e\u003cstrong\u003eLLM Twin Course\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eAn End-to-End Framework for Production-Ready LLM \u0026amp; RAG Systems by Building Your LLM Twin\u003c/p\u003e\n\u003cp\u003eGithub：\u003ca href=\"http://github.com/decodingml/llm-twin-course\"\u003egithub.com/decodingml/llm-twin-course\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"docker-proxy\"\u003eDocker-Proxy\u003c/h3\u003e\n\u003cp\u003e自建Docker镜像加速服务，基于官方Docker Registry 一键部署Docker、K8s、Quay、Ghcr、Mcr等镜像加速\\管理服务。支持部署到Render\\Koyeb\u003c/p\u003e\n\u003cp\u003eGitHub: \u003ca href=\"http://github.com/dqzboy/Docker-Proxy\"\u003egithub.com/dqzboy/Docker-Proxy\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"mediago\"\u003eMediaGo\u003c/h3\u003e\n\u003cp\u003e支持流媒体下载、视频下载、m3u8 文件下载以及 B 站视频下载\u003c/p\u003e\n\u003cp\u003eGitHub：\u003ca href=\"https://github.com/caorushizi/mediago\"\u003egithub.com/caorushizi/mediago\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"pptx2md\"\u003e\u003cstrong\u003ePPTX2MD\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003ea pptx to markdown converter\u003c/p\u003e","title":"2024-07 月刊"},{"content":" If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.\n— from Build a search engine, not a vector DB\n一个优秀的RAG背后， 一定有一个优秀的搜索引擎；\n很多时候，在针对通用场景下， 如果能够调用常见的搜索引擎，如google 或者bing， RAG的效果一般不会太差；\n但，当考虑到一些垂直场景， 我们需要构造属于自己的“搜索引擎”，而往往根据特定的场景，在索引构建上会有特殊处理；可以参考devv，在针对代码场景下， 对RAG的检索部份也做了不少的工作和设计；\n很显然， 检索不是仅仅使用向量匹配和向量库就能搞定的事情；\n考虑到前LLM时代的NLP检索（甚至是图像检索），或者是推荐系统， 其实会发现， 这些东西并没有什么太大的变化，思路和技术依旧是可以复用的。\nvector-search is not all you need 纯粹基于向量匹配实现起来很简单，但不是万能的\n其问题来源有：\nembedding模型一般在通用预料上做训练， 在特定领域下， 其效果并不会太好，很多时候需要在特定领域语料上再做微调；\n语义匹配不见得是万能的，对于一些特定场景，其局限性尤为明显。例如，当用户询问一个具体的名词时，就会发现单纯使用向量匹配只能匹配到大致的相似度，对于具体的关键词是无能为力的。 这种情况下，语义匹配往往难以捕捉到用户提问中的精确细节。例如，用户可能询问特定型号的苹果电脑（如“MacBook Pro 2021”），但语义匹配算法可能会返回类似类型或年份的苹果电脑，而不是确切的型号。此时语义匹配的效果并不会太好。\nembedding模型计算的是输入的两个文本（A和B）的相似度。然而，在实际应用中，输入的是一个问句（query），需要匹配的是一段文字。在这种情况下，我们关注的重点是文本之间的相关性而非简单的相似性。因此，通常在使用embedding模型后，还需要一个reranking模型，根据相关性对结果进行排序，以确保返回最相关的答案。\n对于组合的问题无效；如果query是一些复杂的需要一些推理的问题（如多跳），使用vector- search显然不是好办法；这类问题通常涉及多个子问题，每个子问题的答案都是解答下一个问题的基础， 举例来说，回答“谁是发明电话的人的孙子在2020年的职业是什么？”需要知道发明电话的人是谁，接着找出他的孙子，然后查询这个孙子的职业。面对需要多步推理的复杂问题时，向量搜索的能力有限，原因在于它不可能进行复杂的逻辑推理和多步信息整合。在这样的场景下，需要的是Agent将query进行分解，然后分别调用工具（搜索引擎）；\n其实我们可能过于迷信向量搜索。事实上，对于一些简单的场景，基于倒排索引的关键词匹配效果更好、更加简单且成本更低。倒排索引是一种高效的文本检索方法，通过建立关键词到文档的映射，可以快速找到包含特定关键词的文档。对于不需要复杂推理的简单查询，倒排索引可以提供快速且准确的搜索结果。\n不止只有向量检索 稀疏向量（Sparse）和稠密向量（Dense）是通过不同的算法计算的。稀疏向量主要由零值组成，只有少数几个非零值，而稠密向量则主要包含非零值。稀疏嵌入由算法如BM25和SPLADE生成，而稠密嵌入则由机器学习模型如GloVe和Transformers生成。\nTF-IDF(Term-Frequency Inverse-Document Frequency)是一种用于评估文本中一个词对一个文档的重要程度的统计方法。其原理基于两个指标：词频（TF）和逆文档频率（IDF）。\n词频（TF）：表示某个词在文档中出现的频率，计算公式为：\n$$ \\text{TF}(t,d) = \\frac{\\text{词t在文档d中出现的次数}}{\\text{文档d中的总词数}} $$\n逆文档频率（IDF）：衡量某词在整个文档集合中重要性的指标。公式为：\n$$ \\text{IDF}(t) = \\log\\left(\\frac{\\text{总文档数}}{\\text{包含词t的文档数} + 1}\\right) $$\nTF-IDF值通过将词频和逆文档频率相乘计算得到：\n$$ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $$\nBM25（Best Matching 25）是一种基于概率模型的文本检索算法，它在TF-IDF（词频-逆文档频率）的基础上进行改进。BM25通过引入二元独立模型，并添加归一化惩罚来计算文档长度相对于数据库中所有文档平均长度的权重。\nBM25的核心公式如下：\n$$ \\text{BM25}(q, D) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})} $$\n其中：\nq 代表查询\nD 代表文档\n$q_i$ 是查询中的第i个词\n$f(q_i, D)$ 是词 ($q_i$) 在文档 (D) 中的词频\n|D| 是文档 (D) 的长度（以词计）\n$\\text{avgdl}$ 是集合中所有文档的平均长度\n$k_1$ 和 b 是调节参数，通常 $k_1$ 在1.2到2之间，b 取0.75\n$\\text{IDF}(q_i)$是词 $q_i$ 的逆文档频率\nDense vector 使用稠密向量表示存储在数据库中的信息，包括文本、图像和其他类型的数据，。这些嵌入由机器学习模型生成，将数据转换为向量。也就是最近两年大家常说的向量检索。\n如何提升检索效果：\n自然而然的想法，不再使用单一的向量检索， 而是采用多种检索方式混合的方式，不论是混合不同的embedding模型的检索还是Sparse-vector和Dense-vector混合使用以获得二者的优势。通常来说，稠密向量擅长理解查询的上下文，而稀疏向量则擅长关键词匹配。\n那么需要将二者合并，but how？\nReciprocal Rank Fusion (RRF) 其核心思想是通过计算每个文档在不同排名列表中的倒数排名之和来确定最终排名。这样，排名较高的文档会得到更高的分数，而排名较低的文档会受到惩罚。\nRRF的计算公式如下：\n$$ \\text{RRF}(d) = \\sum{i=1}^{N} \\frac{1}{k + r_{i}(d)} $$\n其中：\n(d) 代表文档\n(N) 是排名列表的数量\n(k) 是一个常数，用于平滑处理，通常取值为60\n($r_{i}(d)$) 是文档 (d) 在第 (i) 个排名列表中的排名\n通过这个公式，每个文档的最终RRF分数是其在所有排名列表中的倒数排名之和，分数越高，排名越靠前。\n在langchain **ensemble-retriever**的源码中可以看到其具体的思想逻辑：\ndef weighted_reciprocal_rank( self, doc_lists: List[List[Document]] ) -\u0026gt; List[Document]: \u0026#34;\u0026#34;\u0026#34; Perform weighted Reciprocal Rank Fusion on multiple rank lists. You can find more details about RRF here: https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf Args: doc_lists: A list of rank lists, where each rank list contains unique items. Returns: list: The final aggregated list of items sorted by their weighted RRF scores in descending order. \u0026#34;\u0026#34;\u0026#34; if len(doc_lists) != len(self.weights): raise ValueError( \u0026#34;Number of rank lists must be equal to the number of weights.\u0026#34; ) # Associate each doc\u0026#39;s content with its RRF score for later sorting by it # Duplicated contents across retrievers are collapsed \u0026amp; scored cumulatively rrf_score: Dict[str, float] = defaultdict(float) for doc_list, weight in zip(doc_lists, self.weights): for rank, doc in enumerate(doc_list, start=1): rrf_score[ doc.page_content if self.id_key is None else doc.metadata[self.id_key] ] += weight / (rank + self.c) # Docs are deduplicated by their contents then sorted by their scores all_docs = chain.from_iterable(doc_lists) sorted_docs = sorted( unique_by_key( all_docs, lambda doc: doc.page_content if self.id_key is None else doc.metadata[self.id_key], ), reverse=True, key=lambda doc: rrf_score[ doc.page_content if self.id_key is None else doc.metadata[self.id_key] ], ) return sorted_docs 排序模型 在信息检索和问答系统中，处理用户查询通常分为两个步骤：召回（retrieval）和重排序（reranking）。\n召回的目标是用非常短的时间在海量数据中找到一定范围的备选项。这一过程的精度可能不会特别高，但可以将候选项从千万量级压缩到百量级。（这一过程涉及ANN也就是Approximate Nearest Neighbor， 在本文的后段会提及）\n重排序阶段，需要从召回阶段的备选项中选择符合需要的选项。由于候选项数量已经大幅减少，可以采用精度较高但相对计算时间较长的算法对这些备选项进行重新排序，以提高最终结果的准确性。\n重排序阶段场景的模型如`BAAI/bge 系列，从其huggingface仓库中的config.json可以发现：\n\u0026#34;_name_or_path\u0026#34;: \u0026#34;BAAI/bge-m3\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;XLMRobertaForSequenceClassification\u0026#34; ], 其本质是基于**XLM-RoBERTa**的序列分类模型。在下游任务中，对于序列分类任务，XLMRobertaForSequenceClassification 在 XLM-RoBERTa 的基础上添加了一个分类层（一个全连接层）来进行分类；简单来说，对于信息检索或问答系统中，输入用户问题（query），和匹配到的doc，通过合并query和doc形成一个sequence（[CLS]query[SEP]doc[SEP]），通过XLMRobertaForSequenceClassification 可以计算出二者的相关性分数，遍历候选集中的doc（一般在百数量级左右），即可得到query同候选集相关性分数。\n一些新的工作 Matryoshka Representation Learning（MRL） 来源于OpenAI 的embedding 技术：New embedding models and API updates\n和NeurIPS 2022发表的论文：Matryoshka Representation Learning\nMotivation：现有的深度学习模型通常学习固定维度的表示，无法灵活适应任务需求；表示维度的难以确定；表示维度缺乏粗到细（coarse-to-fine）的粒度；\nMRL旨在学习具有不同粒度的表示，允许单个embedding适应下游任务的计算约束。其设计思想是为了在不同的embedding维度上使用损失函数，从而训练一个模型，使其在不同的embedding维度上都能保持良好的性能\n训练过程：MRL 在多个嵌套的维度上训练模型，使得每个低维度的表示都能作为数据点的有用表示。这样，模型在不同的任务和计算约束下都能表现良好。\n推理过程：在推理过程中，MRL 可以根据需要使用不同维度的嵌入。对于分类任务，可以从低维度开始，逐步使用更高维度的表示。对于检索任务，可以先使用低维度表示进行初步筛选，然后使用高维度表示进行精细排序。\n（具体可以参考论文和sentence_transformers中所实现的MatryoshkaLoss）\nbge-m3 paper\ngithub\nm3—即Multi-Linguality 支持多语言， Multi-Functionality 支持多种检索方式（Dense-retrieval，Sparse-Retrieval， Multi-Vec Retrieval），以及Multi-Granularity 支持多粒度的检索（sentence-level，Passage-level，和Doc-level）\npip install -U FlagEmbedding Dense Embedding from FlagEmbedding import BGEM3FlagModel model = BGEM3FlagModel(\u0026#39;BAAI/bge-m3\u0026#39;, use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation sentences_1 = [\u0026#34;What is BGE M3?\u0026#34;, \u0026#34;Defination of BM25\u0026#34;] model.encode(sentences_1) print(model.encode(sentences_1)[\u0026#39;dense_vecs\u0026#39;].shape) # (2, 1024) Sparse Embedding (Lexical Weight) output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False) output_1 \u0026#34;\u0026#34;\u0026#34; {\u0026#39;dense_vecs\u0026#39;: array([[-0.03412 , -0.04706 , -0.0009317, ..., 0.0483 , 0.007576 , -0.02959 ], [-0.01026 , -0.0449 , -0.02432 , ..., -0.00828 , 0.01502 , 0.011086 ]], dtype=float16), \u0026#39;lexical_weights\u0026#39;: [defaultdict(int, {\u0026#39;4865\u0026#39;: 0.0836, \u0026#39;83\u0026#39;: 0.0814, \u0026#39;335\u0026#39;: 0.1295, \u0026#39;11679\u0026#39;: 0.252, \u0026#39;276\u0026#39;: 0.17, \u0026#39;363\u0026#39;: 0.2695, \u0026#39;32\u0026#39;: 0.04083}), defaultdict(int, {\u0026#39;262\u0026#39;: 0.04996, \u0026#39;5983\u0026#39;: 0.1367, \u0026#39;2320\u0026#39;: 0.04483, \u0026#39;111\u0026#39;: 0.06335, \u0026#39;90017\u0026#39;: 0.2517, \u0026#39;2588\u0026#39;: 0.3335})], \u0026#39;colbert_vecs\u0026#39;: None} \u0026#34;\u0026#34;\u0026#34; # you can see the weight for each token: print(model.convert_id_to_token(output_1[\u0026#39;lexical_weights\u0026#39;])) # [{\u0026#39;What\u0026#39;: 0.08356, \u0026#39;is\u0026#39;: 0.0814, \u0026#39;B\u0026#39;: 0.1296, \u0026#39;GE\u0026#39;: 0.252, \u0026#39;M\u0026#39;: 0.1702, \u0026#39;3\u0026#39;: 0.2695, \u0026#39;?\u0026#39;: 0.04092}, # {\u0026#39;De\u0026#39;: 0.05005, \u0026#39;fin\u0026#39;: 0.1368, \u0026#39;ation\u0026#39;: 0.04498, \u0026#39;of\u0026#39;: 0.0633, \u0026#39;BM\u0026#39;: 0.2515, \u0026#39;25\u0026#39;: 0.3335}] # compute the scores via lexical mathcing print(model.compute_lexical_matching_score(output_1[\u0026#39;lexical_weights\u0026#39;][0], output_1[\u0026#39;lexical_weights\u0026#39;][1])) # 0 Multi-Vector (ColBERT) sentences_1 = [\u0026#34;What is BGE M3?\u0026#34;, \u0026#34;Defination of BM25\u0026#34;] output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False) print(output_1[\u0026#39;colbert_vecs\u0026#39;][0].shape) # (8, 1024) print(model.colbert_score(output_1[\u0026#39;colbert_vecs\u0026#39;][0], output_1[\u0026#39;colbert_vecs\u0026#39;][1])) # tensor(0.4768) ColBERT score： 在计算完查询和文档之间的相似度矩阵后，对每个查询token选择与文档中token的最大相似度，将所有最大相似度累加，得到最终的colbert_score。\n基于LLM实现embedding 几个代表模型：\nAlibaba-NLP/gte-Qwen2-7B-instruct\nintfloat/e5-mistral-7b-instruct\ne5-mistral-7b-instruct论文：Improving Text Embeddings with Large Language Models\nconfig.json:\n{ \u0026#34;_name_or_path\u0026#34;: \u0026#34;mistralai/Mistral-7B-v0.1\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;MistralModel\u0026#34; ], \u0026#34;bos_token_id\u0026#34;: 1, \u0026#34;eos_token_id\u0026#34;: 2, \u0026#34;hidden_act\u0026#34;: \u0026#34;silu\u0026#34;, \u0026#34;hidden_size\u0026#34;: 4096, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 14336, \u0026#34;max_position_embeddings\u0026#34;: 32768, ... 通过LLM生成数据（合成数据）， 采用对比学习微调Mistral-7B；\nGiven a pretrained LLM, we append an [EOS] token to the end of the query and document, and then feed them into the LLM to obtain the query and document embeddings $(h_{q_{inst}^+}, h_d^+)$, by taking the last layer [EOS] vector.\n附录 如何找到合适的embedding模型和reranker模型 首先推荐从 huggingface MTEB Leaderboard上选择排名靠前的模型\n其次，实验！根据自己的场景构造测试集验证，才能选择出适合自己场景的embedding模型和reranker模型；\nembedding模型和reranker模型的注意事项 上下文长度限制。 这些模型大多本质是Bert-based 模型， 而Bert的上下文长度限制一般是512，具体可以在huggingface中的config.json中查看；以BAAI/bge-base-en-v1.5 为例\n{ \u0026#34;_name_or_path\u0026#34;: \u0026#34;/root/.cache/torch/sentence_transformers/BAAI_bge-base-en/\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;BertModel\u0026#34; ], \u0026#34;attention_probs_dropout_prob\u0026#34;: 0.1, \u0026#34;classifier_dropout\u0026#34;: null, \u0026#34;gradient_checkpointing\u0026#34;: false, \u0026#34;hidden_act\u0026#34;: \u0026#34;gelu\u0026#34;, \u0026#34;hidden_dropout_prob\u0026#34;: 0.1, \u0026#34;hidden_size\u0026#34;: 768, \u0026#34;id2label\u0026#34;: { \u0026#34;0\u0026#34;: \u0026#34;LABEL_0\u0026#34; }, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 3072, \u0026#34;label2id\u0026#34;: { \u0026#34;LABEL_0\u0026#34;: 0 }, \u0026#34;layer_norm_eps\u0026#34;: 1e-12, \u0026#34;max_position_embeddings\u0026#34;: 512, \u0026#34;model_type\u0026#34;: \u0026#34;bert\u0026#34;, \u0026#34;num_attention_heads\u0026#34;: 12, \u0026#34;num_hidden_layers\u0026#34;: 12, \u0026#34;pad_token_id\u0026#34;: 0, \u0026#34;position_embedding_type\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;torch_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;transformers_version\u0026#34;: \u0026#34;4.30.0\u0026#34;, \u0026#34;type_vocab_size\u0026#34;: 2, \u0026#34;use_cache\u0026#34;: true, \u0026#34;vocab_size\u0026#34;: 30522 } 其中`\u0026quot;max_position_embeddings\u0026quot;: 512, 也就是它的上下文长度，但输入sequence长度超过512个token，则模型只看到了最前面的512个token；\n向量维度\n微调\n如何微调embedding 模型 采用FlagEmbedding，参考：How to fine-tune bge embedding model?\n微调Sentence Transformer，参考：Training Overview\n还有哪些类型的reranker模型 根据ColBert文章\n作者将query-document matching 任务（reranking）分类成：\nRepresentation-based Similarity, 双塔，独立对查询和文档进行编码，然后通过计算它们的向量相似度进行匹配， 但query和doc间无更精细的交互。\nQuery-Document interaction： 在查询和文档之间直接进行交互，通常使用CNN来捕捉查询和文档之间的交互特征， 但交互计算复杂度较高，尤其是当查询和文档长度较长时。\nAll-to-All Interaction，基于Bert 的Cross-encoder结构， 查询和文档的每个词与对方的每个词进行全连接交互。这种方式可以捕捉细粒度的交互信息，通常需要较高的计算资源。\nLate Interaction， 迟交互型，在对查询和文档的每个词进行独立编码后，再进行交互计算。通常使用最大池化（MaxSim）来选择最相关的词进行匹配，从而结合了高效性和细粒度交互的优点。\n此外， 当然也有基于LLMs的reranker，参考：llm_reranker\n以 BAAI/bge-reranker-v2-gemma 为例， 只是做这样一件事\nprompt = \u0026#34;Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either \u0026#39;Yes\u0026#39; or \u0026#39;No\u0026#39;.\u0026#34; 以输出“Yes” 的logit 作为相关分数\n对比学习 在之前不断提到了embedding，当然， 和transformer中的embedding层不是一个东西；\n这里所指的embedding是通过对比学习的方式，将语义相近的文本映射到高维参数空间中的相近位置，使它们的距离尽可能短。相反，语义差别大的文本则被映射到参数空间中远离的位置。类似地，这种方法也可以应用于图像（相似图像检索）以及文本和图像的多模态语义匹配（如CLIP）。那么，对比学习有三个核心：loss function，网络架构和采样。\n对比学习有三个核心要素：网络架构、损失函数（loss function）和采样策略。\n常见的损失函数有contrastive loss、triplet loss，以及N-pair loss。\n网络架构方面,对比学习通常采用双塔(siamese network)或者三塔(triplet)结构。双塔结构包含两个相同或相似的编码器,分别用于处理正样本对;三塔结构则增加了一个用于处理负样本的编码器。这些编码器可以是各种深度学习模型,如CNN、RNN或Transformer等,具体选择取决于任务域和数据类型。这一步将原始较为复杂的数据类型，通过深度学习的方法降到相对较低的维度便于之后的计算。\n对于双塔结构， 一般采用Contrastive Loss， 其用于度量两个样本之间的相似度。其公式为：\n$$ L = \\frac{1}{2N} \\sum_{i=1}^N (y_i d_i^2 + (1 - y_i) \\max(0, m - d_i)^2) $$\n其中：\n$d_i$是样本对$(x_i, x_j)$ 的欧几里得距离。\n$y_i$ 是样本对的标签，1 表示正样本对，0 表示负样本对。\nm 是一个超参数，表示负样本对之间的最小距离，也称margin。\nTriplet Loss则使用于triplet-network结构, 其通过构造三元组 ( anchor, positive, negative) 来训练模型，使得正样本（positive）与锚点（anchor）的距离小于负样本（negative）与锚点的距离。其公式为：\n$$ L = \\sum_{i=1}^N \\left[ |f(x_i^a) - f(x_i^p)|_2^2 - |f(x_i^a) - f(x_i^n)|_2^2 + \\alpha \\right] $$\n其中：\n$f(x)$ 表示样本 x 的特征向量。\n$\\alpha$是一个超参数，表示正负样本对之间的最小距离差。\nN-pair Loss 是 Triplet Loss 的推广，通过引入多个负样本来提升训练效果。\n$$ L = \\sum_{i=1}^N \\log \\left( 1 + \\sum_{j \\neq i} \\exp \\left( f(x_i)^\\top f(x_j^n) - f(x_i)^\\top f(x_i^p) \\right) \\right) $$\n在训练过程中， 采样策略则很关键；但运气不好， 选择的样本都是Easy Negative mining，也就是$d(a,p)+margin\u0026lt;d(a,n)$，则模型能够很快收敛，但什么也没有学会，相反在另一个极端中， 采用的多是hard negative mining，也就是$d(a,n)\u0026lt;d(a,p)$的情况，此时模型训练将很难收敛；实际情况中， 往往采用semi-hard 或者是distance-weighted sampling 来寻找难度适中的负样本,以避免模型过于关注极端情况。而由于在高维空间下， 因为高维空间中大部分体积都集中在高维球壳上，所以随机采样得到的点之间的距离往往非常接近，这也是不采用随机采样的策略的原因（Sampling Matters in Deep Embedding Learning）。\n向量库是个什么东西 向量库不是“库”，但也是“库”；\n考虑到传统关系型数据库， 数据以表格形式存储， 但在AI时代， 大量非结构化数据的存储和查询，以表格形式存在这不太合理了；因为数据维度高， 需要一些特殊的查询方式；\n不同于传统数据库的精确匹配查询,向量库主要用于相似性搜索。这种搜索基于向量间的距离,通常使用欧几里得距离或余弦相似度。以暴力遍历的最近邻查询为例(Nearest neighbor search),假设我们有n个d维向量,查询复杂度为$O(nd)$。当n和d都很大时,这种方法变得非常耗时。\n而近似最近邻搜索(Approximate Nearest Neighbor search, ANN)则能将时间复杂度降低到亚线性,通常为$O(log n)$或更优。\n常见的ANN算法：\n基于树的：k-D树(k-D tree):将空间递归地划分为子空间,适用于低维数据。\n基于哈希的：局部敏感哈希(LSH, Locality-Sensitive Hashing):将相似的向量映射到相同的\u0026quot;桶\u0026quot;中。\n乘积量化(Product Quantization):将高维向量分解为低维子向量,并对每个子向量进行量化。\n基于Graph的：分层可导航小世界图(HNSW, Hierarchical Navigable Small World):构建多层图结构,在图上进行快速导航搜索。\n而以上算法的思路都可以归介于—将相似的样本放在一起（减少搜索空间）， 和构建高效的搜索结构。\n参考 Build a search engine, not a vector DB\nHybrid Search Explained\n再谈大模型RAG问答中的三个现实问题：兼看RAG-Fusion多query融合策略、回答引文生成策略及相关数据集概述\nLLM - RAG文档应用处理与召回经验之谈\n[RAG] BGE M3-Embedding | 什么？我的RAG底座模型又要换了？\nColBERT: A complete guide\n搜索引擎技术\nUtilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\nLLM4IR-Survey\nFine-Tuning LLaMA for Multi-Stage Text Retrieval\n","permalink":"https://niraya666.github.io/posts/rag%E6%A3%80%E7%B4%A2/","summary":"\u003cblockquote\u003e\n\u003cp\u003eIf you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e— from \u003ca href=\"https://blog.elicit.com/search-vs-vector-db/\"\u003eBuild a search engine, not a vector DB\u003c/a\u003e\u003c/p\u003e","title":"RAG工具箱：检索"},{"content":"AI 新闻 Claude 3.5 Sonnet https://www.anthropic.com/news/claude-3-5-sonnet\n相比其前身 Claude 3 Opus，Claude 3.5 Sonnet 的运行速度提升了两倍，同时在多项基准测试中表现优异。此外，Claude 3.5 Sonnet 还引入了名为“Artifacts”的新功能，使用户可以在与 AI 对话的过程中实时查看和编辑生成的内容。\n开源模型 Qwen 2 https://qwenlm.github.io/zh/blog/qwen2/\n一共开源了Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及Qwen2-72B 5个模型**。增大了上下文长度支持，最高达到128K** tokens（Qwen2-72B-Instruct）。 代码和数学能力显著提升。多个评测基准上的领先表现。 同时还开源了一个智能体解决方案，用于高效处理100万tokens级别的上下文： **使用Qwen-Agent将上下文记忆扩展到百万量级。**\nGemma2 https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/\n包括两个主要版本，分别是 9 B参数和 27 B参数的模型。27B 的 Gemma 2 模型在多个基准测试中表现优异，超过了 Llama 3 70B 模型的性能。\nGemma 2 采用了新的架构设计，使其在推理时更加高效\nNemotron-4 340B https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/\n包含基础模型、指示模型和奖励模型的综合性数据生成流水线，用于训练大型语言模型。\n提供了一种生成合成训练数据的新方法，旨在提升定制语言模型的性能和鲁棒性。\nDeepSeek-Coder-V2 代码模型 https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct\n开源的专家混合模型，性能媲美GPT4-Turbo，专为代码特定任务优化，支持多源高质量语料库预训练，显著提升编程和数学推理能力，支持338种编程语言，扩展上下文长度至128K。\n值得关注的开源项目 Perplexica： 开源版本Perplexity+支持本地LLM。 GitHub：github.com/ItzCrazyKns/Perplexica\ntranslation-agent：吴恩达老师开源了一套 AI Agent 翻译工作流。通过LLM初步翻译， 而后对翻译结果进行反思，并提出改进建议，最后再根据意见进行最终版本的翻译。GitHub： github.com/andrewyng/translation-agent\nStreamer-Sales 销冠 —— 卖货主播 LLM 大模型： 一个能够根据给定的商品特点从激发用户购买意愿角度出发进行商品解说的卖货主播大模型。 另外还集成了 LMDeploy 加速推理🚀、RAG检索增强生成 📚、TTS文字转语音🔊、数字人生成 🦸、 Agent 使用网络查询实时信息🌐、ASR 语音转文字🎙️。 GitHub：github.com/PeterH0323/Streamer-Sales\nsocial-auto-upload： 自动化上传视频到社交媒体：抖音、小红书、视频号、tiktok、youtube、bilibili。 GitHub：github.com/dreammis/social-auto-upload\ngptpdf： 将 PDF 解析为 Markdown 的开源工具，使用了多模态大模型GPT4-o。\n实现几乎完美地解析排版、数学公式、表格、图片、图表。\ngithub.com/CosmosShadow/gptpdf\nEidos—Offline alternative to Notion： Eidos is an extensible framework for managing your personal data throughout your lifetime in one place. github.com/mayneyao/eidos\nComposio： AI Agent集成工具集，支持100+工具和多种框架。 GitHub: github.com/ComposioHQ/composio\nWiseFlow首席情报官： ****一个敏捷的信息挖掘工具，可以从网站、微信公众号、社交平台等各种信息源中按设定的关注点提炼讯息，自动做标签归类并上传数据库。“我们缺的其实不是信息，我们需要的是从海量信息中过滤噪音，从而让有价值的信息显露出来！” GitHub: github.com/TeamWiseFlow/wiseflow\ncomfyui-sound-lab： 声音生成版本的comfyui。GitHub: github.com/shadowcz007/comfyui-sound-lab\nAmphi-ETL： Low-code ETL for structured and unstructured data. Generates Python code you can deploy anywhere. GitHub: github.com/amphi-ai/amphi-etl\n我的足迹 育音堂凯旋路店在7月份将被拆除， 在6月底去看了最后一眼。\n无数年轻乐队从这里开始，从这里走向更大的舞台，只可惜“小白楼”不在了， 年轻乐队能够“演的起”的场地也不多了。\n影音记录 精选歌单 Live演出 06.09 Sun. Alfa Mist 摩登天空\n06.26 Wed. 惘闻 万代南梦宫\n电影 正值第26届上海国际电影节（SIFF）\n二刷 《完美的日子》，也亲眼见到了役所广司；\n《万物理论》，抢票时没仔细看， 以为是同名的霍金自传电影，结果是一部23年的一部打着多重宇宙幌子，对于黑白惊悚电影的拙劣模仿之作，属实是不推荐；\n《她》，在十年之后的AI元年再重新看这部电影，发现电影中的场景已然发生。\n《蒸发》，自90年代日本经济泡沫破裂后，大量人口由于经济等原因，选择人间蒸发。 与其说是“消失”，不如说是对于其设定角色的“逃离”。\n书\u0026amp;阅读摘录 本月完成： 《日本之镜：日本文化中的英雄与恶人》，《深度关系》。\n越来越喜欢肤浅的快乐，是我们的错吗？ https://mp.weixin.qq.com/s/t55b2swIFc9WcwIRLAMxgg\n网络上的东西，越来越快，越来越肤浅，“劲儿”越来越大。即使难得有深度、优质的创作，也难得静下心来，沉浸地去阅读、去感知。\n互联网和手机应用强调第一时间的快感反馈，通过直观的刺激，抢先进行引导，再使用各种各样的手段延长加强的刺激，让人停不下来。这个过程，相当于是绕过了大脑里负责思考的“系统2”，迫使人去做出直接的、即时的反应。 … 但从注意力的角度来讲，用脑来判断的时候也越来越少了，人们会不知不觉地跟着技术设计的脚步走，顺从“系统1”（也就是直觉）带来的简单判断。 … 这种设计，相当于占用了人们更长的空闲时间。 …\n技术设计原本是要侵占碎片时间，结果所有的时间，连同我们的注意力，都被切得粉碎。\n县城体制内女孩婚恋困局：不想将就，又怕结不了婚 https://news.ifeng.com/c/8YxP14GIPYq\n“比如说以前有几十个人天天来催你，到了29、30岁，可能只有几个人。30岁以上，几乎没有什么人来给你说亲了。因为你的机会、资源越来越少，周围人的信心也越来越弱。” … 如果没法在社会“约定俗成”的年纪顺利结婚，取而代之的，就是对个人价值的全方位否定。 晓媛参加一个亲戚的婚礼，看见台上像是强行凑成一对的新人，新娘年轻漂亮，至于新郎，“长得很不行。”\n县域体制内女青年普遍想选择比自己更优秀的体制内男性青年为婚配对象，但县域体制内男青年无论是数量还是质量上均无法满足这一择偶需求。\n她常说，自己年过40岁，才回归到传统中国文化中，理解中国人的行为。为什么红白喜事总是特别隆重，因为它代表着一种社群的仪式，承载着情感，以及人与人之间的链接。 …县城、乡镇就像一个中国传统家庭，讲人情，有温度，但也有越过边界的关心。\n为什么对于有的人来说 ChatGPT 并没有什么卵用？ https://baoyu.io/blog/ai/why-do-someone-think-chatgpt-doesnot-really-work-for-them\nLLM 擅长处理和生成文本，但是受限于上下文窗口长度，并且对于复杂的数据格式，需要用格式化的数据格式表达，并且要借助提示工程将复杂的任务进行拆解。\n如果要用好 LLM，最好是充分利用 LLM 的长处，把一些高频的使用场景，但是原本不适合或者无法自动化的工作流的，借助 LLM 的文本处理能力，变成自动化的工作流。\nAI 应用爆发前夜的三个信号｜鹅库2402 https://mp.weixin.qq.com/s/Fj3WfSP5-tUBT_-OfusYpw\n三个信号\n下一代大模型效果的提升和成本的大幅下降 已有玩家+AI 后的营收大幅增长 共识的应用层创业者浮出水面 我们在使用大语言模型 (LLMs) 构建产品一年中的经验总结 https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/\n战术\nn-shot 提示:\n如果 n 过低，模型可能会过度依赖这些特定示例，影响其泛化能力。一般来说，n 应该不小于 5，甚至可以达到几十个。 如果您使用支持工具的大语言模型，您的 n-shot 示例也应包括您希望智能体使用的工具。\n输入输出结构化:\nClaude 偏好 xml，而 GPT 则偏好 Markdown 和 JSON。\n编写小而精的提示词，专注做好一件事:\n将单一提示词拆分成了多个简单、专注且易于理解的提示词。这样，我们可以分别迭代和评估每个提示词的效果。\nRAG:\n别忘了关键词搜索：把它作为基线并在混合搜索中使用\n我们需要超越单一提示，拥抱工作流:\n将智能体行为设计成线性链、DAG 和状态机的形式；不同的依赖关系和逻辑关系适用于不同的任务规模。能否通过不同的任务架构来优化性能？\n一个有效的方法是让智能体系统生成确定性的计划，并以结构化、可重复的方式执行。\n缓存:\n缓存可以节省成本并消除响应延迟，因为它避免了对相同输入重新计算响应的需要。此外，如果一个响应之前已经经过安全审核，我们可以提供这些审核过的响应，从而减少提供有害或不适当内容的风险。\nLLM-as-Judge 可以工作，但它不是万能的:\n使用成对比较: 呈现两个选项并让它选择较好的一个 控制位置偏差: 选项的呈现顺序会影响大语言模型的决策。为了减少这种偏差，每次成对比较时都交换选项的顺序进行两次 允许平局 使用 Chain-of-Thought 方法 控制回复长度 我们发现传统分类器和奖励模型比 LLM-as-Judge 更准确，且成本更低、延迟更短。在代码生成方面，LLM-as-Judge 的表现可能不如直接执行代码的评估策略。\n对吴恩达 workflow 概念产品化的思考！ https://mp.weixin.qq.com/s/aO1NhCFCy3JFoN80cQjyVA\n吴恩达、Itamar Friedman、Harrison Chase等人一直在提倡 workflow、flow engineering 之类的概念，意在通过多次、分阶段的 LLM 调用、迭代来实现更好的应用效果。\n用自然语言描述清楚一项复杂工作是非常困难的，没有人能够光用嘴巴教你怎么样造火箭或者量子物理。\n要想进一步拓宽上限，就必须采用形式化的表达\n以下两种产品设计思路的方向是截然相反的，我们认为应该采取第一种：\n现有任务的集合，再决定设计出哪些节点（node）。 先设计出节点（node），再思考我设计出的节点能够完成哪些现实世界的任务。 当我们能够对任务进行形式化表达的时候，就能够思考对各种任务的中间过程状态的拆解以及合并归类的方法和策略，进而设计出合理的流程节点。\n经过思考，我们得出了两点结论，这两点结论指引我们对于 workflow 产品的设计：\nworkflow 的系统设计可以形式化地表达为 DFA 的构造。 workflow 的流程节点设计是给定约束条件下的 DFA 状态数量最小化问题。 workflow 系统是一个确定有限自动机:\n整个工作流产品可以用五元组来表示：\n有限状态集合 ：给定任务集合 （包括初始状态、中间状态、结束状态）所包含状态的有限集合。 有限流程节点集合 ：workflow 产品所提供的流程节点的集合。 状态迁移函数δ ： ，状态迁移函数的值是状态集合 的子集。迁移函数定义了在经过流程节点的处理后，workflow 如何从一个状态迁移到另一个状态。 初始状态 ：workflow 的初始状态， 结束状态集合 ：任务的结束状态集合， 夕小瑶：资本寒冬下的AI创业一年 https://zhuanlan.zhihu.com/p/701574775\n至此，我才终于有了比较清晰的融资思路——\n你要找到手里有钱，受资本寒冬影响尽可能小的投资机构 你要找到多次在你所处的赛道和阶段有较多投资记录的机构 你要找到跟你的团队优势、价值观匹配的投资机构 创业的核心是让你的产品跟市场匹配，简称PMF。而加速PMF的方法只有一个——尽可能的缩短你的试错成本。 更快的投向市场拿反馈，更快的迭代，迭代，再迭代，就是技术型创业的核心方法论。\n最后，我们发现我们高估了agent技术成熟的速度和市场接纳的速度。一方面agent的概念没有像ChatGPT一样深入到用户和客户的心智，如果你去年要做这个方向，首先要面临教育市场的问题。而一个天使轮的小公司，是不具备教育市场的能力的，营销成本很高；\n另一方面，agent的概念虽然好，但真实的业务场景细节很多，非标问题很多，还要去挑战甲方公司旧的利益分配体系，这意味着在agent技术变得充分通用、鲁棒之前，你很难用比较低的成本来满足客户需求。尽管我们通过一系列的手段，将大部分agent的定制交付周期从几周缩短到几天，但发现想再往下降成本的时候几乎就是技术不可解的问题了。因此交付成本很高；\nMilestone 忙着打老头环DLC\n","permalink":"https://niraya666.github.io/monthly/2024-06/","summary":"\u003ch1 id=\"ai-新闻\"\u003eAI 新闻\u003c/h1\u003e\n\u003ch2 id=\"claude-35-sonnet\"\u003e\u003cstrong\u003eClaude 3.5 Sonnet\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.anthropic.com/news/claude-3-5-sonnet\"\u003ehttps://www.anthropic.com/news/claude-3-5-sonnet\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e相比其前身 Claude 3 Opus，Claude 3.5 Sonnet 的运行速度提升了两倍，同时在多项基准测试中表现优异。此外，Claude 3.5 Sonnet 还引入了名为“Artifacts”的新功能，使用户可以在与 AI 对话的过程中实时查看和编辑生成的内容。\u003c/p\u003e\n\u003ch2 id=\"开源模型\"\u003e开源模型\u003c/h2\u003e\n\u003ch3 id=\"qwen-2\"\u003eQwen 2\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://qwenlm.github.io/zh/blog/qwen2/\"\u003ehttps://qwenlm.github.io/zh/blog/qwen2/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e一共开源了Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B以及\u003cstrong\u003eQwen2-72B\u003c/strong\u003e 5个模型**。\u003cstrong\u003e增大了上下文长度支持，最高达到\u003c/strong\u003e128K** tokens（Qwen2-72B-Instruct）。 代码和数学能力显著提升。多个评测基准上的领先表现。 同时还开源了一个智能体解决方案，用于高效处理100万tokens级别的上下文： \u003ca href=\"https://qwenlm.github.io/zh/blog/qwen-agent-2405/\"\u003e**使用Qwen-Agent将上下文记忆扩展到百万量级\u003c/a\u003e。**\u003c/p\u003e\n\u003ch3 id=\"gemma2\"\u003eGemma2\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/\"\u003ehttps://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e包括两个主要版本，分别是 9 B参数和 27 B参数的模型。27B 的 Gemma 2 模型在多个基准测试中表现优异，超过了 Llama 3 70B 模型的性能。\u003c/p\u003e\n\u003cp\u003eGemma 2 采用了新的架构设计，使其在推理时更加高效\u003c/p\u003e\n\u003ch3 id=\"nemotron-4-340b\"\u003eNemotron-4 340B\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/\"\u003ehttps://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e包含基础模型、指示模型和奖励模型的综合性数据生成流水线，用于训练大型语言模型。\u003c/p\u003e\n\u003cp\u003e提供了一种生成合成训练数据的新方法，旨在提升定制语言模型的性能和鲁棒性。\u003c/p\u003e\n\u003ch3 id=\"deepseek-coder-v2-代码模型\"\u003eDeepSeek-Coder-V2 代码模型\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct\"\u003ehttps://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e开源的专家混合模型，性能媲美GPT4-Turbo，专为代码特定任务优化，支持多源高质量语料库预训练，显著提升编程和数学推理能力，支持338种编程语言，扩展上下文长度至128K。\u003c/p\u003e\n\u003ch1 id=\"值得关注的开源项目\"\u003e值得关注的开源项目\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003ePerplexica\u003c/strong\u003e： 开源版本Perplexity+支持本地LLM。 GitHub：\u003ca href=\"http://github.com/ItzCrazyKns/Perplexica\"\u003egithub.com/ItzCrazyKns/Perplexica\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003etranslation-agent\u003c/strong\u003e：吴恩达老师开源了一套 AI Agent 翻译工作流。通过LLM初步翻译， 而后对翻译结果进行反思，并提出改进建议，最后再根据意见进行最终版本的翻译。GitHub： \u003ca href=\"http://github.com/andrewyng/translation-agent\"\u003egithub.com/andrewyng/translation-agent\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStreamer-Sales 销冠 —— 卖货主播 LLM 大模型\u003c/strong\u003e： 一个能够根据给定的商品特点从激发用户购买意愿角度出发进行商品解说的卖货主播大模型。 另外还集成了 LMDeploy 加速推理🚀、RAG检索增强生成 📚、TTS文字转语音🔊、数字人生成 🦸、 Agent 使用网络查询实时信息🌐、ASR 语音转文字🎙️。 GitHub：\u003ca href=\"http://github.com/PeterH0323/Streamer-Sales\"\u003egithub.com/PeterH0323/Streamer-Sales\u003c/a\u003e\u003c/p\u003e","title":"2024-06 月刊"},{"content":"本文将简单介绍如何评价LLM的tool-using 能力。\n引言 在工具使用评估方面，过去的研究主要有以下几种思路：\n对比工具使用和纯LLM在基准测试上的分数：例如Toolformer和LATM。\n在Toolformer研究中，通过下游任务如语言模型评估基准测试、数学推理任务和问答任务来验证工具使用的有效性。\nLATM则采用了来自BigBench的六个数据集进行评估。\n测试工具使用的准确率和响应质量：例如API-Bank。\n在评估过程中，首先初始化评估系统，确保每个API的数据库包含默认值。然后，将预测的API调用与手动标注的API调用进行比较，以确定它们的一致性。响应评估则使用ROUGE-L指标。 利用LLM对工具使用的效果进行评价：例如Tool-bench。\ntwo evaluation metrics:\nPass Rate: Calculates the proportion of successfully completing an instruction within limited OpenAI API calls.\nPreference: Measured by comparing two answers (action sequences) for a given instruction.We pre-define a set of criteria for a better answer, which are organized as prompts for ChatGPT.\n构造虚拟运行环境，测试代理与环境的交互结果：例如ToolAlpaca。\n利用LLM模拟环境（用户代理和助手代理），并使用GPT-4对ToolAlpaca模型进行机器评估，评估其使用各种未见工具的能力。 对于绝大多数企业和垂直场景下， 其中思路1需要构建额外的测试集成本比较高（但还是有必要的）， 而思路4构造虚拟运行环境实际上并不现实； 所以还是选择思路2，外加通过思路3辅助判断；换句话说， 根据场景，构造工具列表和工具调用的ground-truth（包括函数名， 和传入参数） ，在存在歧义时，采用ROUGE评价响应质量， 或者使用LLM判断响应结果并评价。\n顺带提一下Langchain 项目中有关Agent的tool-using能力测试的内容， 不过由于Langchain项目又臭又长，且有很大的局限性， 这里只讨论其思路。\n在 Benchmarking Agent Tool Use 一文中，提出了4种指标用于评价Agent 的tool-using 能力：\nCorrectness 即通过LLM判断工具调用结果是否同ground truth 相同；\nCorrect final state 即检查agent 所交互环境的最终状态是否同预期相同；\nIntermediate step correctness 除了最终状态， 也可以检查中间状态；\nRatio of steps taken to the expected steps 即实际步骤与预期步骤的比例。\n在LangChain Benchmarks 项目提供了4个（3个）任务用于测试tool-using， 本质上也是一种构造虚拟环境并与之交互。\nMultiverse Math 任务 ： 要求模型使用提供的工具来解决数学问题。为了确保模型不依赖于内在知识，数学运算被修改以产生不同于预期的结果。例如，乘法和加法的结果会有所不同，但仍保留某些数学性质。 任务示例包括基本的加减乘除运算、幂运算、对数运算等。\ncode \u0026#34;\u0026#34;\u0026#34;Solve basic math question using the provided tools. Must use the provided tools to solve the math question. To make sure that innate knowledge is not used, the math operations have been altered to yield different results than expected. The modified operations should yield different results, but still retain appropriate properties. For example, the modified multiplication operation should still be commutative. Please note that the modified operations are not guaranteed to even make sense in the real world since not all properties will be retained (e.g., distributive property). For example, I ate 1 apple and 2 oranges every day for 7 days. How many fruits did I eat? One would expect the answer to be 21, but in this universe, the answer is 32.34. In addition, it depends on how the operations are grouped: (1 + 2) * 7 = 32.34 But: 1 * 7 + 2 * 7 = 24.3 Due to these changes certain questions are not allowed as inputs as they would yield different results if evaluated in different ways. For example, \u0026#34;convert 15 degrees to radians\u0026#34; is not allowed as an input as it could be interpreted as either: divide(multiply(15, pi()), 180) or multiply(divide(15, 180), pi()) \u0026#34;\u0026#34;\u0026#34; Relational Data任务 ：要求模型使用提供的工具来回答关于关系数据的问题。环境中包含关于用户及其位置和喜好食物的虚假数据，模型需要使用工具查询这些数据以回答问题。 任务示例包括查询用户的位置信息、喜好食物等\ncode \u0026#34;\u0026#34;\u0026#34;Answer questions about relational data using the provided tools. The environment contains fake data about users and their locations and favorite foods. The environment provides a set of tools that can be used to query the data. All questions can be answered by using the provided tools. The answers include the expected result as well as the most efficient way to answer the question using the tools. \u0026#34;\u0026#34;\u0026#34; Typewriter(single-tool, 26-tools): 要求模型使用工具逐字打印给定的字符串。模型需要调用工具来模拟打字机的行为，每次打印一个字符，直到完成整个字符串。 任务示例包括打印特定的句子或段落。\ncode ```python \u0026quot;\u0026quot;\u0026quot;A task where the agent must type a given string one letter at a time. In this variation of the task, the agent is given a single function, that takes a letter as an argument. \u0026quot;\u0026quot;\u0026quot; ``` 根据综述文章《What Are Tools Anyway? A Survey from the Language Model Perspective》，在验证代理或模型的工具使用能力时，可以关注以下指标：\nTask completion：任务完成度，或pass-ratio 这是一个相对宏观的指标， 工具调用的最核心目的就是为了完成任务， 任务的成功与否表征了工具调用的最终结果。\nTool selection： 如何在众多工具中挑选到最合适的工具，这考验了模型在特定场景下对任务的理解和最优化的工具选择；\nTool reusability： 指的是一个工具在不同情境下或解决不同问题时能够被多次使用的能力， 不过往往和工具生成（tool-marking）相关， 不在这次的讨论范围中。\n方法论 综合以上讨论， tool-using 的测试可以简单分为：\n单轮简单测试 在这种测试中，主要测试function_name 和传入参数。关注的指标是pass-rate 和 Preference。具体方法是不需要使用LLM进行判断，只需将返回的function_name 和 function_arguments 与ground-truth进行比对，确认是否一致。最终使用pass-ratio 评价测试结果。\nJson-mode验证 由于并非所有模型都能严格按要求输出所需要的json格式，考虑到这种情况，可以进行Json-mode的验证，以确保输出格式的正确性。\n优化版本\n通过ROUGE判断function_arguments\n目的：避免严格一致所带来的问题。 通过LLM辅助判断function_arguments\n优点：准确率更高。\n缺点：成本也更高。\n多轮调用 以上单轮tool-using测试在复杂场景下不太适用。\n在一些复杂场景中，如代码生成与执行、SQL生成并执行等，需要按照一定顺序调用多个tools。这些场景需要LLM根据前几次tool-using的结果反馈，调整function_arguments 或 function_name。\n这些场景无外乎都需要LLM根据前几次tool-using的结果反馈， 修改function_arguments 或者是function_name；\n将tool-using的过程抽象为强化学习中agent同环境(env)的交互过程。目标是减少agent与环境的交互次数，从而降低成本，并争取一次或者几次就能做对，避免重复错误。可以借用langchain中的思想，增加对中间和最终状态的检查，并比对实际交互次数和预期交互次数。这类测试需要额外构建虚拟环境，超出此次讨论范围。\n测试集 NousResearch/func-calling-eval-glaive\nNousResearch/func-calling-eval-singleturn\nNousResearch/func-calling-eval\n上手：测试LLM的tool-using 能力 这里，选择最简单的tool-using用于演示， 即直接调用OpenAI 的tool-using API\n环境 import os # os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = \u0026#34;...\u0026#34; from openai import OpenAI import json client = OpenAI() 定义工具函数 考虑了一个简单的医疗场景，共4个工具；\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_medical_definition\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the definition of a medical term\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;term\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The medical term to define\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;term\u0026#34;] } } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_treatment_options\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get treatment options for a disease\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;disease\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The disease to get treatment options for\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;disease\u0026#34;] } } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_medication_side_effects\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the potential side effects of a medication\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;medication\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The name of the medication\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;medication\u0026#34;] } } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_disease_symptoms\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the symptoms associated with a specific disease\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;disease\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The disease to get symptoms for\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;disease\u0026#34;] } } } ] 定义测试用例 创建测试用例数据集，每个测试用例包含查询、预期函数调用及其参数、以及期望的响应。\n使用GPT-4构建。\ntest_cases = [ { \u0026#34;query\u0026#34;: \u0026#34;What are the symptoms of influenza?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_disease_symptoms\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;disease\u0026#34;: \u0026#34;influenza\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Symptoms of influenza include fever, chills, muscle aches, cough, congestion, runny nose, headaches, and fatigue.\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What is the definition of asthma?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_medical_definition\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;term\u0026#34;: \u0026#34;asthma\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Asthma is a condition in which your airways narrow and swell and may produce extra mucus, making breathing difficult and triggering coughing, wheezing, and shortness of breath.\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What treatments are available for rheumatoid arthritis?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_treatment_options\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;disease\u0026#34;: \u0026#34;rheumatoid arthritis\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Treatment options for rheumatoid arthritis include medications such as NSAIDs, steroids, DMARDs, and biologics, as well as physical therapy and lifestyle changes.\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What are the side effects of metformin?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_medication_side_effects\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;medication\u0026#34;: \u0026#34;metformin\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Common side effects of metformin include gastrointestinal issues such as diarrhea, nausea, and abdominal discomfort, as well as lactic acidosis in rare cases.\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What symptoms are associated with hypothyroidism?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_disease_symptoms\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;disease\u0026#34;: \u0026#34;hypothyroidism\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Symptoms of hypothyroidism include fatigue, weight gain, cold intolerance, dry skin, constipation, and depression.\u0026#34; }, ... ] 定义测试metrics 自定义了pass-ratio，即function_name, function_arguments 严格一致为pass。\n运行测试并验证响应 def call_openai_tool(model, prompt, tools): response = client.chat.completions.create( model=model, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt} ], tools=tools, tool_choice=\u0026#34;auto\u0026#34; ) return response def run_tests(test_cases, model=deployment_name): passed_tests = 0 failed_tests = 0 for case in test_cases: response = call_openai_tool(model, case[\u0026#39;query\u0026#39;], tools) if case[\u0026#39;expected_function\u0026#39;] and case[\u0026#39;function_arguments\u0026#39;]: if response.choices[0].message.tool_calls: tool_calls = response.choices[0].message.tool_calls # 验证函数调用名称和参数 try: assert tool_calls[0].function.name == case[\u0026#39;expected_function\u0026#39;], f\u0026#34;Expected function {case[\u0026#39;expected_function\u0026#39;]}, but got {tool_calls[0].function.name}\u0026#34; assert json.loads(tool_calls[0].function.arguments) == case[\u0026#39;function_arguments\u0026#39;], f\u0026#34;Expected arguments {case[\u0026#39;function_arguments\u0026#39;]}, but got {tool_calls[0].function.arguments}\u0026#34; print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 except AssertionError as e: print(e) print(f\u0026#34;Test failed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test failed for query (no tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: # # 验证响应内容 # response_text = response.choices[0].message.content.strip() # 验证没有调用工具 if response.choices[0].message.tool_calls: print(f\u0026#34;Test failed for query (unexpected tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 total_tests = passed_tests + failed_tests accuracy = (passed_tests / total_tests) * 100 if total_tests \u0026gt; 0 else 0 print(f\u0026#34;Total tests: {total_tests}, Passed: {passed_tests}, Failed: {failed_tests}, Accuracy: {accuracy:.2f}%\u0026#34;) run_tests(test_cases) 结果：\nTest passed for query: What are the symptoms of chronic kidney disease? Test passed for query: What does the medical term \u0026#39;bipolar disorder\u0026#39; mean? Expected arguments {\u0026#39;disease\u0026#39;: \u0026#39;chronic obstructive pulmonary disease\u0026#39;}, but got { \u0026#34;disease\u0026#34;: \u0026#34;COPD\u0026#34; } Test failed for query: What are the treatment options for chronic obstructive pulmonary disease (COPD)? Test passed for query: What side effects are associated with the use of aspirin? Test passed for query: Tell me a joke. Test passed for query: What is the capital of France? Total tests: 22, Passed: 21, Failed: 1, Accuracy: 95.45% 对于test failed case， 这只是个模凌两可的错误。\n如果采用LLM作为裁判是可以避免的。\n基于ROUGE的响应判断 环境\n! pip install rouge-score --quiet from rouge_score import rouge_scorer def calculate_rouge_l(predicted, expected): scorer = rouge_scorer.RougeScorer([\u0026#39;rougeL\u0026#39;], use_stemmer=True) scores = scorer.score(predicted, expected) return scores[\u0026#39;rougeL\u0026#39;].fmeasure def run_tests_with_rouge(test_cases, model=deployment_name, rouge_l_threshold=0.8): passed_tests = 0 failed_tests = 0 for case in test_cases: response = call_openai_tool(model, case[\u0026#39;query\u0026#39;], tools) if case[\u0026#39;expected_function\u0026#39;] and case[\u0026#39;function_arguments\u0026#39;]: if response.choices[0].message.tool_calls: tool_calls = response.choices[0].message.tool_calls # 验证函数调用名称和参数 try: assert tool_calls[0].function.name == case[\u0026#39;expected_function\u0026#39;], f\u0026#34;Expected function {case[\u0026#39;expected_function\u0026#39;]}, but got {tool_calls[0].function.name}\u0026#34; predicted_arguments = json.loads(tool_calls[0].function.arguments) # 计算ROUGE-L分数 rouge_l_score = calculate_rouge_l(json.dumps(predicted_arguments), json.dumps(case[\u0026#39;function_arguments\u0026#39;])) assert rouge_l_score \u0026gt;= rouge_l_threshold, f\u0026#34;ROUGE-L score {rouge_l_score} is less than threshold {rouge_l_threshold}\u0026#34; print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 except AssertionError as e: print(e) print(f\u0026#34;Test failed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test failed for query (no tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: # 验证没有调用工具 if response.choices[0].message.tool_calls: print(f\u0026#34;Test failed for query (unexpected tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 total_tests = passed_tests + failed_tests accuracy = (passed_tests / total_tests) * 100 if total_tests \u0026gt; 0 else 0 print(f\u0026#34;Total tests: {total_tests}, Passed: {passed_tests}, Failed: {failed_tests}, Accuracy: {accuracy:.2f}%\u0026#34;) 使用ROUGE-L 计算function_arguments 同预期的差距，若大于一定阈值，则判断失败。\n基于LLM验证和判断 def chat_completion_request(messages, tools=None, tool_choice=None, model=deployment_name, json_mode=False): try: params = { \u0026#39;model\u0026#39;: model, \u0026#39;messages\u0026#39;: messages, \u0026#39;tools\u0026#39;: tools, \u0026#39;tool_choice\u0026#39;: tool_choice } if json_mode: params[\u0026#39;response_format\u0026#39;] = { \u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34; } response = client.chat.completions.create(**params) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e def evaluate_with_llm(predicted_arguments, expected_arguments, model, tools): prompt = f\u0026#34;\u0026#34;\u0026#34; Compare the following predicted arguments and expected arguments for a function call. Predicted Arguments: {json.dumps(predicted_arguments, indent=2)} Expected Arguments: {json.dumps(expected_arguments, indent=2)} Evaluate if the predicted arguments sufficiently match the expected arguments, considering minor differences acceptable. Respond with a JSON object containing a single key \u0026#34;match\u0026#34; with a boolean value indicating if they match. Example response: {{ \u0026#34;match\u0026#34;: true }} \u0026#34;\u0026#34;\u0026#34; response = chat_completion_request(messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: prompt}],model = model, json_mode=True) json_response = json.loads(response.choices[0].message.content) return json_response.get(\u0026#34;match\u0026#34;, False) def run_tests_with_llm(test_cases, model=deployment_name, rouge_l_threshold=0.8): passed_tests = 0 failed_tests = 0 for case in test_cases: response = call_openai_tool(model, case[\u0026#39;query\u0026#39;], tools) if case[\u0026#39;expected_function\u0026#39;] and case[\u0026#39;function_arguments\u0026#39;]: if response.choices[0].message.tool_calls: tool_calls = response.choices[0].message.tool_calls # 验证函数调用名称和参数 try: assert tool_calls[0].function.name == case[\u0026#39;expected_function\u0026#39;], f\u0026#34;Expected function {case[\u0026#39;expected_function\u0026#39;]}, but got {tool_calls[0].function.name}\u0026#34; predicted_arguments = json.loads(tool_calls[0].function.arguments) # 使用 LLM 来判断参数匹配 match = evaluate_with_llm(predicted_arguments, case[\u0026#39;function_arguments\u0026#39;], model, tools) assert match, \u0026#34;LLM evaluation determined the arguments do not match\u0026#34; print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 except AssertionError as e: print(e) print(f\u0026#34;Test failed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test failed for query (no tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: # 验证没有调用工具 if response.choices[0].message.tool_calls: print(f\u0026#34;Test failed for query (unexpected tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 total_tests = passed_tests + failed_tests accuracy = (passed_tests / total_tests) * 100 if total_tests \u0026gt; 0 else 0 print(f\u0026#34;Total tests: {total_tests}, Passed: {passed_tests}, Failed: {failed_tests}, Accuracy: {accuracy:.2f}%\u0026#34;) 附: prompt 生成示例 def chat_completion_request(messages, tools=None, tool_choice=None, model=deployment_name, json_mode=False): try: params = { \u0026#39;model\u0026#39;: model, \u0026#39;messages\u0026#39;: messages, \u0026#39;tools\u0026#39;: tools, \u0026#39;tool_choice\u0026#39;: tool_choice } if json_mode: params[\u0026#39;response_format\u0026#39;] = { \u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34; } response = client.chat.completions.create(**params) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e def generate_test_cases_from_tools(tools, examples): tools_descriptions = \u0026#34;\\n\u0026#34;.join([ f\u0026#34;- {tool[\u0026#39;function\u0026#39;][\u0026#39;name\u0026#39;]}: {tool[\u0026#39;function\u0026#39;][\u0026#39;description\u0026#39;]} (Parameters: {\u0026#39;, \u0026#39;.join(tool[\u0026#39;function\u0026#39;][\u0026#39;parameters\u0026#39;][\u0026#39;properties\u0026#39;].keys())})\u0026#34; for tool in tools ]) prompt = f\u0026#34;\u0026#34;\u0026#34; Create a series of test cases for evaluating the tool-using capabilities of a language model. The test cases should be relevant to the domain and should utilize the provided tools. Each test case should include the following fields: - query: A question or request related to the domain. - expected_function: The function that should be called to answer the query. - function_arguments: The arguments that should be passed to the function. - expected_response: The expected response from the function. The available tools are: {tools_descriptions} Here are some examples to follow: {examples} Now, generate 10 more test cases in the same format utilizing the provided tools. Make sure the output format is JSON and enclose the result in triple backticks (```): \u0026#34;\u0026#34;\u0026#34; return prompt # Example usage tools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_medical_definition\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the definition of a medical term\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;term\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The medical term to define\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;term\u0026#34;] } } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_treatment_options\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get treatment options for a disease\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;disease\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The disease to get treatment options for\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;disease\u0026#34;] } } } ] examples = \u0026#34;\u0026#34;\u0026#34; [ { \u0026#34;query\u0026#34;: \u0026#34;What is diabetes?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_medical_definition\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;term\u0026#34;: \u0026#34;diabetes\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Diabetes is a chronic condition that affects the way the body processes blood sugar (glucose).\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;How can hypertension be treated?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_treatment_options\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;disease\u0026#34;: \u0026#34;hypertension\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Treatment options for hypertension include lifestyle changes, such as diet and exercise, and medications like ACE inhibitors, beta-blockers, and diuretics.\u0026#34; } ] \u0026#34;\u0026#34;\u0026#34; sys_prompt = generate_test_cases_from_tools(tools, examples) messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: sys_prompt} ] resp = chat_completion_request(messages, json_mode=True) test_cases = eval(resp.choices[0].message.content) 参考 Function-calling \u0026amp; JSON-mode Evaluation\nBenchmarking Agent Tool Use\nWhat Are Tools Anyway? A Survey from the Language Model Perspective\nBerkeley Function Calling Leaderboard\nToolformer\nLATM\nAPI-Bank\nTool-bench\nToolAlpaca\n","permalink":"https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B%E7%9A%84tool-using%E8%83%BD%E5%8A%9B/","summary":"\u003cp\u003e本文将简单介绍如何评价LLM的tool-using 能力。\u003c/p\u003e\n\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e在工具使用评估方面，过去的研究主要有以下几种思路：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e对比工具使用和纯LLM在基准测试上的分数\u003c/strong\u003e：例如\u003ca href=\"https://arxiv.org/abs/2302.04761\"\u003eToolformer\u003c/a\u003e和\u003ca href=\"https://arxiv.org/abs/2305.17126\"\u003eLATM\u003c/a\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e在Toolformer研究中，通过下游任务如语言模型评估基准测试、数学推理任务和问答任务来验证工具使用的有效性。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLATM则采用了来自BigBench的六个数据集进行评估。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e测试工具使用的准确率和响应质量\u003c/strong\u003e：例如\u003ca href=\"https://arxiv.org/abs/2304.08244\"\u003eAPI-Bank\u003c/a\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在评估过程中，首先初始化评估系统，确保每个API的数据库包含默认值。然后，将预测的API调用与手动标注的API调用进行比较，以确定它们的一致性。响应评估则使用ROUGE-L指标。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e利用LLM对工具使用的效果进行评价\u003c/strong\u003e：例如Tool-bench。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003etwo evaluation metrics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePass Rate\u003c/strong\u003e: Calculates the proportion of successfully completing an instruction within limited OpenAI API calls.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePreference\u003c/strong\u003e: Measured by comparing two answers (action sequences) for a given instruction.We pre-define a set of criteria for a better answer, which are organized as prompts for ChatGPT.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e构造虚拟运行环境，测试代理与环境的交互结果\u003c/strong\u003e：例如ToolAlpaca。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e利用LLM模拟环境（用户代理和助手代理），并使用GPT-4对ToolAlpaca模型进行机器评估，评估其使用各种未见工具的能力。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于绝大多数企业和垂直场景下， 其中思路1需要构建额外的测试集成本比较高（但还是有必要的）， 而思路4构造虚拟运行环境实际上并不现实； 所以还是选择思路2，外加通过思路3辅助判断；换句话说， 根据场景，构造工具列表和工具调用的ground-truth（包括函数名， 和传入参数）  ，在存在歧义时，采用ROUGE评价响应质量， 或者使用LLM判断响应结果并评价。\u003c/p\u003e\n\u003cp\u003e顺带提一下Langchain 项目中有关Agent的tool-using能力测试的内容， 不过由于Langchain项目又臭又长，且有很大的局限性， 这里只讨论其思路。\u003c/p\u003e","title":"Agent学习笔记： 如何验证模型的tool-using能力"},{"content":"Sistemas Inestables是来自智利圣地亚哥的器乐三重奏乐队。他们的音乐以行进和复杂的结构为特征，并伴随着噪音和氛围的层次。\n风格与特点 多元音乐风格：Sistemas Inestables 融合了噪音、氛围、数学摇滚和后摇滚等多种音乐风格。他们通过混合装置、合成器和鼓机编织出复杂的音纹和和谐，形成了独特的音乐特点。\n即兴创作与实验：乐队在创作过程中强调即兴演奏与实验，探索人与机器的对话和斗争，创造出既真实又虚拟、既模拟又数字、既人工又自然的音乐体验。\n影响与灵感 主要影响：Sistemas Inestables 受到多位艺术家的影响，包括 Godspeed You! Black Emperor, Akinetón Retard, Cómo Asesinar a Felipes, tortuganónima, Pol del Sur, Jaga Jazzist, Floating Points, Brandt Brauer Frick, Jon Hopkins, Nils Frahm, The Cinematic Orchestra, Battles, Tortoise, Do Make Say Think 等。\n灵感来源：他们的创作灵感来自环境、过程、情感和挑战，音乐表现出对当下社会问题的深刻反思和回应，也是对当前事件的抗议和改变的呼吁。\n成立与早期阶段 2016年，Mauricio、Javier Hechenleitner 和 José Tomás 决定开始一个新项目。经过一年的实验，Sistemas Inestables 诞生。在创作首张专辑《O》的过程中，他们进行了大量即兴演奏，寻找三人之间的音乐连接，逐渐形成了他们的音乐风格。\n早期演出与发展 2017年，乐队首次在圣地亚哥的 La Batuta 现场演出，并在哥伦比亚和阿根廷进行了巡演。2018年，他们参加了由 LeRockPsicophonique 唱片公司组织的首届音乐节，并在智利各地巡演。\n国际巡演与成长 2019年，乐队首次前往欧洲，在巴塞罗那的 Primavera Sound 音乐节和比利时的 Dunk! Fest 演出。年底，他们邀请长期担任乐队音响工程师的 Santiago Corvalán 加入乐队担任贝斯手。\n新音乐与当前发展 尽管受到智利社会动荡和新冠疫情的影响，Sistemas Inestables 在此期间创作了新音乐，并在2022年发布了新 EP《Oppidum》。该 EP 的两首曲目“Signum”和“Praedatum”描绘了城市的反乌托邦景象，回应了社会和环境危机。\n乐队简介（翻译自官方Spotify介绍） Sistemas Inestables 是来自智利圣地亚哥的一个器乐三重奏。他们的音乐以进步和复杂的结构为特征，并伴随着噪音和氛围的层次。清晰的旋律和明确的部分从这种声音中浮现，创造出音乐旅程的演变，并通过不同的环境进行叙述。\nSistemas Inestables es un trío instrumental de Santiago de Chile. Su música se caracteriza por ser progresiva y de estructuras complejas, junto a capas de ruido y texturas ambientales. Melodías claras y secciones definidas emergen de este paisaje sonoro, creando una evolución en el viaje musical y narrando a través de diferentes ambientes.\nJavier、Santiago和José Tomás发挥到极致，操作一套混合乐器设备，将传统爵士和摇滚乐器与电子键盘、合成器、鼓机和模拟效果器的组合形成对比，赋予他们独特且充满活力的声音。所有这些元素与抽象的视觉方案同步，创造出一种简约却强大的现场体验。在 Sistemas Inestables 的音乐中，没有图像或形象化的表达，这是一场真实且原始的旅程。\nJavier, Santiago y José Tomás operan (al límite de sus capacidades) un set híbrido que contrasta la utilización de instrumentos tradicionales del jazz y el rock, con un conjunto electrónico de teclados, sintetizadores, cajas de ritmos y efectos análogos que les confiere un sonido único y vibrante. Todos estos recursos se sincronizan con una propuesta visual abstracta para crear una experiencia minimalista pero poderosa en directo. En S/I no hay imágenes ni representación, es un viaje concreto y elemental.\n自2017年4月首次登场以来，Sistemas Inestables 已在国际上巡回演出，在美国、阿根廷、乌拉圭、秘鲁、厄瓜多尔、哥伦比亚、比利时、西班牙和智利等国家的各种场馆和音乐节上演出。2018年9月，他们发行了首张专辑《O》，随后在2019年比利时 Dunk! 后摇滚音乐节上录制了一张现场和重新编排的专辑。目前，Sistemas Inestables 正在发布一系列 EP，这些 EP 将形成一个三部曲，最终将在未来几个月内发行他们的下一张完整专辑。\nDesde su debut en abril de 2017, SISTEMAS INESTABLES ha realizado giras internacionales, presentándose en diversas salas y festivales de países como Estados Unidos, Argentina, Uruguay, Perú, Ecuador, Colombia, Bélgica, España y Chile. En septiembre de 2018, lanzaron su álbum debut \u0026ldquo;O\u0026rdquo;, al que desde entonces le ha seguido un álbum de recomposiciones y en directo, grabado en el festival belga de post-rock Dunk! Festival en 2019. Actualmente S/I se encuentra lanzando una serie de EPs, trilogía que conformará el próximo larga duración de la banda a lanzarse los próximos meses.\n参考 Spotify:Sistemas Inestables\nTREEFORT FEST 2024 PROFILE: SISTEMAS INESTABLES\nFOCUS // SISTEMAS INESTABLES: HOW TO FUNCTION IN THE MIDST OF CHAOS\nSISTEMAS INESTABLES\n","permalink":"https://niraya666.github.io/musik/intorduction_of_si/","summary":"\u003cp\u003eSistemas Inestables是来自智利圣地亚哥的器乐三重奏乐队。他们的音乐以行进和复杂的结构为特征，并伴随着噪音和氛围的层次。\u003c/p\u003e\n\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/artist/6RGPbHSp1Rby5CCzH7psxm?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\n\u003ch3 id=\"风格与特点\"\u003e风格与特点\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e多元音乐风格\u003c/strong\u003e：Sistemas Inestables 融合了噪音、氛围、数学摇滚和后摇滚等多种音乐风格。他们通过混合装置、合成器和鼓机编织出复杂的音纹和和谐，形成了独特的音乐特点。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e即兴创作与实验\u003c/strong\u003e：乐队在创作过程中强调即兴演奏与实验，探索人与机器的对话和斗争，创造出既真实又虚拟、既模拟又数字、既人工又自然的音乐体验。\u003c/p\u003e\n\u003ch3 id=\"影响与灵感\"\u003e影响与灵感\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e主要影响\u003c/strong\u003e：Sistemas Inestables 受到多位艺术家的影响，包括 Godspeed You! Black Emperor, Akinetón Retard, Cómo Asesinar a Felipes, tortuganónima, Pol del Sur, Jaga Jazzist, Floating Points, Brandt Brauer Frick, Jon Hopkins, Nils Frahm, The Cinematic Orchestra, Battles, Tortoise, Do Make Say Think 等。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e灵感来源\u003c/strong\u003e：他们的创作灵感来自环境、过程、情感和挑战，音乐表现出对当下社会问题的深刻反思和回应，也是对当前事件的抗议和改变的呼吁。\u003c/p\u003e\n\u003ch3 id=\"成立与早期阶段\"\u003e成立与早期阶段\u003c/h3\u003e\n\u003cp\u003e2016年，Mauricio、Javier Hechenleitner 和 José Tomás 决定开始一个新项目。经过一年的实验，Sistemas Inestables 诞生。在创作首张专辑《O》的过程中，他们进行了大量即兴演奏，寻找三人之间的音乐连接，逐渐形成了他们的音乐风格。\u003c/p\u003e\n\u003ch3 id=\"早期演出与发展\"\u003e早期演出与发展\u003c/h3\u003e\n\u003cp\u003e2017年，乐队首次在圣地亚哥的 La Batuta 现场演出，并在哥伦比亚和阿根廷进行了巡演。2018年，他们参加了由 LeRockPsicophonique 唱片公司组织的首届音乐节，并在智利各地巡演。\u003c/p\u003e\n\u003ch3 id=\"国际巡演与成长\"\u003e国际巡演与成长\u003c/h3\u003e\n\u003cp\u003e2019年，乐队首次前往欧洲，在巴塞罗那的 Primavera Sound 音乐节和比利时的 Dunk! Fest 演出。年底，他们邀请长期担任乐队音响工程师的 Santiago Corvalán 加入乐队担任贝斯手。\u003c/p\u003e","title":"【冷门乐队推荐】Sistemas Inestables"},{"content":"上周看了《坂本龙一：Opus》，特别喜欢其中有一首未收录在之前作品中的曲目《for Johann》\n第一反应是这首曲子可能是献给已故的冰岛作曲家Johann Johannsson，也是我最喜欢的音乐人之一\n不过，我找不到确切的证据\n万一是巴赫（Johann Sebastian Bach） 也有可能\n总觉得这首歌的旋律走向和Johann Johannsson的作品有某种神似，也有可能是我的错觉罢了吧\n试着弹一下这首歌， 顺带用AnthemScore扒了下谱子 可能有些小错误，但我希望能将这份音乐传递下去\n希望大家喜欢\nHe’s gone, but the music remains.\nありがとうございます\n感谢教授的最后礼物， 也感谢同样被迫包场的另两位陌生人\n","permalink":"https://niraya666.github.io/musik/for_johann/","summary":"\u003cp\u003e上周看了《坂本龙一：Opus》，特别喜欢其中有一首未收录在之前作品中的曲目《for Johann》\u003c/p\u003e\n\u003cp\u003e第一反应是这首曲子可能是献给已故的冰岛作曲家Johann Johannsson，也是我最喜欢的音乐人之一\u003c/p\u003e\n\u003cp\u003e不过，我找不到确切的证据\u003c/p\u003e\n\u003cp\u003e万一是巴赫（Johann Sebastian Bach） 也有可能\u003c/p\u003e\n\u003cp\u003e总觉得这首歌的旋律走向和Johann Johannsson的作品有某种神似，也有可能是我的错觉罢了吧\u003c/p\u003e\n\u003cp\u003e试着弹一下这首歌， 顺带用AnthemScore扒了下谱子\n可能有些小错误，但我希望能将这份音乐传递下去\u003c/p\u003e\n\u003cp\u003e希望大家喜欢\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/music/for_johnann/1.png\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/img/music/for_johnann/2.png\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/img/music/for_johnann/3.png\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/img/music/for_johnann/4.png\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHe’s gone, but the music remains.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eありがとうございます\u003c/p\u003e\n\u003cp\u003e感谢教授的最后礼物， 也感谢同样被迫包场的另两位陌生人\u003c/p\u003e","title":"【钢琴谱分享】坂本龙一《for Johann》"},{"content":"AI GPT4-o 发布 OpenAI 推出了新一代多模态模型 GPT-4o，它不仅支持文本输入，还能处理语音和视频输入。这款模型在速度、成本和性能方面都得到了显著提升，相比之前的版本快两倍，成本降低了50%，并且能够处理更长的输出\nChatGPT数据分析 最新的 ChatGPT 界面允许用户通过选择行或列来提问，并进行数据可视化。\nMistral-7B-v0.3 Mistral-7B-v0.3 模型引入了词表扩展功能，并支持工具调用\nDeepSeek-V2 该模型通过多头潜在注意力（MLA）和 DeepSeekMoE 架构，实现了高效推理，每个token仅激活 210 亿参数，从而显著降低了计算成本。\n关注的开源项目 Cover-Agent Cover-Agent\nCodiumAI Cover-Agent: An AI-Powered Tool for Automated Test Generation and Code Coverage Enhancement!\n自动化和增强测试的生成，目前主要是单元测试\nAnyNode v0.1 AnyNode\nA ComfyUI Node that uses the power of LLMs to do anything with your input to make any type of output.\nLlamaFS LlamaFS\n基于Llama 3模型的自组织文件管理系统，能自动重命名和组织文件，支持多种文件类型，包括图像和音频\nDSPy DSPy\nDSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system without DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change.\nHoarder Hoarder\n自托管的全能书签管理应用，支持链接、笔记和图片的存储，具备基于AI的自动标签化和全文搜索功能\n我的足迹 杭州，大阪，泉州\n影音\u0026amp;阅读记录 精选歌单 演出 5.03 Fri. 闪千手音乐节 草东\u0026amp;万青 .et.al 杭州 第一次在现场看到了草东和万青，也发现了一些之前没接触过的乐队， 如HONNE。虽然很累， 但也非常开心。不论是草东的全场大合唱，还是惘闻演出时远处意外出现的烟花， 抑或是万青在《大石碎胸口》结尾处的solo。我他妈地热爱这个世界。\n5.16 Thur. Athletics \u0026amp; Pg.lost 万代南梦宫 仍然处在Pg.lost 的截断反应中\n5.22 Wed. the Ocean \u0026amp; A burial at sea 南梦宫2F 5.25 Sat. Max Cooper\u0026amp;吴卓玲 万代南梦宫 电影 《坂本龙一：杰作》\n书\u0026amp;阅读摘录 本月完成：《一人企业方法论》，《疯狂的尿酸：不只是痛风》\n中文互联网正在加速崩塌 我们曾经以为互联网可以保留一切，但结果是一切都没能保留。\n主要原因可能是两个： 一是经济原因。\n二是监管原因。\n本质上，互联网内容和生命一样，也受进化论的支配。其存在的标准只有一条：以尽可能低的成本争取尽可能多的注意力。\n新生代也许还能看看朋友圈，但是朋友圈也越来越多“三天可见”，越来越沉默不语。\nElon Musk 访谈： 讲述自己对于突破社交障碍的心得 不要把注意力集中在自己身上 - 告诉自己「别紧张」反而会更紧张 - 你应该把注意力向外发散 询问对方问题，倾听对方回答 - 社交能力是天生的，你只需要发挥它 - 精通这项技能需要3-4年，但是非常值得\n分享一下我的信息流戒断方法（from X 上的 Andrew Bamboo） 社媒账号（即刻、推特、公众号）。作为记事本来用（我的日常随笔记录在 flomo 上，私人的内容仅自己可见，对别人有用的内容会同步在社媒平台上，长篇的可以跨越时间的会进一步沉淀在公众号上）；\nPerplexity 产品开发的新模式 精简管理层： 他们优先考虑自我驱动的独立贡献者，避免聘请那些主要通过指导他人来发挥作用的管理者。 因此，管理流程或领导团队的能力并不是首要考虑的。我们更看重那些能对用户产生显著影响的优秀个体贡献者，而不是仅限于他们所在公司的影响。 但我们不再那么需要那些主导他人工作的人才，因为有了人工智能，这已经不是必需的了。\n对未来的洞察： Johnny 预见到，随着时间的推移，那些既懂技术又具备产品敏感度的项目经理或工程师，将成为公司中最具价值的资产。 我认为未来，整个行业的管理层会趋于精简。如果我要预测，具有技术背景且懂得产品美学的产品经理或工程师，将成为公司最宝贵的资产。\n虽然高层的目标和方向是由上而下制定的，但许多新的想法却是从基层团队中产生的。\nMilestone 第一次独自一人出国：日本 第一次音乐节：闪千手音乐节 这个月发了5篇blog 在小红书发攻略：达到了1000+阅读和100+收藏赞 把游记同步到了马蜂窝 ","permalink":"https://niraya666.github.io/monthly/2024-05/","summary":"\u003ch2 id=\"ai\"\u003eAI\u003c/h2\u003e\n\u003ch3 id=\"gpt4-o-发布\"\u003eGPT4-o 发布\u003c/h3\u003e\n\u003cp\u003eOpenAI 推出了新一代多模态模型 GPT-4o，它不仅支持文本输入，还能处理语音和视频输入。这款模型在速度、成本和性能方面都得到了显著提升，相比之前的版本快两倍，成本降低了50%，并且能够处理更长的输出\u003c/p\u003e\n\u003ch3 id=\"chatgpt数据分析\"\u003eChatGPT数据分析\u003c/h3\u003e\n\u003cp\u003e最新的 ChatGPT 界面允许用户通过选择行或列来提问，并进行数据可视化。\u003c/p\u003e\n\u003ch3 id=\"mistral-7b-v03\"\u003eMistral-7B-v0.3\u003c/h3\u003e\n\u003cp\u003eMistral-7B-v0.3 模型引入了词表扩展功能，并支持工具调用\u003c/p\u003e\n\u003ch3 id=\"deepseek-v2\"\u003eDeepSeek-V2\u003c/h3\u003e\n\u003cp\u003e该模型通过多头潜在注意力（MLA）和 DeepSeekMoE 架构，实现了高效推理，每个token仅激活 210 亿参数，从而显著降低了计算成本。\u003c/p\u003e\n\u003ch2 id=\"关注的开源项目\"\u003e关注的开源项目\u003c/h2\u003e\n\u003ch3 id=\"cover-agent\"\u003e\u003cstrong\u003eCover-Agent\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Codium-ai/cover-agent\"\u003eCover-Agent\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCodiumAI Cover-Agent: An AI-Powered Tool for Automated Test Generation and Code Coverage Enhancement!\u003c/p\u003e\n\u003cp\u003e自动化和增强测试的生成，目前主要是单元测试\u003c/p\u003e\n\u003ch3 id=\"anynode-v01\"\u003e\u003cstrong\u003eAnyNode v0.1\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/lks-ai/anynode\"\u003eAnyNode\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA ComfyUI Node that uses the power of LLMs to do anything with your input to make any type of output.\u003c/p\u003e\n\u003ch3 id=\"llamafs\"\u003e\u003cstrong\u003eLlamaFS\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/iyaja/llama-fs\"\u003eLlamaFS\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e基于Llama 3模型的自组织文件管理系统，能自动重命名和组织文件，支持多种文件类型，包括图像和音频\u003c/p\u003e\n\u003ch3 id=\"dspy\"\u003e\u003cstrong\u003eDSPy\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/stanfordnlp/dspy\"\u003eDSPy\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDSPy is a framework for algorithmically optimizing LM prompts and weights\u003c/strong\u003e, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system \u003cem\u003ewithout\u003c/em\u003e DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change.\u003c/p\u003e","title":"2024-05 月刊"},{"content":"2024年 夏\n这是我第一次踏上泉州这片土地，虽然我是厦门人，已经快三十岁了，却一直没有机会来这座近在咫尺的城市。\n泉州在我想象中，是一座充满历史韵味的地方，同时也被一些暴发户的繁荣所点缀。几年前，这座城市成功申遗，从此逐渐成为小众旅游的热门选择。泉州的古老街区、传统建筑和浓厚的文化氛围，让人忍不住想一探究竟。\n大学时，我有一位好友家在泉州。我们是班里唯二的福建人，因此自然成了很好的朋友。每次听他讲起泉州的美食、风景，总让我心生向往。\n这次借着回家的机会，我终于决定亲自走一趟泉州。\n泉州，我来了！\n初印象 初到泉州，路上穿梭的电动车给我留下了深刻的第一印象。这些电动车在街头巷尾自由穿行，让我一度产生了身处越南的错觉。或许，这正是泉州的独特之处——在传统与现代之间，在历史与现实之中，找到一种和谐的平衡。\n作为古城，鲤城区似乎尽可能地保留着过去的模样。漫步在古城，却能产生一种记忆中的故乡的错觉，这一切似乎都太像了。\n那些熟悉的景象和气息，让人倍感亲切。有人说过，“离开了，故乡才称之为故乡。”庆幸的是，故乡以另一种形式呈现在我的面前，尽管这不是我的故乡。\n在鲤城区，街头的小贩、古早风格的便利店、上了年纪的理发店，无时无刻不把我拉回到记忆的深处。\n人 历史离不开人，特别是一个个普通人\n正是这些普通人，构成了这座城市的血脉与灵魂。他们的日常琐事，他们的勤劳与坚韧，使得这座古老的城市充满了生机与活力。在他们的身上，我看到了泉州的过去、现在和未来。\n宗教 泉州作为海上丝绸之路的起点，在那辉煌的历史长河中，不仅是贸易和文化交流的中心，也吸引了大量的外来文化和宗教。佛教、道教、伊斯兰教、基督教等多种宗教在这里交汇融合，形成了独特的宗教文化景观。经过千年的融合与发展，这些宗教在泉州和谐共存，互相尊重，不分彼此。\n吃 牛肉店 阿秋牛排馆\n牛排并不是传统意义上的steak，而是采用香料和咖喱炖煮而成的牛排骨。招牌牛排非常推荐，炖得非常软烂，入口即化，肉质瞬间脱骨。咖喱的香气与牛肉的原汁原味相得益彰，不会掩盖肉本身的美味。用汤汁拌饭，可谓是一绝。\n不过，对于从小习惯了晋江牛肉店的闽南人来说，阿秋牛排馆的汤味道可能稍显清淡，不够浓郁，因此未必合所有人的口味。此外，这家店的价格也比其他店稍高一些。\n除此之外，我的收藏夹里还有几家值得尝试的牛排店：\n阿波牛肉店\n天财牛肉店\n阿泉全牛馆\n面线糊 平阿面线糊\n我去的这家平阿面线糊，在周末早上8点左右已经挤满了人，足见其受欢迎程度。\n与厦门地区加入了猪血、虾米的面线糊相比，泉州的面线糊更多地呈现出字面上的意思——就是很单纯的面线糊。所用的面线更加细碎，完全不用担心吃太慢会坨，因为这甚至可以用面线汤来形容。但喝上一口汤，你会发现这绝不是一碗普通的面线汤，在汤头上，店家一定是下足了功夫的。\n食客可以根据自己的喜好添加额外的食材，这种模式在闽南地区基本相同。在泉州，根据我对本地食客的观察，基本上必加的食材有：醋肉、卤大肠、套肠、豆干等等，当然绝对不能忘了加一根油条。\n除了平阿面线糊，我还收藏了几家值得一试的面线糊店：\n水门国仔老店\n后城面线糊\n文啊面线糊\n烧肉粽 东街肉粽店\n“烧”在闽南语中的含义既包含了特定的烹饪方法，也体现了食物的热度（与“烫”同音）。在“烧肉粽”中，“烧”不仅指将肉粽通过炖、煮、蒸等方式烹饪至入味，还强调了粽子食用时热乎的特质。\n与在厦门吃的肉粽不同，除了甜辣酱外，泉州的肉粽更喜欢加入花生酱，增添了一份独特的香气和口感。\n原本计划去侯阿婆烧肉粽，但可惜饭点人太多，只能临时改变计划，选择了不远处的东街肉粽店。不过这家感觉有些失望，对粽子没有太多感觉，店里的海蛎煎下的油太多，实在无法恭维。\n可惜这次时间有限，还有很多地方想去但没来得及去， 有很多想吃的没吃上\n下次再来\n","permalink":"https://niraya666.github.io/travel/%E6%B3%89%E5%B7%9E%E8%A1%8C%E8%AE%B0%E5%8F%A4%E5%9F%8E%E6%BC%AB%E6%AD%A5%E4%B8%8E%E5%91%B3%E8%95%BE%E4%B9%8B%E6%97%85/","summary":"\u003cp\u003e2024年 夏\u003c/p\u003e\n\u003cp\u003e这是我第一次踏上泉州这片土地，虽然我是厦门人，已经快三十岁了，却一直没有机会来这座近在咫尺的城市。\u003c/p\u003e\n\u003cp\u003e泉州在我想象中，是一座充满历史韵味的地方，同时也被一些暴发户的繁荣所点缀。几年前，这座城市成功申遗，从此逐渐成为小众旅游的热门选择。泉州的古老街区、传统建筑和浓厚的文化氛围，让人忍不住想一探究竟。\u003c/p\u003e\n\u003cp\u003e大学时，我有一位好友家在泉州。我们是班里唯二的福建人，因此自然成了很好的朋友。每次听他讲起泉州的美食、风景，总让我心生向往。\u003c/p\u003e\n\u003cp\u003e这次借着回家的机会，我终于决定亲自走一趟泉州。\u003c/p\u003e\n\u003cp\u003e泉州，我来了！\u003c/p\u003e\n\u003ch2 id=\"初印象\"\u003e初印象\u003c/h2\u003e\n\u003cp\u003e初到泉州，路上穿梭的电动车给我留下了深刻的第一印象。这些电动车在街头巷尾自由穿行，让我一度产生了身处越南的错觉。或许，这正是泉州的独特之处——在传统与现代之间，在历史与现实之中，找到一种和谐的平衡。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/50F93B5D-5373-486C-A785-4BB341FD0679_1_105_c.jpeg\" alt=\"50F93B5D-5373-486C-A785-4BB341FD0679_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e作为古城，鲤城区似乎尽可能地保留着过去的模样。漫步在古城，却能产生一种记忆中的故乡的错觉，这一切似乎都太像了。\u003c/p\u003e\n\u003cp\u003e那些熟悉的景象和气息，让人倍感亲切。有人说过，“离开了，故乡才称之为故乡。”庆幸的是，故乡以另一种形式呈现在我的面前，尽管这不是我的故乡。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/458F4F0B-B35C-4AC9-A447-0A38E69CC35D_1_105_c.jpeg\" alt=\"458F4F0B-B35C-4AC9-A447-0A38E69CC35D_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e在鲤城区，街头的小贩、古早风格的便利店、上了年纪的理发店，无时无刻不把我拉回到记忆的深处。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/218DC3D6-D630-49FE-9796-64202907D7E3_1_105_c.jpeg\" alt=\"218DC3D6-D630-49FE-9796-64202907D7E3_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"人\"\u003e人 \u003c/h2\u003e\n\u003cp\u003e历史离不开人，特别是一个个普通人\u003c/p\u003e\n\u003cp\u003e正是这些普通人，构成了这座城市的血脉与灵魂。他们的日常琐事，他们的勤劳与坚韧，使得这座古老的城市充满了生机与活力。在他们的身上，我看到了泉州的过去、现在和未来。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/E5CBBA91-C9A6-4F93-853D-009FD2D568C5_1_105_c.jpeg\" alt=\"E5CBBA91-C9A6-4F93-853D-009FD2D568C5_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/BC390CC0-02B4-45A6-9266-1C7638C75D76_1_105_c.jpeg\" alt=\"BC390CC0-02B4-45A6-9266-1C7638C75D76_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/3F165220-A99D-4E4F-92AB-94F572A0B231_1_105_c.jpeg\" alt=\"3F165220-A99D-4E4F-92AB-94F572A0B231_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/0DA901A1-F620-4B8A-A722-63E95EDAA536_1_105_c.jpeg\" alt=\"0DA901A1-F620-4B8A-A722-63E95EDAA536_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/46A3B7FB-2306-4661-8B47-7DEDB3FF20AE_1_105_c.jpeg\" alt=\"46A3B7FB-2306-4661-8B47-7DEDB3FF20AE_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"宗教\"\u003e宗教\u003c/h2\u003e\n\u003cp\u003e泉州作为海上丝绸之路的起点，在那辉煌的历史长河中，不仅是贸易和文化交流的中心，也吸引了大量的外来文化和宗教。佛教、道教、伊斯兰教、基督教等多种宗教在这里交汇融合，形成了独特的宗教文化景观。经过千年的融合与发展，这些宗教在泉州和谐共存，互相尊重，不分彼此。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/22205648-89A8-41BE-B99C-56850A28D252_1_105_c.jpeg\" alt=\"22205648-89A8-41BE-B99C-56850A28D252_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/BB0999F1-EDEA-4AA2-BA72-D6954CD12BA8_1_105_c.jpeg\" alt=\"BB0999F1-EDEA-4AA2-BA72-D6954CD12BA8_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/5A6D2D91-D86E-4BBA-9248-272095117CE9_1_105_c.jpeg\" alt=\"5A6D2D91-D86E-4BBA-9248-272095117CE9_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"吃\"\u003e吃\u003c/h2\u003e\n\u003ch3 id=\"牛肉店\"\u003e牛肉店\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/FEA7D1E0-80AD-45BC-B2F9-E3BF31A1130A_1_201_a.jpg\" alt=\"FEA7D1E0-80AD-45BC-B2F9-E3BF31A1130A_1_201_a.jpg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e阿秋牛排馆\u003c/p\u003e\n\u003cp\u003e牛排并不是传统意义上的steak，而是采用香料和咖喱炖煮而成的牛排骨。招牌牛排非常推荐，炖得非常软烂，入口即化，肉质瞬间脱骨。咖喱的香气与牛肉的原汁原味相得益彰，不会掩盖肉本身的美味。用汤汁拌饭，可谓是一绝。\u003c/p\u003e\n\u003cp\u003e不过，对于从小习惯了晋江牛肉店的闽南人来说，阿秋牛排馆的汤味道可能稍显清淡，不够浓郁，因此未必合所有人的口味。此外，这家店的价格也比其他店稍高一些。\u003c/p\u003e\n\u003cp\u003e除此之外，我的收藏夹里还有几家值得尝试的牛排店：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e阿波牛肉店\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e天财牛肉店\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e阿泉全牛馆\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"面线糊\"\u003e面线糊\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/FC2DD610-927E-4E1E-829A-420ACD1A0418_1_105_c.jpeg\" alt=\"FC2DD610-927E-4E1E-829A-420ACD1A0418_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e平阿面线糊\u003c/p\u003e\n\u003cp\u003e我去的这家平阿面线糊，在周末早上8点左右已经挤满了人，足见其受欢迎程度。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/0615C07D-8BA9-484A-BBFC-FF59E764955B_1_201_a.jpg\" alt=\"0615C07D-8BA9-484A-BBFC-FF59E764955B_1_201_a.jpg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e与厦门地区加入了猪血、虾米的面线糊相比，泉州的面线糊更多地呈现出字面上的意思——就是很单纯的面线糊。所用的面线更加细碎，完全不用担心吃太慢会坨，因为这甚至可以用面线汤来形容。但喝上一口汤，你会发现这绝不是一碗普通的面线汤，在汤头上，店家一定是下足了功夫的。\u003c/p\u003e\n\u003cp\u003e食客可以根据自己的喜好添加额外的食材，这种模式在闽南地区基本相同。在泉州，根据我对本地食客的观察，基本上必加的食材有：醋肉、卤大肠、套肠、豆干等等，当然绝对不能忘了加一根油条。\u003c/p\u003e\n\u003cp\u003e除了平阿面线糊，我还收藏了几家值得一试的面线糊店：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e水门国仔老店\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e后城面线糊\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e文啊面线糊\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"烧肉粽\"\u003e烧肉粽\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/F2CA54E0-ADE3-42B7-A243-D90C9B559D7B_1_105_c.jpeg\" alt=\"F2CA54E0-ADE3-42B7-A243-D90C9B559D7B_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e东街肉粽店\u003c/p\u003e\n\u003cp\u003e“烧”在闽南语中的含义既包含了特定的烹饪方法，也体现了食物的热度（与“烫”同音）。在“烧肉粽”中，“烧”不仅指将肉粽通过炖、煮、蒸等方式烹饪至入味，还强调了粽子食用时热乎的特质。\u003c/p\u003e\n\u003cp\u003e与在厦门吃的肉粽不同，除了甜辣酱外，泉州的肉粽更喜欢加入花生酱，增添了一份独特的香气和口感。\u003c/p\u003e\n\u003cp\u003e原本计划去侯阿婆烧肉粽，但可惜饭点人太多，只能临时改变计划，选择了不远处的东街肉粽店。不过这家感觉有些失望，对粽子没有太多感觉，店里的海蛎煎下的油太多，实在无法恭维。\u003c/p\u003e","title":"泉州行记：古城漫步与味蕾之旅"},{"content":"2024-春夏之交 大阪\n旅行时间2天3夜\n在多邻国上学了快一年的散装日语，受到了看大河剧、日本电影和玩日本游戏的影响，我决定利用刚获得的三年多次入境签证，以及51假期的空闲时间，去日本看看。考虑到时间限制，我选择了关西的交通枢纽和经济中心——大阪，作为这次日本之行的第一站。\n对于日本，尤其是大阪，我的第一印象是热门景区和场所人非常多。这主要是因为赶上了日本的10天黄金周，同时由于日元汇率低迷，大量外国人涌入日本消费和旅游。然而，在居民区，特别是我所住的浪速区，情况则大为不同，晚上和清晨几乎见不到几个人，给人一种非常萧条的感觉。中国人会说，这叫做“缺乏人气”。此外，随处可见的大乌鸦加深了这种萧条感。或许这也可能是宫崎英高在《黑魂》系列中乌鸦的灵感来源吧。在日本，乌鸦似乎被视为吉祥的象征。\n游玩篇 出了民宿，我的第一站是不远处的难波八阪神社。这里以其狮子头形状的大狮子殿闻名，成为了一处热门的打卡地点。据说，这个狮子头具有驱除灾难和带来好运的力量。\n经过两站地下铁，即可到达动物园前站， 这里是新世界的入口。\n新世界，曾是日本向西方看齐、力图“脱亚入欧”时的产物。在20世纪初，日本大量吸纳西方文化元素，并在大阪建立了这一街区。然而时光流转，21世纪的今天，这里更像是一个保存良好的“旧世界”。街区内充满昭和时代的气息，从游戏厅、炸串店到浮夸的店面招牌，甚至粉红色影院，处处透露出时代的印记。\n作为泡沫经济时期产物， 粉红影院现在主要吸引了一部分中老年男性和非传统性别的人士。影院的“学生半价”标志显得有些讽刺，同时也透露出一种时代变迁的哀愁。鉴于这类场所的特殊性，不推荐旅行者单独前往。\n新世界的标志性建筑通天阁，则见证了区域的多重历史。这座铁塔最初在1912年仿照巴黎的埃菲尔铁塔建造，但在二战中因为“献纳”政策而被拆除，用其材料支持军工。通天阁的现代版本建于1956年，为了迎接万博会而重建，塔身醒目地印有日立的广告，成为了大阪的新地标。\n西成区，与新世界仅一路之隔，是大阪的旧城区。这个区域以较复杂的治安情况、流浪汉的较高出现率，以及成人娱乐业而知名。其中，著名的酒吧一条街和在中文互联网上广为人知的飞田新地均位于此地。游客在此区域应特别小心，避免随意使用手机或拍照，以尊重当地文化和增加个人安全。\n往新世界东北边走， 则能到达天王寺车站，商圈，以及以此命名此地的四天王寺。这里的天王寺车站作为从关西机场到大阪市区的两条主要电车线路之一JR西日本的终点站，同前面走过的西成区和新世界相比， 自然非常繁华。\n天王寺站前\nJR西日本机场线之HARUKA， 以Hello- Kitty涂装著称。\n日本的许多古迹因二战的轰炸和历史上的大地震而稀少，多数是在原址上修复或重建的。四天王寺就是这样的例子，它是日本历史悠久的寺庙之一，最初由飞鸟时期的圣德太子创建。虽然历经多次战火，我们今天所见的四天王寺主要是1957年后的重建。\n前往天王寺途中，偶遇的一心寺。\n在天王寺附近，我意外地发现了一个旧书集市。在那里，我用400日元买到了一本1994年的旧书。虽然还有很多书我想买想看，但考虑到海关的限制和行李空间的限制，我只能忍痛放弃。\n不可免俗的道顿堀格利高小人\n蟹道乐\n夜幕降临， 抵达梅田，乘坐hep five摩天轮，体验大阪夜景。\nhep five摩天轮\n第二天，前往大阪历史博物馆和大阪城， 以及shooping。\n大阪城\n大阪城始建于1583年，由丰臣秀吉下令建造。大阪城的天守阁如今是博物馆，展示了丰臣秀吉的生平、战国时期的武器和铠甲，以及大阪城的历史变迁。\n如果对于历史感兴趣的小伙伴， 大阪城公园边上的大阪历史博物馆也值得逛一逛。 大阪历史博物馆展示了古代大阪作为日本重要商业城市的发展过程，以及现代大阪的城市化进程。参观路线从10楼一路到7楼，6楼为特别展。\n日式连锁中餐王将的碳水加碳水再加碳水的定食\n剩下的shopping时间，逛了堂吉柯德，在西斋心桥发现一家二手唱片店\n位于 Big Step 大楼的地下一层，可惜根据网友的评论，似乎店面越来越小，东西越来越少了。\n似乎在日本不同城市，优衣库会有限定的特别款式， 在大阪则是御好烧（大阪烧）和章鱼烧的图案。\n美国村边上的小自由女神像\n攻略篇 入境 Visit Japan Web （VJW）为了替代入境卡而设计的， 需要在入境前提前申报，在入境时扫码即可。和纸质入境卡相比， 至少从我的观察而言， 使用VJW会更加快速，而没有存在需要排队的情况。\n交通 在大阪，从关西机场到市区有两条主要的铁路线路，南海电铁和JR西日本。两条线路隶属于不同的公司运营，在使用购票机购票时需要注意区分。平均时间在半个小时到一个小时不等， 票价在970到1800日元间，取决于所乘坐车次和制定席与否。除了特急列车（Rapi:t \u0026amp; HARUKA）,均和国内乘坐地铁没有太大差别。\n对于特急列车， 可以在网上买票，通过扫二维码进站，或者是在自助购票机上操作，以及通过刷西瓜卡进站，在候车处附近的机器上购买特急券。理论上有售票员会查票。\n赶了6:00的第一班Rapi:t alpha\n吃 居酒屋会在刚入座上小菜，也叫お通し（o-to-shi）,这部分一般是按人头收费，价格在330日元左右，一般是强制收取不给退的。\n日本餐厅一般是不提供打包服务的， 因为内用和外带属于不同执照，并且会因为害怕顾客吃坏肚子店家要负责任， 所以在日本餐厅，店家对于打包吃剩的食物十分抗拒。\n点餐时， 特别在旅游城市， 一般都会有多国语言版本或者是带图片版本的菜单， 有的店家甚至有扫码点单，可谓是非常友好的。所以， 只需要会几个日语单词即可，如\n日语单词 意思 罗马音 注文 点单 chūmon 会計 结账 kaikei メニュー 菜单 menyū 一つ 一个 hitotsu 二つ 两个 futatsu 三つ 三个 mittsu 少々 少量 shōshō 大盛り 大份 ōmori おすすめ 推荐 osusume トイレ 洗手间 toire 在便利店消费时，有几个常用的日语单词和表达可以帮助你更方便地交流。例如，店员可能会询问你是否需要袋子（ふくろ，fukuro）或加热食物（温め，atatame）。在不需要这些服务时，可以回答“大丈夫”（daijōbu，没关系）。\n在7-11一般是又店员负责扫码和年龄检查（买烟酒时），扫完后，有顾客在旁边的一台机器上选择付款方式并付款。当然，也可以直接出示Alipay 给店员， 店员会帮你操作的。\n大阪周游卡（Osaka Amazing Pass） or 大阪乐游券（Osaka e-Pass） 目前周游卡已停售(二日券)，一日券购买有效期截止到2024年6月30日。\n其中大阪周游卡需要购买后兑换成实体卡，兑换位置可查看 售票及换票点；而大阪乐游券直接展示二维码即可。二者都支持在Klook上购买。周游卡价格在¥3300包括了市区地铁随意乘坐， 乐游券价格为¥2400，只包括景点。\n用GPT-4 整理了一个周游卡和乐游券差异的表格（不能保证100%正确）：\n地点 (日本語) 地点 (中文) Location Regular fee Osaka Amazing Pass Osaka e-Pass 梅田スカイビル空中庭園展望台 梅田天空大厦空中庭园观景台 梅田 ¥ 1500 ✔️ ✔️ 梅田スカイビル・衣川邦彦天空美術館 梅田天空大厦衣川邦彦天空美术馆 梅田 ¥ 1000 ✔️ ✔️ HEP FIVE観覧車 HEP FIVE摩天轮 梅田 ¥ 600 ✔️ ✔️ 通天閣 通天阁 新世界 ¥ 1000 ✔️ ✔️ 通天閣タワースライダー 通天阁塔滑梯 新世界 ¥ 1000 ✔️ ✔️ 四天王寺 (中心伽藍・本坊庭園) 四天王寺（中心伽蓝、本坊庭园） 四天王寺 ¥ 300 ✔️ ✔️ とんぼりリバークルーズ 道顿堀河巡航 道顿堀 ¥ 1200 ✔️ ✔️ 天王寺動物園 天王寺动物园 四天王寺 ¥ 500 ✔️ ❌ 中之島川クルーズ 中之岛河巡航 北区中之岛 ¥ 1500 ✔️ ✔️ 大阪国立国際美術館 大阪国立国际美术馆 北区中之岛 ¥ 430 ✔️ ❌ 道頓堀ZAZA 道顿堀ZAZA 道顿堀 ¥ 800 ✔️ ✔️ 上方浮世絵館 上方浮世绘馆 难波 ¥ 700 ✔️ ✔️ 大阪歴史博物館 大阪历史博物馆 大阪城 ¥ 600 ✔️ ❌ 大阪平和の殿堂 大阪和平馆 大阪城 ¥ 250 ✔️ ✔️ 大阪城天守閣 大阪城天守阁 大阪城 ¥ 600 ✔️ ❌ ワンダークルーズ 奇迹巡航 ¥ 1200 ✔️ ✔️ 大阪くらしの今昔館 大阪生活今昔馆 天神橋 ¥ 600 ✔️ ✔️ 天保山大観覧車 天保山大观览车 天保山 ¥ 1200 ✔️ ❌ グリオンミュージアム GLION博物馆 天保山 ¥ 1200 ✔️ ✔️ サンタマリア日帰りクルーズ 圣玛丽亚日游巡航 ¥ 1800 ✔️ ✔️ サンタマリア夜景クルーズ 圣玛丽亚夜景巡航 ¥ 2300 ✔️ ✔️ キャプテンライン 队长线 ¥ 1500 ✔️ ✔️ レゴランド・ディスカバリー・センター大阪 大阪乐高探索中心 ¥2800～¥3000 ✔️ ✔️ 咲洲コスモタワー展望台 咲洲宇宙塔展望台 ¥1000 ✔️ ✔️ ボートレース住之江 住之江赛艇 ¥ 1500 ✔️ ✔️ 大阪水上バスアクアライナー 大阪水上巴士Aqua-Liner ¥ 1800 ✔️ ✔️ 桜川お花見クルーズ 樱川赏花巡航 ¥ 1200 ✔️ ✔️ 夕日クルーズ「寄道」 夕阳巡航“YORIMICHI” ¥ 1500 ✔️ ✔️ 大阪城御座船 大阪城御座船 ¥ 1500 ✔️ ✔️ エンタープライズイノベーションミュージアム 创业创新博物馈 ¥ 300 ✔️ ✔️ 大阪城重要文化財櫓 大阪城重要文化财瞭 ¥ 800 ✔️ ❌ 大阪城西の丸庭園 大阪城西之丸庭园 ¥ 200 ✔️ ❌ 咲くやこの花館 咲く家这花馆 ¥ 500 ✔️ ❌ 長居植物園 长居植物园 ¥ 200 ✔️ ❌ 大阪市立自然史博物館 大阪市立自然历史博物馆 ¥ 300 ✔️ ❌ 堺利晶の杜 堺利晶之森 ¥ 300 ✔️ ❌ 堺市博物館 堺市博物馆 ¥ 200 ✔️ ❌ 万博記念公園（自然文化園、日本庭園） 万博纪念公园（自然文化园、日本庭园） ¥ 260 ✔️ ❌ OSAKA WHEEL（日本最大の観覧車） 大阪摩天轮（日本最大） 天保山 ¥ 1000 ✔️ ✔️ 大阪科学館 大阪科学馆 ¥ 400 ✔️ ❌ 桂陵園 桂陵园 ¥ 150 ✔️ ❌ 日向の湯 日向温泉 ¥ 800 ❌ ✔️ 海洋堂ホビーランド 海洋堂模型乐园 ¥ 1000 ❌ ✔️ 忍者トリックハウス（エディオン難波本店） 忍者特技屋（EDION难波本店） ¥ 1000 ❌ ✔️ 道頓堀ミュージアムなみき座 道顿堀剧场 ❌ ✔️ 更多的攻略建议参考：小气少年的部落格 这是我所发现的最完整的日本旅行攻略，成功解决了我的旅行前焦虑。 此外，在推荐一本书：《日本點餐完全圖解》，这本书详细介绍了在日本点餐的注意事项，甚至教会你怎么吃。\n感受篇 在这次的旅途中，我在网上上结识了两位旅游搭子，分别各搭了一天。\nA是一位出生于1998年的年轻人，帅气的外表下藏着近期感情伤痕的痛楚。他独自一人前来关西，试图通过旅行洗涤感情上的创伤。尽管我不愿轻易给人贴上标签，诸如“渣男”之类的称呼，但A的行为难免让人费解：他同时与四五名女生保持联系，不断在Tinder上寻找新的可能，对每一个路过的女性都有自己的评价。然而，从他的话语中，我逐渐感知到他背后隐藏的故事——那些使他变得如此的伤痕累累。通过他，我也间接了解了日本的一些风俗业及他的个人感受体验。\nR则是一位三十多岁的体制内员工，表面上看似普通，内心却充满了对现状的不满和对自由的渴望。他的思想深受那个时代的河殇文化与十年前“意林”文学的影响。在他的眼中，日本社会的方方面面几乎都是模范，而国内的绝大多数则恰恰相反。他甚至考虑过一个大胆的想法：辞去体制内的工作，“润”到日本。用互联网上的用语形容他的思想就是妥妥“反贼”和“润人”，但又何妨。R还是个爱酒如命的人。\n日本人究竟相信什么？道德伦理？法律？还是抽象的社会潜规则 事实上，日本社会的一个显著特点是高度的循规蹈矩。人们极其重视集体，与社会脱节被视为所能想到的最严酷的惩罚之一。也许正是因为日本人对意识形态和教条缺乏关注，他们格外重视外在形象，注重符合各种场合的姿态，因为“面子”比“里子”更重要。这演化成了一种行为模式，即在私下里怎么搞都行，但在公众面前一定要维护自己的“形象”。这意味着日本人本质上追求的是集体认同，极度避免被社会边缘化。\n“日本的情况则不同，原始信仰从未遭到更精致的官方律条的碾压。尽管不同宗教之间的区分不甚清晰，尤其是在最广泛的层面上，但佛寺和神社仍然共存并立。两种教派的仪式人们都参与，尽管不一定同时同地。这或许是因为日本人对意识形态和教条缺乏关注，却格外重视外在，重视符合各种场合的姿态，因为“面子”比“里子”更重要。”\n在这种文化氛围下，个人行为被严格的社会规范和集体利益所约束。脱离集体，被视为严重的社会惩罚，比法律制裁更为严酷。社会关系和集体认同对个人生活至关重要，因此日本人普遍面临巨大的心理压力，必须时刻注意自己的行为是否符合社会期望，以避免被边缘化。\n这种面子文化强调外在形象的维护。日本人非常重视在各种场合的表现和形象管理，不论是在公共场合的礼仪，还是在工作中的表现，维护良好的外在形象是必不可少的。在私下场合，日本人可能会表现出更大的灵活性和自由度，但在公开场合，他们会极力维护自己的公众形象，遵守社会规范和礼仪，以获得集体的认可和尊重。\n不论是飞田新地的挂羊头卖狗肉，还是游戏厅的老虎机，日本人总是有能力绕过所谓的规则，显示出一种极强的变通能力；但在公共区域场合下， 日本人又总能展现出一种死板地对于规则的严格遵守。\n日本社会的这种双重性在于，私下和公开行为的巨大反差。这种现象可以通过对集体认同和避免边缘化的强烈需求来解释。正如《日本之镜：日本文化中的英雄与恶人》一书中所提到的，“制约日本人行为的不是社会规则，而是一套抽象的道德教训。”\n然而，这种“抽象的道德”在中文语境下显得陌生。中国古代追求的是儒家的仁义道德，而日本的道德观念更多来源于神道教的泛神信仰，强调避免在社会上出丑。例如，须佐之男被放逐的故事就是一种道德教训，提醒人们避免类似的羞辱。日本人常说，只要不被逮个正着，避免在社会上出丑，就能够为所欲为，不必担心后果。\n","permalink":"https://niraya666.github.io/travel/%E5%A4%A7%E9%98%AA%E6%B8%B8%E8%AE%B0%E6%97%B6%E5%85%89%E4%BA%A4%E9%94%99%E4%B8%8B%E7%9A%84%E6%97%A5%E6%9C%AC%E6%8E%A2%E7%B4%A2/","summary":"\u003cp\u003e2024-春夏之交 大阪\u003c/p\u003e\n\u003cp\u003e旅行时间2天3夜\u003c/p\u003e\n\u003cp\u003e在多邻国上学了快一年的散装日语，受到了看大河剧、日本电影和玩日本游戏的影响，我决定利用刚获得的三年多次入境签证，以及51假期的空闲时间，去日本看看。考虑到时间限制，我选择了关西的交通枢纽和经济中心——大阪，作为这次日本之行的第一站。\u003c/p\u003e\n\u003cp\u003e对于日本，尤其是大阪，我的第一印象是热门景区和场所人非常多。这主要是因为赶上了日本的10天黄金周，同时由于日元汇率低迷，大量外国人涌入日本消费和旅游。然而，在居民区，特别是我所住的浪速区，情况则大为不同，晚上和清晨几乎见不到几个人，给人一种非常萧条的感觉。中国人会说，这叫做“缺乏人气”。此外，随处可见的大乌鸦加深了这种萧条感。或许这也可能是宫崎英高在《黑魂》系列中乌鸦的灵感来源吧。在日本，乌鸦似乎被视为吉祥的象征。\u003c/p\u003e\n\u003ch2 id=\"游玩篇\"\u003e游玩篇\u003c/h2\u003e\n\u003cp\u003e出了民宿，我的第一站是不远处的难波八阪神社。这里以其狮子头形状的大狮子殿闻名，成为了一处热门的打卡地点。据说，这个狮子头具有驱除灾难和带来好运的力量。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/D22F4C9A-23D6-472A-B359-5A9DC656DE03_1_201_a.jpeg\" alt=\"D22F4C9A-23D6-472A-B359-5A9DC656DE03_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e经过两站地下铁，即可到达动物园前站， 这里是新世界的入口。\u003c/p\u003e\n\u003cp\u003e新世界，曾是日本向西方看齐、力图“脱亚入欧”时的产物。在20世纪初，日本大量吸纳西方文化元素，并在大阪建立了这一街区。然而时光流转，21世纪的今天，这里更像是一个保存良好的“旧世界”。街区内充满昭和时代的气息，从游戏厅、炸串店到浮夸的店面招牌，甚至粉红色影院，处处透露出时代的印记。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/6DA5908E-2DB7-48B0-9254-7B189FB7848A_1_201_a.jpeg\" alt=\"6DA5908E-2DB7-48B0-9254-7B189FB7848A_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/AF7FD137-8145-4DE2-91D7-116908DC6985_1_201_a.jpeg\" alt=\"AF7FD137-8145-4DE2-91D7-116908DC6985_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e作为泡沫经济时期产物， 粉红影院现在主要吸引了一部分中老年男性和非传统性别的人士。影院的“学生半价”标志显得有些讽刺，同时也透露出一种时代变迁的哀愁。鉴于这类场所的特殊性，不推荐旅行者单独前往。\u003c/p\u003e\n\u003cp\u003e新世界的标志性建筑通天阁，则见证了区域的多重历史。这座铁塔最初在1912年仿照巴黎的埃菲尔铁塔建造，但在二战中因为“献纳”政策而被拆除，用其材料支持军工。通天阁的现代版本建于1956年，为了迎接万博会而重建，塔身醒目地印有日立的广告，成为了大阪的新地标。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/6C8F7905-91AC-46AA-BEC9-79AF3A362781_1_105_c.jpeg\" alt=\"6C8F7905-91AC-46AA-BEC9-79AF3A362781_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/0B1EA1C4-DFF3-4FDA-ACF8-EFDB75E7A3C0_1_201_a.jpeg\" alt=\"0B1EA1C4-DFF3-4FDA-ACF8-EFDB75E7A3C0_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e西成区，与新世界仅一路之隔，是大阪的旧城区。这个区域以较复杂的治安情况、流浪汉的较高出现率，以及成人娱乐业而知名。其中，著名的酒吧一条街和在中文互联网上广为人知的飞田新地均位于此地。游客在此区域应特别小心，避免随意使用手机或拍照，以尊重当地文化和增加个人安全。\u003c/p\u003e\n\u003cp\u003e往新世界东北边走， 则能到达天王寺车站，商圈，以及以此命名此地的四天王寺。这里的天王寺车站作为从关西机场到大阪市区的两条主要电车线路之一JR西日本的终点站，同前面走过的西成区和新世界相比， 自然非常繁华。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/2E815B2C-F25B-4CB4-A8FB-8D993265A540_1_105_c.jpeg\" alt=\"2E815B2C-F25B-4CB4-A8FB-8D993265A540_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e天王寺站前\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/42E69559-0A2E-4F15-8B70-C4CD15948641_1_201_a.jpeg\" alt=\"42E69559-0A2E-4F15-8B70-C4CD15948641_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eJR西日本机场线之HARUKA， 以Hello- Kitty涂装著称。\u003c/p\u003e\n\u003cp\u003e日本的许多古迹因二战的轰炸和历史上的大地震而稀少，多数是在原址上修复或重建的。四天王寺就是这样的例子，它是日本历史悠久的寺庙之一，最初由飞鸟时期的圣德太子创建。虽然历经多次战火，我们今天所见的四天王寺主要是1957年后的重建。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/C316DC6A-106E-494C-9926-6F5ED52B11E3_1_105_c.jpeg\" alt=\"C316DC6A-106E-494C-9926-6F5ED52B11E3_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/30F7FC2C-4BFA-494F-A0EC-1C07AEBA1A5E_1_105_c.jpeg\" alt=\"30F7FC2C-4BFA-494F-A0EC-1C07AEBA1A5E_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/3BFFEA31-F958-4239-99FD-23B5D8F91CFF_1_105_c.jpeg\" alt=\"3BFFEA31-F958-4239-99FD-23B5D8F91CFF_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e前往天王寺途中，偶遇的一心寺。\u003c/p\u003e\n\u003cp\u003e在天王寺附近，我意外地发现了一个旧书集市。在那里，我用400日元买到了一本1994年的旧书。虽然还有很多书我想买想看，但考虑到海关的限制和行李空间的限制，我只能忍痛放弃。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/A0D7A507-9B4F-44D5-AA9C-6B771FA520D2_1_201_a.jpeg\" alt=\"A0D7A507-9B4F-44D5-AA9C-6B771FA520D2_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/96FA58A9-C223-40F4-9BC8-33B6A50D4BF0_1_105_c.jpeg\" alt=\"96FA58A9-C223-40F4-9BC8-33B6A50D4BF0_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e不可免俗的道顿堀格利高小人\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/17DC7E7D-8CD2-4FCD-A19E-EC4BCE661FF1_1_105_c.jpeg\" alt=\"17DC7E7D-8CD2-4FCD-A19E-EC4BCE661FF1_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e蟹道乐\u003c/p\u003e\n\u003cp\u003e夜幕降临， 抵达梅田，乘坐hep five摩天轮，体验大阪夜景。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/90FE62D5-F899-43B8-BBB7-D44B242C2CEB_1_201_a.jpeg\" alt=\"90FE62D5-F899-43B8-BBB7-D44B242C2CEB_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003ehep five摩天轮\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/5C716546-81DE-4FE8-BEAF-EA239F27DAB4_1_201_a.jpeg\" alt=\"5C716546-81DE-4FE8-BEAF-EA239F27DAB4_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e第二天，前往大阪历史博物馆和大阪城， 以及shooping。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/FDCCBE76-D57A-4EC6-8D79-D19D9066390F_1_105_c.jpeg\" alt=\"FDCCBE76-D57A-4EC6-8D79-D19D9066390F_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e大阪城\u003c/p\u003e\n\u003cp\u003e大阪城始建于1583年，由丰臣秀吉下令建造。大阪城的天守阁如今是博物馆，展示了丰臣秀吉的生平、战国时期的武器和铠甲，以及大阪城的历史变迁。\u003c/p\u003e\n\u003cp\u003e如果对于历史感兴趣的小伙伴， 大阪城公园边上的大阪历史博物馆也值得逛一逛。 大阪历史博物馆展示了古代大阪作为日本重要商业城市的发展过程，以及现代大阪的城市化进程。参观路线从10楼一路到7楼，6楼为特别展。\u003c/p\u003e","title":"大阪游记：时光交错下的日本探索"},{"content":"为什么要进行文本分块？ 大型语言模型（LLM）和嵌入式模型都有其处理文本的长度限制。为了有效处理超出这些限制的长文本，我们需要进行文本分块。\n如何进行文本分块 **块分割戒律：**我们的目标不仅仅是为了划分数据块。更重要的是，我们要以一种便于日后检索和提取价值的格式来整理我们的数据。\n文本分块的根本目的不仅仅是简单地将数据分割成块。更重要的是，我们要以便于检索和提取价值的方式来组织这些数据块。以下是几种常见的分块逻辑：\n句子分割（Sentence Splitting） 这种方法只是简单地将文本切分成N个字符的片段，不考虑内容或形式的连贯性。\ntext = \u0026#34;...\u0026#34; # 你的文本 from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter( separator = \u0026#34;\\n\\n\u0026#34;, chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 递归字符文本分割（Recursive Character Text Splitting） 这种方法首先尝试用一个分隔符将文本切分成小块。如果初始尝试未能达到预期的块大小或结构，就递归地用不同的分隔符重新分割结果块，直到得到满意的结果。\ntext = \u0026#34;...\u0026#34; # 你的文本 from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 根据langchain 的默认分隔条件 [\u0026quot;\\n\\n\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;] ,也就是会将text根据该分割条件的顺序（两个换行-\u0026gt;一个换行-\u0026gt;空格）将文本进行递归分割。\n针对特定文档的分割方法（Document Specific Splitting） 例如针对Markdown或Python代码的分割策略，这些策略根据文档的结构来进行分割，如类定义或Markdown标题。\nlangchain 提供了一些常见文档的分割方法：\nmardown的分割逻辑\n[ # First, try to split along Markdown headings (starting with level 2) \u0026#34;\\n#{1,6} \u0026#34;, # Note the alternative syntax for headings (below) is not handled here # Heading level 2 # --------------- # End of code block \u0026#34;```\\n\u0026#34;, # Horizontal lines \u0026#34;\\n\\\\*\\\\*\\\\*+\\n\u0026#34;, \u0026#34;\\n---+\\n\u0026#34;, \u0026#34;\\n___+\\n\u0026#34;, # Note that this splitter doesn\u0026#39;t handle horizontal lines defined # by *three or more* of ***, ---, or ___, but this is not handled \u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;, ] python的分割逻辑：\n[ # First, try to split along class definitions \u0026#34;\\nclass \u0026#34;, \u0026#34;\\ndef \u0026#34;, \u0026#34;\\n\\tdef \u0026#34;, # Now split by the normal type of lines \u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;, ] 表格 重点提示： 后来在我们对数据块进行语义搜索时，直接从表格中匹配嵌入可能会比较困难。开发者常用的做法是，在提取了表格之后，对其进行关键信息提取。然后对这些关键信息的总结进行嵌入。如果这个总结的嵌入与你的搜索目标匹配，那么就可以把原始表格交给你的大语言模型处理。\n语义分块 目前的文本分块方法大多基于结构或字符数量，忽视了文本的实际语义内容。有效的语义分块应当确保内容上相似的数据被归类在一起，这样不仅有助于数据的组织，还能提升后续处理的效率和准确性。\n根据Greg的做法：寻找连续句子间的分割点， 即从第一句话开始，获取其嵌入，然后与第二句进行比较，以此类推。在寻找嵌入距离较大的“分割点”。如果距离超过了一定阈值，就认为这标志着新的语义段落的开始。\n可以通过以下步骤来进行语义分块：\n文档初始处理：首先将你的文档分割成句子，每个句子通常包含关于一个主题的单一观点。\n句子嵌入：利用语言模型获取每个句子的嵌入表示。\n句子比较与分块：从第一句话开始，逐句比较嵌入的相似度。当相邻句子之间的嵌入距离超过预设的阈值时，标记为新的语义段落的开始。\n聚类相似句子：将语义上接近的句子聚集成块，同时保持句子的原始顺序。\n伪代码示例\narticle = \u0026#34;你的完整文章\u0026#34; chunks = [article[:1]] # 默认第一部分为一个独立分块 # 文章逐句审查 for sentence in article[1:]: if is_similar(sentence, chunks[-1][-1]): chunks[-1].append(sentence) # 如果新句子与当前块的最后一个句子相似，则加入当前块 else: chunks.append([sentence]) # 如果不相似，则创建新的分块 # 持续这个过程直至文章末尾 # 在这种方法中，is_similar 函数需要定义为比较句子嵌入之间的距离，并判断它们是否足够接近。 特殊分块以及后处理 不见得一定要在召回后，返回原始的chunks。\nSummary(摘要) 处理和存储大量文档的原始数据块往往不是最高效的方法。通过为每个文档创建摘要并进行嵌入，我们不仅可以减少处理的数据量，还能提高检索的速度和准确性。\nhttps://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary\ncreate a summary for each document, embed that along with (or instead of) the document.\nimport uuid from langchain_core.documents import Document from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI chain = ( {\u0026#34;doc\u0026#34;: lambda x: x.page_content} | ChatPromptTemplate.from_template(\u0026#34;Summarize the following document:\\n\\n{doc}\u0026#34;) | ChatOpenAI(max_retries=0) | StrOutputParser() ) summaries = chain.batch(docs, {\u0026#34;max_concurrency\u0026#34;: 5}) # The vectorstore to use to index the child chunks vectorstore = Chroma(collection_name=\u0026#34;summaries\u0026#34;, embedding_function=OpenAIEmbeddings()) # The storage layer for the parent documents store = InMemoryByteStore() id_key = \u0026#34;doc_id\u0026#34; # The retriever (empty to start) retriever = MultiVectorRetriever( vectorstore=vectorstore, byte_store=store, id_key=id_key, ) doc_ids = [str(uuid.uuid4()) for _ in docs] summary_docs = [ Document(page_content=s, metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries) ] retriever.vectorstore.add_documents(summary_docs) retriever.docstore.mset(list(zip(doc_ids, docs))) Hypothetical questions HyDE\nhypothetical_document_embeddings\nHyDE (Hypothetical Document Embeddings)\nHyDE（Hypothetical Document Embeddings，假设性文档嵌入），通过生成假设性的答案并对这些生成的文档进行嵌入，而非嵌入原始文本。这种方法特别适用于用户问题简短且与需要查找的原始文本在语义上相似度不高的情况。\n当用户的问题与知识库中的原始文本在语义上相差较大时，传统的文档检索方法可能无法有效地匹配到相关内容。HyDE通过使用大型语言模型（LLMs）根据用户问题生成格式上类似于知识库中文本的假设性答案，从而提升匹配的准确率。\n尽管HyDE可以提升检索的准确性，但这种方法也会一定程度上增加检索的耗时。为了平衡检索准确率和响应速度，可以考虑使用更强大的重排模型（reranking models）。这些模型能够在检索后的阶段进一步优化结果，通过精准地评估和排序生成的假设性答案，以提供最相关的回答。\nParent Document Retriever (PDR) from Advanced RAG Techniques: an Illustrated Overview\nA.K.A Multi-size-chunks(Parent Document Retriever/ parent-child chunks retriever/ Auto Merging Retriever)\n核心思想是，尽管小片段的数据更容易在语义上与短小的用户查询匹配，但这些小片段往往缺乏提供全面回答所需的足够上下文。PDR的解决方案是在检索时优先获取这些小片段，然后识别并返回包含这些小片段的更大父文档，以此为基础进行问题的回答。\n在使用embedding模型召回时，尽可能的将对应文本拆成句子级别，因为往往人的问题都是比较短的句子，这样在语义匹配上能够较好的匹配上；但是在使用LLM做回答时，由需要足够的上下文让模型能够有足够多的背景进行回答；因此在召回的时候倾向于使用small-chunks用于匹配，再通过某些聚合方式合并成一个具备相对完整上下文的较大chunks用作模型回答之背景；\n在Langchain中提供了Parent Document Retriever这一方法。\nDuring retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n使用方法：\nfrom langchain.retrievers import ParentDocumentRetriever from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter # Retrieving larger chunks # This text splitter is used to create the parent documents parent_splitter = RecursiveCharacterTextSplitter(separators=\u0026#34;\\n\u0026#34;,chunk_size=2000) # This text splitter is used to create the child documents # It should create documents smaller than the parent child_splitter = RecursiveCharacterTextSplitter(separators=\u0026#34;\\n\u0026#34;,chunk_size=400) # define your docs = embeddings, and vectorstore # docs = loader.load(...) # emebeddings = OpenAIEmbeddings(...) # vectorstore = Chroma(...) # The storage layer for the parent documents store = InMemoryStore() retriever = ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter, ) retriever.add_documents(docs) 而在llama_index中也提供了类似的方法。\n# 创建较小的子块 from llama_index.schema import IndexNode sub_chunk_sizes = [256, 512, 864] sub_node_parsers = [ SimpleNodeParser.from_defaults(chunk_size=c) for c in sub_chunk_sizes ] all_nodes = [] for base_node in base_nodes: for n in sub_node_parsers: sub_nodes = n.get_nodes_from_documents([base_node]) sub_inodes = [IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes] all_nodes.extend(sub_inodes) original_node = IndexNode.from_text_node(base_node, base_node.node_id) all_nodes.append(original_node) all_nodes_dict = {n.node_id: n for n in all_nodes} # 创建索引、检索器和查询引擎 vector_index_chunk = VectorStoreIndex( all_nodes, service_context=service_context ) vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2) retriever_chunk = RecursiveRetriever( \u0026#34;vector\u0026#34;, retriever_dict={\u0026#34;vector\u0026#34;: vector_retriever_chunk}, node_dict=all_nodes_dict, verbose=True, ) query_engine_chunk = RetrieverQueryEngine.from_args( retriever_chunk, service_context=service_context ) response = query_engine_chunk.query( \u0026#34;Can you tell me about the key concepts for safety finetuning\u0026#34; ) print(str(response)) Sentence Window Retrieval 为了更好地分析找到的语境，我们在检索到的最相关单句之前后各扩展 k 个句子，然后把这个扩展后的语境送给 LLM 进行推理\nfrom Advanced RAG Techniques: an Illustrated Overview\n分块可视化小工具 推荐一个可视化chunk-spliting 工具chunkviz\n推荐阅读 Text splitting (chunking) for RAG applications\n5_Levels_Of_Text_Splitting\nlangchain:Semantic Chunking\nChunking Strategies for LLM Applications\nAdvanced RAG Techniques: an Illustrated Overview\nRAG系列05：基于语义的Chunk分割\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%9C%AC%E5%88%86%E5%9D%97/","summary":"\u003ch2 id=\"为什么要进行文本分块\"\u003e\u003cstrong\u003e为什么要进行文本分块？\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e大型语言模型（LLM）和嵌入式模型都有其处理文本的长度限制。为了有效处理超出这些限制的长文本，我们需要进行文本分块。\u003c/p\u003e\n\u003ch2 id=\"如何进行文本分块\"\u003e\u003cstrong\u003e如何进行文本分块\u003c/strong\u003e\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e**块分割戒律：**我们的目标不仅仅是为了划分数据块。更重要的是，我们要以一种便于日后检索和提取价值的格式来整理我们的数据。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e文本分块的根本目的不仅仅是简单地将数据分割成块。更重要的是，我们要以便于检索和提取价值的方式来组织这些数据块。以下是几种常见的分块逻辑：\u003c/p\u003e\n\u003ch3 id=\"句子分割sentence-splitting\"\u003e\u003cstrong\u003e句子分割（Sentence Splitting）\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e这种方法只是简单地将文本切分成N个字符的片段，不考虑内容或形式的连贯性。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;...\u0026#34;\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 你的文本\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003elangchain.text_splitter\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCharacterTextSplitter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext_splitter\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCharacterTextSplitter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eseparator\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003echunk_size\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e256\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003echunk_overlap\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e20\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edocs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etext_splitter\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate_documents\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"递归字符文本分割recursive-character-text-splitting\"\u003e\u003cstrong\u003e递归字符文本分割（Recursive Character Text Splitting）\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e这种方法首先尝试用一个分隔符将文本切分成小块。如果初始尝试未能达到预期的块大小或结构，就递归地用不同的分隔符重新分割结果块，直到得到满意的结果。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;...\u0026#34;\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 你的文本\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003elangchain.text_splitter\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext_splitter\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003echunk_size\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e256\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003echunk_overlap\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e20\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edocs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etext_splitter\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate_documents\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e根据langchain 的\u003ca href=\"https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L842\"\u003e默认分隔条件\u003c/a\u003e \u003ccode\u003e[\u0026quot;\\n\\n\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;]\u003c/code\u003e ,也就是会将text根据该分割条件的顺序（两个换行-\u0026gt;一个换行-\u0026gt;空格）将文本进行递归分割。\u003c/p\u003e\n\u003ch3 id=\"针对特定文档的分割方法document-specific-splitting\"\u003e\u003cstrong\u003e针对特定文档的分割方法（Document Specific Splitting）\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e例如针对Markdown或Python代码的分割策略，这些策略根据文档的结构来进行分割，如类定义或Markdown标题。\u003c/p\u003e\n\u003cp\u003elangchain 提供了一些常见文档的分割方法：\u003c/p\u003e\n\u003cp\u003emardown的分割逻辑\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# First, try to split along Markdown headings (starting with level 2)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e#{1,6} \u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# Note the alternative syntax for headings (below) is not handled here\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# Heading level 2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# ---------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# End of code block\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;```\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# Horizontal lines\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003e*\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003e*\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003e*+\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e---+\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e___+\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# Note that this splitter doesn\u0026#39;t handle horizontal lines defined\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# by *three or more* of ***, ---, or ___, but this is not handled\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34; \u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003epython的分割逻辑：\u003c/p\u003e","title":"RAG工具箱：文本分块"},{"content":"写在开头 本文是基于最近组内技术交流的文字稿整理。\nWhat is Agent？ 在探讨复杂的人工智能技术之前，让我们先回顾一下生活中的一个简单例子：扫地机器人。这种智能设备在房间中自主导航，避开障碍物，寻找最有效的清洁路径。它就是一个现实生活中的Agent——一个可以自主决策和行动的实体。\n在人工智能领域，Agent指的是任何可以感知其环境并根据感知结果做出决策的实体。这些决策旨在完成某些任务或达成特定的目标。Agent的行为可以简单如游戏里的机器人，也可以复杂如自动驾驶汽车。\n开始于强化学习 在强化学习中， 我们往往能见到agent的概念。强化学习是一种机器学习方法，它教导Agent通过试错法找到最佳行动路径。就像训练小狗一样，我们通过奖励来引导Agent做出正确的决策。Agent的目标是在与环境的交互中寻找最优策略。理想情况下，如果Agent能够获取足够多的真实环境数据，它就能找到最佳解决方案。然而，由于真实环境的复杂性，完全模拟真实世界是不现实的。\n目前，强化学习主要适用于环境简单、问题定义明确的场景，如围棋或视频游戏。这种方法在虚拟环境中通过大量试错来探索解决方案，这种方法虽然有效，但缺乏灵活性和高效性。与人类学习新技能的方式相比，强化学习的效率远低。人们通常通过少量的尝试就能迅速掌握新技能，而强化学习可能需要成千上万次的试错。\n这时候，如果agent具有大脑就好了。\n将LLMs作为大脑: 赋能智能Agent的关键技术 相较于基于强化学习的Agent，人类的优势在于我们天生具备的记忆能力和逻辑判断能力，甚至包括反思和从经验中学习的能力。这些能力使得我们能够通过极少的试错迅速适应和掌握新技能。\n语言模型（LLMs）为AI领域带来了革命性的变化。LLMs通过其深度学习的新范式，以及在思维链和自然语言理解方面的强大能力，预示着Agent将拥有更强大的学习和迁移能力。这种能力的提升将使得创建广泛应用且实用的Agent成为可能。\n虽然LLM是否真正具备了推理能力仍然存疑，但LLM的出现无疑改变了很多。以COT（Chain of Thought）为例，这种模型通过打印解题的中间步骤，加强了在数学和逻辑推理方面的能力，减少了幻觉的出现。这一突破性的工作揭示了LLM在不同场景下的强大\u0026quot;in-context learning\u0026quot;能力，以及其在未经微调的情况下的泛化能力。\n将LLM视为AI Agent的大脑，为自动化系统提供了一种全新的构思方式。这种基于LLM的agent系统综合了规划、记忆、工具使用和行动的能力，通过API调用与外部世界互动，显示出了前所未有的灵活性和效率。\n根据LLM Powered Autonomous Agents一文对LLM-based agent系统的定义， agent需要具备的基本能力：规划+记忆+工具+行动。\n规划能力：将复杂的任务分解成小任务， 并管理每一个子任务的目标， 并从过去的失败中反思，以吸取经验。\n记忆：LLM的上下文长度有限， 通过额外的记忆系统以提升LLM作为大脑的能力。\n工具调用\u0026amp;行动：LLM通过API调用的方式，执行任务， 与外界交互，而不是只是输出文字。\n探索AI代理的独特能力：人类与单一LLM无法比拟 AI系统的主要优势在于它们的规模和效率。这些系统能够执行以下任务，超越人类能力：\n大规模数据处理：AI能够高效地分析和处理超出人类理解范围的数据量。\n无需休息的持续操作：AI系统可以不间断地运行，而无需像人类那样休息和恢复。\n超快速计算：AI可以迅速执行复杂的计算，处理速度和效率远超人类。\nAI代理与单一LLM的不同:\n根据Andrew Ng在讲座中分享的内容，使用相对“简单”模型的代理工作流程（例如GPT-3.5）在实际应用中往往能够超越使用“智能”模型（如GPT-4）的零次提示。这说明在特定场景下，选择适当的AI模型和策略可能比单一的高级模型更有效。\nAI代理在决策制定中的应用也显示出其独特的优势。它们可以在没有情感偏见的情况下，基于大量数据做出快速且精确的决策。这种能力在需要快速响应和高精确度的领域尤为重要，如金融交易和紧急响应系统。\nAgent的规划和思维过程 AI Agent在处理复杂任务时，通过将大任务分解成小任务来提高效率。此外，自我反思能力允许Agent从过去的行动中学习，通过评估过去的决策来改善未来的表现。\nCoT，Chain of Thought， Wei et al. 2022。 即“思维链”，是一种使Agent逐步思考的方法。它通过要求模型展示解决问题的中间步骤来加强其逻辑推理能力，从而提高决策的质量和准确性。\nTree of Thoughts， ToT (Yao et al. 2023)\n尽管语言模型在许多领域表现出色，但在需要复杂规划和全局决策的任务上，它们的能力受到了限制。ToT框架应运而生，旨在通过一个结构化的思考和评估过程来弥补这一缺陷。\nToT框架借鉴了人类心理学中的双系统决策理论，通过整合快速直觉判断和慢速深思熟虑的决策过程，极大地提升了模型的决策能力。这一框架通过自我评估的方式，允许模型在面对多种可能的决策路径时，能够进行有效的选择和全局优化。\nToT框架旨在克服现有语言模型在处理需要复杂规划或搜索任务的局限。它通过结构化的思想树来探索和评估不同的决策路径。ToT允许模型在考虑多个推理路径时自我评估其选择，以做出最佳的决策。此外，ToT结合了语言生成和搜索算法（如BFS和DFS），使模型能够在进行决策时前瞻和回溯，以实现全局最优选择。\nprompt example：\ncot_prompt = \u0026#39;\u0026#39;\u0026#39; Write a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} Make a plan then write. Your output should be of the following format: Plan: Your plan here. Passage: Your passage here. \u0026#39;\u0026#39;\u0026#39; vote_prompt = \u0026#39;\u0026#39;\u0026#39;Given an instruction and several choices, decide which choice is most promising. Analyze each choice in detail, then conclude in the last line \u0026#34;The best choice is {s}\u0026#34;, where s the integer id of the choice. \u0026#39;\u0026#39;\u0026#39; compare_prompt = \u0026#39;\u0026#39;\u0026#39;Briefly analyze the coherency of the following two passages. Conclude in the last line \u0026#34;The more coherent passage is 1\u0026#34;, \u0026#34;The more coherent passage is 2\u0026#34;, or \u0026#34;The two passages are similarly coherent\u0026#34;. \u0026#39;\u0026#39;\u0026#39; score_prompt = \u0026#39;\u0026#39;\u0026#39;Analyze the following passage, then at the last line conclude \u0026#34;Thus the coherency score is {s}\u0026#34;, where s is an integer from 1 to 10. \u0026#39;\u0026#39;\u0026#39; GoT， Graph of Thoughts，\nhttps://arxiv.org/abs/2308.09687\nhttps://arxiv.org/abs/2401.14295\nGoT（Graph of Thoughts）是在ToT（Tree of Thoughts）基础上的一种进阶，它通过引入图状结构，增强了模型处理复杂推理任务的能力。这种结构允许在生成的思考中进行更为灵活的图状转换，如合并多个思考或在思考间进行循环。\nGoT的核心特点在于其灵活的图状结构，这使得它能够通过合并和循环不同的思考节点来模拟更复杂的推理过程。每个思考节点可以从多个父节点聚合信息，也可以向多个子节点扩展，形成一个密集的思考网络。这种结构特别适合处理那些传统树状模型难以解决的复杂推理和决策任务。\nGoT设计之初就为了处理比ToT更为复杂的推理任务。通过允许广泛的依赖关系和中间步骤的聚合，GoT能够更有效地解决那些需要非线性思考路径的问题。例如，在处理涉及多个相互关联子问题的任务时，GoT的图状结构提供了一种比传统线性或树状结构更为有效的解决方案。\nXoT, Everything of Thoughts, https://arxiv.org/abs/2311.04254\nXoT代表了在AI领域中对复杂思考和决策过程的一次显著革新，它通过结合蒙特卡洛树搜索（MCTS）和大型语言模型求解器（LLM solver）来优化决策路径。\nXoT框架包含以下几个关键步骤和组件：\n预训练的MCTS模块：该模块通过在特定任务上的预训练，获得了有效导航复杂思考路径的能力。\n动态思考搜索：在实际应用中，MCTS模块继续利用其策略和价值网络来探索和优化潜在的思考轨迹。\n思考修正与优化：LLM评估并修正MCTS的输出，确保思考路径的逻辑严密性和有效性。\n决策执行：修正后的思考路径被送回LLM，用于解决具体的问题和挑战。\nReAct (Reason to Act) 是由 Yao et al. 2023 提出的一种新颖的提示框架，旨在通过交错的思考、行动和观察步骤，提高大型语言模型（LLM）的动态推理和计划调整能力。\nReAct通过“思考-行动-观察”结构，使LLM能够生成口头推理迹线和与任务相关的行动。这种结构允许模型动态地创建、维护并调整高层次的行动计划，同时与外部环境（如Wikipedia）互动，引入额外信息以丰富推理。\nReAct的独特特点\n直观且易于设计：ReAct的设计直观，易于实现，并能快速适应不同的应用场景。\n通用性与灵活性：适用于多种任务，能够灵活处理复杂的情境。\n高性能与鲁棒性：提供稳定且有效的推理输出。\n与人类行为一致且可控：增强了模型的可控性，与人类的推理和行动方式更为一致。\nReAct与CoT的对比\n相对于CoT（Chain of Thought），ReAct通过交错的思考、行动和观察步骤，改进了模型的根据性和可信度。然而，这种结构在提高模型实地操作的同时，也略微降低了在推理步骤制定的灵活性。此外，ReAct的有效性很大程度上依赖于成功检索到信息丰富的知识。\nThought: ... Action: ... Observation: ... ... (Repeated many times) Reflexion是由Shinn \u0026amp; Labash 2023提出的一种创新的决策支持框架。它包括三个核心模块：Actor（行动模块）、Evaluator（评估模块）、和Self-Reflection（自我反思模块），旨在通过增强的语言模型反馈机制提升任务处理效率和质量。\n框架组成与工作机制\nActor模块：负责生成针对特定任务的行动指令和文本。\nEvaluator模块：评估Actor的输出，保证行动的适宜性和效果。\nSelf-Reflection模块：反馈生成部分，通过深入分析和总结Actor的表现，指导未来的改进。\nReflexion框架的轻量级设计和对详细反馈的处理能力使其在多个高级任务中表现优异，包括编程、推理和复杂决策。实际应用显示，该框架能显著提升任务处理的正确率和效率。\n思考的快与慢。 在《思考，快与慢》一书中，丹尼尔·卡尼曼提出了人类思考的两种模式：“系统1”和“系统2”。系统1是快速、本能和情感性的，而系统2则更为缓慢、深思熟虑和逻辑性。这两种系统在处理不同类型的记忆和决策时发挥着不同的作用。\nAgent记忆 记忆是人类认知功能的核心，使我们能够从经验中学习并避免重复错误。在人工智能领域，尤其是在开发智能代理（Agent）时，构建有效的记忆系统也显得尤为重要。\nAI系统，尤其是依赖于大型语言模型的Agent，常面临有限的上下文记忆容量问题。这限制了它们在处理持续对话和复杂任务中的表现。这些系统通常受到有限上下文长度的制约，这限制了它们在处理连续对话或复杂任务时的效率。例如，一个没有有效记忆机制的Agent可能会重复犯同样的错误，或者忘记初始的任务目标。\n尽管现代LLMs的上下文长度已经可以处理超过100,000个标记，试图将所有对话历史和相关记忆直接置于模型的上下文中既不经济也不高效。因此，开发一个专门的记忆系统，如检索增强生成（Retrieval-Augmented Generation, RAG）成为提升Agent性能的有效手段。\n如果要为AI构建高效的记忆系统，借鉴和参考人类的记忆系统是必不可少的。人类的记忆系统包含多种类型的记忆，每种记忆都有其独特的生物学基础和功能，这些记忆类型不仅支持我们的日常活动，还是我们进行复杂决策和学习的基础。\n通过了解人类记忆的工作原理，我们可以设计出能够更好地模拟人类认知过程的AI系统。这不仅可以提高机器的任务执行能力，还可以使机器更加自主地适应复杂的环境和需求。\n根据维基百科的定义，人类的记忆系统可细分成：\n工作记忆：支持我们处理日常任务和进行决策。\n工具记忆：帮助我们形成习惯和自动化反应。\n语义记忆：允许我们累积知识和理解世界。\n情节记忆：使我们能够回忆过去的经历和事件。\n动作记忆：通过练习改善我们的物理技能和运动。\n遗忘的机制和认知卸载。记忆的形成经历过滤、压缩、遗忘和重组等环节。在这个过程中，大脑会删除大部分接收到的信息，只保留对当前活动或未来决策最关键的部分。\n建议阅读：\nA Survey on the Memory Mechanism of Large Language Model based Agents\nAgent的工具调用 我们更希望能帮助我们干活的agent 而不是一个只会说话的LLMs。换句话说人实际上并不需要一个聊天的对象，而是帮助解决问题的助手\n如何让LLMs具备工具调用的能力。 根据当前的主要工作， 主要是以openAI 的tool-using 的工作为主（也就是之前的function calling）。\n一句话解释：function calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\n具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\n更详细的tool-using内容，可以查看之前的博文：Agent学习笔记：OpenAI Function Calling完全指南\n除了tool-using意外， 比较重要的工具还有：code interpreter， 在代码开发的场景下， code interpreter充当了RL中的Env角色，让agent能够根据环境的反馈做出对应决策，而不只是生成代码。 除了openAI所提供的code interpreter外， 还有一些开源的code interpreter，如Open Interpreter 可以使用。\n从单Agent到多agent协作 来自斯坦福的虚拟小镇，因其独特的话题性而被广泛讨论。\nGenerative Agents: Interactive Simulacra of Human Behavior\nMetaGPT 在MetaGPT看来，可以将智能体想象成环境中的数字人，其中\n智能体 = 大语言模型（LLM） + 观察 + 思考 + 行动 + 记忆\n多智能体系统可以视为一个智能体社会，其中\n多智能体 = 智能体 + 环境 + 标准流程（SOP） + 通信 + 经济\nCode = SOP(Team) 是核心哲学。我们将SOP具象化，并且用于LLM构成的团队\n一些讨论 LLMs真的具备逻辑推理能力吗 何为推理（reasoning）：是对信息进行处理和推断，形成判断或结论的过程；**推理通常是一个包含多个推断步骤的过程。推理通常被认为需要抽象思维——也就是说，推理的能力不局限于具体的例子，而是更为普遍的。**LLMs的推理能力似乎是”涌现“的结果，而非专门针对性训练，所以LLMs是否真正具备推理能力？从一些结果表明，LLMs似乎是在背答案而非真正的推理。\n在 Can Large Language Models Reason? 这篇文章中作者认为CoT过程中LLM可能是在背答案：\nGPT-4 solved 10 out of 10 problems that had been published before 2021 (GPT-4’s pre-training cutoff date) and zero out of 10 problems that had been published after 2021. 不过，我还是持保留意见的， 作者所观察到的现象和人类很像，更擅长在自己熟悉的领域，因为存在大量的记忆，记忆间存在大量“近道”（shortcut）用于加速推导过程，甚至直接给出答案；所要做的让LLMs去执行“慢思考”而不是“快思考”。\n如果从transformer 的原理上讨论， 一个自回归的模型本身是和推理，逻辑甚至是智能无缘的。\n但人类的语言中， 天生具备了逻辑，或者人类的语言中融入的人类本身的逻辑。那么通过模拟语言的生成，一定程度也能恢复出语言背后的逻辑。\nAgent真的只是商业上的吹捧或者泡沫吗 泡沫的成份一定是有点。但也是未来十年值得做的东西。\n如果把LLM类比成CPU的话， 一个完整的计算机系统，除了CPU以外， 还需要内存，控制，输入输出等组件。不可能让LLM实现所有功能，这并不现实。但通过一定合理的架构，工作流的编排，能够大大提升LLM的准确性，进而加快落地实现。\n传统行业要如何在agent上发力 所有公司都有SOP，和大量存在在员工脑子中的“隐性知识”。 这些“知识”只有转换成agent的工作流（思维方式），或者是LLM的system prompt，才会变得更有价值和意义。现阶段有很多公司在做所谓的知识管理， 本质上是用公司的内部文本作为RAG系统而已，但一个问答系统很难量化出收益，并且对于使用者的要求挺高的。 实际上我认为，发力点应该是将内部人类可使用的工具逐渐API化，标准化以方便agent的工具调用，然后定义智能代理在各种业务场景中的具体任务和行动。这包括从简单的数据检索到复杂的决策制定\nAgent应该从事简单重复劳动还是具有创造性的探索工作 这题让GPT-4来回答吧：\n关于“Agent应该从事简单重复劳动还是具有创造性的探索工作”的问题，我们可以从多个角度进行分析：\n经济效益：从经济角度考虑，如果简单重复的劳动可以被自动化或由机器人执行，这将大幅度降低成本，并提高效率。企业通常会选择自动化这类工作以减少人力成本。然而，对于创造性的探索工作，由于需要不断地创新和解决新问题，这种类型的工作更可能需要人类智能的参与，因此，这种工作对于推动技术进步和经济增长具有重要价值。\n技术发展：随着AI和机器学习技术的发展，越来越多的简单劳动可以被智能系统所取代。例如，数据输入、监控等任务已被自动化系统广泛接管。然而，对于需要高度创造性和判断力的工作，如科学研究、艺术创作、复杂决策制定等，目前的技术还无法完全替代人类。\n人类福祉：从心理学的角度来看，长期从事简单重复的劳动可能导致工作满意度下降，影响心理健康。相反，创造性工作通常能带来更高的个人成就感和职业满意度。因此，鼓励Agent从事创造性探索工作可能更有助于提升整体福祉。\n社会需求和未来趋势：社会对创新和创造的需求日益增加，这种趋势推动了对创造性职业的需求。例如，新兴技术、可持续发展解决方案和个性化产品等领域，都需要大量的创新思维和创造力。因此，社会和市场需求可能更倾向于从事具有创造性的探索工作。\n综上所述，虽然从事简单重复的劳动在某些情况下具有其经济和效率上的优势，但从长远和全面的角度来看，鼓励Agent从事具有创造性的探索工作似乎更具前瞻性和可持续性。这不仅有助于经济和技术的发展，还能改善工作者的心理福祉和职业满意度。\nAgent在制造业良率分析上如何应用 以Yield分析和提升为例，在进行Yield提升和案例分析时，存在大量重复的分析和判断工作，这些可以通过AI进行加速和辅助，而不需人工逐一检查。\n工程师每天可能需要处理超过100个与良率相关的预警。其中约40%可能是由于测试问题、设备故障或是之前已经标注过的案例。这些都需要工程师逐一检查。由于人力有限，他们只能优先审查良率特别低的情况，可能会错过一些潜在的问题。通过使用特定规则和AI Agent，可以自动检查这些数据并作出判断。\n在处理实际案例时，工程师通常会根据案例特征在知识库中查询处理方法，这一过程也可以由Agent自动完成，并缩小后续检查的范围。如果需要进一步分析，工程师会调用不同的工具用于检查，这些分析也可以通过Agent来执行。如果仍无法找到问题，可以提醒人工介入。当Agent成功分析出结果时，还会将分析过程和结果整理成图文并茂的报告，供工程师检查和使用。一旦这些分析被人工审核并采用，它们就会成为高质量的知识，自动加入知识库中供下次使用。\nReading LLM Powered Autonomous Agents\nA busy person’s Intro to AI Agents\n基于大语言模型的AI Agents—Part 1\nChain of Thought\nTree of Thoughts\nGraph of Thoughts\nReAct\nReflexion\nMetaGPT: Meta Programming for Multi-Agent Collaborative Framework\nGenerative Agents: Interactive Simulacra of Human Behavior\nLLM Agents Papers\nAI Agent \u0026amp; 大模型经典论文推荐\nAI Agents are disrupting automation: Current approaches, market solutions and recommendations\nAGI-Edgerunners-LLM-Agents-Papers\nThe Rise and Potential of Large Language Model Based Agents: A Survey\nAwesome AI Agents\nLarge Language Model based Multi-Agents: A Survey of Progress and Challenges\nThe Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n","permalink":"https://niraya666.github.io/posts/%E5%9F%BA%E4%BA%8E%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84-agent%E7%A7%91%E6%99%AE%E5%90%91/","summary":"\u003ch2 id=\"写在开头\"\u003e写在开头\u003c/h2\u003e\n\u003cp\u003e本文是基于最近组内技术交流的文字稿整理。\u003c/p\u003e\n\u003ch2 id=\"what-is-agent\"\u003eWhat is Agent？\u003c/h2\u003e\n\u003cp\u003e在探讨复杂的人工智能技术之前，让我们先回顾一下生活中的一个简单例子：扫地机器人。这种智能设备在房间中自主导航，避开障碍物，寻找最有效的清洁路径。它就是一个现实生活中的Agent——一个可以自主决策和行动的实体。\u003c/p\u003e\n\u003cp\u003e在人工智能领域，Agent指的是任何可以感知其环境并根据感知结果做出决策的实体。这些决策旨在完成某些任务或达成特定的目标。Agent的行为可以简单如游戏里的机器人，也可以复杂如自动驾驶汽车。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%9f%ba%e4%ba%8e%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%20Agent%ef%bc%9a%e7%a7%91%e6%99%ae%e5%90%91-assets/Pasted%202024-05-10-14-58-28.webp\" alt=\"Pasted 2024-05-10-14-58-28.webp\"  /\u003e\n\u003c/p\u003e\n\u003ch3 id=\"开始于强化学习\"\u003e开始于强化学习\u003c/h3\u003e\n\u003cp\u003e在强化学习中， 我们往往能见到agent的概念。强化学习是一种机器学习方法，它教导Agent通过试错法找到最佳行动路径。就像训练小狗一样，我们通过奖励来引导Agent做出正确的决策。Agent的目标是在与环境的交互中寻找最优策略。理想情况下，如果Agent能够获取足够多的真实环境数据，它就能找到最佳解决方案。然而，由于真实环境的复杂性，完全模拟真实世界是不现实的。\u003c/p\u003e\n\u003cp\u003e目前，强化学习主要适用于环境简单、问题定义明确的场景，如围棋或视频游戏。这种方法在虚拟环境中通过大量试错来探索解决方案，这种方法虽然有效，但缺乏灵活性和高效性。与人类学习新技能的方式相比，强化学习的效率远低。人们通常通过少量的尝试就能迅速掌握新技能，而强化学习可能需要成千上万次的试错。\u003c/p\u003e\n\u003cp\u003e这时候，如果agent具有大脑就好了。\u003c/p\u003e\n\u003ch3 id=\"将llms作为大脑-赋能智能agent的关键技术\"\u003e将LLMs作为大脑: \u003cstrong\u003e赋能智能Agent的关键技术\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e相较于基于强化学习的Agent，人类的优势在于我们天生具备的记忆能力和逻辑判断能力，甚至包括反思和从经验中学习的能力。这些能力使得我们能够通过极少的试错迅速适应和掌握新技能。\u003c/p\u003e\n\u003cp\u003e语言模型（LLMs）为AI领域带来了革命性的变化。LLMs通过其深度学习的新范式，以及在思维链和自然语言理解方面的强大能力，预示着Agent将拥有更强大的学习和迁移能力。这种能力的提升将使得创建广泛应用且实用的Agent成为可能。\u003c/p\u003e\n\u003cp\u003e虽然LLM是否真正具备了推理能力仍然存疑，但LLM的出现无疑改变了很多。以COT（Chain of Thought）为例，这种模型通过打印解题的中间步骤，加强了在数学和逻辑推理方面的能力，减少了幻觉的出现。这一突破性的工作揭示了LLM在不同场景下的强大\u0026quot;in-context learning\u0026quot;能力，以及其在未经微调的情况下的泛化能力。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%9f%ba%e4%ba%8e%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%20Agent%ef%bc%9a%e7%a7%91%e6%99%ae%e5%90%91-assets/%e6%88%aa%e5%b1%8f2024-05-08%2015.37.21.png\" alt=\"截屏2024-05-08 15.37.21.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e将LLM视为AI Agent的大脑，为自动化系统提供了一种全新的构思方式。这种基于LLM的agent系统综合了规划、记忆、工具使用和行动的能力，通过API调用与外部世界互动，显示出了前所未有的灵活性和效率。\u003c/p\u003e\n\u003cp\u003e根据\u003ca href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\u003eLLM Powered Autonomous Agents\u003c/a\u003e一文对LLM-based agent系统的定义， agent需要具备的基本能力：规划+记忆+工具+行动。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e规划能力\u003c/strong\u003e：将复杂的任务分解成小任务， 并管理每一个子任务的目标， 并从过去的失败中反思，以吸取经验。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e记忆\u003c/strong\u003e：LLM的上下文长度有限， 通过额外的记忆系统以提升LLM作为大脑的能力。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e工具调用\u0026amp;行动\u003c/strong\u003e：LLM通过API调用的方式，执行任务， 与外界交互，而不是只是输出文字。\u003c/p\u003e\n\u003ch3 id=\"探索ai代理的独特能力人类与单一llm无法比拟\"\u003e\u003cstrong\u003e探索AI代理的独特能力：人类与单一LLM无法比拟\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eAI系统的主要优势在于它们的规模和效率。这些系统能够执行以下任务，超越人类能力：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e大规模数据处理\u003c/strong\u003e：AI能够高效地分析和处理超出人类理解范围的数据量。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e无需休息的持续操作\u003c/strong\u003e：AI系统可以不间断地运行，而无需像人类那样休息和恢复。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e超快速计算\u003c/strong\u003e：AI可以迅速执行复杂的计算，处理速度和效率远超人类。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAI代理与单一LLM的不同:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e根据Andrew Ng在讲座中分享的内容，使用相对“简单”模型的代理工作流程（例如GPT-3.5）在实际应用中往往能够超越使用“智能”模型（如GPT-4）的零次提示。这说明在特定场景下，选择适当的AI模型和策略可能比单一的高级模型更有效。\u003c/p\u003e\n\u003cp\u003eAI代理在决策制定中的应用也显示出其独特的优势。它们可以在没有情感偏见的情况下，基于大量数据做出快速且精确的决策。这种能力在需要快速响应和高精确度的领域尤为重要，如金融交易和紧急响应系统。\u003c/p\u003e\n\u003ch2 id=\"agent的规划和思维过程\"\u003e\u003cstrong\u003eAgent的规划和思维过程\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eAI Agent在处理复杂任务时，通过将大任务分解成小任务来提高效率。此外，自我反思能力允许Agent从过去的行动中学习，通过评估过去的决策来改善未来的表现。\u003c/p\u003e\n\u003cp\u003eCoT，Chain of Thought， \u003ca href=\"https://arxiv.org/abs/2201.11903\"\u003eWei et al. 2022\u003c/a\u003e。 即“思维链”，是一种使Agent逐步思考的方法。它通过要求模型展示解决问题的中间步骤来加强其逻辑推理能力，从而提高决策的质量和准确性。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%9f%ba%e4%ba%8e%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%20Agent%ef%bc%9a%e7%a7%91%e6%99%ae%e5%90%91-assets/%e6%88%aa%e5%b1%8f2024-05-08%2015.37.21.png\" alt=\"截屏2024-05-08 15.37.21.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTree of Thoughts， ToT\u003c/strong\u003e  (\u003ca href=\"https://arxiv.org/abs/2305.10601\"\u003eYao et al. 2023\u003c/a\u003e)\u003c/p\u003e","title":"基于大语言模型的 Agent：科普向"},{"content":"值得关注的模型 mistralai/Mixtral-8x22B-Instruct-v0.1\nCohereForAI/c4ai-command-r-plus\nmeta-llama/Meta-Llama-3-8B-Instruct\nmeta-llama/Meta-Llama-3-70B-Instruct\nmicrosoft/Phi-3-mini-128k-instruct\ngpt2-chatbot: 一个神秘的模型\n基础建设 Groq提供了免费且非常高速的API服务\nGroqCloud\n值得关注的开源项目 Cleanlab RAG 基础数据质量处理组件 \u0026ndash; 自动发现数据问题并进行修复，提高数据质量和价值\nhttps://github.com/cleanlab/cleanlab\n支持任何数据，包括图片、文本、音频、表格等\n检测数据标注问题、改进、训练模型、价值提升\n支持任何框架或模型：OpenAI、HuggingFace、PyTorch\njina reader 抓取URL，将正文转换为良好的Markdown格式的工具\nReader API\n一些类似的工具，by ：\n九原客 on Twitter / X\nhttps://markdowndown.vercel.app\nWeb-scraper(open source):\nhttps://github.com/zzzgydi/webscraper…\ncode-html-to-markdown:\nhttps://github.com/SivilTaram/code-html-to-markdown…（偏向于更好地处理代码块）\nhttps://github.com/JimLiu/gpt-api\nhttps://www.firecrawl.dev/?x\nChainForge An open-source visual programming environment for battle-testing prompts to LLMs.\nGitHub - ianarawjo/ChainForge: An open-source visual programming environment for battle-testing prompts to LLMs.\nRAGFlow 一个RAG框架\nhttps://github.com/infiniflow/ragflow/blob/main/README_zh.md\n亮点： 分块后的可视化和修改功能\nSuperMemory 一个第二大脑项目， 提供插件保存网页作为memory和基于memory对话的功能。\nhttps://github.com/Dhravya/supermemory\nLLM Agents Papers Must-read Papers on LLM Agents.\nhttps://github.com/zjunlp/LLMAgentPapers\n本月精选歌单 LIVE记录 0420 数集无限追加-DA\n0428 Arch Enemy\n到过的城市 昆明、大理、苏州\n阅读\u0026amp;摘录 读完了上野千鹤子的《厌女》和弗洛伊德的《心理学导论》\nIlya——塑造世界的人工智能科学家 什么激励我？我喜欢思考根本问题，基本问题。我们的系统不能做什么，而人类却可以做到？我几乎以哲学的方式去思考这些问题。比如，什么是学习？什么是经验？什么是思考？大脑又是如何运作的呢？\n如果多个团队处于竞相开发通用人工智能的军备竞赛态势中，他们就会没有足够的时间来确保他们构建的通用人工智能会真正关心人类。因为在我看来，这就像是通用人工智能发展的雪崩，一发不可收拾。 我认为将来整个地球的表面很可能布满太阳能板和数据中心。考虑到这些担忧，未来的通用人工智能的建设应该是多国间的合作项目。\n台积电在美国郊区建造芯片工厂的艰辛历程 美国工程师们纷纷表示，公司的等级制度僵硬且效率低下；而久经沙场的台湾台积电员工则觉得他们的美国同事缺少了构筑公司顶尖成功的必要精神：奉献与服从。\n在某些部门，为了测试美国员工是否能承受和台湾同事相同的工作压力，管理层会突然宣布需要即刻完成的任务。据两名工程师透露，有时候这种做法会导致美国员工在同事面前受到羞辱，甚至被建议放弃工程职业。…此外，一些台湾员工还参加了有关美国文化的培训课程，了解到相较于批评，美国人更喜欢通过鼓励来提高工作效率。\n“台湾人对每个任务都制造一种不切实际的紧迫感，总是催促你必须立即完成所有工作。但这对于想保持正常工作与生活平衡的人来说，显然是不现实的。” 多位前美国员工表示，他们不介意加班，只要这些加班的工作富有意义。另一位前台积电工程师回忆说：“我曾多次询问我的经理‘你的最高优先级是什么？’他总是回答‘一切都是优先级’。结果我加班加点完成的工作，最后发现根本不是必需的。”\n但美国员工对每周撰写工作报告感到厌烦，并且对台湾同事无故加班表示反感。“这真让人难以忍受，”Bruce 表示，“他们这么做完全是为了表面功夫。”\n五位前美国员工向 Rest of World 透露，台积电的工程师有时会伪造或挑选数据以满足客户和管理层的需求。有时，员工会调整测试工具或晶圆的数据，以满足上级不切实际的期望。另一些时候，一位工程师透露：“因为人手严重不足，他们会想尽办法减轻自己的工作负担。”四位美国员工形容台积电的文化是“面子文化”，员工通常会不惜牺牲效率和员工福利，只为让团队、部门或公司显得更有面子。\n“一切都源于努力工作。没有这种文化，台积电不可能成为世界第一，”他充满激情地说。“我要支持台积电成为伟大的公司。这是我的信仰。”\nZ Potentials | 张佳圆，95后离开字节创业AI搜索，三个月达超百万月访 垂直的好处是可以针对性地构建index，Devv的底层包括了开发文档、代码，也包括搜索引擎里的一些数据，这是它和general search engine的区别。… 垂直搜索引擎最大的差异点就在于能够基于场景更好地构建index，这是我们认为Phind没有做好的。\n做一个60分水平的RAG完全可以通过包装一些外部的LangChain组件来实现，但如果要做到八九十分的RAG系统，需要做的优化点会非常多。\n“Good artists copy，great artists steal”\nCode Review 时，曾被我忽视的 3 件重要小事 人类是一种神奇的智慧生物，阅读一段文字，不仅能从中获取到信息，更能从字里行间感受到情绪，有时，这份情绪甚至会盖过信息，影响他们做出判断。因此，当你在参与评审时，请谨记这一点：保持谦逊、尊重他人。\n工业化时代建立的秩序正在加速瓦解 Andrew Bamboo on Twitter / X\n但就像之前我们将经济下行归咎于疫情，而忽略了债务周期才是主导因素一样，一些迹象表明，我们也不能因为债务周期的存在，而忽略了背后的时代趋势：工业化时代的秩序正在加速瓦解，生产关系正迈向原子化。\n我们看到，大厂占据着绝对的生态位，养着无数员工，人们集中在一起，按照标准化流程进行分工合作，虽然号称白领中产，但与工业时代的流水线工人有何区别？仍然是原子化的生活，集约化标准化的生产。\n几十年的发展，使得供应链、物流变得愈发繁荣，互联网大厂的内卷，使得拿起手机，即可发布几万几十万人看到的信息构建个人影响力，即可促成交易，即可靠 AI 完成专业人士才能完成的工作，这背后，成本得到了转移，带来的是属于个体和小团队的机会。\n近来明显可以看到，身边的自媒体人/独立开发者/小创业团队逐渐多了起来，YC 投资的 AIGC 公司清单（见附图），大部分公司都是 2-4 人。在互联网增量逐渐见顶的时候，利用大公司提供的优秀基建，个体和小团队也可以依靠利基市场生存下来。而随着利基市场不断被挖掘，社会分工进一步细化，人们可以利用更为丰富的细分服务完成更多的事情，又进一步扩充了个人能力的边界。随着越来越多人从工业时代的集约化标准化生活解放出来，又会产生更为多样化的需求及新的利基市场。工业化秩序的瓦解，生产的原子化正变为一种愈发明显的趋势，而得益还是受害，也取决于个人的选择了吧——老生常谈的，每个人都应当成为超级个体，都需要有个自己的小生意。\n吴恩达分享Agentic reasoning 而agent工作流程则更加迭代和自主,它会先分析问题,必要时进行研究,生成初步方案,然后不断自我检查和修正。\n这种工作方式能够显著提高语言模型的表现,有时甚至可以让较小的模型超越更大的模型。因此,agentic reasoning的设计模式对于提升语言模型的性能非常重要。\nAI agents的四种设计模式:\n第一种是自反,即让语言模型自己检查和修正生成的内容,比如代码;\n第二种是工具使用,语言模型可以使用各种工具进行分析、信息获取和行动;\n第三种是规划,语言模型可以自主地制定计划,执行一系列步骤以完成复杂任务;\n第四种是多智能体协作,通过prompt让同一语言模型扮演不同角色,互相配合完成任务。\nAndrew认为,通往AGI(人工通用智能)是一个旅程而非终点,agent工作流程的出现让我们向这个目标又迈进了一步。\nMilestone 昆明-大理旅行 3篇blog 画了个大饼用来“骗GPU” 完成了RAG-chatbot的初步架构设计 ","permalink":"https://niraya666.github.io/monthly/2024-04/","summary":"\u003ch2 id=\"值得关注的模型\"\u003e值得关注的模型\u003c/h2\u003e\n\u003cp\u003emistralai/Mixtral-8x22B-Instruct-v0.1\u003c/p\u003e\n\u003cp\u003eCohereForAI/c4ai-command-r-plus\u003c/p\u003e\n\u003cp\u003emeta-llama/Meta-Llama-3-8B-Instruct\u003c/p\u003e\n\u003cp\u003emeta-llama/Meta-Llama-3-70B-Instruct\u003c/p\u003e\n\u003cp\u003emicrosoft/Phi-3-mini-128k-instruct\u003c/p\u003e\n\u003cp\u003egpt2-chatbot: 一个神秘的模型\u003c/p\u003e\n\u003ch3 id=\"基础建设\"\u003e基础建设\u003c/h3\u003e\n\u003cp\u003eGroq提供了免费且非常高速的API服务\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://console.groq.com/docs/quickstart\"\u003eGroqCloud\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"值得关注的开源项目\"\u003e值得关注的开源项目\u003c/h2\u003e\n\u003ch3 id=\"cleanlab\"\u003eCleanlab\u003c/h3\u003e\n\u003cp\u003eRAG 基础数据质量处理组件 \u0026ndash; 自动发现数据问题并进行修复，提高数据质量和价值\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/cleanlab/cleanlab\"\u003ehttps://github.com/cleanlab/cleanlab\u003c/a\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e支持任何数据，包括图片、文本、音频、表格等\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e检测数据标注问题、改进、训练模型、价值提升\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e支持任何框架或模型：OpenAI、HuggingFace、PyTorch\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"jina-reader\"\u003ejina reader\u003c/h3\u003e\n\u003cp\u003e抓取URL，将正文转换为良好的Markdown格式的工具\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://jina.ai/reader/\"\u003eReader API\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e一些类似的工具，by ：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/9hills/status/1779830990598549998\"\u003e九原客 on Twitter / X\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://t.co/buDyt3ixUJ\"\u003ehttps://markdowndown.vercel.app\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWeb-scraper(open source):\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://t.co/inuuZWQlwt\"\u003ehttps://github.com/zzzgydi/webscraper…\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecode-html-to-markdown:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://t.co/PsxPWBkOKf\"\u003ehttps://github.com/SivilTaram/code-html-to-markdown…\u003c/a\u003e（偏向于更好地处理代码块）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/JimLiu/gpt-api\"\u003ehttps://github.com/JimLiu/gpt-api\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.firecrawl.dev/?x\"\u003ehttps://www.firecrawl.dev/?x\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"chainforge\"\u003e\u003cstrong\u003eChainForge\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eAn open-source visual programming environment for battle-testing prompts to LLMs.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ianarawjo/ChainForge?tab=readme-ov-file#installation\"\u003eGitHub - ianarawjo/ChainForge: An open-source visual programming environment for battle-testing prompts to LLMs.\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"ragflow\"\u003e\u003cstrong\u003eRAGFlow\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e一个RAG框架\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/infiniflow/ragflow/blob/main/README_zh.md\"\u003ehttps://github.com/infiniflow/ragflow/blob/main/README_zh.md\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e亮点： 分块后的可视化和修改功能\u003c/p\u003e\n\u003ch3 id=\"supermemory\"\u003e\u003cstrong\u003eSuperMemory\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e一个第二大脑项目， 提供插件保存网页作为memory和基于memory对话的功能。\u003c/p\u003e","title":"2024-04 月刊"},{"content":"写在最开始 当我们在讨论基于大型语言模型（LLM-based）的智能代理（agent）时，我们究竟在谈论什么？根据Lilian W在其文章《LLM Powered Autonomous Agents》中的讨论，一个智能代理需要具备几个核心能力：规划（Planning）、记忆（Memory）、以及工具使用（Tool use）。特别地，工具使用方面的进展，得益于OpenAI在API中提供的function calling功能，为我们开启了新的可能性。\nOpenAI function calling，作为智能代理与外部工具交互的基本方式，对于每位从业者来说都是必备技能。随着技术的发展，我们期望的不只是能与我们对话的LLM，而是能够辅助我们使用各种工具、做出决策的智能伙伴。\n不过需要特别指出的是，最近OpenAI在Chat Completions API中已经废弃了“函数（function）”的使用，转而采用“工具（tool）”。这一变更旨在拓宽LLM集成的功能范围，为更复杂的交互模式铺平道路，如构建能够相互作用的多代理系统。\n尽管如此，由于语言习惯的原因，本文中仍然会使用function calling的术语来描述OpenAI的tool using功能，因为“function calling”的说法已经深入人心了。\n核心内容概览\nFunction Calling的定义：解释什么是function calling，以及它在智能代理工作中的作用。\nOpenAI Cookbook示例：提供实际的function calling示例，帮助读者理解其在实际应用中的用途。\n开源LLM的Tool Using：探索如何在开源大型语言模型中实现工具使用，以及LLM在tool using的时候经历了什么。\n评价与训练：讨论如何评价开源模型的工具使用能力，以及如何训练LLM进行有效的工具使用。\n鉴于整理笔记的速度远赶不上更新的速度， 会将第四部份作为单独的部分整理。\n何为function calling 一句话解释：function calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\n具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\nfunction calling的应用范围广泛，如\n创建智能助手：通过调用外部API回答问题。\n转换指令：将自然语言指令转换成API调用指令。\n数据提取：从文本中提取结构化数据。\nfunction calling的过程涵盖了从定义函数集、通过模型生成遵循自定义模式的JSON对象字符串，到在代码中解析这个字符串并调用相应函数的全过程。这一连串操作不仅自动化了交互过程，还确保了执行操作的安全性和准确性。\n一些常见的问题 JSON mode json mode 和tool-using 有什么关系？有了json mode 还需要用到tool-using吗？\n从json mode 的本质， 更多的是在system prompt 增加一句类似“请以json格式输出”之类的话，然后在LLM输出时增加json结果检查和格式转换。在使用时只需要在client.chat.completions.create 中 增加response_format={ \u0026quot;type\u0026quot;: \u0026quot;json_object\u0026quot; } 即可。\n那么json mode 什么时候会用到呢？一般在做文本提取，内容提取时可以使用；以RAG场景为例， 当我们希望LLM能够帮我们对用户的query进行改写时，我们肯定是希望模型能够返回干净的json格式改写结果，这样的结果可以直接使用，而不是在模型输出一些内容后，如：\n\u0026#34;\u0026#34;\u0026#34; 好的，以下是我的改写内容： ``` real-rewrite-query ``` \u0026#34;\u0026#34;\u0026#34; 其中包含了一些模型喜欢输出的客套话，此时我们需要通过正则匹配等方法将真正希望使用内容提取出来。而这时候json mode可以直接输出需要的内容， 而跳过了额外的提取步骤。在json mode 出现之前，这样的处理我们也尝试使用过tool-using 的模式，但有点大材小用了。\n很显然， tool-using的真正强大之处并不只是对输出格式进行处理， 而是能够让模型从提供的多个tools中选择需要使用的。\n如何使用OpenAI function calling 天气查询的简单示例 环境配置\n首先，我们需要安装一些必要的Python库。这些库将帮助我们与OpenAI的API进行交互，以及完成一些辅助功能。\n!pip install scipy --quiet !pip install tenacity --quiet !pip install tiktoken --quiet !pip install termcolor --quiet !pip install openai --quiet os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = \u0026#34;...\u0026#34; from openai import OpenAI from tenacity import retry, wait_random_exponential, stop_after_attempt from termcolor import colored client = OpenAI() 工具函数定义\n接下来，我们定义一些实用函数，这些函数旨在方便我们向Chat Completions API发送请求，并管理与跟踪对话状态。\n@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3)) def chat_completion_request(messages, tools=None, tool_choice=None, model=\u0026#34;gpt-3.5-turbo\u0026#34;): try: response = client.chat.completions.create( model=model, messages=messages, tools=tools, tool_choice=tool_choice, ) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e def pretty_print_conversation(messages): role_to_color = { \u0026#34;system\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;function\u0026#34;: \u0026#34;magenta\u0026#34;, } for message in messages: if message[\u0026#34;role\u0026#34;] == \u0026#34;system\u0026#34;: print(colored(f\u0026#34;system: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;user\u0026#34;: print(colored(f\u0026#34;user: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;assistant\u0026#34; and message.get(\u0026#34;function_call\u0026#34;): print(colored(f\u0026#34;assistant: {message[\u0026#39;function_call\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;assistant\u0026#34; and not message.get(\u0026#34;function_call\u0026#34;): print(colored(f\u0026#34;assistant: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;function\u0026#34;: print(colored(f\u0026#34;function ({message[\u0026#39;name\u0026#39;]}): {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) 函数规范定义\n我们还需要创建一些函数规范，以接口化与假设的天气API的交互。\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;The temperature unit to use. Infer this from the users location.\u0026#34;, }, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;format\u0026#34;], }, } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_n_day_weather_forecast\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get an N-day weather forecast\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;The temperature unit to use. Infer this from the users location.\u0026#34;, }, \u0026#34;num_days\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The number of days to forecast\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;format\u0026#34;, \u0026#34;num_days\u0026#34;] }, } }, ] 对话示例\n当我们请求当前天气时， LLM会要求澄清问题，如地址等参数信息：\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like today\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message output:\nChatCompletionMessage(content=\u0026#39;Sure, may I know your current location?\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None) 当补充缺失的信息后， LLM将生成适当的函数参数\nmessages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;I\u0026#39;m in Shanghai, China.\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message output:\nChatCompletionMessage(content=None, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_VdqOOMp9pagf5ho39Y2HmYV4\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;San Francisco, CA\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;,\\n \u0026#34;num_days\u0026#34;: 4\\n}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)]) 完整流程如下：\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;what is the weather going to be like in Glasgow, Scotland over the next x days\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;in 5 days\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message 当然， 我们也可以强制要求使用特定的函数\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Give me a weather report for Toronto, Canada.\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools, tool_choice={\u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;get_n_day_weather_forecast\u0026#34;}} ) chat_response.choices[0].message output:\nChatCompletionMessage(content=None, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_0ldecDpV8Vdq8mGPoUewlue3\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Toronto, Canada\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;,\\n \u0026#34;num_days\u0026#34;: 1\\n}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)]) 并行函数调用\n对于一些特定模型，如**gpt-4-turbo-preview**, gpt-4-0125-preview, gpt-4-1106-preview, gpt-3.5-turbo-0125, and **gpt-3.5-turbo-1106**支持并行函数调用， 允许我们在单个回合中调用多个函数。\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;what is the weather going to be like in San Francisco and Glasgow over the next 4 days\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message.tool_calls assistant_message output:\n[ChatCompletionMessageToolCall(id=\u0026#39;call_tfl8eTCW64sHvHjiiatoYzku\u0026#39;, function=Function(arguments=\u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;San Francisco, CA\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;fahrenheit\u0026#34;, \u0026#34;num_days\u0026#34;: 4}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;), ChatCompletionMessageToolCall(id=\u0026#39;call_bAqj55RygP2Y1T85RHqgskku\u0026#39;, function=Function(arguments=\u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;Glasgow, UK\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;, \u0026#34;num_days\u0026#34;: 4}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)] 实现本地函数调用\n首先我们需要构造两个用于演示的假function\nimport json def get_current_weather(location, format=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Simulates getting the current weather for a given location. The response is hardcoded for demonstration purposes. Args: location (str): The city and state, e.g., San Francisco, CA. format (str, optional): The temperature unit to use. Defaults to \u0026#34;fahrenheit\u0026#34;. Returns: str: JSON string with the current weather data. \u0026#34;\u0026#34;\u0026#34; if \u0026#34;tokyo\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Partly Cloudy\u0026#34;}) elif \u0026#34;san francisco\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;San Francisco\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;72\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Sunny\u0026#34;}) elif \u0026#34;paris\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;Paris\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Rainy\u0026#34;}) else: return json.dumps({\u0026#34;location\u0026#34;: location, \u0026#34;temperature\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Data Unavailable\u0026#34;}) def get_n_day_weather_forecast(location, num_days, format=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Simulates getting an N-day weather forecast for a given location. The response is hardcoded for demonstration purposes. Args: location (str): The city and state, e.g., San Francisco, CA. num_days (int): The number of days to forecast. format (str, optional): The temperature unit to use. Defaults to \u0026#34;fahrenheit\u0026#34;. Returns: str: JSON string with the N-day weather forecast data. \u0026#34;\u0026#34;\u0026#34; # This example just returns a fixed response regardless of the input. # In a real scenario, the response would depend on the location, num_days, and format. forecast = [ {\u0026#34;day\u0026#34;: 1, \u0026#34;temperature\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Sunny\u0026#34;}, {\u0026#34;day\u0026#34;: 2, \u0026#34;temperature\u0026#34;: \u0026#34;18\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Cloudy\u0026#34;}, {\u0026#34;day\u0026#34;: 3, \u0026#34;temperature\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Rainy\u0026#34;} ] # Return only the forecast for the requested number of days. return json.dumps(forecast[:num_days]) available_functions = { \u0026#34;get_current_weather\u0026#34;: get_current_weather, \u0026#34;get_n_day_weather_forecast\u0026#34;: get_n_day_weather_forecast, } 尝试请求天气\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like in Tokyo!\u0026#34;}) chat_response = chat_completion_request(messages, tools=tools, tool_choice=\u0026#34;auto\u0026#34;) assistant_message = chat_response.choices[0].message assistant_message = json.loads(assistant_message.model_dump_json()) assistant_message[\u0026#34;content\u0026#34;] = str(assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;]) #a temporary patch but this should be handled differently # remove \u0026#34;function_call\u0026#34; from assistant message del assistant_message[\u0026#34;function_call\u0026#34;] assistant_message \u0026#34;\u0026#34;\u0026#34; {\u0026#39;content\u0026#39;: \u0026#39;{\\\u0026#39;arguments\\\u0026#39;: \\\u0026#39;{\\\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\\\n}\\\u0026#39;, \\\u0026#39;name\\\u0026#39;: \\\u0026#39;get_current_weather\\\u0026#39;}\u0026#39;, \u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;tool_calls\u0026#39;: [{\u0026#39;id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;function\u0026#39;: {\u0026#39;arguments\u0026#39;: \u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\n}\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;}, \u0026#39;type\u0026#39;: \u0026#39;function\u0026#39;}]} \u0026#34;\u0026#34;\u0026#34; messages.append(assistant_message) 可以看到，LLM返回了json格式的参数信息（其实是str）以及需要调用的function名称， 拥有这些信息之后就可以调用之前定义的函数了\n# get the weather information to pass back to the model function_name_to_call = assistant_message[\u0026#39;tool_calls\u0026#39;][0][\u0026#39;function\u0026#39;][\u0026#39;name\u0026#39;] function_arguments = assistant_message[\u0026#39;tool_calls\u0026#39;][0][\u0026#39;function\u0026#39;][\u0026#39;arguments\u0026#39;] weather = available_functions[function_name_to_call](function_arguments) 将函数执行结果和对话历史返回给LLM\nmessages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;], \u0026#34;content\u0026#34;: weather}) messages \u0026#34;\u0026#34;\u0026#34; [{\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#34;What\u0026#39;s the weather like in Tokyo!\u0026#34;}, {\u0026#39;content\u0026#39;: \u0026#39;{\\\u0026#39;arguments\\\u0026#39;: \\\u0026#39;{\\\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\\\n}\\\u0026#39;, \\\u0026#39;name\\\u0026#39;: \\\u0026#39;get_current_weather\\\u0026#39;}\u0026#39;, \u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;tool_calls\u0026#39;: [{\u0026#39;id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;function\u0026#39;: {\u0026#39;arguments\u0026#39;: \u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\n}\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;}, \u0026#39;type\u0026#39;: \u0026#39;function\u0026#39;}]}, {\u0026#39;role\u0026#39;: \u0026#39;tool\u0026#39;, \u0026#39;tool_call_id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;fahrenheit\u0026#34;}\u0026#39;}] \u0026#34;\u0026#34;\u0026#34; 最终输出结果（for user）\nfinal_response = chat_completion_request(messages, tools=tools) final_response.choices[0].message.content output：\n\u0026#39;The current weather in Tokyo is partly cloudy with a temperature of 10°C (50°F).\u0026#39; 通过function calling 实现sql执行 下载演示所用的sqlite数据\n!wget https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip -O chinook.zip !unzip chinook.zip 连接数据库，和定义一些function\nimport sqlite3 conn = sqlite3.connect(\u0026#34;/content/chinook.db\u0026#34;) print(\u0026#34;Opened database successfully\u0026#34;) def get_table_names(conn): \u0026#34;\u0026#34;\u0026#34;Return a list of table names.\u0026#34;\u0026#34;\u0026#34; table_names = [] tables = conn.execute(\u0026#34;SELECT name FROM sqlite_master WHERE type=\u0026#39;table\u0026#39;;\u0026#34;) for table in tables.fetchall(): table_names.append(table[0]) return table_names def get_column_names(conn, table_name): \u0026#34;\u0026#34;\u0026#34;Return a list of column names.\u0026#34;\u0026#34;\u0026#34; column_names = [] columns = conn.execute(f\u0026#34;PRAGMA table_info(\u0026#39;{table_name}\u0026#39;);\u0026#34;).fetchall() for col in columns: column_names.append(col[1]) return column_names def get_database_info(conn): \u0026#34;\u0026#34;\u0026#34;Return a list of dicts containing the table name and columns for each table in the database.\u0026#34;\u0026#34;\u0026#34; table_dicts = [] for table_name in get_table_names(conn): columns_names = get_column_names(conn, table_name) table_dicts.append({\u0026#34;table_name\u0026#34;: table_name, \u0026#34;column_names\u0026#34;: columns_names}) return table_dicts 获取db 的schema\ndatabase_schema_dict = get_database_info(conn) database_schema_string = \u0026#34;\\n\u0026#34;.join( [ f\u0026#34;Table: {table[\u0026#39;table_name\u0026#39;]}\\nColumns: {\u0026#39;, \u0026#39;.join(table[\u0026#39;column_names\u0026#39;])}\u0026#34; for table in database_schema_dict ] ) 定义tools工具列表\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ask_database\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Use this function to answer user questions about music. Input should be a fully formed SQL query.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; SQL query extracting info to answer the user\u0026#39;s question. SQL should be written using this database schema: {database_schema_string} The query should be returned in plain text, not in JSON. \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, } } ] 定义用于执行sql的function\ndef ask_database(conn, query): \u0026#34;\u0026#34;\u0026#34;Function to query SQLite database with a provided SQL query.\u0026#34;\u0026#34;\u0026#34; try: results = str(conn.execute(query).fetchall()) except Exception as e: results = f\u0026#34;query failed with error: {e}\u0026#34; return results def execute_function_call(message): if message.tool_calls[0].function.name == \u0026#34;ask_database\u0026#34;: query = json.loads(message.tool_calls[0].function.arguments)[\u0026#34;query\u0026#34;] results = ask_database(conn, query) else: results = f\u0026#34;Error: function {message.tool_calls[0].function.name} does not exist\u0026#34; return results messages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Answer user questions by generating SQL queries against the Chinook Music Database.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi, who are the top 5 artists by number of tracks?\u0026#34;}) chat_response = chat_completion_request(messages, tools) assistant_message = chat_response.choices[0].message messages.append(assistant_message) if assistant_message.tool_calls: results = execute_function_call(assistant_message) messages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message.tool_calls[0].id, \u0026#34;name\u0026#34;: assistant_message.tool_calls[0].function.name, \u0026#34;content\u0026#34;: results}) 此时的messages\n[{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Answer user questions by generating SQL queries against the Chinook Music Database.\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Hi, who are the top 5 artists by number of tracks?\u0026#39;}, ChatCompletionMessage(content=\u0026#39;Function(arguments=\\\u0026#39;{\\\\n \u0026#34;query\u0026#34;: \u0026#34;SELECT artists.Name, COUNT(tracks.TrackId) AS num_tracks FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY artists.ArtistId ORDER BY num_tracks DESC LIMIT 5\u0026#34;\\\\n}\\\u0026#39;, name=\\\u0026#39;ask_database\\\u0026#39;)\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_vSLysEQncbGvMgXGhttIow6v\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;query\u0026#34;: \u0026#34;SELECT artists.Name, COUNT(tracks.TrackId) AS num_tracks FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY artists.ArtistId ORDER BY num_tracks DESC LIMIT 5\u0026#34;\\n}\u0026#39;, name=\u0026#39;ask_database\u0026#39;), type=\u0026#39;function\u0026#39;)]), {\u0026#39;role\u0026#39;: \u0026#39;tool\u0026#39;, \u0026#39;tool_call_id\u0026#39;: \u0026#39;call_vSLysEQncbGvMgXGhttIow6v\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;ask_database\u0026#39;, \u0026#39;content\u0026#39;: \u0026#34;[(\u0026#39;Iron Maiden\u0026#39;, 213), (\u0026#39;U2\u0026#39;, 135), (\u0026#39;Led Zeppelin\u0026#39;, 114), (\u0026#39;Metallica\u0026#39;, 112), (\u0026#39;Deep Purple\u0026#39;, 92)]\u0026#34;}] 最终输出：\nfinal_response = chat_completion_request(messages, tools=tools) final_response.choices[0].message.content # output \u0026#34;\u0026#34;\u0026#34; The top 5 artists by number of tracks are: 1. Iron Maiden - 213 tracks 2. U2 - 135 tracks 3. Led Zeppelin - 114 tracks 4. Metallica - 112 tracks 5. Deep Purple - 92 tracks \u0026#34;\u0026#34;\u0026#34; 基于function calling构建智能代理：自动获取与深度解析arXiv学术文章 在这一节中将介绍如何构建一个能够从arXiv上查找论文， 下载分析并总结学术论文的Agent。这个Agent不仅可以帮助用户快速了解特定领域的最新研究动态，而且还能深入分析和总结选定文章的核心内容。\nAgent核心功能\n获取arXiv文章 get_articles 代理通过**arxiv**库搜索关于特定主题的文章，为用户提供简要的文章摘要和链接。\n阅读并总结文章 read_article_and_summarize 利用**PyPDF2**库读取选中文章的PDF文件，代理能够提炼出文章的主要论点、支撑证据和结论。\n环境配置\n!pip install scipy --quiet !pip install tenacity --quiet !pip install tiktoken==0.3.3 --quiet !pip install termcolor --quiet !pip install openai --quiet !pip install arxiv --quiet !pip install pandas --quiet !pip install PyPDF2 --quiet !pip install tqdm --quiet import os import arxiv import ast import concurrent import json import os import pandas as pd import tiktoken from csv import writer from IPython.display import display, Markdown, Latex from openai import OpenAI from PyPDF2 import PdfReader from scipy import spatial from tenacity import retry, wait_random_exponential, stop_after_attempt from tqdm import tqdm from termcolor import colored GPT_MODEL = \u0026#34;gpt-3.5-turbo-0613\u0026#34; EMBEDDING_MODEL = \u0026#34;text-embedding-ada-002\u0026#34; client = OpenAI() 所有下载的论文都被存储在本地**./data/papers目录中，并且每篇文章的详细信息（包括其嵌入向量）都记录在arxiv_library.csv**文件中。\ndirectory = \u0026#39;./data/papers\u0026#39; # Check if the directory already exists if not os.path.exists(directory): # If the directory doesn\u0026#39;t exist, create it and any necessary intermediate directories os.makedirs(directory) print(f\u0026#34;Directory \u0026#39;{directory}\u0026#39; created successfully.\u0026#34;) else: # If the directory already exists, print a message indicating it print(f\u0026#34;Directory \u0026#39;{directory}\u0026#39; already exists.\u0026#34;) # Set a directory to store downloaded papers data_dir = os.path.join(os.curdir, \u0026#34;data\u0026#34;, \u0026#34;papers\u0026#34;) paper_dir_filepath = \u0026#34;./data/arxiv_library.csv\u0026#34; # Generate a blank dataframe where we can store downloaded files df = pd.DataFrame(list()) df.to_csv(paper_dir_filepath) 我们将定义一些utils function用于：\n文章获取与存储：通过**get_articles**函数查询主题相关的文章，系统自动下载文章并记录重要信息及embedding向量。\n文章选择与内容提取：根据用户的查询，系统通过计算embedding向量的相似度来选择最相关的文章，并提取出文章的文本内容。\n内容分块与总结：长文本被分割成多个较小的块，每个块被独立总结。\n汇总总结：所有独立块的总结被汇总成一篇全面的总结，更好地回应用户的查询。\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def embedding_request(text): response = client.embeddings.create(input=text, model=EMBEDDING_MODEL) return response @retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def get_articles(query, library=paper_dir_filepath, top_k=5): \u0026#34;\u0026#34;\u0026#34;This function gets the top_k articles based on a user\u0026#39;s query, sorted by relevance. It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize. \u0026#34;\u0026#34;\u0026#34; client = arxiv.Client() search = arxiv.Search( query = query, max_results = top_k, sort_by = arxiv.SortCriterion.SubmittedDate ) result_list = [] for result in client.results(search): result_dict = {} result_dict.update({\u0026#34;title\u0026#34;: result.title}) result_dict.update({\u0026#34;summary\u0026#34;: result.summary}) # Taking the first url provided result_dict.update({\u0026#34;article_url\u0026#34;: [x.href for x in result.links][0]}) result_dict.update({\u0026#34;pdf_url\u0026#34;: [x.href for x in result.links][1]}) result_list.append(result_dict) # Store references in library file response = embedding_request(text=result.title) file_reference = [ result.title, result.download_pdf(data_dir), response.data[0].embedding, ] # Write to file with open(library, \u0026#34;a\u0026#34;) as f_object: writer_object = writer(f_object) writer_object.writerow(file_reference) f_object.close() return result_list def strings_ranked_by_relatedness( query: str, df: pd.DataFrame, relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), top_n: int = 100, ) -\u0026gt; list[str]: \u0026#34;\u0026#34;\u0026#34;Returns a list of strings and relatednesses, sorted from most related to least.\u0026#34;\u0026#34;\u0026#34; query_embedding_response = embedding_request(query) query_embedding = query_embedding_response.data[0].embedding strings_and_relatednesses = [ (row[\u0026#34;filepath\u0026#34;], relatedness_fn(query_embedding, row[\u0026#34;embedding\u0026#34;])) for i, row in df.iterrows() ] strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True) strings, relatednesses = zip(*strings_and_relatednesses) return strings[:top_n] def read_pdf(filepath): \u0026#34;\u0026#34;\u0026#34;Takes a filepath to a PDF and returns a string of the PDF\u0026#39;s contents\u0026#34;\u0026#34;\u0026#34; # creating a pdf reader object reader = PdfReader(filepath) pdf_text = \u0026#34;\u0026#34; page_number = 0 for page in reader.pages: page_number += 1 pdf_text += page.extract_text() + f\u0026#34;\\nPage Number: {page_number}\u0026#34; return pdf_text # Split a text into smaller chunks of size n, preferably ending at the end of a sentence def create_chunks(text, n, tokenizer): \u0026#34;\u0026#34;\u0026#34;Returns successive n-sized chunks from provided text.\u0026#34;\u0026#34;\u0026#34; tokens = tokenizer.encode(text) i = 0 while i \u0026lt; len(tokens): # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens j = min(i + int(1.5 * n), len(tokens)) while j \u0026gt; i + int(0.5 * n): # Decode the tokens and check for full stop or newline chunk = tokenizer.decode(tokens[i:j]) if chunk.endswith(\u0026#34;.\u0026#34;) or chunk.endswith(\u0026#34;\\n\u0026#34;): break j -= 1 # If no end of sentence found, use n tokens as the chunk size if j == i + int(0.5 * n): j = min(i + n, len(tokens)) yield tokens[i:j] i = j def extract_chunk(content, template_prompt): \u0026#34;\u0026#34;\u0026#34;This function applies a prompt to some input content. In this case it returns a summarized chunk of text\u0026#34;\u0026#34;\u0026#34; prompt = template_prompt + content response = client.chat.completions.create( model=GPT_MODEL, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt}], temperature=0 ) return response.choices[0].message.content def summarize_text(query): \u0026#34;\u0026#34;\u0026#34;This function does the following: - Reads in the arxiv_library.csv file in including the embeddings - Finds the closest file to the user\u0026#39;s query - Scrapes the text out of the file and chunks it - Summarizes each chunk in parallel - Does one final summary and returns this to the user\u0026#34;\u0026#34;\u0026#34; # A prompt to dictate how the recursive summarizations should approach the input paper summary_prompt = \u0026#34;\u0026#34;\u0026#34;Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\u0026#34;\u0026#34;\u0026#34; # If the library is empty (no searches have been performed yet), we perform one and download the results library_df = pd.read_csv(paper_dir_filepath).reset_index() if len(library_df) == 0: print(\u0026#34;No papers searched yet, downloading first.\u0026#34;) get_articles(query) print(\u0026#34;Papers downloaded, continuing\u0026#34;) library_df = pd.read_csv(paper_dir_filepath).reset_index() library_df.columns = [\u0026#34;title\u0026#34;, \u0026#34;filepath\u0026#34;, \u0026#34;embedding\u0026#34;] library_df[\u0026#34;embedding\u0026#34;] = library_df[\u0026#34;embedding\u0026#34;].apply(ast.literal_eval) strings = strings_ranked_by_relatedness(query, library_df, top_n=1) print(\u0026#34;Chunking text from paper\u0026#34;) pdf_text = read_pdf(strings[0]) # Initialise tokenizer tokenizer = tiktoken.get_encoding(\u0026#34;cl100k_base\u0026#34;) results = \u0026#34;\u0026#34; # Chunk up the document into 1500 token chunks chunks = create_chunks(pdf_text, 1500, tokenizer) text_chunks = [tokenizer.decode(chunk) for chunk in chunks] print(\u0026#34;Summarizing each chunk of text\u0026#34;) # Parallel process the summaries with concurrent.futures.ThreadPoolExecutor( max_workers=len(text_chunks) ) as executor: futures = [ executor.submit(extract_chunk, chunk, summary_prompt) for chunk in text_chunks ] with tqdm(total=len(text_chunks)) as pbar: for _ in concurrent.futures.as_completed(futures): pbar.update(1) for future in futures: data = future.result() results += data # Final summary print(\u0026#34;Summarizing into overall summary\u0026#34;) response = client.chat.completions.create( model=GPT_MODEL, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;\u0026#34;\u0026#34;Write a summary collated from this collection of key points extracted from an academic paper. The summary should highlight the core argument, conclusions and evidence, and answer the user\u0026#39;s query. User query: {query} The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions. Key points:\\n{results}\\nSummary:\\n\u0026#34;\u0026#34;\u0026#34;, } ], temperature=0, ) return response 实现一个**Conversation**类，用于支持与API进行多轮对话\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def chat_completion_request(messages, functions=None, model=GPT_MODEL): try: response = client.chat.completions.create( model=model, messages=messages, functions=functions, ) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e class Conversation: def __init__(self): self.conversation_history = [] def add_message(self, role, content): message = {\u0026#34;role\u0026#34;: role, \u0026#34;content\u0026#34;: content} self.conversation_history.append(message) def display_conversation(self, detailed=False): role_to_color = { \u0026#34;system\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;function\u0026#34;: \u0026#34;magenta\u0026#34;, } for message in self.conversation_history: print( colored( f\u0026#34;{message[\u0026#39;role\u0026#39;]}: {message[\u0026#39;content\u0026#39;]}\\n\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]], ) ) 完成以上基本工作， 接下来是agent的核心内容\n定义工具列表 tools\narxiv_functions = [ { \u0026#34;name\u0026#34;: \u0026#34;get_articles\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Use this function to get academic papers from arXiv to answer user questions.\u0026#34;\u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; User query in JSON. Responses should be summarized and should include the article URL reference \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, }, { \u0026#34;name\u0026#34;: \u0026#34;read_article_and_summarize\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Use this function to read whole papers and provide a summary for users. You should NEVER call this function before get_articles has been called in the conversation.\u0026#34;\u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; Description of the article in plain text based on the user\u0026#39;s query \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, } ] 定义function用于工具调用\ndef chat_completion_with_function_execution(messages, functions=[None]): \u0026#34;\u0026#34;\u0026#34;This function makes a ChatCompletion API call with the option of adding functions\u0026#34;\u0026#34;\u0026#34; response = chat_completion_request(messages, functions) full_message = response.choices[0] if full_message.finish_reason == \u0026#34;function_call\u0026#34;: print(f\u0026#34;Function generation requested, calling function\u0026#34;) return call_arxiv_function(messages, full_message) else: print(f\u0026#34;Function not required, responding to user\u0026#34;) return response def call_arxiv_function(messages, full_message): \u0026#34;\u0026#34;\u0026#34;Function calling function which executes function calls when the model believes it is necessary. Currently extended by adding clauses to this if statement.\u0026#34;\u0026#34;\u0026#34; if full_message.message.function_call.name == \u0026#34;get_articles\u0026#34;: try: parsed_output = json.loads( full_message.message.function_call.arguments ) print(\u0026#34;Getting search results\u0026#34;) results = get_articles(parsed_output[\u0026#34;query\u0026#34;]) except Exception as e: print(parsed_output) print(f\u0026#34;Function execution failed\u0026#34;) print(f\u0026#34;Error message: {e}\u0026#34;) messages.append( { \u0026#34;role\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;name\u0026#34;: full_message.message.function_call.name, \u0026#34;content\u0026#34;: str(results), } ) try: print(\u0026#34;Got search results, summarizing content\u0026#34;) response = chat_completion_request(messages) return response except Exception as e: print(type(e)) raise Exception(\u0026#34;Function chat request failed\u0026#34;) elif ( full_message.message.function_call.name == \u0026#34;read_article_and_summarize\u0026#34; ): parsed_output = json.loads( full_message.message.function_call.arguments ) print(\u0026#34;Finding and reading paper\u0026#34;) summary = summarize_text(parsed_output[\u0026#34;query\u0026#34;]) return summary else: raise Exception(\u0026#34;Function does not exist and cannot be called\u0026#34;) arXiv conversation, Start with a system message\npaper_system_message = \u0026#34;\u0026#34;\u0026#34;You are arXivGPT, a helpful assistant pulls academic papers to answer user questions. You summarize the papers clearly so the customer can decide which to read to answer their question. You always provide the article_url and title so the user can understand the name of the paper and click through to access it. Begin!\u0026#34;\u0026#34;\u0026#34; paper_conversation = Conversation() paper_conversation.add_message(\u0026#34;system\u0026#34;, paper_system_message) # Add a user message paper_conversation.add_message(\u0026#34;user\u0026#34;, \u0026#34;Hi, how does PPO reinforcement learning work?\u0026#34;) chat_response = chat_completion_with_function_execution( paper_conversation.conversation_history, functions=arxiv_functions ) assistant_message = chat_response.choices[0].message.content paper_conversation.add_message(\u0026#34;assistant\u0026#34;, assistant_message) display(Markdown(assistant_message)) output:\nFunction generation requested, calling function Getting search results Got search results, summarizing content I found several papers related to PPO reinforcement learning. Here are a few summaries: Title: \u0026#34;Bandit Profit-maximization for Targeted Marketing\u0026#34; Summary: This paper presents near-optimal algorithms for optimizing profit over multiple demand curves, which are dependent on different ancillary variables while maintaining the same price. It is relevant to PPO reinforcement learning as it tackles a sequential profit-maximization problem. Article URL: Link Title: \u0026#34;Inferring potential landscapes: A Schrödinger bridge approach to Maximum Caliber\u0026#34; Summary: This work extends Schrödinger bridges to account for integral constraints along paths, specifically in the context of Maximum Caliber, a Maximum Entropy principle applied in a dynamic context. While not directly related to PPO reinforcement learning, it can provide insights into stochastic dynamics and inference of time-varying potential landscapes. Article URL: Link Title: \u0026#34;a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification\u0026#34; Summary: This paper proposes an architecture-agnostic detection cost function (a-DCF) for evaluating spoofing-robust automatic speaker verification (ASV) systems. Although it does not focus on PPO reinforcement learning, it provides a metric for evaluating ASV systems in the presence of spoofing attacks. Article URL: Link These papers should provide insights into different aspects of reinforcement learning and related topics. Add another user message to induce our system to use the second tool\npaper_conversation.add_message( \u0026#34;user\u0026#34;, \u0026#34;Can you read the PPO sequence generation paper for me and give me a summary\u0026#34;, ) updated_response = chat_completion_with_function_execution( paper_conversation.conversation_history, functions=arxiv_functions ) display(Markdown(updated_response.choices[0].message.content)) output:\nFunction generation requested, calling function Finding and reading paper Chunking text from paper Summarizing each chunk of text 100%|██████████| 4/4 [00:04\u0026lt;00:00, 1.11s/it] Summarizing into overall summary Core Argument: The paper discusses the potential of using a general-purpose large language model (LLM) to learn the structural biophysics of DNA. The authors show that fine-tuning a LLM, specifically chatGPT 3.5-turbo, can enhance its ability to analyze and design DNA sequences and their structures. The study focuses on the formation of secondary structures in DNA, which are governed by base pairing and stacking bonds. The authors propose a method that involves chaining together models fine-tuned for subtasks and using a chain-of-thought approach to improve the model\u0026#39;s performance. Evidence: The authors use the NUPACK software suite to provide data for training and validation. The expert pipeline approach involves using models that have been fine-tuned for subtasks and feeding their outputs into each other. The models perform better when they explicitly consider the nearest neighbor window and the reverse complement of the sequences. The pipeline approach, where a separate model determines the reverse complement and feeds it to another model for secondary structure prediction, enhances the accuracy of the predictions. The performance of the models improves with larger training sets. Conclusions: The study demonstrates the potential of using LLMs to learn DNA structural biophysics. Integrating experimental data and machine learning is important in scientific research. The expert pipeline approach and breaking down the problem into smaller subtasks improve the performance of the models in DNA sequence analysis. The combination of chain-of-thought and model pipeline provides the best results in analysis tasks. The CoT approach, combined with the reverse complement transformation, yields the highest accuracy in design tasks. The addition of an error checking layer further improves accuracy in design tasks. Sequence design is more challenging than analysis, but error correction can compensate for the increased difficulty. Larger training sets benefit design tasks more. Future research directions include exploring chaining smaller models for performance improvement and using an LLM architecture involving both an encoder and decoder for direct sequence comparison. Function calling via open source LLMs 在考虑成本和隐私性的背景下，我们可能会倾向于在开源的大型语言模型（LLM）上实现函数调用功能。目前，有几个框架支持以类似OpenAI API的形式调用工具（tools call）：\nXinference\nText Generation Inference (TGI)\n而在开源大型语言模型（LLM）方面，支持工具调用的主要有：\nLlama-3\nMixtral-8x7B-Instruct-v0.1\nqwent\nchatGLM-6B\nNexusRaven-13B\ngorilla-openfunctions-v1\n等等\n以下以Xinference和chatGLM-6B为例，探索如何通过OpenAI API的形式调用开源模型的函数调用功能。\n环境\n%pip install -U -q xinference[transformers] openai langchain !pip install typing-extensions --upgrade ## Start Local Server !nohup xinference-local \u0026gt; xinference.log 2\u0026gt;\u0026amp;1 \u0026amp; 模型加载\n!xinference launch -u my-llm --model-name chatglm3 --size-in-billions 6 --model-format pytorch ## Interact with the running model import openai messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Who are you?\u0026#34; } ] client = openai.Client(api_key=\u0026#34;empty\u0026#34;, base_url=f\u0026#34;http://0.0.0.0:9997/v1\u0026#34;) client.chat.completions.create( model=\u0026#34;my-llm\u0026#34;, messages=messages, ) # ChatCompletion(id=\u0026#39;chatda6056ac-da01-11ee-b92e-0242ac1c000c\u0026#39;, choices=[Choice(finish_reason=\u0026#39;stop\u0026#39;, index=0, logprobs=None, message=ChatCompletionMessage(content=\u0026#34;I am an AI assistant named ChatGLM3-6B, which is developed based on the language model jointly trained by Tsinghua University KEG Lab and Zhipu AI Company in 2023. My job is to provide appropriate answers and support to users\u0026#39; questions and requests.\u0026#34;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None))], created=1709541198, model=\u0026#39;my-llm\u0026#39;, object=\u0026#39;chat.completion\u0026#39;, system_fingerprint=None, usage=CompletionUsage(completion_tokens=-1, prompt_tokens=-1, total_tokens=-1)) without tool using\ncompletion = client.chat.completions.create( model=\u0026#34;my-llm\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the weather like in London?\u0026#34;}] ) # print(handle_response(completion)) print(completion.choices[0].message.content) \u0026#34;\u0026#34;\u0026#34; London has a temperate climate with warm summers and cool winters. The average temperature during the summer months (June to August) is around 18°C, while the winter months (December to February) are around 6°C. The city experiences heavy rainfall throughout the year, with an annual precipitation of around 350 mm. The average precipitation on the weekends is around 40 mm. London\u0026#39;s cloudy skies are common throughout the year, but they are especially prevalent in December and January. \u0026#34;\u0026#34;\u0026#34; tool using\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;]}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, }, } ] def get_completion(messages, model=\u0026#34;my-llm\u0026#34;, temperature=0, max_tokens=500, tools=None, tool_choice=None): response = client.chat.completions.create( model=model, messages=messages, temperature=temperature, max_tokens=max_tokens, tools=tools, tool_choice=tool_choice ) return response.choices[0].message # Defines a dummy function to get the current weather def get_current_weather(location, unit=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Get the current weather in a given location\u0026#34;\u0026#34;\u0026#34; weather = { \u0026#34;location\u0026#34;: location, \u0026#34;temperature\u0026#34;: \u0026#34;50\u0026#34;, \u0026#34;unit\u0026#34;: unit, } return json.dumps(weather) messages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like in Boston!\u0026#34;}) assistant_message = get_completion(messages, tools=tools, tool_choice=\u0026#34;auto\u0026#34;) assistant_message = json.loads(assistant_message.model_dump_json()) assistant_message[\u0026#34;content\u0026#34;] = str(assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;]) #a temporary patch but this should be handled differently # remove \u0026#34;function_call\u0026#34; from assistant message del assistant_message[\u0026#34;function_call\u0026#34;] messages.append(assistant_message) # get the weather information to pass back to the model weather = get_current_weather(messages[1][\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;arguments\u0026#34;]) messages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;], \u0026#34;content\u0026#34;: weather}) final_response = get_completion(messages, tools=tools) final_response \u0026#34;\u0026#34;\u0026#34; ChatCompletionMessage(content=\u0026#39;The current weather in Boston is 50 degrees Fahrenheit.\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[]) \u0026#34;\u0026#34;\u0026#34; LLM 在执行function calling时经历了什么 可惜我们并不能看到openAI的模型在服务器端发生了什么，但是根据开源的模型和推理框架，我们某种程度上，也能对LLM在执行function calling的背后逻辑一探究竟。\n这部分内容可以从推理框架和开源模型的源码中找到答案。\n根据xinference 的源码：https://github.com/xorbitsai/inference/blob/main/xinference/model/llm/utils.py#L42\n我们主要关注ChatGLM3 和Qwen\n当我们使用以下假设对话时：\nmessages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;今天北京的天气怎么样？\u0026#34; } ] tools = [ { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, } ] 根据GLM的官方文档，则最终给到ChatGLM3模型的prompt应该长这样：\n\u0026lt;|system|\u0026gt; Answer the following questions as best as you can. You have access to the following tools: [ { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, } ] \u0026lt;|user|\u0026gt; 今天北京的天气怎么样？ \u0026lt;|assistant|\u0026gt; 好的，让我们来查看今天的天气 \u0026lt;|assistant|\u0026gt;get_current_weather ```python tool_call(location=\u0026#34;beijing\u0026#34;, unit=\u0026#34;celsius\u0026#34;) ``` \u0026lt;|observation|\u0026gt; {\u0026#34;temperature\u0026#34;: 22} \u0026lt;|assistant|\u0026gt; 根据查询结果，今天北京的气温为 22 摄氏度。 根据xinference 中有关qwen的代码\nelif prompt_style.style_name == \u0026#34;QWEN\u0026#34;: if tools: tool_desc = \u0026#34;\u0026#34;\u0026#34;{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters} Format the arguments as a JSON object.\u0026#34;\u0026#34;\u0026#34; react_instruction = \u0026#34;\u0026#34;\u0026#34;Answer the following questions as best you can. You have access to the following APIs: {tools_text} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tools_name_text}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated zero or more times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin!\u0026#34;\u0026#34;\u0026#34; tools_text = [] tools_name_text = [] for func_info in tools: parameters = [] required_parameters = func_info[\u0026#34;function\u0026#34;][\u0026#34;parameters\u0026#34;].get( \u0026#34;required\u0026#34;, [] ) for name, p in func_info[\u0026#34;function\u0026#34;][\u0026#34;parameters\u0026#34;][ \u0026#34;properties\u0026#34; ].items(): param = dict({\u0026#34;name\u0026#34;: name}, **p) if name in required_parameters: param[\u0026#34;required\u0026#34;] = True parameters.append(param) name = func_info[\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;] desc = func_info[\u0026#34;function\u0026#34;][\u0026#34;description\u0026#34;] tool_string = tool_desc.format( name_for_model=name, name_for_human=name, # Hint: You can add the following format requirements in description: # \u0026#34;Format the arguments as a JSON object.\u0026#34; # \u0026#34;Enclose the code within triple backticks (`) at the beginning and end of the code.\u0026#34; description_for_model=desc, parameters=json.dumps(parameters, ensure_ascii=False), ) tools_text.append(tool_string) tools_name_text.append(name) tools_text_string = \u0026#34;\\n\\n\u0026#34;.join(tools_text) tools_name_text_string = \u0026#34;, \u0026#34;.join(tools_name_text) tool_system = react_instruction.format( tools_text=tools_text_string, tools_name_text=tools_name_text_string, ) else: tool_system = \u0026#34;\u0026#34; ret = f\u0026#34;\u0026lt;|im_start|\u0026gt;system\\n{prompt_style.system_prompt}\u0026lt;|im_end|\u0026gt;\u0026#34; for message in chat_history: role = get_role(message[\u0026#34;role\u0026#34;]) content = message[\u0026#34;content\u0026#34;] ret += prompt_style.intra_message_sep if tools: if role == \u0026#34;user\u0026#34;: if tool_system: content = tool_system + f\u0026#34;\\n\\nQuestion: {content}\u0026#34; tool_system = \u0026#34;\u0026#34; else: content = f\u0026#34;Question: {content}\u0026#34; elif role == \u0026#34;assistant\u0026#34;: tool_calls = message.get(\u0026#34;tool_calls\u0026#34;) if tool_calls: func_call = tool_calls[0][\u0026#34;function\u0026#34;] f_name, f_args = ( func_call[\u0026#34;name\u0026#34;], func_call[\u0026#34;arguments\u0026#34;], ) content = f\u0026#34;Thought: I can use {f_name}.\\nAction: {f_name}\\nAction Input: {f_args}\u0026#34; elif content: content = f\u0026#34;Thought: I now know the final answer.\\nFinal answer: {content}\u0026#34; elif role == \u0026#34;tool\u0026#34;: role = \u0026#34;function\u0026#34; content = f\u0026#34;Observation: {content}\u0026#34; else: raise Exception(f\u0026#34;Unsupported message role: {role}\u0026#34;) if content: content = content.lstrip(\u0026#34;\\n\u0026#34;).rstrip() ret += f\u0026#34;\u0026lt;|im_start|\u0026gt;{role}\\n{content}\u0026lt;|im_end|\u0026gt;\u0026#34; else: ret += f\u0026#34;\u0026lt;|im_start|\u0026gt;{role}\\n\u0026#34; return ret 会稍微复杂一些， 利用了react的COT方式（代码中的react_instruction），要求模型以一系列的**Thought（思考）、Action（行动）、Action Input（行动输入）和Observation**（观察结果）步骤，最终给出问题的答案，以增加正确性。\n假设使用以下工具列表和对话历史：\n# 工具列表 tools = [ { \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;geo_lookup\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Retrieves geographical information.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The query to lookup.\u0026#34; } } } } } ] chat_history = [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the population of Tokyo?\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;tool_calls\u0026#34;: [ { \u0026#34;function\u0026#34;: \u0026#34;geo_lookup\u0026#34;, \u0026#34;arguments\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;Tokyo\u0026#34; } } ], \u0026#34;content\u0026#34;: \u0026#34;The population of Tokyo is about 14 million.\u0026#34; } ] 可以推断出，最终输入Qwen的prompt应该长这样：\nsystem Answer the following questions as best you can. You have access to the following APIs: geo_lookup: Call this tool to interact with the geo_lookup API. What is the geo_lookup API useful for? Retrieves geographical information. Parameters: [{\u0026#34;name\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The query to lookup.\u0026#34;, \u0026#34;required\u0026#34;: true}] Format the arguments as a JSON object. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [geo_lookup] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated zero or more times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! user Question: What is the population of Tokyo? assistant Thought: I can use geo_lookup to find the information. Action: geo_lookup Action Input: {\u0026#34;query\u0026#34;: \u0026#34;Tokyo\u0026#34;} Observation: The population of Tokyo is about 14 million. Thought: I now know the final answer. Final answer: The population of Tokyo is about 14 million. 在这个例子中，用户询问东京的人口数量。助手利用**geo_lookup**工具进行查询，具体的行动步骤包括：\n思考：助手决定可以使用**geo_lookup**工具来查找信息。\n行动：实际调用**geo_lookup**工具。\n行动输入：向工具传递的参数，即查询**\u0026quot;Tokyo\u0026quot;**。\n观察：观察到的结果，这里是东京的人口大约为1400万。\n最终思考：基于观察结果，助手得出了最终答案。\n最终答案：向用户提供的答案，即东京的人口数量。\n更详细的内容，建议看这篇知乎文章：Qwen Function Calling 的对话模板及训练方法总结; 以及qwen的官方文档：ReAct Prompting 示例\n参考 openAI function calling\nHow to call functions with chat models\nHow_to_call_functions_for_knowledge_retrieval\nQwen Function Calling 的对话模板及训练方法总结\nqwen的官方文档\nGLM的官方文档\ntool-using via Groq API\nJson mode in Groq\nOpenAI JSON Mode \u0026amp; Seeding\nOpenAI API Guide: Using JSON Mode\n","permalink":"https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0openai-function-calling%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/","summary":"\u003ch2 id=\"写在最开始\"\u003e写在最开始\u003c/h2\u003e\n\u003cp\u003e当我们在讨论基于大型语言模型（LLM-based）的智能代理（agent）时，我们究竟在谈论什么？根据Lilian W在其文章《LLM Powered Autonomous Agents》中的讨论，一个智能代理需要具备几个核心能力：规划（Planning）、记忆（Memory）、以及工具使用（Tool use）。特别地，工具使用方面的进展，得益于OpenAI在API中提供的function calling功能，为我们开启了新的可能性。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png\" alt=\"AGI\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eOpenAI function calling，作为智能代理与外部工具交互的基本方式，对于每位从业者来说都是必备技能。随着技术的发展，我们期望的不只是能与我们对话的LLM，而是能够辅助我们使用各种工具、做出决策的智能伙伴。\u003c/p\u003e\n\u003cp\u003e不过需要特别指出的是，最近OpenAI在Chat Completions API中已经废弃了“函数（function）”的使用，转而采用“工具（tool）”。这一变更旨在拓宽LLM集成的功能范围，为更复杂的交互模式铺平道路，如构建能够相互作用的多代理系统。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/Agent%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/tool-using/%e6%88%aa%e5%b1%8f2024-03-28%2015.16.54.png\" alt=\"截屏2024-03-28 15.16.54.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/Agent%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/tool-using/%e6%88%aa%e5%b1%8f2024-03-28%2015.17.38.png\" alt=\"截屏2024-03-28 15.17.38.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e尽管如此，由于语言习惯的原因，本文中仍然会使用function calling的术语来描述OpenAI的tool using功能，因为“function calling”的说法已经深入人心了。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e核心内容概览\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFunction Calling的定义\u003c/strong\u003e：解释什么是function calling，以及它在智能代理工作中的作用。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAI Cookbook示例\u003c/strong\u003e：提供实际的function calling示例，帮助读者理解其在实际应用中的用途。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e开源LLM的Tool Using\u003c/strong\u003e：探索如何在开源大型语言模型中实现工具使用，以及LLM在tool using的时候经历了什么。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cdel\u003e评价与训练\u003c/del\u003e\u003c/strong\u003e\u003cdel\u003e：讨论如何评价开源模型的工具使用能力，以及如何训练LLM进行有效的工具使用。\u003c/del\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e鉴于整理笔记的速度远赶不上更新的速度， 会将第四部份作为单独的部分整理。\u003c/p\u003e\n\u003ch2 id=\"何为function-calling\"\u003e何为function calling\u003c/h2\u003e\n\u003cp\u003e一句话解释：\u003cstrong\u003efunction calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。\u003cstrong\u003e这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003efunction calling的应用范围广泛，如\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e创建智能助手：通过调用外部API回答问题。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e转换指令：将自然语言指令转换成API调用指令。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e数据提取：从文本中提取结构化数据。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efunction calling的过程涵盖了从定义函数集、通过模型生成遵循自定义模式的JSON对象字符串，到在代码中解析这个字符串并调用相应函数的全过程。这一连串操作不仅自动化了交互过程，还确保了执行操作的安全性和准确性。\u003c/p\u003e\n\u003ch2 id=\"一些常见的问题\"\u003e一些常见的问题\u003c/h2\u003e\n\u003ch3 id=\"json-mode\"\u003eJSON mode\u003c/h3\u003e\n\u003cp\u003ejson mode 和tool-using 有什么关系？有了json mode 还需要用到tool-using吗？\u003c/p\u003e\n\u003cp\u003e从json mode 的本质， 更多的是在system prompt 增加一句类似“请以json格式输出”之类的话，然后在LLM输出时增加json结果检查和格式转换。在使用时只需要在\u003ccode\u003eclient.chat.completions.create \u003c/code\u003e中 增加\u003ccode\u003eresponse_format={ \u0026quot;type\u0026quot;: \u0026quot;json_object\u0026quot; }\u003c/code\u003e 即可。\u003c/p\u003e","title":"Agent学习笔记：OpenAI Function Calling完全指南"},{"content":"个人介绍 后现代人类 AI从业者与终身学习者 工作狂 世界探索者 INFJ 阅读 听歌 创作 欢迎来到我的个人博客！\n","permalink":"https://niraya666.github.io/about/","summary":"\u003ch3 id=\"个人介绍\"\u003e个人介绍\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e后现代人类\u003c/li\u003e\n\u003cli\u003eAI从业者与终身学习者\u003c/li\u003e\n\u003cli\u003e工作狂\u003c/li\u003e\n\u003cli\u003e世界探索者\u003c/li\u003e\n\u003cli\u003eINFJ\u003c/li\u003e\n\u003cli\u003e阅读 听歌 创作\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e欢迎来到我的个人博客！\u003c/p\u003e","title":"About"},{"content":" 引言 在信息化时代，数据和知识的快速提取变得尤为重要。特别是对于学术和技术文档，如何从格式丰富且结构复杂的PDF文件中准确地提取信息，是提高研究效率的关键。本文将探讨几种先进的技术方法，用于优化学术论文的PDF解析及其表格内容的处理。\n对于文档解析，langchain 和 llama_index 提供的 document loader 能够支持多种文件类型，覆盖绝大多数文档格式的需求。但对于pdf解析而言还存在着一些挑战。\n人类与机器的阅读差异 尽管PDF格式适合人类阅读，提供了优秀的视觉体验和格式保真，但它并不适合机器阅读。PDF文件通常包含复杂的布局和多样的内容元素，如多列布局、公式和表格，这些都给自动解析带来了挑战。\n常见的PDF解析问题 使用传统的PDF解析库可能遇到多种问题：\n多列布局导致的文本流读取错误。\n公式和表格的解析效果差，难以正确提取信息。\n解析过程中结构化信息（如标题和列表）的丢失。\n影印版PDF的文本无法被标准OCR工具识别。\n高级解析技术 根据unstractued提供的方案，文档解析可以大致分为两种方法：结构化解析和基于图像的解析。结构化解析侧重于从文档中提取文字和其它字符数据。而图像基解析则包括以下两种技术：\nOCR技术：首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字。\n基于Transformer的端到端解析：使用深度学习模型直接将图像转换为相应的文字内容。在这方面，Dount 和 Nougat 模型表现出色，尤其是 Nougat 能够将图像中的文章整理成Markdown或Latex格式，非常适合需要保留结构信息（如标题层级、公式和表格）的场景。\n只可惜Nougat只针对英文的学术论文做了训练，但遇到其他语言如中文论文的效果就不是那么的好了。\n必须强调一点的是，Markdown格式因其简洁性和易于解析的特点，被广泛认为是LLM（大型语言模型）友好的文档格式。Markdown通过明确的标记语法，帮助模型更好地理解文档结构和内容，从而提高信息提取的准确性和效率。特别在存在大量公式，表格的学术论文场景下，Markdown可能是更合适的格式选择。\n快速上手：使用Nougat将pdf解析成适合LLM读取的markdown 依赖按照\n!pip install -q pymupdf python-Levenshtein nltk !pip install -q git+https://github.com/huggingface/transformers.git Load model and processor\nfrom transformers import AutoProcessor, VisionEncoderDecoderModel import torch processor = AutoProcessor.from_pretrained(\u0026#34;facebook/nougat-base\u0026#34;) model = VisionEncoderDecoderModel.from_pretrained(\u0026#34;facebook/nougat-base\u0026#34;) device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model.to(device) 将pdf转成图像\nfrom typing import Optional, List import io import fitz from pathlib import Path def rasterize_paper( pdf: Path, outpath: Optional[Path] = None, dpi: int = 96, return_pil=False, pages=None, ) -\u0026gt; Optional[List[io.BytesIO]]: \u0026#34;\u0026#34;\u0026#34; Rasterize a PDF file to PNG images. Args: pdf (Path): The path to the PDF file. outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None. dpi (int, optional): The output DPI. Defaults to 96. return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False. pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None. Returns: Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None. \u0026#34;\u0026#34;\u0026#34; pillow_images = [] if outpath is None: return_pil = True try: if isinstance(pdf, (str, Path)): pdf = fitz.open(pdf) if pages is None: pages = range(len(pdf)) for i in pages: page_bytes: bytes = pdf[i].get_pixmap(dpi=dpi).pil_tobytes(format=\u0026#34;PNG\u0026#34;) if return_pil: pillow_images.append(io.BytesIO(page_bytes)) else: with (outpath / (\u0026#34;%02d.png\u0026#34; % (i + 1))).open(\u0026#34;wb\u0026#34;) as f: f.write(page_bytes) except Exception: pass if return_pil: return pillow_images from transformers import StoppingCriteria, StoppingCriteriaList from collections import defaultdict class RunningVarTorch: def __init__(self, L=15, norm=False): self.values = None self.L = L self.norm = norm def push(self, x: torch.Tensor): assert x.dim() == 1 if self.values is None: self.values = x[:, None] elif self.values.shape[1] \u0026lt; self.L: self.values = torch.cat((self.values, x[:, None]), 1) else: self.values = torch.cat((self.values[:, 1:], x[:, None]), 1) def variance(self): if self.values is None: return if self.norm: return torch.var(self.values, 1) / self.values.shape[1] else: return torch.var(self.values, 1) class StoppingCriteriaScores(StoppingCriteria): def __init__(self, threshold: float = 0.015, window_size: int = 200): super().__init__() self.threshold = threshold self.vars = RunningVarTorch(norm=True) self.varvars = RunningVarTorch(L=window_size) self.stop_inds = defaultdict(int) self.stopped = defaultdict(bool) self.size = 0 self.window_size = window_size @torch.no_grad() def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor): last_scores = scores[-1] self.vars.push(last_scores.max(1)[0].float().cpu()) self.varvars.push(self.vars.variance()) self.size += 1 if self.size \u0026lt; self.window_size: return False varvar = self.varvars.variance() for b in range(len(last_scores)): if varvar[b] \u0026lt; self.threshold: if self.stop_inds[b] \u0026gt; 0 and not self.stopped[b]: self.stopped[b] = self.stop_inds[b] \u0026gt;= self.size else: self.stop_inds[b] = int( min(max(self.size, 1) * 1.15 + 150 + self.window_size, 4095) ) else: self.stop_inds[b] = 0 self.stopped[b] = False return all(self.stopped.values()) and len(self.stopped) \u0026gt; 0 将pdf转成markdown\nfrom tqdm import tqdm def trans_pdf_to_markdown(filepath): output = \u0026#34;\u0026#34; images = rasterize_paper(pdf=filepath, return_pil=True) for i in tqdm(range(len(images))): image = Image.open(images[i]) pixel_values = processor(images=image, return_tensors=\u0026#34;pt\u0026#34;).pixel_values # autoregressively generate tokens, with custom stopping criteria (as defined by the Nougat authors) outputs = model.generate(pixel_values.to(device), min_length=1, max_length=3584, bad_words_ids=[[processor.tokenizer.unk_token_id]], return_dict_in_generate=True, output_scores=True, stopping_criteria=StoppingCriteriaList([StoppingCriteriaScores()]), ) generated = processor.batch_decode(outputs[0], skip_special_tokens=True)[0] generated = processor.post_process_generation(generated, fix_markdown=True) output+=generated output+=\u0026#34;\\n\\n\u0026#34; return output filepath = \u0026#39;xxx.pdf\u0026#39; makrdown_ = trans_pdf_to_markdown(filepath) 具体参考Nougat的官方示例。\n表格类数据的RAG问题 表格类目前的解决方案主要有两种：\n对于内容较小的表格，一般采用LLM对表格进行summary， 以table summary构建查询的index，在召回后， 进行表格内容的复原或者根据问题后处理提供至LLM进行生成。\n对于内容较多的表格， 一般采取结构化处理， 也就是存储至数据库， 在查询阶段根据问题匹配对应的表schema， 有模型执行text-to-sql任务，生成sql并执行获得结果，最终根据结果回答内容。\n对于第一种方案，在Langchain 的cookbook中提到了Semi-structured RAG的方法值得借鉴；\n此外在HRoT这篇工作中， 也提到了类似的方法；\n在HRoT这篇工作中， 增加了将table基于问题进行重构的算法，\nType-Aware Table Reconstruction algorithm\n该算法的实现步骤如下：\n问题分类：首先，算法对问题进行分类，判断问题是算术问题还是跨列选择问题。这有助于确定需要从表格中检索的信息类型。\n获取表格和证据：对于算术问题，算法获取与问题相关的表格集合。然后，从这些表格中提取出作为证据的文本和表格片段。\n表格分区：对于每个表格，算法将其分区为多个子表格。这是通过获取表格的跨度列表（Lt）来完成的，该列表包含了表格中每个头部（行和列）的起始和结束位置。\n确定保留的行和列：算法根据问题类型和证据来确定哪些行和列是回答问题所必需的。这是通过分析每个证据的子标题（即表格中的行和列标题）来实现的。\n重建表格：在确定了需要保留的行和列之后，算法根据这些信息重建表格。重建的表格只包含对问题回答有帮助的信息，从而减少了无关信息的干扰。\n在TableQuery 这篇工作中， 利用一系列模块组件，实现了基于LLM对数据库的高效查询。\nTableQuery的架构设计包括以下几个主要模块：\nDatastore：Datastore是一个表格集合，可以通过自然语言查询。这些表格可以是数据库或电子表格目录。除了表格数据，Datastore还包含模式文件，这些文件包含每个表的元数据，如表关键词、列名、列类型、列关键词（用户为列名提供的关键词）等。模式文件可以手动为这些表创建，也可以通过应用各种启发式方法自动生成。\nTable Selector：表选择器根据输入查询从Datastore中选择合适的表。这是通过从输入查询中提取关键词，并找到与问题关键词（以及模式中的表关键词、列名、列关键词等）重叠系数最大的表来完成的。\nKnown Fields Extractor：已知字段提取器提取查询中已经给出值的列。对于这些列中的每一个，也提取了相应的值。这是通过一个预训练的深度学习模型来完成的，该模型用于在自由文本上执行问答。\nUnknown Fields Extractor：未知字段提取器提取需要从选定表中检索值的列。这是通过排除已知字段提取器已经从查询中提取的列，并找到其列关键词与问题关键词重叠系数最大的列来完成的。\nAggregate Function Classifier：一些查询可能需要对选定表中的未知字段的值执行进一步操作。这是通过在生成的SQL查询中包含SQL聚合函数（如COUNT、SUM、MIN、MAX、AVG等）来实现的。聚合函数分类器决定给定输入查询要使用的聚合函数。为了执行这项任务，作者训练了一个两层的神经网络，该网络以使用通用句子编码器编码的查询作为输入，并输出要使用的适当聚合函数（如果有的话）。\nSQL Generator：结合已知字段和值、未知字段和聚合函数来构建SQL查询，当该查询在选定的表上运行时，返回所需的结果。\n关于text-to-sql的更详尽内容， 可以参考这篇综述文章：Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey。 限于篇幅就不展开了。\n参考文献 langchain: Document loaders\nNougat: Neural Optical Understanding for Academic Documents\nlangchain: Semi-structured RAG\nHRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering\nTableQuery： Querying tabular data with natural language\nNatural Language Interfaces for Tabular Data Querying and Visualization: A Survey\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/post-cover/rag_toolkits_2.JPG\" alt=\"image\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"引言\"\u003e\u003cstrong\u003e引言\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e在信息化时代，数据和知识的快速提取变得尤为重要。特别是对于学术和技术文档，如何从格式丰富且结构复杂的PDF文件中准确地提取信息，是提高研究效率的关键。本文将探讨几种先进的技术方法，用于优化学术论文的PDF解析及其表格内容的处理。\u003c/p\u003e\n\u003cp\u003e对于文档解析，\u003cstrong\u003e\u003ccode\u003elangchain\u003c/code\u003e\u003c/strong\u003e 和 \u003cstrong\u003e\u003ccode\u003ellama_index\u003c/code\u003e\u003c/strong\u003e 提供的 \u003cstrong\u003e\u003ccode\u003edocument loader\u003c/code\u003e\u003c/strong\u003e 能够支持多种文件类型，覆盖绝大多数文档格式的需求。但对于pdf解析而言还存在着一些挑战。\u003c/p\u003e\n\u003ch3 id=\"人类与机器的阅读差异\"\u003e\u003cstrong\u003e人类与机器的阅读差异\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e尽管PDF格式适合人类阅读，提供了优秀的视觉体验和格式保真，但它并不适合机器阅读。PDF文件通常包含复杂的布局和多样的内容元素，如多列布局、公式和表格，这些都给自动解析带来了挑战。\u003c/p\u003e\n\u003ch3 id=\"常见的pdf解析问题\"\u003e\u003cstrong\u003e常见的PDF解析问题\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e使用传统的PDF解析库可能遇到多种问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e多列布局导致的文本流读取错误。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e公式和表格的解析效果差，难以正确提取信息。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e解析过程中结构化信息（如标题和列表）的丢失。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e影印版PDF的文本无法被标准OCR工具识别。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"高级解析技术\"\u003e\u003cstrong\u003e高级解析技术\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e根据\u003ca href=\"https://unstructured.io/\"\u003eunstractued\u003c/a\u003e提供的方案，文档解析可以大致分为两种方法：结构化解析和基于图像的解析。结构化解析侧重于从文档中提取文字和其它字符数据。而图像基解析则包括以下两种技术：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOCR技术\u003c/strong\u003e：首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e基于Transformer的端到端解析\u003c/strong\u003e：使用深度学习模型直接将图像转换为相应的文字内容。在这方面，\u003cstrong\u003e\u003ccode\u003eDount\u003c/code\u003e\u003c/strong\u003e 和 \u003cstrong\u003e\u003ccode\u003eNougat\u003c/code\u003e\u003c/strong\u003e 模型表现出色，尤其是 \u003cstrong\u003e\u003ccode\u003eNougat\u003c/code\u003e\u003c/strong\u003e 能够将图像中的文章整理成Markdown或Latex格式，非常适合需要保留结构信息（如标题层级、公式和表格）的场景。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e只可惜Nougat只针对英文的学术论文做了训练，但遇到其他语言如中文论文的效果就不是那么的好了。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://unstructured-io.github.io/unstructured/_images/strategy.png\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e必须强调一点的是，Markdown格式因其简洁性和易于解析的特点，被广泛认为是LLM（大型语言模型）友好的文档格式。Markdown通过明确的标记语法，帮助模型更好地理解文档结构和内容，从而提高信息提取的准确性和效率。特别在存在大量公式，表格的学术论文场景下，Markdown可能是更合适的格式选择。\u003c/p\u003e\n\u003ch2 id=\"快速上手使用nougat将pdf解析成适合llm读取的markdown\"\u003e快速上手：使用Nougat将pdf解析成适合LLM读取的markdown\u003c/h2\u003e\n\u003cp\u003e依赖按照\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eq\u003c/span\u003e \u003cspan class=\"n\"\u003epymupdf\u003c/span\u003e \u003cspan class=\"n\"\u003epython\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eLevenshtein\u003c/span\u003e \u003cspan class=\"n\"\u003enltk\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eq\u003c/span\u003e \u003cspan class=\"n\"\u003egit\u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"n\"\u003ehttps\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"o\"\u003e//\u003c/span\u003e\u003cspan class=\"n\"\u003egithub\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecom\u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"n\"\u003ehuggingface\u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"n\"\u003etransformers\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egit\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eLoad model and processor\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etransformers\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoProcessor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eVisionEncoderDecoderModel\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eprocessor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoProcessor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;facebook/nougat-base\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eVisionEncoderDecoderModel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;facebook/nougat-base\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edevice\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecuda\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eis_available\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;cpu\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edevice\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e将pdf转成图像\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eOptional\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eList\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eio\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003efitz\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003epathlib\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003ePath\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003erasterize_paper\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ePath\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eoutpath\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eOptional\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ePath\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edpi\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e96\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ereturn_pil\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eOptional\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eList\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eio\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eBytesIO\u003c/span\u003e\u003cspan class=\"p\"\u003e]]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Rasterize a PDF file to PNG images.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Args:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        pdf (Path): The path to the PDF file.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        dpi (int, optional): The output DPI. Defaults to 96.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Returns:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epillow_images\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eoutpath\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ereturn_pil\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"nb\"\u003eisinstance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ePath\u003c/span\u003e\u003cspan class=\"p\"\u003e)):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003epdf\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003efitz\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003epages\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003epages\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003epages\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003epage_bytes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003ebytes\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_pixmap\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edpi\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edpi\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epil_tobytes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eformat\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;PNG\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ereturn_pil\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"n\"\u003epillow_images\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eio\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eBytesIO\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epage_bytes\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eoutpath\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"si\"\u003e%02d\u003c/span\u003e\u003cspan class=\"s2\"\u003e.png\u0026#34;\u003c/span\u003e \u003cspan class=\"o\"\u003e%\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;wb\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epage_bytes\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ereturn_pil\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003epillow_images\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etransformers\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eStoppingCriteria\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eStoppingCriteriaList\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ecollections\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultdict\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eRunningVarTorch\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e15\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enorm\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eL\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enorm\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecat\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e]),\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecat\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e:],\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e]),\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003evariance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eStoppingCriteriaScores\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eStoppingCriteria\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.015\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e200\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003esuper\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evars\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eRunningVarTorch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enorm\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evarvars\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eRunningVarTorch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultdict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultdict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003ebool\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nd\"\u003e@torch.no_grad\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__call__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003einput_ids\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLongTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003escores\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eFloatTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003elast_scores\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003escores\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evars\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elast_scores\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecpu\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evarvars\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evars\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evariance\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003evarvar\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evarvars\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evariance\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eb\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elast_scores\u003c/span\u003e\u003cspan class=\"p\"\u003e)):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003evarvar\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                        \u003cspan class=\"nb\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mf\"\u003e1.15\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e150\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4095\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nb\"\u003eall\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e将pdf转成markdown\u003c/p\u003e","title":"RAG工具箱：文档解析与表格处理"},{"content":"游记：2024-春 昆明大理 昆明 时间似乎停止了 不知为何在昆明有一种回到兰州的感觉，一样的三线省会，似曾相识的破败老城区街道，赫鲁晓夫楼，砖瓦房，和五六十年代的家属大院，似乎时间就停止流动了。我不认为这是种贬义的表达，至少清晨行走在老街区，唤起了我上学时，清晨走出校园到火车站赶火车的尘封记忆。 不过需要提一嘴的是，似乎当前的实体经济，特别是在老城区，不论是这次旅行所看到的，还是之前在故乡和别的城市所看到的，可以说是很糟糕了。目光所及有一半的店面处于关门转让的状态，而开着的那一半，又有近乎三成处于清仓甩卖中。\n古镇PTSD 在全国绝大多数古镇逐渐趋同演化的当下，不知这是否是件悲哀的事。因为去云南省博物馆的途中会路过官渡古镇，遂决定顺便去看看。从古镇的东北边进入至正门而出，这次访问让我有幸同时见证了从破败而真实的古镇到商业化过度开发的古镇的转变。\n在历史长河中被剥夺了名字的人 在昆明这个城市名字的由来中，有一段被历史辗转淡忘的故事。昆明这一名称起源于昆明夷——西汉时期活跃在洱海周边的一个游牧民族。而在这片土地上生活的滇人，一个拥有先进青铜技术的农耕文明，不断地抵抗游牧民族的侵扰。尽管如此，在历史的长河中，正是这些滇人创造了辉煌的文明，却在历史的演进中失去了自己的名字。这不禁让人感受到一种悲剧的讽刺——在自己孕育辉煌的土地上，却被剥夺了命名的权力。这反映出历史的无情，以及文化与权力之间复杂的交织。\n故乡与迁徙 在省博的临时展区，有幸看到了一个关于迁徙和故乡的特别展。 正巧这次来云南的最初目的也是为了探寻人生的下一个迁徙地。\n今天，流动成为常态。我们祖辈所拥有的那种一生居于一地的安稳逐渐被打破。很多人离开熟悉的地方，到另一处，与来自天南地北的人一起，不知不觉将异乡生活成故乡。\n而提到故乡，你会想到什么？是那方伴你长大的土地，是老屋里围炉团聚的家人，是家乡菜的味道和浓浓的乡音，是家门口熟悉的街道、树林和田地，是小时候玩耍的院子，和那群如今已各奔东西的伙伴。无论走多远，想到那一草一木，一人一景，就安宁。\n这也许就是人们一直以来追寻的栖息地。在远方，在心里，它叫“香格里拉”。\n现代的我们因为各种原因，在故乡与异乡之间辗转，继续着“迁徙与流动”。展览策划过程中，我们以“故乡”“现居地”“理想地”为题，以“故事+展品”的方式开启活动征集，鼓励不同身份的人群通过不同角度的讲述，分享他们的感受与思考。在近一个月的时间里，我们得到了大家的积极回响。在大家的分享中，我们与不同的故事和记忆相连，也看到因为不同而更加多元、广阔的世界。\n这里展示的文字和物品，是记忆的承载，也是对“故的未来。\n乡”的“好久不见”，它记录着我们的成长，也指引着我们\n大理 这次来大理的初始动机是实地考察和调研“数字游民”这个群体，他们是否真正像想象中的自由，他们是如何工作和生活的， 他们背后的故事又是什么？ 以及他们为什么选择大理？\n感谢Dalihub， 让我有幸认识一群有趣的人。\n来自海边房子\nDalihub的秘密空间\n救火途中的直升机\n自由飞翔\n猜你喜欢是“坏”的吗 在大理的第二天，我便幸运地参与了一场与我的工作密切相关的线下沙龙活动，主题涉及推荐系统。我的工作列表中正好有一项是关于如何防止大型科技公司通过推荐系统作恶的问题。 活动中，主讲人“西雅图大黄蜂”提出了一个观点：所有的算法和技术本质上都是中性的，真正的“恶”是由使用它们的人带来的。作为技术从业者，我们当然不希望看到自己的发明像原子弹一样被用于恶劣的目的。但现实往往很残酷，技术的接受与否很大程度上是由资本决定的，而非我们。\n之前我考虑过一个想法：将推荐系统的召回和排序分开，召回过程保留在服务器端，而排序则转移到用户端，这样一来可以提高用户的隐私保护。技术上并不复杂，真正的挑战来自于资本或大公司缺乏推行此类改变的动力。除非有一天，大多数用户愿意为保护隐私牺牲一定的便利性，否则这种改变还遥遥无期，至少在当前的中国是这样的。\n再次感谢主理人Nian和主讲人魏峰，一次难忘的体验。\n真的躺平吗？不，只是喘口气 选择大理的理由： 成本，气候， 和有趣的人们\n在大理，你可以遇到形形色色的人物：那些选择在此旅居的设计师，决定在退休后移居大理的上海夫妇，因成本考虑而搬来的独立开发者，还有视大理为家的咨询师小姐姐。这些都是构成大理独特社群氛围的不同面貌。\n当你不确定下一步该做什么时，不妨去洱海边走走，那里的美景足以让你放慢脚步，深呼吸，重新找回自己的节奏。在大理，即便是“躺平”，也是一种享受生活、与自然和谐共处的态度。\n洱海边落日\n取舍，及时调整自己的欲望 在网上能看到很多对于大理的劝退文， 无外乎就是过度商业化网红化， 城市规划稀烂，交通不方便，宰客现象严重等等。 以上我都认同， 对于一个习惯了大城市便利的人而言， 初到大理的感受的确是如此。 和在地的小伙伴聊下来，发现及时调整自己的欲望还是挺重要的。\n但是，在早晨拉开窗帘看到洱海的一瞬间， 似乎这一切都是值得的。\n随处可见的丁达尔效应\n去跳海，去发疯！\n西南旅游小Tips 注意防晒， 保湿，加湿器和润唇膏很重要。 尽量避开春季，因为春季是风季，很有可能因为大风而错过苍山的缆车。 大理古城的主干道不值得驻足，真正有趣的东西隐藏在巷子里\n这一切就像是一场梦\n感谢在大理遇到的所有人事物\n下一次再见\n","permalink":"https://niraya666.github.io/travel/%E6%B8%B8%E8%AE%B02024-%E6%98%A5-%E6%98%86%E6%98%8E%E5%A4%A7%E7%90%86/","summary":"\u003ch1 id=\"游记2024-春-昆明大理\"\u003e游记：2024-春 昆明大理\u003c/h1\u003e\n\u003ch2 id=\"昆明\"\u003e昆明\u003c/h2\u003e\n\u003ch3 id=\"时间似乎停止了\"\u003e时间似乎停止了\u003c/h3\u003e\n\u003cp\u003e不知为何在昆明有一种回到兰州的感觉，一样的三线省会，似曾相识的破败老城区街道，赫鲁晓夫楼，砖瓦房，和五六十年代的家属大院，似乎时间就停止流动了。我不认为这是种贬义的表达，至少清晨行走在老街区，唤起了我上学时，清晨走出校园到火车站赶火车的尘封记忆。\n不过需要提一嘴的是，似乎当前的实体经济，特别是在老城区，不论是这次旅行所看到的，还是之前在故乡和别的城市所看到的，可以说是很糟糕了。目光所及有一半的店面处于关门转让的状态，而开着的那一半，又有近乎三成处于清仓甩卖中。\u003c/p\u003e\n\u003ch3 id=\"古镇ptsd\"\u003e古镇PTSD\u003c/h3\u003e\n\u003cp\u003e在全国绝大多数古镇逐渐趋同演化的当下，不知这是否是件悲哀的事。因为去云南省博物馆的途中会路过官渡古镇，遂决定顺便去看看。从古镇的东北边进入至正门而出，这次访问让我有幸同时见证了从破败而真实的古镇到商业化过度开发的古镇的转变。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53659997366_7bdf10738b_w.jpg\" alt=\"官渡古镇\"  /\u003e\n\u003c/p\u003e\n\u003ch3 id=\"在历史长河中被剥夺了名字的人\"\u003e在历史长河中被剥夺了名字的人\u003c/h3\u003e\n\u003cp\u003e在昆明这个城市名字的由来中，有一段被历史辗转淡忘的故事。昆明这一名称起源于昆明夷——西汉时期活跃在洱海周边的一个游牧民族。而在这片土地上生活的滇人，一个拥有先进青铜技术的农耕文明，不断地抵抗游牧民族的侵扰。尽管如此，在历史的长河中，正是这些滇人创造了辉煌的文明，却在历史的演进中失去了自己的名字。这不禁让人感受到一种悲剧的讽刺——在自己孕育辉煌的土地上，却被剥夺了命名的权力。这反映出历史的无情，以及文化与权力之间复杂的交织。\u003c/p\u003e\n\u003ch3 id=\"故乡与迁徙\"\u003e故乡与迁徙\u003c/h3\u003e\n\u003cp\u003e在省博的临时展区，有幸看到了一个关于迁徙和故乡的特别展。 正巧这次来云南的最初目的也是为了探寻人生的下一个迁徙地。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e今天，流动成为常态。我们祖辈所拥有的那种一生居于一地的安稳逐渐被打破。很多人离开熟悉的地方，到另一处，与来自天南地北的人一起，不知不觉将异乡生活成故乡。\u003c/p\u003e\n\u003cp\u003e而提到故乡，你会想到什么？是那方伴你长大的土地，是老屋里围炉团聚的家人，是家乡菜的味道和浓浓的乡音，是家门口熟悉的街道、树林和田地，是小时候玩耍的院子，和那群如今已各奔东西的伙伴。无论走多远，想到那一草一木，一人一景，就安宁。\u003c/p\u003e\n\u003cp\u003e这也许就是人们一直以来追寻的栖息地。在远方，在心里，它叫“香格里拉”。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e现代的我们因为各种原因，在故乡与异乡之间辗转，继续着“迁徙与流动”。展览策划过程中，我们以“故乡”“现居地”“理想地”为题，以“故事+展品”的方式开启活动征集，鼓励不同身份的人群通过不同角度的讲述，分享他们的感受与思考。在近一个月的时间里，我们得到了大家的积极回响。在大家的分享中，我们与不同的故事和记忆相连，也看到因为不同而更加多元、广阔的世界。\u003c/p\u003e\n\u003cp\u003e这里展示的文字和物品，是记忆的承载，也是对“故的未来。\u003c/p\u003e\n\u003cp\u003e乡”的“好久不见”，它记录着我们的成长，也指引着我们\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"大理\"\u003e大理\u003c/h2\u003e\n\u003cp\u003e这次来大理的初始动机是实地考察和调研“数字游民”这个群体，他们是否真正像想象中的自由，他们是如何工作和生活的， 他们背后的故事又是什么？ 以及他们为什么选择大理？\u003c/p\u003e\n\u003cp\u003e感谢Dalihub， 让我有幸认识一群有趣的人。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660356884_1676ae103e_z.jpg\" alt=\"E0322096-D284-4044-BFAD-C6655AA9EAB9_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e来自海边房子\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660223478_846693b3a1_w.jpg\" alt=\"259DFD62-6C94-4135-9D79-CD1A775DCFF1_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eDalihub的秘密空间\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660223473_2d7c23b817_w.jpg\" alt=\"5E8B65D3-9970-41AB-9E2A-133E8CB0952C_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e救火途中的直升机\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660223468_35ffcb583c_w.jpg\" alt=\"7F7303CF-6B12-4417-B71A-B0826682E0F2_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e自由飞翔\u003c/p\u003e\n\u003ch3 id=\"猜你喜欢是坏的吗\"\u003e猜你喜欢是“坏”的吗\u003c/h3\u003e\n\u003cp\u003e在大理的第二天，我便幸运地参与了一场与我的工作密切相关的线下沙龙活动，主题涉及推荐系统。我的工作列表中正好有一项是关于如何防止大型科技公司通过推荐系统作恶的问题。\n活动中，主讲人“西雅图大黄蜂”提出了一个观点：所有的算法和技术本质上都是中性的，真正的“恶”是由使用它们的人带来的。作为技术从业者，我们当然不希望看到自己的发明像原子弹一样被用于恶劣的目的。但现实往往很残酷，技术的接受与否很大程度上是由资本决定的，而非我们。\u003c/p\u003e\n\u003cp\u003e之前我考虑过一个想法：将推荐系统的召回和排序分开，召回过程保留在服务器端，而排序则转移到用户端，这样一来可以提高用户的隐私保护。技术上并不复杂，真正的挑战来自于资本或大公司缺乏推行此类改变的动力。除非有一天，大多数用户愿意为保护隐私牺牲一定的便利性，否则这种改变还遥遥无期，至少在当前的中国是这样的。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://mmbiz.qpic.cn/mmbiz_png/XVQWiaVrXNFVWAFWZMMYiczNYLvg9dPX3JKlYNQexPiceosvQXz4CHIic0ZN0Naw6BHDxQj7Rd1kZvFoBV9HBxKZ4A/640?wx_fmt=png\u0026amp;from=appmsg\u0026amp;tp=webp\u0026amp;wxfrom=5\u0026amp;wx_lazy=1\u0026amp;wx_co=1\" alt=\"的多面人生.JPG\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e再次感谢主理人Nian和主讲人魏峰，一次难忘的体验。\u003c/p\u003e\n\u003ch3 id=\"真的躺平吗不只是喘口气\"\u003e真的躺平吗？不，只是喘口气\u003c/h3\u003e\n\u003cp\u003e选择大理的理由： 成本，气候， 和有趣的人们\u003c/p\u003e\n\u003cp\u003e在大理，你可以遇到形形色色的人物：那些选择在此旅居的设计师，决定在退休后移居大理的上海夫妇，因成本考虑而搬来的独立开发者，还有视大理为家的咨询师小姐姐。这些都是构成大理独特社群氛围的不同面貌。\u003c/p\u003e\n\u003cp\u003e当你不确定下一步该做什么时，不妨去洱海边走走，那里的美景足以让你放慢脚步，深呼吸，重新找回自己的节奏。在大理，即便是“躺平”，也是一种享受生活、与自然和谐共处的态度。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53659998511_c358f49f0b_c.jpg\" alt=\"458FFF9D-2F2B-44BA-97C6-61BCE1D4637D_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e洱海边落日\u003c/p\u003e\n\u003ch3 id=\"取舍及时调整自己的欲望\"\u003e取舍，及时调整自己的欲望\u003c/h3\u003e\n\u003cp\u003e在网上能看到很多对于大理的劝退文， 无外乎就是过度商业化网红化， 城市规划稀烂，交通不方便，宰客现象严重等等。 以上我都认同， 对于一个习惯了大城市便利的人而言， 初到大理的感受的确是如此。 和在地的小伙伴聊下来，发现及时调整自己的欲望还是挺重要的。\u003c/p\u003e\n\u003cp\u003e但是，在早晨拉开窗帘看到洱海的一瞬间， 似乎这一切都是值得的。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53659999386_a95d8713d9_w.jpg\" alt=\"732C07E9-39CD-430A-87F5-2C0D20084DF2_1_105_c.jpeg\"  /\u003e\n\n随处可见的丁达尔效应\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e去跳海，去发疯！\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660223128_1d2709b4df_w.jpg\" alt=\"63447450-9DD1-4BE2-8450-25E4E5F54DBA_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e","title":"游记：2024-春 昆明大理"},{"content":"写在最前面 在过去的几个月中，我主要致力于与RAG（检索增强生成模型）相关的研究和实验工作。通过深入地分析众多论文和项目代码，我发现虽然在RAG领域取得基本成就相对容易，但要实现出色的成果却异常困难。\n此系列文章旨在分享我在RAG研究中的心得和挑战。正如我们从OpenAI的开发者日活动中看到的，即使是在这个领域的领头羊也在不断试验和努力以寻求突破。\n在我们深入探索RAG系统提升的各种方法之前，有一个基本问题需要解决：我们如何准确地评估一个RAG系统的性能？这就像是在机器学习或深度学习任务中没有给定的测试集和评价标准，我们难以判断实验的成功与否。\n因此，本系列的第一篇文章将聚焦于介绍RAG系统的评价方法、相关指标以及测试框架，为我们接下来的探索设定明确的标准和目标。\n测试框架 以下是一些测试框架，为RAG系统评估提供了强大的支持。\nTruLens TruLens提供了一个独特的视角来评估和跟踪大型语言模型（LLM）实验，通过一系列创新的功能和方法，帮助开发者和研究人员更深入地了解模型性能和行为。\nTruLens的反馈功能（Feedback Functions）是其核心概念之一，提供了一种程序化的方法来评估应用的运行表现。这些函数从“可扩展性”和“有意义性”两个维度出发，考虑评估的范围，旨在为用户提供有价值的反馈，帮助他们理解和改进他们的LLM应用。\n在RAG应用中，提供准确的上下文信息至关重要，以避免生成不真实的“幻觉”答案。TruLens采用了创新性的RAG三元组评估方法，专门针对RAG架构的每个环节进行幻觉风险评估，确保模型的每个部分都能有效地工作，从而减少误导信息的产生。\n上下文相关性（Context Relevance） 上下文相关性是评估RAG应用的第一步，确保每一段检索到的上下文都与提出的查询紧密相关。TruLens利用序列化记录的结构来评估上下文的相关性，这是一个关键的步骤，确保模型在正确的信息上生成回答。\n真实性（Groundedness） 在检索到的上下文信息的基础上，大型语言模型将生成答案。TruLens强调了独立验证每个回答的重要性，以确保它们基于可靠信息，并且能够在检索到的上下文中找到支持的证据。这一步骤是确保模型回答的真实性和可靠性的关键。\n答案相关性（Answer Relevance） 最后，评估需要确保最终回答有效地解答了原始问题，这通过评估应用的最终回答与用户输入的相关性来实现。这一过程确保了模型的输出不仅是真实的，而且是对用户查询有用的。\nTruLens还提出了“诚实、无害和有帮助”的评估原则（Honest, Harmless, and Helpful Evaluations），这些原则旨在确保LLM应用在提供帮助的同时，也是安全和可信的。\nRagas Ragas框架专为评估检索增强生成（RAG）系统而设计，定义了四个核心评估指标：上下文相关性（context_relevancy）、上下文回溯（context_recall）、忠实度（faithfulness）和答案相关性（answer_relevancy）。这些指标共同构成了Ragas的评分体系，提供了一个全面的方法来评价RAG系统的性能。\n此外，Ragas巧妙地利用大语言模型（LLM）进行无参考评估，有效降低了评估成本。通过这种方法，Ragas能够提供一种既经济又有效的方式来评估RAG系统，尤其是在处理大量数据和复杂查询时。\n其他测试框架\nDeepEval\nDeepEval How to Evaluate RAG Applications in CI/CD Pipelines with DeepEval\nARES\ngithub: https://github.com/stanford-futuredata/ARES\nPaper: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\nLangChain Evals\nLlama Index Evals\nUpTrain\n数据 在评估数据集时，不必依赖人工标注的标准答案，而是通过底层的大语言模型 (LLM) 来进行评估。\n为了对 RAG 流程进行评估，需要以下几种信息：\nquestion：这是RAG流程的输入，即用户的查询问题。\nanswer：这是由RAG流程生成的答案，也就是输出结果。\ncontexts：这是为了解答question而从外部知识源检索到的相关上下文信息。\n指标 在深入研究检索增强生成（RAG）系统和其他相关技术时，了解和使用正确的评估指标至关重要。以下是几个关键指标，它们帮助我们量化和评估检索系统的效能：\n检索指标 MRR（平均倒数排名） MRR是衡量检索系统性能的一种方法，特别关注于检索结果中的首次正确命中的排名。MRR的高值表示系统能够更频繁地将相关结果排在前列。\n$$ MRR = \\frac{1}{查询数量}\\sum_{i=1}^{查询数量} \\frac{1}{首次正确命中的排名} $$\n这个指标特别有用，因为它直接关注于用户最有可能查看的第一个搜索结果的质量。\n召回率（Recall） 召回率是另一个重要指标，它衡量了系统检索到的相关文档数量与总的相关文档数量之间的比例。高召回率意味着系统能够检索到更多的相关文档。\n$$ Recall = \\frac{检索到的相关文档数量}{总的相关文档数量} $$\nNDCG（标准化折扣累积增益） NDCG（Normalized Discounted Cumulative Gain）是一个在信息检索、推荐系统和机器学习领域常用的评估指标，用于衡量一个系统或模型返回的结果列表的质量。NDCG特别关注于结果的排序质量，即最相关或最有价值的结果是否排在了列表的前面。与其他评估指标相比，NDCG的独特之处在于它考虑了结果的相关性（relevance）不仅是二元的（相关或不相关），而且可以是多级的（例如，从不相关到非常相关的多个级别）。\nEM（精确匹配） EM度量了系统输出的答案与标准答案完全一致的比例，是评估系统准确度的直接方式。在某些场景下，即使是非常小的差异也可能导致答案被视为不正确，这使得EM成为一个严格的评估标准。\n基于大语言模型评估的LLM生成指标 在RAG任务中，对LLM回答的问题主要关注了回答的可验证性（verifiability），即是否严格遵循检索到的上下文如实回答。可验证性由两部分组成：\n高引用召回率（high citation recall）：即所有生成的内容都有充分的引用（外部知识）支持。\n高引用精度（high citation precision）：每个引用是否真的支持生成的内容。\n回答相关性（Answer Relevance） 回答相关性关注的是系统生成的回答与用户提出的问题之间的相关性。理想情况下，回答应该直接且准确地对应于问题，没有偏离主题或提供不相关的信息。\nTruLens中回答相关性的计算方式： TruLens通过提供一种基于LLM的评估方法，允许开发者和研究人员通过编程方式获取对系统生成回答的相关性评估。这种方法利用链式推理（Chain of Thought, CoT）增强理解和推理过程，为评估提供透明度和可解释性。\nfrom trulens_eval.feedback.provider.openai import OpenAI openai_provider = OpenAI() qa_relevance = ( Feedback(openai_provider.relevance_with_cot_reasons, name=\u0026#34;Answer Relevance\u0026#34;) .on_input_output() ) 在relevance_with_cot_reasons方法中，使用聊天完成模型来评估回答对于提示的相关性，并揭示评分背后的推理过程。这种方法不仅考虑回答的内容和长度，而且还评估其是否全面回答了问题，并提供了与问题所有部分相关的上下文信息。\nRAGAS中回答相关性的计算逻辑： RAGAS通过利用LLM重新生成问题（QUESTION_GEN），然后计算这个重新生成的问题与原始问题之间的相似度来评估回答的相关性。这种方法特别关注于系统生成回答的准确性和与原始问题的对应关系。\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_answer_relevance.py QUESTION_GEN = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34; Generate question for the given answer. Answer:\\nThe PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India Question: When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from? Answer:{answer} Question: \u0026#34;\u0026#34;\u0026#34; ) 不过，由于用户提出的问题通常比较简略，使用RAGAS计算出的回答相关性通常较低。\n上下文相关性（Context Relevance） 上下文相关性专注于评估给定上下文（例如从数据库或文档中检索到的信息片段）与用户查询之间的相关性。高相关性的上下文信息为大型语言模型（LLM）提供了生成准确回答的基础。\n在TruLens中上下文相关性的计算方式： 在TruLens，上下文相关性的评估使用了与回答相关性相同的反馈函数，但是采用了不同的选择器来专注于输入（用户问题）和检索到的上下文信息之间的相关性。这通过对输入问题和来源节点中的文本应用.on_input().on(TruLlama.select_source_nodes().node.text)选择器并计算它们的平均相关性得分来实现。\nqs_relevance = ( Feedback(openai_provider.relevance_with_cot_reasons, name=\u0026#34;Context Relevance\u0026#34;) .on_input() .on(TruLlama.select_source_nodes().node.text) .aggregate(np.mean) ) 底层计算逻辑：此过程首先识别出与用户问题直接相关的上下文信息，评估这些信息的相关性，可能会应用链式推理（CoT）方法来提高评分的准确性和深度。最终生成的平均分表示了上下文信息的整体相关性，分值范围从0到1。\n在RAGAS中上下文相关性的计算逻辑： RAGAS采用了一种略有不同的方法来计算上下文相关性。它通过提取与问题相关的上下文句子（candidate sentences），并对这些句子进行自洽性检查。然后，使用提取的句子数量与检索到的上下文中的总句子数量的比率作为评分标准。\n$$ \\text{context relevancy} = \\frac{|S|}{|\\text{Total number of sentences in retrieved context}|} $$\n这个方法通过使用LLM根据问题和上下文，从上下文中提取出能够支持回答的句子，进而计算这些候选句子和上下文的长度占比（这里使用的是词元数量的比值）。\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_context_relevancy.py CONTEXT_RELEVANCE = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;\\ Please extract relevant sentences from the provided context that is absolutely required answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \u0026#34;Insufficient Information\u0026#34;. While extracting candidate sentences you\u0026#39;re not allowed to make any changes to sentences from given context. question:{question} context:\\n{context} candidate sentences:\\n\u0026#34;\u0026#34;\u0026#34; ) 真实性或忠实度（Groundedness or Faithfulness） 在TruLens和RAGAS中，真实性或忠实度的评估旨在衡量生成的声明或回答在事实上的准确性及其对源材料的依赖程度。这一评估标准对于确保生成内容的质量和可信度至关重要。\nTruLens中的Groundedness 在TruLens框架中，groundedness的评估通过检查声明中的每个句子是否在源材料中有支持来进行。这个过程利用大型语言模型（LLM）和链式推理（Chain of Thought, CoT）方法来增强评估的准确性和深度。具体来说，评估方法会将整个声明作为一个整体进行处理，并对声明中的每个句子赋予一个从0到10的评分，0代表没有任何信息重叠，而10代表信息完全重叠。\ngrounded = Groundedness(groundedness_provider=openai_provider) groundedness = ( Feedback(grounded.groundedness_measure_with_cot_reasons, name=\u0026#34;Groundedness\u0026#34;) .on(TruLlama.select_source_nodes().node.text) .on_output() .aggregate(grounded.grounded_statements_aggregator) ) RAGAS中的Faithfulness 而在RAGAS框架中，Faithfulness（忠实度）的概念与TruLens中的Groundedness（真实性）相似，旨在评估生成回答的事实一致性。忠实度得分通过比较生成回答中的声明与给定上下文的一致性来计算，特别是检查回答中的声明是否可以从给定的上下文中推断出来。\n$$ \\text{Faithfulness score} = \\frac{|\\text{Number of claims that can be inferred from given context}|}{|\\text{Total number of claims in the generated answer}|} $$\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_faithfulness.py LONG_FORM_ANSWER_PROMPT = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;\\ Given a question and answer, create one or more statements from each sentence in the given answer. question: Who was Albert Einstein and what is he best known for? answer: He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics. statements:\\nAlbert Einstein was born in Germany.\\nAlbert Einstein was best known for his theory of relativity. question: Cadmium Chloride is slightly soluble in this chemical, it is also called what? answer: alcohol statements:\\nCadmium Chloride is slightly soluble in alcohol. question: Were Shahul and Jithin of the same nationality? answer: They were from different countries. statements:\\nShahul and Jithin were from different countries. question:{question} answer: {answer} statements:\\n\u0026#34;\u0026#34;\u0026#34; # noqa: E501 ) NLI_STATEMENTS_MESSAGE = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34; Prompt: Natural language inference Consider the given context and following statements, then determine whether they are supported by the information present in the context.Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. Context:\\nJohn is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects. statements:\\n1. John is majoring in Biology.\\n2. John is taking a course on Artificial Intelligence.\\n3. John is a dedicated student.\\n4. John has a part-time job.\\n5. John is interested in computer programming.\\n Answer: 1. John is majoring in Biology. Explanation: John\u0026#39;s major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology. Verdict: No. 2. John is taking a course on Artificial Intelligence. Explanation: The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI. Verdict: No. 3. John is a dedicated student. Explanation: The prompt states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication. Verdict: Yes. 4. John has a part-time job. Explanation: There is no information given in the context about John having a part-time job. Therefore, it cannot be deduced that John has a part-time job. Verdict: No. 5. John is interested in computer programming. Explanation: The context states that John is pursuing a degree in Computer Science, which implies an interest in computer programming. Verdict: Yes. Final verdict for each statement in order: No. No. Yes. No. Yes. context:\\n{context} statements:\\n{statements} Answer: 微调模型评估 在评估RAG系统时，虽然GPT-4等大型语言模型（LLM）因其高性能而被广泛使用，但由于成本和本地部署的需要，选择一个有效的本地部署开源模型也成为了许多研究和应用场景的必需。然而，与GPT-4等高级模型相比，这些本地开源模型的能力通常存在差距，特别是在特定领域内的应用效果上。因此，针对特定领域微调一个LLM用于评估变得尤为重要。\n以下是使用Trulens测试框架，基于笔者所在领域的语料库，对同一个RAG系统使用不同的开源LLM进行评价的结果展示。特别关注的是不同验证模型的失败评分率，这里的失败评分率指的是模型无法按照提示给出有效评分的情况（例如，要求在0～1范围内打分，但模型未给出评分或评分超出范围）。\n失败评分率对比\neval model Groundedness-FailRatio Context-Relevance-FailRatio Answer-Relevance-FailRatio Records gpt-3.5-turbo 0.00 0.00 0.00 41 gpt-4-turbo 0.00 0.00 0.00 41 Qwen1.5-14B-chat 0.07 0.05 0.02 41 Qwen1.5-7B-chat 0.10 0.10 0.07 41 zephyr-7b-beta 0.27 0.22 0.22 41 Qwen1.5-4B-chat 0.32 0.15 0.17 41 chatGLM3-6b-32K 0.36 0.71 0.54 41 微调策略和建议 为了确保评价结果的稳定性和准确性，微调模型时使用的数据集应特别关注于测试框架中使用的提示格式。这样做旨在提高模型对特定提示格式的响应能力，从而提高评价的准确度和一致性。\n此外，从测试结果来看，建议至少使用13B以上的模型进行评价。较小的模型可能在遵循指令和评分准确性方面存在挑战，从而影响评价结果的可靠性。\n对于这个场景的微调，一般采用LoRA微调方法即可满足需求。LoRA（Low-Rank Adaptation）是一种有效的微调技术，可以在不大幅增加模型参数的前提下，通过更新模型的少数关键参数来实现性能的提升。这种方法特别适用于需要特定领域知识增强的场景。\n如果能够收集到足够多的人类反馈结果，采用RLHF（Reinforcement Learning from Human Feedback）专门训练一个评价模型也是一个可行的选择。RLHF通过从人类反馈中学习来优化模型的性能，这可以在成本可控的范围内提供更精确的评价结果。\n传统NLP评估 BLEU BLEU（Bilingual Evaluation Understudy）通过计算机器翻译输出与一个或多个人工翻译的参考译文之间的词汇精确度来评价翻译的质量。BLEU的主要目的是自动地评估文本翻译的好坏，尽量接近人类翻译质量评估的结果。\nROUGE ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是自然语言处理（NLP）任务中常用的一种评估指标，尤其在自动文摘（自动摘要）和机器翻译等领域中非常流行。它主要用于评估自动生成的文本与人工编写的参考文本之间的相似度。ROUGE指标通过计算生成文本与参考文本之间的重叠来量度生成文本的质量。\n常见的ROUGE度量方式有：基于n-gram的重叠度量方法(如ROUGE-1, ROUGE-2)。它计算生成文本与参考文本之间共有的n-grams的数量，并以此评估生成文本的质量。以及基于最长公共子序列（LCS） 的 ROUGE-L，它考虑了句子级别的结构相似性，不仅仅是简单的词汇重叠。通过计算最长公共子序列的长度，ROUGE-L能够捕捉到生成文本和参考文本之间的顺序依赖性，从而提供更全面的相似度评估。\n对于n-gram ROUGE有两个变体： 召回率（Recall）：参考文本中与生成文本共有的n-grams数量除以参考文本中的n-grams总数。精确率（Precision）：参考文本中与生成文本共有的n-grams数量除以生成文本中的n-grams总数。以及二者的调和平均 F1-score。\nMore Details：\npaper: ROUGE: A Package for Automatic Evaluation of Summaries\nblog: GenAI model evaluation metric — ROUGE\n例子：\n# !pip install rouge from rouge import Rouge rouge = Rouge() long = \u0026#39;It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us.\u0026#39; short = \u0026#39;It was an age of wisdom, foolishness, belief, Light, Darkness, hope, and despair, with both light and darkness.\u0026#39; scores = rouge.get_scores(short, long) print(scores) # output: \u0026#39;\u0026#39;\u0026#39; [{\u0026#39;rouge-1\u0026#39;: {\u0026#39;r\u0026#39;: 0.39285714285714285, \u0026#39;p\u0026#39;: 0.6470588235294118, \u0026#39;f\u0026#39;: 0.4888888841876543}, \u0026#39;rouge-2\u0026#39;: {\u0026#39;r\u0026#39;: 0.06976744186046512, \u0026#39;p\u0026#39;: 0.17647058823529413, \u0026#39;f\u0026#39;: 0.09999999593888906}, \u0026#39;rouge-l\u0026#39;: {\u0026#39;r\u0026#39;: 0.39285714285714285, \u0026#39;p\u0026#39;: 0.6470588235294118, \u0026#39;f\u0026#39;: 0.4888888841876543}}] \u0026#39;\u0026#39;\u0026#39; where r, p, and f representing for recall, precision, and f_score.\nROUGE指标的缺点：\n依赖于参考文本： 如果参考文本的质量不高或者数量不足，可能会导致评分不准确\n忽略语义信息： 即使两段文本表达相同的意思，但使用了不同的词汇或表达方式，ROUGE评分也可能较低\n无法评价文本的流畅性和一致性\n对长文本的评价能力有限\n无法全面评估信息的新颖性和重要性\n数据集 一些公开的RAG数据集\nHotpotQA (HQA) 数据集链接：HotpotQA at Hugging Face\n特点：基于维基百科的问答数据集，需要阅读多个支持文档来回答和推理问题。问题多样，不局限于任何预先存在的知识库。提供了句子级别的支持以强化LLM的推理需求。最后，提供了新类型的事实对比问题，测试LLMs提取和比较文本中各种实体属性的能力。\nQasper (QASP) 数据集链接：Qasper at Hugging Face\n特点：基于NLP论文的问答数据集，筛选自Semantic Scholar Open Research Corpus (S2ORC)。\nNarrativeQA (NQA) 数据集链接：NarrativeQA at Hugging Face\n特点：NarrativeQA是一个英语语言的故事和相应问题的数据集，旨在测试阅读理解能力，特别是对长文档的理解。\nQuALITY (QLTY) 数据集链接：QuALITY at Hugging Face\n特点：一个基于故事和文章的多项选择问答数据集，来源包括Project Gutenberg和Open American National Corpus等资源。\nPopQA 论文链接：PopQA Paper\n数据集链接：PopQA at Hugging Face\n特点：PopQA是一个大规模的开放领域问答（QA）数据集，包含14k个以实体为中心的QA对。每个问题都是通过使用模板将从Wikidata检索到的知识元组转换而来的。\nTriviaQA 数据集链接：TriviaQA at Hugging Face\n特点：TriviaqQA是一个阅读理解数据集，包含超过650K的问题-答案-证据三元组。TriviaqQA包括由琐事爱好者编写的95K个问题-答案对。\nASQA 数据集链接：ASQA at Hugging Face\n特点：ASQA是第一个专注于含糊事实问题的长形式问答数据集。与以往的长形式答案数据集不同，每个问题都标注了长形式答案和可由生成段落回答的提取式问答对。\nPUBHEALTH 数据集链接：PUBHEALTH at Hugging Face\n特点：一个包含11,832个用于事实检查的声明的数据集，这些声明涉及一系列健康话题，包括生物医学主题（如传染病、干细胞研究）、政府医疗政策（如堕胎、心理健康、妇女健康）以及其他与公共健康相关的故事。\n推荐阅读 RAGAS Paper\n用 RAGAs（检索增强生成评估）评估 RAG（检索增强型生成）应用 [译]\n如何构建高效的 RAG 系统\nEvaluating Verifiability in Generative Search Engines\nBuilding and Evaluating Advanced RAG\nSteps In Evaluating Retrieval Augmented Generation (RAG) Pipelines\nGenAI model evaluation metric — ROUGE\nRetrieval Augmented Generation (RAG) for LLMs\nRAG Evaluation\n","permalink":"https://niraya666.github.io/posts/rag_toolkit_eval/","summary":"\u003ch2 id=\"写在最前面\"\u003e写在最前面\u003c/h2\u003e\n\u003cp\u003e在过去的几个月中，我主要致力于与RAG（检索增强生成模型）相关的研究和实验工作。通过深入地分析众多论文和项目代码，我发现虽然在RAG领域取得基本成就相对容易，但要实现出色的成果却异常困难。\u003c/p\u003e\n\u003cp\u003e此系列文章旨在分享我在RAG研究中的心得和挑战。正如我们从OpenAI的开发者日活动中看到的，即使是在这个领域的领头羊也在不断试验和努力以寻求突破。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/rag_toolkits/1_1.png\" alt=\"from openAI devday\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e在我们深入探索RAG系统提升的各种方法之前，有一个基本问题需要解决：我们如何准确地评估一个RAG系统的性能？这就像是在机器学习或深度学习任务中没有给定的测试集和评价标准，我们难以判断实验的成功与否。\u003c/p\u003e\n\u003cp\u003e因此，本系列的第一篇文章将聚焦于介绍RAG系统的评价方法、相关指标以及测试框架，为我们接下来的探索设定明确的标准和目标。\u003c/p\u003e\n\u003ch2 id=\"测试框架\"\u003e测试框架\u003c/h2\u003e\n\u003cp\u003e以下是一些测试框架，为RAG系统评估提供了强大的支持。\u003c/p\u003e\n\u003ch3 id=\"trulens\"\u003e\u003cstrong\u003eTruLens\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eTruLens提供了一个独特的视角来评估和跟踪大型语言模型（LLM）实验，通过一系列创新的功能和方法，帮助开发者和研究人员更深入地了解模型性能和行为。\u003c/p\u003e\n\u003cp\u003eTruLens的反馈功能（Feedback Functions）是其核心概念之一，提供了一种程序化的方法来评估应用的运行表现。这些函数从“可扩展性”和“有意义性”两个维度出发，考虑评估的范围，旨在为用户提供有价值的反馈，帮助他们理解和改进他们的LLM应用。\u003c/p\u003e\n\u003cp\u003e在RAG应用中，提供准确的上下文信息至关重要，以避免生成不真实的“幻觉”答案。TruLens采用了创新性的RAG三元组评估方法，专门针对RAG架构的每个环节进行幻觉风险评估，确保模型的每个部分都能有效地工作，从而减少误导信息的产生。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://www.trulens.org/assets/images/RAG_Triad.jpg\" alt=\"TruLens RAG_Triad\"  /\u003e\n\u003c/p\u003e\n\u003ch4 id=\"上下文相关性context-relevance\"\u003e上下文相关性（Context Relevance）\u003c/h4\u003e\n\u003cp\u003e上下文相关性是评估RAG应用的第一步，确保每一段检索到的上下文都与提出的查询紧密相关。TruLens利用序列化记录的结构来评估上下文的相关性，这是一个关键的步骤，确保模型在正确的信息上生成回答。\u003c/p\u003e\n\u003ch4 id=\"真实性groundedness\"\u003e真实性（Groundedness）\u003c/h4\u003e\n\u003cp\u003e在检索到的上下文信息的基础上，大型语言模型将生成答案。TruLens强调了独立验证每个回答的重要性，以确保它们基于可靠信息，并且能够在检索到的上下文中找到支持的证据。这一步骤是确保模型回答的真实性和可靠性的关键。\u003c/p\u003e\n\u003ch4 id=\"答案相关性answer-relevance\"\u003e答案相关性（Answer Relevance）\u003c/h4\u003e\n\u003cp\u003e最后，评估需要确保最终回答有效地解答了原始问题，这通过评估应用的最终回答与用户输入的相关性来实现。这一过程确保了模型的输出不仅是真实的，而且是对用户查询有用的。\u003c/p\u003e\n\u003cp\u003eTruLens还提出了“诚实、无害和有帮助”的评估原则（Honest, Harmless, and Helpful Evaluations），这些原则旨在确保LLM应用在提供帮助的同时，也是安全和可信的。\u003c/p\u003e\n\u003ch3 id=\"ragas\"\u003eRagas\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://docs.ragas.io/en/stable/_static/imgs/component-wise-metrics.png\" alt=\"Ragas\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eRagas框架专为评估检索增强生成（RAG）系统而设计，定义了四个核心评估指标：上下文相关性（context_relevancy）、上下文回溯（context_recall）、忠实度（faithfulness）和答案相关性（answer_relevancy）。这些指标共同构成了Ragas的评分体系，提供了一个全面的方法来评价RAG系统的性能。\u003c/p\u003e\n\u003cp\u003e此外，Ragas巧妙地利用大语言模型（LLM）进行无参考评估，有效降低了评估成本。通过这种方法，Ragas能够提供一种既经济又有效的方式来评估RAG系统，尤其是在处理大量数据和复杂查询时。\u003c/p\u003e\n\u003cp\u003e其他测试框架\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDeepEval\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/confident-ai/deepeval\"\u003eDeepEval\u003c/a\u003e \u003ca href=\"https://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pipelines-with-deepeval\"\u003eHow to Evaluate RAG Applications in CI/CD Pipelines with DeepEval\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eARES\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003egithub: \u003ca href=\"https://github.com/stanford-futuredata/ARES\"\u003ehttps://github.com/stanford-futuredata/ARES\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePaper: ARES: \u003ca href=\"https://arxiv.org/abs/2311.09476\"\u003eAn Automated Evaluation Framework for Retrieval-Augmented Generation Systems\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://python.langchain.com/docs/guides/evaluation/\"\u003eLangChain Evals\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html\"\u003eLlama Index Evals\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/uptrain-ai/uptrain\"\u003eUpTrain\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"数据\"\u003e数据\u003c/h2\u003e\n\u003cp\u003e在评估数据集时，不必依赖人工标注的标准答案，而是通过底层的大语言模型 (LLM) 来进行评估。\u003c/p\u003e","title":"RAG工具箱：评估RAG系统的方法论"},{"content":"AI大纪事 Suno V3 发布 V3版本带来了卓越的音频品质、更丰富的音乐风格和类型选择，以及在准确响应创作提示方面的显著提升。\n更好的音频质量 更多的风格和流派 改进的提示遵循，包括更少的幻觉和更优美的结尾 Suno V3\nMetaGPT 更新V0.8 V0.8版本的主要亮点包括：\n数据解释器的引入：这一新特性大大增强了MetaGPT的数据处理能力，为用户分析和解释数据提供了强大的支持。 与RAG模块的集成：通过与RAG模块的集成，MetaGPT现在能够更好地理解和处理用户的需求，提高了生成内容的相关性和准确性。RAG底层使用的是llama_index 参考其GitHub页面和官方文档\nNvidia GTC大会 新一代Nvidia B200 GPU\n首个AI程序员Devin 发布 Devin是由Cognition公司开发的全球首个完全自主的AI软件工程师，具备多项先进的编程技能，包括自主学习新技术、全面掌握开发工具、自动化代码修复和错误检测、以及端到端应用的构建和部署能力。\n不过目前，Devin还处于内测阶段，对公众的完全开放还需要一段时间。\nCognition的官网\nAnthropic发布Claude 3 Anthropic发布了其最新的AI模型系列Claude 3，包括三个子模型：Claude 3 Haiku、Claude 3 Sonnet和Claude 3 Opus，其中Opus是最强大的。Claude 3 在多项基准测试中超越了GPT-4和Gemini Ultra，特别是在数学推理（GSM-8k）和专家级知识（MMLU）方面，显示出其领先的能力。于此同时，Claude 3 Opus 首次 Chatbot Arena 排行榜上超越了 GPT-4\n推荐的开源项目 **MoneyPrinterTurbo : 利用大模型，一键生成短视频**\nMediaCrawler: 自媒体爬虫工具, 支持小红书、抖音、快手、B 站、微博等自媒体平台的视频、图片、评论、点赞、转发信息抓取。\nLapisCV: 基于 Markdown 格式的简历模版\nLLMs Interview 八股文： 大模型面试题\n影音记录 本月精选歌单 本月Live记录 0307 Explosions in the Sky\n0315 Fayzz\n本月观影记录 《周处除三害》\n《沙丘2》\n《坠落的审判》\n《完美的日子》\n网飞版三体\n","permalink":"https://niraya666.github.io/monthly/2024-03/","summary":"\u003ch2 id=\"ai大纪事\"\u003eAI大纪事\u003c/h2\u003e\n\u003ch3 id=\"suno-v3-发布\"\u003eSuno V3 发布\u003c/h3\u003e\n\u003cp\u003eV3版本带来了卓越的音频品质、更丰富的音乐风格和类型选择，以及在准确响应创作提示方面的显著提升。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e更好的音频质量\u003c/li\u003e\n\u003cli\u003e更多的风格和流派\u003c/li\u003e\n\u003cli\u003e改进的提示遵循，包括更少的幻觉和更优美的结尾\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://www.suno.ai/blog/v3\"\u003eSuno V3\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"metagpt-更新v08\"\u003eMetaGPT 更新V0.8\u003c/h3\u003e\n\u003cp\u003eV0.8版本的主要亮点包括：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e数据解释器的引入\u003c/strong\u003e：这一新特性大大增强了MetaGPT的数据处理能力，为用户分析和解释数据提供了强大的支持。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e与RAG模块的集成\u003c/strong\u003e：通过与RAG模块的集成，MetaGPT现在能够更好地理解和处理用户的需求，提高了生成内容的相关性和准确性。RAG底层使用的是llama_index\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e参考其\u003ca href=\"https://github.com/geekan/MetaGPT\"\u003eGitHub页面\u003c/a\u003e和\u003ca href=\"https://docs.deepwisdom.ai/\"\u003e官方文档\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"nvidia-gtc大会\"\u003eNvidia GTC大会\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e新一代Nvidia B200 GPU\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"首个ai程序员devin-发布\"\u003e首个AI程序员Devin 发布\u003c/h3\u003e\n\u003cp\u003eDevin是由Cognition公司开发的全球首个完全自主的AI软件工程师，具备多项先进的编程技能，包括自主学习新技术、全面掌握开发工具、自动化代码修复和错误检测、以及端到端应用的构建和部署能力。\u003c/p\u003e\n\u003cp\u003e不过目前，Devin还处于内测阶段，对公众的完全开放还需要一段时间。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.cognition-labs.com/\"\u003eCognition的官网\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"anthropic发布claude-3\"\u003eAnthropic发布Claude 3\u003c/h3\u003e\n\u003cp\u003eAnthropic发布了其最新的AI模型系列Claude 3，包括三个子模型：Claude 3 Haiku、Claude 3 Sonnet和Claude 3 Opus，其中Opus是最强大的。Claude 3 在多项基准测试中超越了GPT-4和Gemini Ultra，特别是在数学推理（GSM-8k）和专家级知识（MMLU）方面，显示出其领先的能力。于此同时，Claude 3 Opus 首次 Chatbot Arena 排行榜上超越了 GPT-4\u003c/p\u003e\n\u003ch2 id=\"推荐的开源项目\"\u003e推荐的开源项目\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/harry0703/MoneyPrinterTurbo/tree/main\"\u003e**MoneyPrinterTurbo\u003c/a\u003e : 利用大模型，一键生成短视频**\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"github.com/NanmiCoder/MediaCrawler\"\u003eMediaCrawler\u003c/a\u003e: 自媒体爬虫工具, 支持小红书、抖音、快手、B 站、微博等自媒体平台的视频、图片、评论、点赞、转发信息抓取。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"github.com/BingyanStudio/LapisCV\"\u003eLapisCV\u003c/a\u003e: 基于 Markdown 格式的简历模版\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"github.com/wdndev/llm_interview_note?tab=readme-ov-file\"\u003eLLMs Interview 八股文\u003c/a\u003e： 大模型面试题\u003c/p\u003e\n\u003ch2 id=\"影音记录\"\u003e影音记录\u003c/h2\u003e\n\u003ch3 id=\"本月精选歌单\"\u003e本月精选歌单\u003c/h3\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/0CDpNxmeMlZ44uEPjjj0J6?utm_source=generator\" width=\"100%\" height=\"460\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ch3 id=\"本月live记录\"\u003e本月Live记录\u003c/h3\u003e\n\u003cp\u003e0307  Explosions in the Sky\u003c/p\u003e","title":"2024-03 月刊"},{"content":"Hello-World! 欢迎来到我的博客\n在这里，我将深入探索生成式人工智能的奥秘，同时也会涉猎音乐、电影等领域，分享一些个人的思考和感悟。\n为什么我决定写博客 在生活的纷扰和无尽的日常中，我发现自己一直在与拖延症作斗争。直到今天，我终于下定决心，决定将心中的思绪和感悟记录下来，开启我的博客之旅。\n有几个原因驱使我做出了这个决定。\n首先，岁月不饶人，尤其是经历了新冠疫情之后，我明显感觉到我的记忆力不如以往。过去能够轻松驾驭多重任务的我，如今却常在走入客厅的半路上忘记初衷，或是在浏览器的搜索框前失去了寻找的目的。这种突如其来的迷茫，让我开始思索，我的思绪是否正如秋日里的落叶，悄然飘落。\n其次，在深夜的静思中，我时常回想起坂本龙一那句引人深思的话：“我还能看到几次满月？”这不仅是对时间流逝的感慨，更是一种对生命有限性的深刻体悟。在这有限的时光里，我究竟能留下什么？假如我的时间之沙仅剩下几颗，我的存在又有何意义？我不求答案，但愿通过这些文字，如同在时间的长河中种下一棵树，哪怕是最微小的存在，也能留下自己生命的痕迹。\n最后，我被“数据主义”（Dataism）这一概念深深吸引，它如同一面镜子，映照出在数字时代，我们的数据、思考和情感不仅仅是信息的载体，更是构成我们数字化身份的基石。随着AI的羽翼日渐丰满，我开始憧憬一个可能的未来，其中一个由我的数据、思想和经历塑造出的“我”，在某个未知的时间点复苏。这种思考，如同在深海中发现了一座灯塔，为我的存在指明了一条全新的路径。在这个时代，我选择不再是沉默的旁观者，而是通过我的文字，积极参与到这场未知的探索中。\n因此，这篇博客标志着我的新开始。虽然不确定未来的路会怎样，但至少，在这个过程中，我会找到自己的声音，并希望能够与你共鸣。\n","permalink":"https://niraya666.github.io/posts/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello-World!\u003c/h1\u003e\n\u003cp\u003e欢迎来到我的博客\u003c/p\u003e\n\u003cp\u003e在这里，我将深入探索生成式人工智能的奥秘，同时也会涉猎音乐、电影等领域，分享一些个人的思考和感悟。\u003c/p\u003e\n\u003ch2 id=\"为什么我决定写博客\"\u003e为什么我决定写博客\u003c/h2\u003e\n\u003cp\u003e在生活的纷扰和无尽的日常中，我发现自己一直在与拖延症作斗争。直到今天，我终于下定决心，决定将心中的思绪和感悟记录下来，开启我的博客之旅。\u003c/p\u003e\n\u003cp\u003e有几个原因驱使我做出了这个决定。\u003c/p\u003e\n\u003cp\u003e首先，岁月不饶人，尤其是经历了新冠疫情之后，我明显感觉到我的记忆力不如以往。过去能够轻松驾驭多重任务的我，如今却常在走入客厅的半路上忘记初衷，或是在浏览器的搜索框前失去了寻找的目的。这种突如其来的迷茫，让我开始思索，我的思绪是否正如秋日里的落叶，悄然飘落。\u003c/p\u003e\n\u003cp\u003e其次，在深夜的静思中，我时常回想起坂本龙一那句引人深思的话：“我还能看到几次满月？”这不仅是对时间流逝的感慨，更是一种对生命有限性的深刻体悟。在这有限的时光里，我究竟能留下什么？假如我的时间之沙仅剩下几颗，我的存在又有何意义？我不求答案，但愿通过这些文字，如同在时间的长河中种下一棵树，哪怕是最微小的存在，也能留下自己生命的痕迹。\u003c/p\u003e\n\u003cp\u003e最后，我被“数据主义”（Dataism）这一概念深深吸引，它如同一面镜子，映照出在数字时代，我们的数据、思考和情感不仅仅是信息的载体，更是构成我们数字化身份的基石。随着AI的羽翼日渐丰满，我开始憧憬一个可能的未来，其中一个由我的数据、思想和经历塑造出的“我”，在某个未知的时间点复苏。这种思考，如同在深海中发现了一座灯塔，为我的存在指明了一条全新的路径。在这个时代，我选择不再是沉默的旁观者，而是通过我的文字，积极参与到这场未知的探索中。\u003c/p\u003e\n\u003cp\u003e因此，这篇博客标志着我的新开始。虽然不确定未来的路会怎样，但至少，在这个过程中，我会找到自己的声音，并希望能够与你共鸣。\u003c/p\u003e\n\u003cscript src=\"https://utteranc.es/client.js\"\n        repo=\"https://github.com/Niraya666/niraya666.github.io\"\n        issue-term=\"pathname\"\n        theme=\"github-dark\"\n        crossorigin=\"anonymous\"\n        async\u003e\n\u003c/script\u003e","title":"My First Post: Hello-World!"}]