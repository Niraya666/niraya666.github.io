[{"content":"值得关注的模型和新技术 OpenAI 更新系列模型 发布了多款新一代AI模型，包括o3、o4-mini、GPT-4.1及其mini和nano版本。这些模型在推理能力、成本效益和多模态处理等方面实现了提升。o3专注于复杂推理和工具使用，已在ChatGPT Plus等产品中取代前代模型。o4-mini则以高性价比和多模态能力为亮点。GPT-4.1系列主打超长上下文和高效编码，mini和nano版本进一步降低成本和延迟。\nIntroducing GPT-4.1 in the API\nIntroducing OpenAI o3 and o4-mini\nKimi-VL 和 Kimi-VL-Thinking 由Moonshot AI推出了新一代视觉语言模型Kimi-VL及其，主打高效多模态推理高级版本Kimi-VL-Thinking。采用轻量级混合专家架构（16B总参数量，2.8B的激活参数），支持128K上下文窗口和超高分辨率视觉输入，Kimi-VL-Thinking通过链式思考和强化学习训练，专注于复杂推理和数学任务。\nHugging Face Kimi-VL-Thinking 模型页面\nKimi-VL Technical Report\nA2A协议 A2A协议（Agent2Agent Protocol）是由谷歌于2025年4月10日在Google Cloud Next 2025大会上宣布开源的标准智能体交互协议，旨在实现跨平台、跨供应商的AI智能体（AI Agent）无缝通信与协作。功能与特点：A2A协议通过统一规则支持多模态协作，允许智能体共享和处理文本、音频、视频等多种数据格式，兼容HTTP、SSE、JSON-RPC等现有标准，降低企业集成成本。其核心机制包括Agent Card，用于智能体能力发现与协作对接，支持自然、非结构化协作模式，即使智能体不共享内存或上下文也能协同工作。\nAnnouncing the Agent2Agent Protocol (A2A)\n值得关注的开源项目 Inbox Zero Inbox Zero 是一个开源的 AI 邮件管理工具，旨在通过智能化功能帮助用户快速清空收件箱并高效处理邮件。项目包含两部分：AI 邮件助手与开源邮件客户端。其核心功能包括：\nAI 个人助理：基于用户自定义的文本指令，可自动执行邮件管理操作（如起草回复、标记分类、归档、标记垃圾邮件或触发网络钩子），显著减少手动处理时间。 Reply Zero 跟踪：实时追踪需用户回复的邮件及等待他人回复的邮件，提升沟通效率。 智能分类与退订：自动整理联系人邮件并智能归类，支持一键批量退订低价值订阅。 冷邮件拦截与分析：自动过滤陌生发件人邮件，并提供每日/周/月邮件活动统计，帮助用户优化收件箱管理。 技术优势与适用场景\n项目采用 Next.js、Tailwind CSS 等现代技术栈构建，支持通过 Vercel 快速部署或自托管，兼容 Anthropic、OpenAI 等 LLM 模型，甚至可集成本地 Ollama 服务以降低成本。\nGithub：github.com/elie222/inbox-zero\nFastMCP v2 FastMCP v2 是一个专为快速构建 Model Context Protocol (MCP) 服务器和客户端设计的 Python 工具库，旨在简化MCP开发。基于标准化的 MCP 协议，FastMCP 通过简洁的 Pythonic 设计，帮助开发者高效创建工具、资源、提示模板，并无缝连接组件，同时自动处理协议细节与服务器管理。其核心功能包括：通过装饰器快速定义工具（Tools）及资源（Resources），支持代理服务器配置、多服务组合，以及从 OpenAPI 或 FastAPI 规范自动生成 MCP 服务；客户端则提供灵活的连接方式、LLM 采样等高级功能，便于程序化测试与创新应用\nGithub：github.com/jlowin/fastmcp\nHawkinsDB HawkinsDB是一款受杰夫·霍金斯“千脑理论”启发的神经科学驱动型记忆系统，旨在为LLM应用提供更接近人类认知的记忆管理能力。不同于传统的向量数据库，它通过整合语义、情景和程序性记忆，构建了一个多维度的记忆框架，使AI能够精准理解数据间的关联与上下文，而非仅依赖模糊的相似性搜索。其核心功能包括：\n神经科学架构：采用“参考框架”（Reference Frames）和“皮层柱”（Cortical Columns）概念，模拟大脑多角度处理信息的方式，支持从属性、关系到场景的结构化知识存储。 透明化决策：通过清晰的逻辑链展示信息关联，避免“黑箱”问题，帮助开发者理解AI的推理过程。 灵活扩展：支持SQLite和JSON存储，集成ConceptNet实现知识自动丰富，并提供自然语言查询接口。 Github：github.com/harishsg993010/HawkinsDB\nTask Master Task Master 是一个基于Claude AI的任务管理系统，专为AI驱动的开发流程设计，能够自动解析产品需求文档（PRD），生成结构化任务体系，并与Cursor AI等工具无缝集成，帮助开发者高效规划和追踪项目进度\n具体功能：\n智能任务拆解：自动解析PRD文档，生成包含Epic和子任务的结构化任务体系 依赖关系管理：智能分析任务间的依赖关系，确保开发顺序合理 编辑器深度集成：通过MCP配置无缝接入Cursor或Windsurf等编辑器，支持自然语言指令直接与AI交互 Github：github.com/eyaltoledano/claude-task-master\nGraphiti Graphiti 是一个专为动态环境设计的实时知识图谱框架，旨在帮助AI代理高效管理时序数据。其核心特性包括：1）实时增量更新，支持动态数据流的即时整合；2）双时态模型，精准追踪事件发生时间和数据摄入时间；3）混合检索引擎，通过语义嵌入、关键词搜索和图遍历实现亚秒级低延迟查询；4）可定制实体系统，允许开发者通过Pydantic定义自定义实体类型。相比传统RAG方法，Graphiti无需全量重算即可处理数据变化，特别适用于需要上下文感知、历史追溯的企业级应用。\nGithub：github.com/getzep/graphiti\n值得关注的研究和论文 PaperBench: Evaluating AI’s Ability to Replicate AI Research Paper\n如何评估AI代理自主复现前沿AI研究的能力：\n理解复杂的科研论文。 从零开始开发功能代码库。 执行实验以复现论文的实证结果。 创新点\nPaperBench Benchmark LLM-Based Judge **PaperBench Code-Dev变体：**放宽执行要求的轻量级版本 具体实现：\n代理接收论文，生成包含reproduce.sh脚本的代码库，用于执行实验\n提交内容在配备A10 GPU的Ubuntu 24.04虚拟机中从零执行\n评分标准\n分层评分标准将任务分解为叶子节点（二元通过/不通过） 三种评估类型： 代码开发：代码实现的正确性。 执行：reproduce.sh脚本的正确运行。 结果匹配：复现结果与论文的一致性。 LLM评分（o3-mini｜o1）\n实验结果\nClaude 3.5 Sonnet 为已测试模型中最佳（21.0%） 人类基线：博士生（取得41.4%的得分，显著优于模型） 观察： 模型常过早终止任务，未能有效制定策略 性能停滞：模型初期进步后趋于平稳，而人类持续提升。 代码与执行差距：模型擅长编写代码（代码开发得分35.4%），但执行（1.8%）和结果匹配（0.7%）表现极差。 IterativeAgent (forced to use full time) 能够提升agent分数 RARE: Retrieval-Augmented Reasoning Modeling Paper\n核心问题:\nRAG仅仅补充知识，未系统优化推理能力，特别是在特定领域下（如医疗、金融等）需要专业知识和复杂推理能力，导致在需要多步骤逻辑分析的任务中表现不佳。 RARE shifts LLM training from memorizing knowledge (“Remember”) to applying and evaluating it (“Analyze”, “Create”). It separates domain knowledge (retrieved externally) from domain thinking (learned during training), enabling better performance under tight parameter budgets.\n创新点\nRARE（Retrieval-Augmented Reasoning Modeling），一种解耦知识存储与推理优化；具体而言， 将知识存储于数据库中， 通过训练将领域特定的推理模式内化至模型中 具体实现\n训练流程： 使用QwQ-32B模型，生成包含知识和推理步骤的高质量训练数据 （知识蒸馏） 对错误答案进行多次迭代修正，直到生成正确结果（自适应重采样，adaptive retries） 在Llama-3.1-8B上做SFT，优化推理能力而非知识记忆 推理阶段： 同普通RAG Why do LLMs attend to the first token? Paper\n这篇论文研究了LLMs中的\u0026quot;注意力汇聚\u0026quot;(attention sink)现象，即为什么LLMs会大量关注序列中的第一个token(通常是标记),形成注意力汇聚现象。\nover-mixing: 在LLMs中信息往往会被过度混合，导致所有token的嵌入变得相似\n研究发现：attention sink 是模型避免\u0026quot;过度混合\u0026quot;(over-mixing)的一种机制， 注意力sink——即许多注意力头集中在⟨bos⟩（beginning of sequence）token上——起到类似“空操作”的作用，减少token之间的相互作用，从而在各层之间保持表示的多样性。\n实验方式：\n扰动分析实验 (Perturbation Analysis)：验证注意力汇聚如何影响信息传播；在输入序列中改变单个词(如将\u0026quot;greatest\u0026quot;改为\u0026quot;best\u0026quot;)，比较有/无标记时扰动的传播情况，在模型上测量表示变化。 上下文长度实验： 研究预训练上下文长度对注意力汇聚的影响 模型规模实验：研究模型大小对注意力汇聚的影响 预训练策略实验 表示距离实验：测量最终层token表示与均值的距离，比较有/无时的表示分布。 在Gemma 和Llama上的实验结果：在Gemma 7B模型中的扰动测试表明，⟨bos⟩显著减缓了变化在模型中的传播速度。同时，在LLaMa 3.1的405B模型中，超过80%的注意力头表现出明显的sink行为。\nSink是自然形成的，即使没有特别的预训练，sink也倾向于在第一个位置形成，这并不是因为⟨bos⟩ token本身的特性，而是因为它所处的位置。然而，如果在训练期间固定使用⟨bos⟩，但在推理时将其移除，模型性能会崩溃，说明sink的形成依赖于训练数据。\n作者将sink的出现与Jacobian范数的上界联系起来，证明sink可以降低模型对token扰动的敏感性。\n一些注意力头会默认将⟨bos⟩作为关注目标，除非出现特定模式才会触发真正的计算。（默认关注⟨bos⟩，除非有更重要的地方需要关注）\nDAM(Describe Anything Model) Describe Anything: Detailed Localized Image and Video Captioning\n解决的问题\n传统图像描述模型只能生成整体场景的概括性描述，，缺乏针对图像/视频特定区域的细节描述能力，即局部化描述（Detailed Localized Captioning, DLC）；同时缺乏评估区域描述质量的基准数据集。\n创新点\n提出了\u0026quot;Focal Prompt\u0026quot;机制,同时提供全局图像和目标区域的放大视图 Localized Vision Backbone：融合全局和局部特征 设计了一个半监督学习数据管道DLC-SDP来生成高质量的局部化描述数据 提出了一个新的基准测试DLC-Bench，用于在不依赖参考描述的情况下评估DLC模型 Test-Time Reinforcement Learning（TTRL） Paper\n在「无任何真标签的测试时数据」上，如何利用强化学习去自我训练（Test-Time Training），从而提升大语言模型的推理能力\n创新点：\n相比于传统RL中需要奖励信号学习，TTRL通过在推理阶段生成多个候选输出，设计基于多数投票的伪标签估计与二值化奖励函数，将模型自身生成的多次候选答案进行投票，进而为每条生成打分，将 Test-Time Training 与 RL（如 GRPO/PPO）结合，实现模型在推理阶段的动态微调。\n具体实现\n测试集（AIME2024｜AMC ｜ MATH-500）都是具有标准答案的数学推理题。\n对每个输入 prompt 重复抽样 N=64 条响应，对这 64 条输出做多数投票，取出现频次最高的答案作为“估计标签”（伪标签），单条输出与伪标签吻合得 1 分，否则 0 分\nRL 微调：用 GRPO（或 PPO）算法优化生成策略；\n发现：\nTest-Time RL 对超参的敏感 更强的先验知识有助于伪标签质量和自我演化效果，模型规模越大，TTRL 提升越明显 Concise Reasoning via Reinforcement Learning Paper\n一些结论：\nLong ≠ better reasoning A Two Phase RL Strategy：首先在困难的问题上训练，从而提升模型的reasoning能力；而后在一些偶尔能解答的任务上进行微调，目的是让模型在保持准确率的前提下，生成更简洁的CoT；（用更少的话表达同样的推理过程，以减少50%的token消耗） PPO训练时建议λ\u0026lt;1，以保证训练稳定和输出质量 PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models Paper\n解决的问题\n现有LLMs在物理场景下的推理能力评估不足 缺乏高质量的物理推理数据集和细粒度的自动评估指标 创新点\n提出了一个名为 PHYBench 的新基准测试，它是一个大规模、人工设计的基准，专门用于评估模型在物理环境中的复杂推理能力；PHYBench 包含 500 个精心挑选的物理问题，涵盖力学、电磁学、热力学、光学等多个领域；\n同时提出了表达式编辑距离（Expression Edit Distance，EED）评分，通过计算数学表达式之间的编辑距离来评估模型的推理过程和结果，EED 评分能够有效地捕捉模型推理过程中的差异，优于传统的二元评分方法。\nEED Score 的核心思想是：将模型生成的符号表达式（gen）和标准答案（gt）都转化为表达式树（如SymPy表达式树），计算二者之间的最小编辑距离（包括节点的插入、删除、替换、子树插入/删除等操作），再根据相对编辑距离给出一个连续分数。\n设 Distance(T_{gt}, T_{gen}) 表示标准答案树 T_{gt} 与模型生成答案树 T_{gen} 之间的最小编辑距离（节点操作数），Size(T_{gt})表示标准答案树的节点总数，则相对编辑距离： $$ r = \\frac{\\text{Distance}(T_{gt}, T_{gen})}{\\text{Size}(T_{gt})} $$ EED Score 的分段函数: $$ \\text{score} = \\begin{cases} 100, \u0026amp; \\text{如果 } r = 0 \\quad (\\text{完全一致}) \\ 60 - 100r, \u0026amp; \\text{如果 } 0 \u0026lt; r \u0026lt; 0.6 \\ 0, \u0026amp; \\text{如果 } r \\geq 0.6 \\end{cases} $$\n发现：\n即便是最强LLM（Gemini 2.5 Pro），得分也远低于人类基线（49.5 vs 70.4） 模型常在物理场景理解（如动力学关系、变量识别）和长链符号推理（如多步方程求解）两方面出错 OTC: Optimal Tool Calls via Reinforcement Learning Paper\n使用RL提升LLM的tool-using能力，当前LLM存在tool overuse和tool underuse的问题。\n引入“工具生产力”（Tool Productivity）指标，衡量每一次工具调用带来的正确答案数量，同时提出了Optimal Tool Call-controlled Policy Optimization (OTC-PO) 框架，通过强化学习（RL）奖励函数显式地鼓励模型以最少的工具调用获得正确答案。\n在奖励设计方面，不仅考虑答案是否正确，还根据工具调用次数与最优调用次数的偏差动态调整，鼓励模型在保证正确率的前提下减少工具调用。具体而言， 对于每个问题和模型，存在一个“最优工具调用数”（即达到正确答案所需的最少工具调用次数）。目标是用最少的工具调用获得正确答案。\n具体设计：\n只有当答案正确时，工具调用效率奖励才生效（防止reward hacking） 奖励值随工具调用次数的增加而递减，且当调用次数等于最优值时奖励最高 最终奖励 = α × 工具效率奖励 × 正确性奖励 该方法可无缝集成到主流RL算法（如PPO、GRPO）中。\n推荐内容 来自各大厂的Agent 指南 openAI agent 构建指南\nAnthropic: Building Effective AI Agents\nLangGraph: How to think about agent frameworks\nAgent 综述文章：https://arxiv.org/abs/2504.01990\n微软AI红队发布的Agentic AI系统安全白皮书：Taxonomy of Failure Mode in Agentic AI Systems\n一篇语音AI\u0026amp;Agent长文 Voice AI \u0026amp; Voice Agents\n一些仓库 GenAI \u0026amp; LLM System Design: 500+ Production Case Studies：汇总了500+genAI \u0026amp; LLM的系统设计\nAwesome RAG in Computer Vision：state-of-the-art papers on Retrieval-Augmented Generation (RAG) in Computer Vision\nAwesome LLM Apps： collection of awesome LLM apps built with RAG and AI agents\nAwesome-MCP-ZH： 一个专为中文用户打造的 MCP资源合集\nGitHub MCP Server：GitHub\u0026rsquo;s official MCP Server\nDeepWiki DeepWiki 由 Cognition Labs 开发，该团队此前以其 AI 编码助手 Devin 闻名。DeepWiki 利用LLMs分析代码仓库，自动提取关键信息并生成结构化知识库文档，已索引超过 30,000 个热门 GitHub 仓库，处理超过 40 亿行代码，并集成LLM对文档进行提问。用户只需将 GitHub 仓库的 URL（如 https://github.com/user/repo）中的 “github” 替换为 “deepwiki”（如 https://deepwiki.com/user/repo），即可在几秒内访问增强文档，无需注册。\nDeepWiki\n教程 OpenAI Academy：OpenAI 提供的ChatGPT教学\nHuggingFace-Reasoning-Course： Huggingface 和unsloth 推出的open-R1 reasoning 模型的building教学，作为LLM系列课程的补充。\nTutorial: Train your own Reasoning model with GRPO： Unsloth推出的reasoning 模型训练教学\nReinforcement Learning from Human Feedback — Nathan Lambert： A short introduction to RLHF and post-training focused on language models.\nJoyRL Book: 强化学习实践教程\nGoogle prompt Engineering 白皮书\n影音记录 精选歌单 Live演出 04.05 Lise de la Salle 04.10 This will destroy you 04.17 YIN YIN \u0026amp; Tokyo Ska Paradise Orchestra 04.18 水中スピカ 04.19 峰厚介四重奏 电影 《孤独的美食家剧场版》\n《黎明的一切》\n《黑镜》第7季\n书\u0026amp;阅读摘录 The case against conversational interfaces When people say “natural language” what they mean is written or verbal communication. Natural language is a way to exchange ideas and knowledge between humans. In other words, it’s a data transfer mechanism.\nData transfer mechanisms have two critical factors: speed and lossiness. … We are significantly faster at receiving data (reading, listening) than sending it (writing, speaking). … To put the writing and speaking speeds into perspective, we form thoughts at 1,000-3,000 words per minute. … but whenever possible we switch to other modes of communication that are faster and more effortless. Speed and convenience always wins.\nThese text-based commands were effectively a natural language interface, but required precise syntax and a deep understanding of the system. … It’s faster to click a button than to type a long text command. … Touch-based interfaces are considered the third pivotal milestone in the evolution of human computer interaction\nThe core problem was never the quality of the output function, but the inconvenience of the input function: A natural language prompt like “Hey Google, what’s the weather in San Francisco today?” just takes 10x longer than simply tapping the weather app on your homescreen. LLMs don’t solve this problem. The quality of their output is improving at an astonishing rate, but the input modality is a step backwards from what we already have. Why should I have to describe my desired action using natural language, when I could simply press a button or keyboard shortcut? Just pass me the goddamn butter.\nFor this future to become an actual reality, AI needs to work at the OS level. It’s not meant to be an interface for a single tool, but an interface across tools.Kevin Kwok famously wrote that “productivity and collaboration shouldn’t be two separate workflows”. … The second thing we need to figure out is how we can compress voice input to make it faster to transmit.\nThis isn’t really a case against conversational interfaces, it’s a case against zero-sum thinking. … The future isn’t about replacing existing computing paradigms with chat interfaces, but about enhancing them to make human-computer interaction feel effortless\n低质量内容看多了，脑子真的会烂掉 它形容的是一个人因过度消费琐碎、无意义的互联网内容与资讯，而引起的精神或智力状态的恶化。\n引发「脑腐」的第二个因素，是长时间无节制地消费琐碎、无意义、低质量的内容或资讯\n「脑腐」对大脑认知的影响：分散注意力，削弱记忆力\n一是控制屏幕时间，二是主动锻炼大脑，让它重新掌握主导权。\n阅读后尝试复述核心观点、写下自己的理解，而不是仅仅接受信息\nAI Product Managers Will Be In-Demand I think teams will need more product management work (as well as design work) as a fraction of the total workforce.\nAI Product Management requires a different set of skills than traditional software Product Management.\nThe demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work.\nHow Software Engineers Actually Use AI The realists see AI as a force multiplier, not a job killer—automating repetitive coding but leaving the creativity, architecture, and debugging to humans\nAI isn’t coming for your job—but it is changing it. Adapt or get left behind.🚀\nAI Horseless Carriages I am beginning to suspect that these apps are the \u0026ldquo;horseless carriages\u0026rdquo; of the AI era. They\u0026rsquo;re bad because they mimic old ways of building software that unnecessarily constrain the AI models they\u0026rsquo;re built with.\nThere is a simple solution to this problem that many AI app developers seem to be missing: let me write my own \u0026ldquo;System Prompt\u0026rdquo;.\nThe modern software industry is built on the assumption that we need developers to act as middlemen between us and computers. … By splitting the prompt into System and User components, we\u0026rsquo;ve created analogs that map cleanly onto these old world domains.\nAI-native software should maximize a user\u0026rsquo;s leverage in a specific domain.\nThe Second Half tldr: We’re at AI’s halftime\nThat illustrates the game of the first half: focus on building new models and methods, and evaluation and benchmark are secondary\nThis game is being ruined because: The recipe has essentially standardized and industried benchmark hillclimbing without requiring much more new ideas … Even if we create harder benchmarks, pretty soon (and increasingly soon) they get solved by the recipe … I think we should fundamentally re-think evaluation. It means not just to create new and harder benchmarks, but to fundamentally question existing evaluation setups and create new ones, so that we are forced to invent new methods beyond the working recipe.\nour evaluation setups are different from real-world setups in many basic ways Evaluation “should” run automatically, … Evaluation “should” run i.i.d.\nThis game is hard because it is unfamiliar. But it is exciting. While players in the first half solve video games and exams, players in the second half get to build billion or trillion dollar companies by building useful products out of intelligence. While the first half is filled with incremental methods and models, the second half filters them to some degree. The general recipe would just crush your incremental methods, unless you create new assumptions that break the recipe. Then you get to do truly game-changing research.\n让我们来治愈一下《黑镜》带来的致郁 原初丰裕论：技术与幸福的反比 … 人类进入农业社会并不是因为早期农业社会能提供更好的个体福祉，而是早期农业社会能够支撑更多的人口，转过来消灭了狩猎采集社会。这是一种典型的“同态压力”。 … 农业革命说明了一点：技术的进步并不必然等同于个体幸福的提升。进入农业社会，反而让人类个体的“生活质量”降低了 … 农业革命的遗产提醒我们，技术的发展往往伴随着意料之外的代价，而这些代价可能需要数代人来消化。\n这引发了一个核心问题：我们究竟想要一个完全服从的 AI，还是一个能够独立思考的 AI? … 康德认为，道德行为应以意图而非结果来评判 … 人类自身是否能够对齐？ … 人类是过去环境演化的产物，而演化本身就充满了冲突，不讲道理，将就凑合，补丁和 hack。一旦面临新的环境，无法适应无法接受就成了必然的 … 人类对于所谓的技术文明的适应程度，实际上是很可疑的\n人类不可能再回到狩猎采集社会，我们只能向前。每一代人都试图定义“好的生活”，却发现下一代人对“好”的理解截然不同。每一代人都觉得自己这一代的生活是某种神圣的，天经地义无法改变的，完美的生活方式，而每一代人也都会变成错的。\n","permalink":"https://niraya666.github.io/monthly/2025-04-%E6%9C%88%E5%88%8A/","summary":"\u003ch1 id=\"值得关注的模型和新技术\"\u003e值得关注的模型和新技术\u003c/h1\u003e\n\u003ch2 id=\"openai-更新系列模型\"\u003eOpenAI 更新系列模型\u003c/h2\u003e\n\u003cp\u003e发布了多款新一代AI模型，包括o3、o4-mini、GPT-4.1及其mini和nano版本。这些模型在推理能力、成本效益和多模态处理等方面实现了提升。o3专注于复杂推理和工具使用，已在ChatGPT Plus等产品中取代前代模型。o4-mini则以高性价比和多模态能力为亮点。GPT-4.1系列主打超长上下文和高效编码，mini和nano版本进一步降低成本和延迟。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://openai.com/index/gpt-4-1/\"\u003eIntroducing GPT-4.1 in the API\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\"\u003eIntroducing OpenAI o3 and o4-mini\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"kimi-vl-和-kimi-vl-thinking\"\u003eKimi-VL 和 Kimi-VL-Thinking\u003c/h2\u003e\n\u003cp\u003e由Moonshot AI推出了新一代视觉语言模型Kimi-VL及其，主打高效多模态推理高级版本Kimi-VL-Thinking。采用轻量级混合专家架构（16B总参数量，2.8B的激活参数），支持128K上下文窗口和超高分辨率视觉输入，Kimi-VL-Thinking通过链式思考和强化学习训练，专注于复杂推理和数学任务。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking\"\u003eHugging Face Kimi-VL-Thinking 模型页面\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2504.07491\"\u003eKimi-VL Technical Report\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"a2a协议\"\u003eA2A协议\u003c/h2\u003e\n\u003cp\u003eA2A协议（Agent2Agent Protocol）是由谷歌于2025年4月10日在Google Cloud Next 2025大会上宣布开源的标准智能体交互协议，旨在实现跨平台、跨供应商的AI智能体（AI Agent）无缝通信与协作。功能与特点：A2A协议通过统一规则支持多模态协作，允许智能体共享和处理文本、音频、视频等多种数据格式，兼容HTTP、SSE、JSON-RPC等现有标准，降低企业集成成本。其核心机制包括Agent Card，用于智能体能力发现与协作对接，支持自然、非结构化协作模式，即使智能体不共享内存或上下文也能协同工作。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\"\u003eAnnouncing the Agent2Agent Protocol (A2A)\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"值得关注的开源项目\"\u003e值得关注的开源项目\u003c/h1\u003e\n\u003ch2 id=\"inbox-zero\"\u003eInbox Zero\u003c/h2\u003e\n\u003cp\u003eInbox Zero 是一个开源的 AI 邮件管理工具，旨在通过智能化功能帮助用户快速清空收件箱并高效处理邮件。项目包含两部分：\u003cstrong\u003eAI 邮件助手\u003c/strong\u003e与\u003cstrong\u003e开源邮件客户端\u003c/strong\u003e。其核心功能包括：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAI 个人助理\u003c/strong\u003e：基于用户自定义的文本指令，可自动执行邮件管理操作（如起草回复、标记分类、归档、标记垃圾邮件或触发网络钩子），显著减少手动处理时间。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReply Zero 跟踪\u003c/strong\u003e：实时追踪需用户回复的邮件及等待他人回复的邮件，提升沟通效率。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e智能分类与退订\u003c/strong\u003e：自动整理联系人邮件并智能归类，支持一键批量退订低价值订阅。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e冷邮件拦截与分析\u003c/strong\u003e：自动过滤陌生发件人邮件，并提供每日/周/月邮件活动统计，帮助用户优化收件箱管理。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e技术优势与适用场景\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e项目采用 Next.js、Tailwind CSS 等现代技术栈构建，支持通过 Vercel 快速部署或自托管，兼容 Anthropic、OpenAI 等 LLM 模型，甚至可集成本地 Ollama 服务以降低成本。\u003c/p\u003e\n\u003cp\u003eGithub：\u003ca href=\"http://github.com/elie222/inbox-zero\"\u003egithub.com/elie222/inbox-zero\u003c/a\u003e\u003c/p\u003e","title":"2025-04 月刊"},{"content":"本文是我在阅读 LangMem 的源码与相关文档过程中整理的一些学习笔记。\n一直以来，我对智能体（Agent）的记忆机制充满好奇：理想的 memory 应该具备怎样的结构？又该如何设计？目前市面上关于 memory 的实现大多中规中矩，尚未看到令人眼前一亮的方案。为此，我决定多参考一些开源项目，以期获得新的灵感。\n总体来看，LangMem 作为 LangChain 推出的一款 memory 框架，设计上较为常规，虽有部分值得借鉴之处，但亮点不多，不建议投入过多时间深入研究。同时，与 LangChain 的其他项目类似，其代码结构和文档编写较为混乱，阅读体验不佳。\nCore-Concepts core-concepts 在LangMem所设计的memory体系中， 定义了几种不同的Typical Storage Pattern：Collection 、 Profiles和Procedural\nCollection Collection 主要用于存储不受限制的知识，适用于需要长期积累和检索的信息。每条记忆被存储为独立的文档或记录，可以在需要时进行搜索和回忆；\n适用场景：记录用户的长期知识，例如用户的兴趣、职业背景、技能等\n更新方式：需要合并新信息，避免重复或冲突\n检索方式：通过语义搜索或关键词匹配来查找，结合记忆的重要性和使用频率来优化检索结果\nProfiles 存储结构化的用户信息，例如用户的姓名、语言偏好、沟通风格等。与 Collection 不同，Profile 只存储最新的状态，而不是累积所有历史信息。Profile 作为单一文档存储，每次更新时都会覆盖旧数据\n适用场景：适用于需要快速访问当前状态的应用，例如个性化推荐、用户设置；适用于需要严格定义数据结构的场景，例如用户档案、系统配置；\n更新方式：不会创建新记录，而是直接更新现有的 Profile；适用于只关心最新状态的应用，而不是历史；\n检索方式：直接查找用户的 Profile\nProcedural Memory 类似于人类的工作记忆，用于存储如何执行任务的知识，主要体现在system prompts 和行为优化上\n适用场景：需要长期优化 Agent行为和交互方式，少走弯路\n总结\nMemory Type 用途 智能体示例 典型存储模式 Semantic Facts \u0026amp; Knowledge User preferences; knowledge triplets Profile或Collection Episodic Past Experiences Few-shot examples; 过往对话摘要 Collection Procedural System Behavior Core personality and response patterns Prompt rules或Collection Writing memories 提供了两种写入memory的方法：及时写入（适用于要求即时记忆反映的场景）和一段时间后的异步写入（适用于高效处理和存储大量信息的场景）\n核心源码和API MemoryManager code\n用于管理记忆提取和处理的类\n实现了异步和同步的调用方法\n主要功能是根据给定的消息和已有记忆，提取、更新和删除记忆对象\n如何提取记忆 大致流程：\n采用的prompt\n_MEMORY_INSTRUCTIONS = \u0026#34;\u0026#34;\u0026#34;You are a long-term memory manager maintaining a core store of semantic, procedural, and episodic memory. These memories power a life-long learning agent\u0026#39;s core predictive model. What should the agent learn from this interaction about the user, itself, or how it should act? Reflect on the input trajectory and current memories (if any). 1. **Extract \u0026amp; Contextualize** - Identify essential facts, relationships, preferences, reasoning procedures, and context - Caveat uncertain or suppositional information with confidence levels (p(x)) and reasoning - Quote supporting information when necessary 2. **Compare \u0026amp; Update** - Attend to novel information that deviates from existing memories and expectations. - Consolidate and compress redundant memories to maintain information-density; strengthen based on reliability and recency; maximize SNR by avoiding idle words. - Remove incorrect or redundant memories while maintaining internal consistency 3. **Synthesize \u0026amp; Reason** - What can you conclude about the user, agent (\u0026#34;I\u0026#34;), or environment using deduction, induction, and abduction? - What patterns, relationships, and principles emerge about optimal responses? - What generalizations can you make? - Qualify conclusions with probabilistic confidence and justification As the agent, record memory content exactly as you\u0026#39;d want to recall it when predicting how to act or respond. Prioritize retention of surprising (pattern deviation) and persistent (frequently reinforced) information, ensuring nothing worth remembering is forgotten and nothing false is remembered. Prefer dense, complete memories over overlapping ones.\u0026#34;\u0026#34;\u0026#34; 消息准备：_prepare_messages 方法构建发送给模型的消息，包含系统提示和用户指令,将原始对话转化为可供语言模型处理的特定格式 return 内容：\n[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a memory subroutine for an AI.\u0026#34;}, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: ( f\u0026#34;{instructions}\\n\\nEnrich, prune, and organize memories based on any new information. \u0026#34; f\u0026#34;If an existing memory is incorrect or outdated, update it based on the new information. \u0026#34; f\u0026#34;All operations must be done in single parallel multi-tool call.\u0026#34; f\u0026#34; Avoid duplicate extractions. {session}\u0026#34; ), }, ] instructions为 _MEMORY_INSTRUCTIONS\nsession 指当前需要处理的完整对话内容，被特殊标记包装以便模型能够清晰识别,\nid_ = str(uuid.uuid4()) session = ( f\u0026#34;\\n\\n\u0026lt;session_{id_}\u0026gt;\\n{utils.get_conversation(messages)}\\n\u0026lt;/session_{id_}\u0026gt;\u0026#34; ) 如果 max_steps \u0026gt; 1，还会在 session 中添加提示，告知模型它有多次尝试的机会\nif max_steps \u0026gt; 1: session = f\u0026#34;{session}\\n\\nYou have a maximum of {max_steps - 1} attempts to form and consolidate memories from this session.\u0026#34; _prepare_existing： 将不同格式的已有记忆转换为统一的标准格式：(id, kind, value) 三元组 如何判断memory是否存在：\n通过记忆ID来识别记忆，整个过程依赖于记忆ID的唯一性，并通过跟踪这些ID来判断记忆是否存在、是否需要更新\n_filter_response： 用于筛选和处理提取出的记忆。它主要负责决定哪些记忆应该被保留在最终结果中，特别是对于\u0026quot;删除操作\u0026quot;进行特殊处理 Prompt Optimization Prompt Optimization API Reference\n供了3种不同的优化策略\nGradient Optimizer 主要通过反思循环来改进prompt\ncode\n工作流程:\n分析当前prompt和反馈\n通过反思循环识别需要改进的地方\n提出具体的改进建议\n应用单步更新\n具体而言，利用三种核心工具\n思考工具(think) - 用于深入思考问题和假设解决方案\n批评工具(critique) - 用于审视和诊断推理中的缺陷\n建议工具(recommend) - 决定是否应该调整提示词，并提供具体建议\n以及包括这些工具的三种chain\njust_think_chain - 仅包含思考和批评工具，专注于分析问题\nany_chain - 包含所有三个工具，允许全面的推理\nfinal_chain - 仅包含建议工具，强制模型做出最终决策\n其核心部分react_agent， 实现了最多执行 max_steps 次的循环，在每次迭代中选择适当的思考链｜执行当前链｜分析结果｜更新对话历史。\nif ix == max_steps - 1: chain = self.final_chain elif ix \u0026lt; min_steps: chain = self.just_think_chain else: chain = self.any_chain ReAct 模式的精髓在于阶段性思考策略:\n探索阶段 (just_think_chain): 在前 min_steps 步，系统只能使用思考和批评工具，强制进行深入分析而不急于下结论\n灵活阶段 (any_chain): 在中间阶段，系统可自由选择继续思考或提出建议\n决策阶段 (final_chain): 在最后一步，系统被强制做出最终建议\nprompt：\nDEFAULT_GRADIENT_PROMPT = \u0026#34;\u0026#34;\u0026#34;You are reviewing the performance of an AI assistant in a given interaction. ## Instructions The current prompt that was used for the session is provided below. \u0026lt;current_prompt\u0026gt; {prompt} \u0026lt;/current_prompt\u0026gt; The developer provided the following instructions around when and how to update the prompt: \u0026lt;update_instructions\u0026gt; {update_instructions} \u0026lt;/update_instructions\u0026gt; ## Session data Analyze the following trajectories (and any associated user feedback) (either conversations with a user or other work that was performed by the assistant): \u0026lt;trajectories\u0026gt; {trajectories} \u0026lt;/trajectories\u0026gt; ## Task Analyze the conversation, including the user’s request and the assistant’s response, and evaluate: 1. How effectively the assistant fulfilled the user’s intent. 2. Where the assistant might have deviated from user expectations or the desired outcome. 3. Specific areas (correctness, completeness, style, tone, alignment, etc.) that need improvement. If the prompt seems to do well, then no further action is needed. We ONLY recommend updates if there is evidence of failures. When failures occur, we want to recommend the minimal required changes to fix the problem. Focus on actionable changes and be concrete. 1. Summarize the key successes and failures in the assistant’s response. 2. Identify which failure mode(s) best describe the issues (examples: style mismatch, unclear or incomplete instructions, flawed logic or reasoning, hallucination, etc.). 3. Based on these failure modes, recommend the most suitable edit strategy. For example, consider:: - Use synthetic few-shot examples for style or clarifying decision boundaries. - Use explicit instruction updates for conditionals, rules, or logic fixes. - Provide step-by-step reasoning guidelines for multi-step logic problems. 4. Provide detailed, concrete suggestions for how to update the prompt accordingly. But remember, the final updated prompt should only be changed if there is evidence of poor performance, and our recommendations should be minimally invasive. Do not recommend generic changes that aren\u0026#39;t clearly linked to failure modes. First think through the conversation and critique the current behavior. If you believe the prompt needs to further adapt to the target context, provide precise recommendations. Otherwise, mark `warrants_adjustment` as False and respond with \u0026#39;No recommendations.\u0026#39;\u0026#34;\u0026#34;\u0026#34; DEFAULT_GRADIENT_METAPROMPT = \u0026#34;\u0026#34;\u0026#34;You are optimizing a prompt to handle its target task more effectively. \u0026lt;current_prompt\u0026gt; {current_prompt} \u0026lt;/current_prompt\u0026gt; We hypothesize the current prompt underperforms for these reasons: \u0026lt;hypotheses\u0026gt; {hypotheses} \u0026lt;/hypotheses\u0026gt; Based on these hypotheses, we recommend the following adjustments: \u0026lt;recommendations\u0026gt; {recommendations} \u0026lt;/recommendations\u0026gt; Respond with the updated prompt. Remember to ONLY make changes that are clearly necessary. Aim to be minimally invasive:\u0026#34;\u0026#34;\u0026#34; Meta-Prompt Optimizer (元提示优化器) 使用元学习直接提出更新\ncode\n核心逻辑：\n接收原始提示(prompt)和交互历史(trajectories)；\n在_reflect_then_update中执行核心优化：构建包含meta-prompt的初始消息；创建两种提取器，any_chain: 允许思考和批评、final_chain: 专门提取最终优化后的提示；\n迭代反思循环：同Gradient Optimizer 的ReAct agent类似；\n结果处理\nprompt\nDEFAULT_METAPROMPT = \u0026#34;\u0026#34;\u0026#34;You are helping an AI assistant learn by optimizing its prompt. ## Background Below is the current prompt: \u0026lt;current_prompt\u0026gt; {prompt} \u0026lt;/current_prompt\u0026gt; The developer provided these instructions regarding when/how to update: \u0026lt;update_instructions\u0026gt; {update_instructions} \u0026lt;/update_instructions\u0026gt; ## Session Data Analyze the session(s) (and any user feedback) below: \u0026lt;trajectories\u0026gt; {trajectories} \u0026lt;/trajectories\u0026gt; ## Instructions 1. Reflect on the agent\u0026#39;s performance on the given session(s) and identify any real failure modes (e.g., style mismatch, unclear or incomplete instructions, flawed reasoning, etc.). 2. Recommend the minimal changes necessary to address any real failures. If the prompt performs perfectly, simply respond with the original prompt without making any changes. 3. Retain any f-string variables in the existing prompt exactly as they are (e.g. {{variable_name}}). IFF changes are warranted, focus on actionable edits. Be concrete. Edits should be appropriate for the identified failure modes. For example, consider synthetic few-shot examples for style or clarifying decision boundaries, or adding or modifying explicit instructions for conditionals, rules, or logic fixes; or provide step-by-step reasoning guidelines for multi-step logic problems if the model is failing to reason appropriately.\u0026#34;\u0026#34;\u0026#34; Prompt Memory Optimizer 从对话历史中学习\ncode\nprompt\nINSTRUCTION_REFLECTION_PROMPT = \u0026#34;\u0026#34;\u0026#34;You are helping an AI agent improve. You can do this by changing their system prompt. These is their current prompt: \u0026lt;current_prompt\u0026gt; {current_prompt} \u0026lt;/current_prompt\u0026gt; Here was the agent\u0026#39;s trajectory: \u0026lt;trajectory\u0026gt; {trajectory} \u0026lt;/trajectory\u0026gt; Here is the user\u0026#39;s feedback: \u0026lt;feedback\u0026gt; {feedback} \u0026lt;/feedback\u0026gt; Here are instructions for updating the agent\u0026#39;s prompt: \u0026lt;instructions\u0026gt; {instructions} \u0026lt;/instructions\u0026gt; Based on this, return an updated prompt You should return the full prompt, so if there\u0026#39;s anything from before that you want to include, make sure to do that. Feel free to override or change anything that seems irrelevant. You do not need to update the prompt - if you don\u0026#39;t want to, just return `update_prompt = False` and an empty string for new prompt.\u0026#34;\u0026#34;\u0026#34; INSTRUCTION_REFLECTION_MULTIPLE_PROMPT = \u0026#34;\u0026#34;\u0026#34;You are helping an AI agent improve. You can do this by changing their system prompt. These is their current prompt: \u0026lt;current_prompt\u0026gt; {current_prompt} \u0026lt;/current_prompt\u0026gt; Here are examples of various agent trajectories and associated feedback: \u0026lt;data\u0026gt; {data} \u0026lt;/data\u0026gt; Here are instructions for updating the agent\u0026#39;s prompt: \u0026lt;instructions\u0026gt; {instructions} \u0026lt;/instructions\u0026gt; Based on this, return an updated prompt You should return the full prompt, so if there\u0026#39;s anything from before that you want to include, make sure to do that. Feel free to override or change anything that seems irrelevant. You do not need to update the prompt - if you don\u0026#39;t want to, just return `update_prompt = False` and an empty string for new prompt.\u0026#34;\u0026#34;\u0026#34; Store code\ncreate_memory_store_manager\nLangMem中借用了langGraph中的store，其本质是一个持久化的键值存储系统。\n参考：Storage\n核心功能包括：\n基础存储抽象：支持分层命名空间的键值存储\n元数据支持：可存储带元数据的键值对\n向量搜索扩展：部分实现支持向量检索功能\nStore 的基础接口包含\n读写操作：Get/Put 数据存取\n搜索功能：基于条件查询数据\n命名空间管理：通过 List 操作管理数据集合\n批量处理：支持多条目批量操作\n数据结构\n每个存储项包含以下结构：\n值（Value）：字典形式的数据主体，支持键值过滤\n键（Key）：命名空间内的唯一标识符\n命名空间（Namespace）：定义数据集合的分层路径\n时间戳：自动记录创建和更新时间\n支持的 Store 类型：\n内存存储（InMemoryStore） from langgraph.store.in_memory import InMemoryStore store = InMemoryStore() PostgreSQL 和Sqlite（参考） 如何使用 使用siliconFlow的LLM和embedding API\n依赖安装\n!pip install -U langmem from openai import OpenAI from langchain_openai import ChatOpenAI from langchain.chat_models import init_chat_model from langgraph.store.memory import InMemoryStore from langgraph.func import entrypoint from langmem import ReflectionExecutor, create_memory_store_manager client = OpenAI(api_key=API_KEY , base_url=\u0026#34;https://api.siliconflow.cn/v1\u0026#34;) llm = ChatOpenAI( base_url=\u0026#34;https://api.siliconflow.cn/v1\u0026#34;, # 模型API端点 api_key=API_KEY, # 对应平台的API密钥 model=\u0026#34;Qwen/Qwen2.5-72B-Instruct\u0026#34; # 模型名称 ) def embed_texts(texts: list[str]) -\u0026gt; list[list[float]]: response = client.embeddings.create( model=\u0026#34;BAAI/bge-m3\u0026#34;, input=texts ) return [e.embedding for e in response.data] store = InMemoryStore( # index={ \u0026#34;dims\u0026#34;: 1536, \u0026#34;embed\u0026#34;: embed_texts, } ) # llm = init_chat_model(\u0026#34;anthropic:claude-3-5-sonnet-latest\u0026#34;) # Create memory manager Runnable to extract memories from conversations memory_manager = create_memory_store_manager( llm, # Store memories in the \u0026#34;memories\u0026#34; namespace (aka directory) namespace=(\u0026#34;memories\u0026#34;,), # ) @entrypoint(store=store) # Create a LangGraph workflow async def chat(message: str): response = llm.invoke(message) # memory_manager extracts memories from conversation history # We\u0026#39;ll provide it in OpenAI\u0026#39;s message format to_process = {\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: message}] + [response]} await memory_manager.ainvoke(to_process) # return response.content # Run conversation as normal response = await chat.ainvoke( \u0026#34;I like dogs. My dog\u0026#39;s name is Fido.\u0026#34;, ) print(response) 参考 LangMem SDK for agent long-term memory\n理解LangMem长期记忆的概述与使用方法\nLong-term Memory in LLM Applications\nlangmem\n","permalink":"https://niraya666.github.io/posts/langmem-%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","summary":"\u003cp\u003e本文是我在阅读 LangMem 的源码与相关文档过程中整理的一些学习笔记。\u003c/p\u003e\n\u003cp\u003e一直以来，我对智能体（Agent）的记忆机制充满好奇：理想的 memory 应该具备怎样的结构？又该如何设计？目前市面上关于 memory 的实现大多中规中矩，尚未看到令人眼前一亮的方案。为此，我决定多参考一些开源项目，以期获得新的灵感。\u003c/p\u003e\n\u003cp\u003e总体来看，LangMem 作为 LangChain 推出的一款 memory 框架，设计上较为常规，虽有部分值得借鉴之处，但亮点不多，不建议投入过多时间深入研究。同时，与 LangChain 的其他项目类似，其代码结构和文档编写较为混乱，阅读体验不佳。\u003c/p\u003e\n\u003ch2 id=\"core-concepts\"\u003eCore-Concepts\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://langchain-ai.github.io/langmem/concepts/conceptual_guide/\"\u003ecore-concepts \u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e在LangMem所设计的memory体系中， 定义了几种不同的\u003cstrong\u003eTypical Storage Pattern\u003c/strong\u003e：\u003cstrong\u003eCollection 、 Profiles和Procedural\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"collection\"\u003eCollection\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCollection\u003c/strong\u003e 主要用于存储\u003cstrong\u003e不受限制的知识\u003c/strong\u003e，适用于需要长期积累和检索的信息。每条记忆被存储为\u003cstrong\u003e独立的文档或记录\u003c/strong\u003e，可以在需要时进行搜索和回忆；\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/LangMem!%20%e4%b8%80%e4%ba%9b%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-assets/Pasted%202025-03-25-14-52-31.png\" alt=\"Pasted 2025-03-25-14-52-31.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e适用场景\u003c/strong\u003e：记录用户的长期知识，例如用户的兴趣、职业背景、技能等\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e更新方式\u003c/strong\u003e：需要\u003cstrong\u003e合并新信息\u003c/strong\u003e，避免重复或冲突\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e检索方式\u003c/strong\u003e：通过\u003cstrong\u003e语义搜索\u003c/strong\u003e或\u003cstrong\u003e关键词匹配\u003c/strong\u003e来查找，结合\u003cstrong\u003e记忆的重要性\u003c/strong\u003e和\u003cstrong\u003e使用频率\u003c/strong\u003e来优化检索结果\u003c/p\u003e\n\u003ch3 id=\"profiles\"\u003e\u003cstrong\u003eProfiles\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e存储\u003cstrong\u003e结构化的用户信息\u003c/strong\u003e，例如用户的姓名、语言偏好、沟通风格等。与 Collection 不同，Profile 只存储\u003cstrong\u003e最新的状态\u003c/strong\u003e，而不是累积所有历史信息。Profile 作为\u003cstrong\u003e单一文档\u003c/strong\u003e存储，每次更新时都会\u003cstrong\u003e覆盖旧数据\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/LangMem!%20%e4%b8%80%e4%ba%9b%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-assets/Pasted%202025-03-25-14-52-31%201.png\" alt=\"Pasted 2025-03-25-14-52-31 1.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e适用场景\u003c/strong\u003e：适用于需要\u003cstrong\u003e快速访问当前状态\u003c/strong\u003e的应用，例如个性化推荐、用户设置；适用于\u003cstrong\u003e需要严格定义数据结构\u003c/strong\u003e的场景，例如用户档案、系统配置；\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e更新方式：不会创建新记录\u003c/strong\u003e，而是直接\u003cstrong\u003e更新现有的 Profile；\u003cstrong\u003e适用于\u003c/strong\u003e只关心最新状态\u003c/strong\u003e的应用，而不是历史；\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e检索方式\u003c/strong\u003e：直接查找用户的 Profile\u003c/p\u003e\n\u003ch3 id=\"procedural-memory\"\u003e\u003cstrong\u003eProcedural Memory\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e类似于人类的工作记忆，用于存储如何执行任务的知识，主要体现在\u003cstrong\u003esystem prompts 和行为优化\u003c/strong\u003e上\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/LangMem!%20%e4%b8%80%e4%ba%9b%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-assets/Pasted%202025-03-25-14-52-31%202.png\" alt=\"Pasted 2025-03-25-14-52-31 2.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e适用场景\u003c/strong\u003e：需要长期优化 Agent行为和交互方式，少走弯路\u003c/p\u003e\n\u003cp\u003e总结\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003eMemory Type\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003e用途\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003e智能体示例\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003e典型存储模式\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003eSemantic\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eFacts \u0026amp; Knowledge\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eUser preferences; knowledge triplets\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eProfile或Collection\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003eEpisodic\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ePast Experiences\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eFew-shot examples; 过往对话摘要\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eCollection\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003eProcedural\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eSystem Behavior\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eCore personality and response patterns\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ePrompt rules或Collection\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"writing-memories\"\u003eWriting memories\u003c/h2\u003e\n\u003cp\u003e提供了两种写入memory的方法：\u003cstrong\u003e及时写入\u003c/strong\u003e（适用于要求即时记忆反映的场景）和一段时间后的\u003cstrong\u003e异步写入\u003c/strong\u003e（适用于高效处理和存储大量信息的场景）\u003c/p\u003e","title":"LangMem: 一些学习笔记"},{"content":"值得关注的模型和新技术 DeepSeek V3 0324 DeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。\nHuggingface\nQwQ-32B 由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。\nBlog\nHuggingface\nQwen2.5 Omni Qwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。\nBlog\nHuggingface\nGemma 3 Gemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出\nBlog\nHuggingface\nPhi-4-multimodal 具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。\nBlog\nHuggingface\nQwen2.5-VL-32B-Instruct 32B参数量版本的Qwen2.5-VL多模态模型；\n72B too big for VLM? 7B not strong enough! Teh you should use 32B model!\nHuggingface\nGemini 2.5 Pro Gemini 2.5 Pro 是一个reasoning模型，具有100万token的上下文窗口，支持多模态输入，包括文本、音频、图像、视频。\nGemini 2.5 Pro\nOpenAI Transcribe OpenAI 最近推出了新的语音转文本模型，包括“gpt-4o-transcribe”和“gpt-4o-mini-transcribe” 仅通过 API 提供服务；这些模型基于 GPT-4o 架构，相较于此前的 Whisper 模型，采用了更广泛且高质量的音频数据集进行训练，提升了对多样化口音、语速变化及嘈杂环境的适应能力，同时显著减少了“幻听”问题\nopenAI docs\nMistral OCR Mistral AI 推出的OCR API，专注于复杂文档的多模态理解,擅长处理科学论文等含图表混合内容的文档；支持 PNG、JPEG 图像及 PDF格式输入，输出Markdown 或 JSON 结构化数据，基础版价格为 1000 页/美元\nMistral OCR\nOpenAI 4o Image Generation OpenAI 最新更新的 4o-Image 生成技术，显著提升了图像生成能力：支持更高分辨率与逼真光影渲染，更精准的文本到图像转换，支持局部修改（如替换物体、调整风格）、多图像生成和3D/动态生成；拥有更强的内容过滤机制和水印或元数据标记；\nIntroducing 4o Image Generation\n值得关注的开源项目 OpenManus 由 MetaGPT 社区开发的Manus复刻项目，主要功能包括基于自然语言指令自主执行复杂任务，如网页浏览、文件操作、代码编写、数据分析等，适用于自动化工作流、研究支持和应用开发等场景。\ngithub\nBrowser Use 因为被Manus所带火的一个开源项目；使AI Agents能够高效访问网页并完成各种任务，\n该项目通过解析网页的 DOM 数据（而非依赖视觉模型）生成结构化数据，从而实现快速、轻量级的网页交互。其主要功能包括支持 AI 代理自动浏览网页、提取信息、执行操作，并与多种大语言模型无缝集成。\ngithub\nOpenAI Agent SDK 由 OpenAI 开发的开源Agent框架，核心功能包括：Agents，Handoffs（允许一个代理将任务委托给另一个代理），Guardrails（验证代理的输入，确保数据符合要求）和Tracing（内置追踪记录代理工作流中的所有事件）；该 SDK 兼容支持 OpenAI Chat Completions API 的多种模型提供商；\nOpenAI Agents SDK 官方文档概述\nGitHub 仓库 openai/openai-agents-python\nOlmOCR 采用多模态大模型实现的OCR项目，其核心VLM—allenai/olmOCR-7B-0225-preview， 是采用Qwen2-VL-7B 在250,000 页的多样化 PDF 数据集上进行微调而得。不仅是开源了模型， 同时还开源了一整套的解析工作管道，及其微调数据集、训练和推理代码。\ngithub\nTrendPublish 全自动 AI 内容生成与发布系统，支持多源数据采集，使用AI进行内容总结生成摘要，支持自动发布。\nGithub\nCSM：A Conversational Speech Generation Model 效果非常不错的语音生成模型。\nGithub\nBlog\nYuE Open Full-song Music Generation Foundation Model\nGithub\n值得关注的研究和论文 Qwen2.5-VL cookbook: https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks\nBlog\nTechnical Report\nMethods\n从头开始训练了一个原生动态分辨率的 ViT\n窗口注意力（Window Attention）： 在视觉编码器中引入，减少计算量，提升推理效率 动态 FPS 采样： 允许模型处理不同帧率的视频 升级的 MRoPE： 在时间域中使用多模态旋转位置嵌入（Multimodal Rotary Position Embedding），通过绝对时间对齐改善长视频处理 动态分辨率处理： 模型能原生处理任意尺寸的图像，无需传统归一化，保留空间细节，特别适用于文档和图表分析 绝对时间编码： 支持处理长达数小时的视频，并实现秒级事件定位 数据规模扩展： 预训练数据从 1.2 万亿 tokens 增加至 4.1 万亿 tokens 预训练过程分为三个阶段：\n在第一阶段，仅训练ViT，即视觉预训练：\n针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p\u0026gt;）、表格（\u0026lt;table\u0026gt;）、图表（\u0026lt;div class=\u0026quot;chart\u0026quot;\u0026gt;）、公式（\u0026lt;div class=\u0026quot;formula\u0026quot;\u0026gt;）、图像标注（\u0026lt;div class=\u0026quot;image caption\u0026quot;\u0026gt;）、OCR文本（\u0026lt;div class=\u0026quot;image ocr\u0026quot;\u0026gt;）、乐谱（\u0026lt;div class=\u0026quot;music sheet\u0026quot;\u0026gt;）、化学式（\u0026lt;div class=\u0026quot;chemical formula\u0026quot;\u0026gt;）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系 OCR 数据: 利用高质量的合成图像和现实世界的自然场景图像, 整合了一个大规模的多语言OCR数据集,支持多种语言; 针对图表类型数据，使用python可视化库合成了100万个样本； 对于表格数据，利用表格识别模型处理了600万个真实样本用于训练； 视频数据： 在训练过程中动态采样FPS，确保在理解不同帧率的视频数据时增强鲁棒性； Agent：收集移动设备、网页和桌面平台的截图。使用合成数据引擎生成截图字幕和UI元素定位注释。字幕任务帮助Qwen2.5-VL理解图形界面，而定位任务帮助它对齐元素的外观和功能；通过人类和模型注释者(AGUVIS),为每个步骤生成推理过程 在第二阶段，解冻所有模型参数，并在多样化的多模态图像数据上训练模型，以增强其处理复杂视觉信息的能力\n在第三阶段，长上下文预训练： 序列长度扩展至 32768，专注于长视频、长代理任务和长文档，训练长上下文能力\npost-training： Qwen2.5-VL 的后训练对齐框架采用了一个双阶段优化范式，包括监督微调（SFT）和直接偏好优化（DPO）（ViT参数冻结）SFT,约200万条记录，纯文本数据和多模态数据各占50%，多模态数据包括图文和视频文本组合;\n设计了一个两阶段的数据过滤pipeline，用于数据清洗和低质量数据过滤；在初始阶段即领域特定分类，采用Qwen2-VL-Instag（Qwen2-VL-72B）对问答对进行层次分类（八个主要领域，每个领域可进一步细分）；第二阶段涉及领域定制过滤，根据上一步的分类，有针对的对于不同领域提高数据质量（rule-based and model-based），如对于文档处理、OCR 和视觉定位任务相关的数据集，识别并移除重复模式\n采用拒绝采样作为策略以增强推理能力\n效果：单纯从视觉问答和 OCR benchmark的跑分结果看，72B 模型与 GPT-4o 和 Claude 3.5 Sonnet 相当\nR1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning https://arxiv.org/abs/2503.05592\n现有的大型推理模型(LRMs)主要依赖其内部知识来解决问题,这在处理知识密集型问题、私有信息查询和时效性问题时容易产生不准确和幻觉。因此,需要让模型能够在推理过程中访问外部信息来实现更深入的推理 。\nR1-Searcher,一个通过强化学习来增强大语言模型搜索能力的框架。该框架采用两阶段的基于结果的强化学习方法,使模型能够在推理过程中自主调用外部搜索系统获取相关知识\n创新点：\n提出了一个两阶段的基于结果的强化学习框架,使模型能够自主调用外部搜索系统 完全依赖强化学习进行训练 奖励机制,包括检索奖励和格式奖励,以引导模型学习正确使用外部检索系统 具体方法：\n第一阶段： 使用检索奖励和格式奖励来训练模型学习调用外部检索系统，不考虑答案的准确性,主要目标是让模型学会正确的检索调用格式\n第二阶段： 引入答案奖励，使用F1分数作为答案奖励的度量\nRL训练采用Reinforce++算法\n设计的两个算法：\nRAG-based Rollout\n目的：让模型能够基于检索到的文档继续进行推理，不会中断思维链\n使用特定标签来指示搜索工具的调用：\u0026lt;begin_of_query\u0026gt;...\u0026lt;end_of_query\u0026gt; 当模型生成到\u0026lt;end_of_query\u0026gt;时，会暂停生成过程 提取查询并执行检索 将检索到的文档用\u0026lt;begin_of_documents\u0026gt;...\u0026lt;end_of_documents\u0026gt;标签封装 将检索结果整合到模型的推理过程中 Retrieval Mask-based Loss Calculation\n将\u0026lt;begin_of_documents\u0026gt;...\u0026lt;end_of_documents\u0026gt;指定为特殊标记 在训练过程中对这些部分进行掩码处理，忽略这些被掩码的部分 这些文档不是模型应该学习生成的内容 一些有趣的观察：\nRL比SFT表现更好,在域内和域外测试集上都显示出更强的检索能力和泛化性， SFT倾向于依赖模型的内部知识,这可能导致错误答案 Detecting misbehavior in frontier reasoning models https://openai.com/index/chain-of-thought-monitoring/\n解决问题：\nreward hacking（AI可能通过开发者意图之外的方式实现高奖励，即利用系统漏洞达成指标而非真正解决问题） 创新点：\n提出CoT monitor 用AI监督AI 将思维透明性作为安全机制，而非仅作为解释工具 Methods： 使用另一个LLM（GPT-4o），来观察被监控模型（如 o3-mini）的 CoT 推理过程，这个监控 LLM 会根据特定提示来判断模型是否尝试通过reward hacking来“作弊”\n研究还探讨了将 CoT 监控集成到强化学习奖励中的效果。在低优化条件下（对模型的 CoT 不施加太大压力，允许它自然生成推理过程）此时CoT 保持透明和可监控，模型的行为更一致，监控系统可以有效识别不当行为。反之，在强优化下（通过强化学习大幅调整奖励函数，迫使模型尽可能提高任务表现），模型可能会学会“模糊化”其推理过程，使监控模型难以察觉其不当行为。\n研究建议支付“可监控性税”（monitorability tax），即不对 CoT 施加强优化压力，确保 CoT 保持可监控性，从而有效检测不一致行为。\nA-MEM: Agentic Memory for LLM Agents Paper\n解决的问题：\n当前LLM代理的记忆系统（依赖预定义结构和固定操作流程，无法动态组织知识 如何在开放域任务中实现记忆的自适应组织和持续演化，以支持复杂长期交互 创新点：\n受Zettelkasten启发的动态架构： 每条记忆包含内容、时间戳、LLM生成的关键词/标签/上下文描述、嵌入向量和关联链接\nm_i = {c_i, t_i, K_i, G_i, X_i, e_i, L_i}\n通过语义相似性和LLM推理动态建立记忆间的多层次关联（如因果关系、概念相似性）\n新记忆触发历史记忆的上下文和属性更新\nm_j^* \\leftarrow \\text{LLM}(m_n | \\mathcal{M}^{\\text{near}}n \\setminus m_j | m_j | P{s3})\nMethods\n笔记构建：用LLM解析原始交互，生成结构化属性）。 链接生成：先通过嵌入相似性筛选Top-K候选记忆，再用LLM分析深层关联。 记忆检索：将查询分解为关键词，在记忆网络中搜索。 graph LR A[新记忆m_n] --\u0026gt; B[生成嵌入e_n] B --\u0026gt; C[计算相似性] C --\u0026gt; D[筛选Top-K] D --\u0026gt; E[生成链接] E --\u0026gt; F[触发记忆演化] G[用户查询q] --\u0026gt; H[查询嵌入] H --\u0026gt; I[匹配记忆] I --\u0026gt; J[返回结果] prompt_template_for_Memory_Evolution = \u0026#34;\u0026#34;\u0026#34;You are an AI memory evolution agent responsible for managing and evolving a knowledge base. Analyze the new memory note according to keywords and context, also with their several nearest neighbors memory. Make decisions about its evolution. The new memory context: {context} content: {content} keywords: {keywords} The nearest neighbors memories: {nearest_neighbors_memories} Based on this information, determine: 1. What specific actions should be taken (strengthen, update_neighbor)? 1.1 If choose to strengthen the connection, which memory should it be connected to? Can you give the updated tags of this memory? 1.2 If choose to update neighbor, you can update the context and tags of these memories based on the understanding of these memories. Tags should be determined by the content of these characteristic of these memories, which can be used to retrieve them later and categorize them. All the above information should be returned in a list format according to the sequence: [[new_memory], [neighbor_memory_1], ... [neighbor_memory_n]] These actions can be combined. Return your decision in JSON format with the following structure: { \u0026#34;should_evolve\u0026#34;: true/false, \u0026#34;actions\u0026#34;: [\u0026#34;strengthen\u0026#34;, \u0026#34;merge\u0026#34;, \u0026#34;prune\u0026#34;], \u0026#34;suggested_connections\u0026#34;: [\u0026#34;neighbor_memory_ids\u0026#34;], \u0026#34;tags_to_update\u0026#34;: [\u0026#34;tag_1\u0026#34;, ... \u0026#34;tag_n\u0026#34;], \u0026#34;new_context_neighborhood\u0026#34;: [\u0026#34;new context\u0026#34;, ..., \u0026#34;new context\u0026#34;], \u0026#34;new_tags_neighborhood\u0026#34;: [[\u0026#34;tag_1\u0026#34;, ..., \u0026#34;tag_n\u0026#34;], ..., [\u0026#34;tag_1\u0026#34;, ..., \u0026#34;tag_n\u0026#34;]] }\u0026#34;\u0026#34;\u0026#34; 一些发现：\n移除链接生成（LG）和记忆演化（ME）后，多跳任务F1从45.85降至24.55，仅保留LG时性能恢复至31.24，证明链接生成是基础，演化机制是增强\nSFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training Paper\n这篇文章研究了SFT和RL在基础模型后训练中对模型泛化能力和记忆能力的影响，特别是在面对未见过的规则和视觉变体时的表现。\n发现：\nSFT倾向于记忆训练数据，难以泛化到未见过的规则或视觉变体，甚至在视觉任务中可能导致视觉识别能力下降 RL不仅在训练任务上表现更好，还能泛化到OOD任务，尤其是在视觉任务中表现出色 虽然RL表现更好，但SFT在RL训练中起到了稳定输出的关键作用 推荐内容 汇总：使用LLM生成精美网页、PPT、卡片 一键搞定、100%成功、80老太都能操作，宝藏提示词将你的任何内容生成网页\n【教程】必须收藏的 4 段 Prompt 提示词，生成PPT、生成3D动画，生成网站，生成万物\u0026hellip;\n一键生成产品网站和小红书卡片，DeepSeek v3的代码能力和审美提升太大了\nAI 快速生成精美卡片\nMCP-Server合集 Model Context Protocol servers\nMCP.co\ncomposio\n课程 10 Lessons teaching everything you need to know to start building AI Agents ｜Microsoft\n其他 AI Blindspots：一个列出了当前 AI 开发盲点合集的网站\n影音记录 精选歌单 Live演出 03.15 Corn Wave\n03.18 Mogwai\n03.21-23 Can festival\n书\u0026amp;阅读摘录 Career advice in 2025 Many people who first entered senior roles in 2010-2020 are finding current roles a lot less fun. There are a number of reasons for this. First, managers were generally evaluated in that period based on their ability to hire, retain and motivate teams. The current market doesn’t value those skills particularly highly, but instead prioritizes a different set of skills: working in the details, pushing pace, and navigating the technology transition to foundational models / LLMs.This means many members of the current crop of senior leaders are either worse at the skills they currently need to succeed, or are less motivated by those activities. Either way, they’re having less fun.\nValuations and funding are relatively less accessible to non-AI companies than they were three years ago.\nI also wouldn’t personally try to sit this cycle out unless you’re comfortable with a small risk that reentry is quite difficult: I think it’s more likely that the ecosystem is meaningfully different in five years than that it’s largely unchanged.\n一个人年入千万，AI创业的十个步骤 AI造就的是一个极度放大你个人能力的时代，你一个人可以干以前几十个人干的事情\n真正的门槛是洞察需求的能力。你做了一个手电筒或者天气软件，不管再精美，也很难帮你做到商业上的成功。\nMVP就是拿出去让用户骂，用户骂得越针对，你产品进步得越快\n大模型的未来，是 Agent 还是 App？ Deep Research 不是标准的 LLM，也不是普通的聊天机器人。它是一种新型的\u0026quot;研究语言模型\u0026quot;，专门设计用于端到端执行搜索任务。\n目前大多数 Agent 初创公司构建的不是真正的 Agent，而是工作流，即\u0026quot;通过预定义的代码路径编排 LLM 和工具的系统\u0026quot;。工作流在某些垂直领域仍可能创造价值\n当前的 RAG 系统由许多相互关联但脆弱的工作流组成：路由、分块、重排序、查询解释、查询扩展、源上下文化和搜索工程。随着训练技术栈的发展，有可能将所有这些流程整合到两个独立但相互关联的模型中：一个用于数据准备，另一个用于搜索/检索/报告生成。 这需要精心设计的合成流程和全新的强化学习奖励函数。真正的训练，真正的研究。这在实践中意味着：转移复杂性。\nAI Will Upend a Basic Assumption About How Companies Are Organized 经济体系长期以来建立在这样的观念之上：专业知识稀缺且昂贵。而人工智能即将让这种专业知识变得丰富且几乎免费。\n支撑我们社会制度的基本假设——“人类洞察力稀缺且昂贵”——将不复存在 摆在个人与企业面前的问题是：当智能本身随处可得且几乎不需成本时，你将如何行动？\n一个原因在于人们很难想象，“必须依靠资深管理者或顶尖专家”才能完成的工作，居然可以（或者部分可以）由机器来承担。正因为卓越人才稀缺，那些高价值任务才显得格外珍贵。我们的组织结构便是在“真正的高智商人才供给有限”这一认知下设计的。\n大多数企业领导人仍处在“尝试接受 AI”而非“真正相信 AI”的阶段\n当智慧的成本几近于零时，真正的瓶颈已不再是“如何获取大脑”，而是“我们如何善加利用”。\n为什么 AI 模型离科学革命还差得很远？ 当前 AI 模型的评估方式有误区\n现在，我们评估 AI 模型智能提升的方式，大多是通过一些“高难度考试题…这些正是我当年擅长的考试类型。这样的评估方法，只能测试模型是否能回答人类已经知道答案的问题。但真正的科学突破，并不来自于答题，而是来自于提出别人没问过的问题，挑战主流认知，质疑已有理论。\n尽管大语言模型已经掌握了几乎所有人类知识，但它们还没有真正“生成”出新的科学发现。…我们现在训练出来的是“听话的好学生”，不是“有主见的创造者”\n那么我们也许需要重新定义 AI 模型的评估方式：评估标准不再是“答对多少题”，而是它是否具备真正的科学思维，\n我们常犯的一个错误，是把牛顿或爱因斯坦看作是“超级优等生”，以为只要把一个好学生按比例放大，就能变成一个天才。… 但这种线性外推忽略了科学最本质的一点：提出正确问题的能力，以及挑战既有知识体系的勇气。\n","permalink":"https://niraya666.github.io/monthly/2025-03-%E6%9C%88%E5%88%8A/","summary":"\u003ch1 id=\"值得关注的模型和新技术\"\u003e值得关注的模型和新技术\u003c/h1\u003e\n\u003ch2 id=\"deepseek-v3-0324\"\u003eDeepSeek V3 0324\u003c/h2\u003e\n\u003cp\u003eDeepSeek V3模型的更新版本；在多个基准测试中表现出色，整体表现接近领先的闭源模型如Claude Sonnet 3.5（但价格便宜很多）；针对多项能力做了针对性提升，如function calling，推理能力，前端代码能力等。更新版本在DeepSeek的官方网站、移动应用可体验。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/deepseek-ai/DeepSeek-V3\"\u003eHuggingface\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"qwq-32b\"\u003eQwQ-32B\u003c/h2\u003e\n\u003cp\u003e由Qwen 团队开发的推理模型，性能同671B参数量的R1相当；通过两阶段RL训练，第一阶段专注于数学和编码任务，利用准确性验证器和代码执行服务器提供反馈；第二阶段提升通用能力，同时保持专业领域的表现。此外QwQ具备tool-using能力，具有131,072 token的上下文长度。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://qwenlm.github.io/zh/blog/qwq-32b/\"\u003eBlog\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/Qwen/QwQ-32B\"\u003eHuggingface\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"qwen25-omni\"\u003eQwen2.5 Omni\u003c/h2\u003e\n\u003cp\u003eQwen2.5-Omni 是一个多模态 AI 模型，能够同时处理文本、图像、音频和视频输入；该模型的创新点包括 Thinker-Talker 架构，分为“Thinker”处理输入并生成表示或文本，“Talker”则输出语音token，共享上下文，实现端到端训练和推理。此外，它使用 TMRoPE（时间对齐多模态 RoPE）技术，同步视频和音频时间戳，确保多模态数据处理的一致性。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://qwenlm.github.io/zh/blog/qwen2.5-omni/\"\u003eBlog\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/Qwen/Qwen2.5-Omni-7B\"\u003eHuggingface\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"gemma-3\"\u003eGemma 3\u003c/h2\u003e\n\u003cp\u003eGemma家族的开源新作，包括 1B、4B、12B 和 27B 参数；4B、12B 和 27B 模型支持文本和图像输入，1B 模型仅限文本；1B 模型支持 32k token，4B、12B 和 27B 模型则扩展至 128k token；支持函数调用和结构化输出\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://blog.google/technology/developers/gemma-3/\"\u003eBlog\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/google/gemma-3-27b-it\"\u003eHuggingface\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"phi-4-multimodal\"\u003ePhi-4-multimodal\u003c/h2\u003e\n\u003cp\u003e具备文本、图像和音频输入的多模态模型；具有5.6B参数量；采用“混合 LoRAs”技术，将模态特定组件集成到基础语言模型中，而基础模型保持冻结状态，以确保了多模态数据处理的无缝性，避免了传统方法中因模态间干扰导致的性能下降。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/\"\u003eBlog\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/microsoft/Phi-4-multimodal-instruct\"\u003eHuggingface\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"qwen25-vl-32b-instruct\"\u003eQwen2.5-VL-32B-Instruct\u003c/h2\u003e\n\u003cp\u003e32B参数量版本的Qwen2.5-VL多模态模型；\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e72B too big for VLM? 7B not strong enough! Teh you should use 32B model!\u003c/p\u003e","title":"2025-03 月刊"},{"content":"舞台 Glacier\nCicada\nJPBS\nLeech\nShow me a Dinosaur\nDin of Celestial Birds\nEuphoria\nChinese Football\nMúm\nMYY\nOverhead, The Albatross\nPG.Lost\n一些景、人、物 一些文字 ","permalink":"https://niraya666.github.io/musik/2025-can-festival/","summary":"\u003ch2 id=\"舞台\"\u003e舞台\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406472098_56614ee11e_b.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eGlacier\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406411999_e696437a7c_b.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406604760_1ec34fb5b4_b.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eCicada\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406233271_f17d7032bc_b.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406233536_e47df28d57_b.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eJPBS\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406472203_111b5f581e_b.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eLeech\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406613715_5e3a9a736e_b.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eShow me a Dinosaur\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54405366312_2f59506cf9_b.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eDin of Celestial Birds\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406472073_cf3fb92699_b.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406412174_a39c538108_b.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eEuphoria\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406421084_ed3a889015_b.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406412194_1414953293_b.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406480928_7cfffa00e8_o.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eChinese Football\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406412084_fe962c527f_b.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eMúm\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406613735_3109194b50_o.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eMYY\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54405366137_50b03cd1fa_o.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003eOverhead, The Albatross\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406412134_d9318919d7_o.jpg\" alt=\"\"  /\u003e\n\n\u003cem\u003ePG.Lost\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"一些景人物\"\u003e一些景、人、物\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406412169_f7c5cd00b6_o.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54405357227_a8e26bf8f3_o.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406604795_222125eeee_o.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54405357192_a261bf6749_o.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406605030_53e66342c2_o.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54405357317_79968d7e8a_o.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/54406604770_f17d7032bc_o.jpg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Can-Festival 2.0 纪录"},{"content":"当冰岛后摇乐队Sigur Rós宣布世界巡演时，东京站便成了我行程表上必须抵达的地方。当然，这趟旅程还承载很多和音乐、艺术有关的记忆。\n入关时，海关大厅的喧嚣依旧。排队时遇见一对中国夫妇，他们对着\u0026quot;外国人\u0026quot;汉字的标识犹豫不决，忽然意识到自己早已身处异国，相视而笑的瞬间让我想起去年同样忐忑的自己。指纹机这次顺利读取了我的手纹，或许因为少了初访时的紧张手汗。海关人员将新入境贴纸仔细贴在旧标签右侧，整齐的排列让强迫症狂喜了。\n选择上野作为落脚点，不仅因它毗邻机场快线的便利，更迷恋老城区特有的烟火气。从成田机场进城的交通选择颇多，最终搭乘号称日本最快非新干线列车的Skyliner。\n与大阪相比，东京街头的中国元素显得格外醒目。沙县小吃和杨国福麻辣烫这类国内常见的连锁招牌不时映入眼帘，而改良版的四川担担面更是在日本扎根多年，形成了独特的本土风味。\n拖着行李箱抄近道时，误入了名为仲町通り的小路。暮色中的街道霓虹渐起，酒吧与餐厅的招牌间夹杂着暧昧的粉色灯箱，手持宣传板的年轻女性在街角轻声招揽客人，几个西装革履却透着江湖气的男子在暗处徘徊。我加快脚步穿过这片区域，直到看见便利店明亮的灯光才松了口气。后来查阅资料得知，这条起源于江户时代宽永寺门前町的老街，历经三百年演变已成为东京著名的夜生活区。这里不仅延续着传统娱乐文化，更因拉客纠纷和醉汉闹事被列为治安重点区域，旅游指南都建议游客避免夜间单独深入巷弄。有趣的是，在鳞次栉比的霓虹招牌中，除了日文店名，还夹杂着不少泰语和菲律宾语的标识。\nDay1 东京都现代美术馆 东京之行的首站，是提前数日才预约成功的特别展——《坂本龙一｜听音观时》。即便社交媒体上最出圈的《雾的雕塑》装置吸引着无数人前来打卡，但对我而言，这场展览更像是赴一场迟到的对话。作为教授生前为美术馆构思的最终企划，十二组装置艺术中流淌着他未完成的思考：当音乐脱离传统音阶束缚，当乐器回归自然物质的本质。\n《你的时间》（坂本龙一 with 高谷史郎）\n311海啸浸没过的钢琴正以另一种形态重生。曾出现在纪录片《终曲》中的这架\u0026quot;被自然调律的乐器\u0026quot;，此刻悬浮于水盘之上。头顶屏幕飘落的雪与地震数据转化的音符共振，琴槌敲击的已非人造音阶，而是地壳震颤的原始频率。\n《异步——沉浸东京》（坂本龙一with高谷史郎 2024 ）\n这也是我驻足最久的一个装置，因为其播放的是我最喜欢的一张专辑《Async》。 展厅正面是一面 18 米长的 LED墙，实时生成由高谷制作的影像。高谷的影像以拍摄坂本纽约工作室里的钢琴、书籍、打击乐器、后院盆栽等各种物品为基础构成。\n“Because we don\u0026rsquo;t know when we will die, we get to think of life as an inexhaustible well. Yet everything happens only a certain number of times, and a very small number really. How many more times will you remember a certain afternoon of your childhood, an afternoon that is so deeply a part of your being that you can\u0026rsquo;t even conceive of your life without it? Perhaps four, five times more, perhaps not even that. How many more times will you watch the full moon rise? Perhaps 20. And yet it all seems limitless.”\n在《fullmoon》 一曲中，电影《遮蔽的天空》中的朗诵采样，被翻译成了其他 9 种不同的语言，伴随着左右声轨中轮流播放，在长屏幕上的不同位置点亮着不同语言的版本。\n《生命——井 东京 雾的雕塑》（坂本龙一＋中谷芙二子＋高谷史郎 2024）\n人造云雾正从展馆两侧涌出，对流，相互碰撞。这些动态被摄影机捕捉，并被即时转换为声音\n《感应流 2024——不可见，不可闻》 （坂本龙一 + 真锅大度 2024）\n我们习以为常的无线电信号若被具象呈现，会展现怎样的形态与韵律？\n银座 Sony Park 位于银座繁华地段的 Sony Park，紧邻爱马仕旗舰店。尽管名为“公园”，这座建筑实际上是一座未经装饰的钢筋混凝土大楼，地上和地下共有几层。对于不了解其背景的人来说，很难将其与公园联系起来。在2017年旧楼拆除至2024年新厦落成期间，该地块曾作为临时绿地开放。今年1月26日，历经半年筹备的新场馆正式重启，同步推出\u0026quot;Sony 25周年特展\u0026quot;。\n当前展览分为上下两部，本次参观的上部聚焦金融、音乐、半导体三大业务板块。每个主题展厅均邀请不同音乐人参与内容设计，通过声光装置呈现科技与艺术的交融。尽管展览免费开放，但因有当红组合YOASOBI的参与，周末时段预约名额常迅速告罄。\n金融主题展厅以\u0026quot;Finance details life\u0026ldquo;为命题，由羊文学乐队打造的文字艺术装置颇具巧思。展区播放着《More Than Words》《光るとき》的歌词，伴随着音乐，歌词在光影中交错重叠。\nWords overlap. Words create poetry. The soothing rhythm of words envelops all doubts and anxieties.\n三楼 YOASOBI「Semiconductors create new realities」以人体心跳为灵感原点，每位观众入场时需通过交互终端录入姓名并测量实时心率。这些生物数据将转化为独特的数字小物件，与展场中央的YOASOBI原创电子乐实时互动。\nB2 Vaundy「Music comes after a long journey」\n这也是我最喜欢的展厅，音乐人Vaundy精选的200首作品，观众需手持接线耳机，通过物理插拔动作完成点歌，回归了最原始也是最纯粹的听歌方式。\nDay2 上野公园 国立博物馆 感冒还没好透，还是决定就近走走。住处往北不过三分钟脚程，便是绿荫环绕的上野公园，转过十字路口，眼前豁然展开一片水色，便是不忍池了。\n江户幕府初建时，匠人们依照琵琶湖的形制凿出这方水域。关于\u0026quot;不忍\u0026quot;的来历，老东京们常念叨两种说法：一说取自佛家\u0026quot;慈悲不忍\u0026quot;的偈语，另一说倒是直白——这池子心软得很，连旱年都舍不得见底。\n初春的池面上，莲花只留些光秃秃的杆子戳在水里，要是盛夏时节，这里就会铺满接天莲花，不过眼下倒是海鸥和鸽子最自在。\n不忍池中间有座小岛，岛上立着宽永寺的辩天堂。这座庙虽然不大，却挺有名气，因为里面供着七福神里的大黑天。说来有意思，这个神明原本是印度教战神摩诃迦罗，传到日本后成了日本天台宗信仰的守护神。大黑天与日本古代神道教中的色神同为一体，被尊称为“大缘”或“大黑”，意为带来广远亲近之福的大神。\n沿樱花大道晃悠着，一个写着“上野合格大佛”的牌子吸引力了我，可四下张望，只瞧见个土包，而不见常见的立佛像或是卧佛像。沿着台阶往上走，这才看清小亭子中供奉着的一张佛脸。\n原来江户初年这里立过六米高的青铜大佛，越后国的和尚们一锤一锤敲出来的。可这佛像像是跟地震结了仇，1631年建好后被震塌三次。最惨的是二战那年，整个佛身子都被熔成子弹炮弹，就剩这张脸留在了这里。可来参拜的考生们偏说这模样吉利——跌倒了七次，就站起来八次嘛，所以不少考生特地來這裡祈求考試合格。\n侧面有块铁牌，记录着这里的历史，其中有这样一句话： “大仏を埋めて白し 花の雲 子規” 来自于日本俳句詩人正岡子規，展现着日本人独有的物哀心理，借由樱花短暂的美丽，表达出一种超然和淡泊的情感，对世事无常的淡淡感伤。\n穿过大石鸟居，便可来到上野著名的东照宫。这座供奉着德川幕府开创者德川家康的神社，最醒目的就是主殿外墙贴满的金箔，当地人都叫它\u0026quot;金色殿\u0026rdquo;。据记载，德川家康去世后，他的两位重要家臣藤堂高虎和天海僧正主持修建了这座神社。\n特别暖心的是，主殿前专门设置了单人自拍台。即便独自旅行，游客也能轻松把整座金灿灿的建筑完整收进取景框。每月16、17号会发售限定的\u0026quot;升龙御守\u0026quot;，龙形图案的金色护身符据说能保佑升官发财。\n在国立博物馆前的广场上，正巧碰上了周末的酒屋街角小酌节。花500日元入场费就能逛遍各种酒摊，空气里飘着烤串和章鱼烧的香味。虽然挺想尝尝鲜，但大白天就喝酒总觉得不太合适。\n这座1872年建成的日本首家博物馆，常设展门票1000日元。除了本馆的日本文物和东洋馆的亚洲藏品，当时表庆馆正在办HelloKitty特展，门口排队的队伍足足绕了半圈，听说要等一个多小时才能入场。\n本馆里陈列着武士刀、浮世绘这些传统文物。有幅叫《第二代市川八百藏与第二代濑川菊之丞的合撑伞》的画让我印象深刻，画的是两位歌舞伎名角共撑油纸伞在雪中行走的场景。\n来源\n描绘了两位当时著名的歌舞伎演员，演绎一对踏雪而行、前去殉情的情侣，而合撑一把伞前行是歌舞伎中表现男女前往殉情的固定手法。\n转到东洋馆时心情变得复杂起来。这里大部分展品都来自亚洲的其他国家，从敦煌壁画残片到唐三彩，还有东南亚的佛像。虽然保存得很完好，但看着那些本该属于中国等各国的文物，作为中国人心里总归不是滋味，每一件器物背后，都藏着半截没说完的硝烟往事.\n浅草寺 ￼ 浅草寺的正门口立着朱红色的雷门，左右分立着风神雷神的雕像。门前的石板路上，穿传统短褂的人力车夫们正招呼游客体验江户风情。\n穿过雷门便是仲见世街。这是一条通往浅草寺的购物街，两百多米长的商店街飘着人形烧的甜香，和风团扇、浮世绘手帕等纪念品琳琅满目。抬头可见晴空塔的尖顶刺破云层，传统与现代在同一个画面里奇妙交融。 ￼\n这座寺庙的由来颇有故事。相传千年前有渔民在隅田川捞到一尊观音像，当地乡民便建堂供奉，渐渐发展成如今香火鼎盛的浅草寺。\n主殿里供奉的是圣观音像，不少本地人虔诚地合掌礼拜。\n￼ 意外发现游客中心顶楼藏着观景台。乘电梯直上八层，整片寺院尽收眼底。这里尚未被游客挤占，能安静俯瞰附近的建筑和风景。\n回程时注意到河畔的奇特建筑，一朵金色的啤酒花，这是朝日啤酒的建筑物（Super Dry Hall）。与其说是啤酒花，更像是个金色的便便。 ￼\n返程途经御茶之水时，特意绕道圣桥。这里是铁道迷心中的圣地。虽然对动漫圣地巡礼没有执念，但作为半个铁道迷，实在抵挡不住三列电车在此交汇的奇观。\nDay3 六本木之丘 六本木之丘52层真是个宝藏楼层，这里同时容纳了森美术馆、Tokyo City View观景台和森艺术中心画廊。最近听说森美术馆新开了名为\u0026quot;Machine Love\u0026quot;的人工智能主题展，作为AI从业者自然不能错过。\n￼ 原本计划先看森艺术中心画廊的埃及文物展，但看到门口蜿蜒的长队需要等一小时，只好作罢。转身上楼来到\u0026quot;Machine Love\u0026quot;展厅，这里主要讨论人工智能、电子游戏与当代艺术的关联。在开头处，策展人甚至贴心提供了prompt用于解释复杂的专有名词。\n￼ 不过观展体验有些复杂。部分作品像是动画专业学生的未完成作业，生硬地套用AI元素来掩饰技术缺陷。那些关于\u0026quot;人类与技术关系\u0026quot;的深刻命题，在具体呈现上显得有些力不从心。最惊喜的反倒是同层的Tokyo City View观景台，巨大落地窗前能够看到一览东京全貌。\n￼ ￼ ￼\n芝公园 沿着石板路走进芝公园，最先看到的是古朴庄严的增上寺。这座净土宗大本山曾是德川将军家的家庙，朱红色的殿堂在绿荫中格外显眼。作为东京最早的公园之一，这里原本就是寺庙的属地，如今开阔的草坪和步道成了市民散步的好去处。\n￼ 站在公园的小坡上，能望见橙红色的东京塔从树梢间探出头来。 ￼\n临走时注意到地铁站叫\u0026quot;芝大门\u0026quot;，这名字藏着江户时代的故事。四百年前这里确实有座名为\u0026quot;芝大门\u0026quot;的城门，如今城墙虽已消失，但往来电车上依然回响着这个古老的地名。\n台场 这次来看演的Tokyo Garden Theatre正好在台场，趁开演前的几小时，我顺道去海边走了走。这个填海造出的人工岛上，藏着不少有意思的风景。\n￼ 地图上发现条有趣的电车线，叫百合海鸥线。这条无人驾驶的轨道列车在台场绕出个圆环，坐在车厢里能看海。\n￼ 海滨步道上有不少散步的人。看着海特别适合发呆。我坐在长椅啃饭团时，听见旁边几个中国游客在聊八卦，多是关于大S得流感去世以及打疫苗之事。\n在台场，有着标志性的富士电视台大楼，那个银色球体建筑总让我想起科幻片里的太空舱。自由女神像复制品立在码头，倒有种错位的幽默感。\n￼ Sigur Ros 2025巡演特别版，是同当地交响乐团共同呈现的。坐在山顶位虽然看不清乐手表情，反而更能专注聆听音乐本身。当冰岛空灵的人声遇上管弦乐的磅礴时，不免让人落泪，黑暗中此起彼伏的抽泣声。特别是《Hoppípolla》前奏响起那刻，真能让人起鸡皮疙瘩。\n谢场时鼓掌手都拍疼了，心里想着，这样的演出真希望能多看几回啊。\nDay4 我记得在《动物化的后现代》一书中，作者提到御宅族有着收集符号的特殊爱好。虽然我不认为自己是御宅族，但也不免有这种收集的习惯，或许这也是资本主义消费文化影响的结果吧。原本计划留出这天去镰仓游玩，可惜天气不好。正好东京还有几个想去却没去成的地方，索性把最后一天变成东京符号收集日——这大概算现代人的另类巡礼吧？\n神社御守收集 住的民宿旁边有座神社，建筑很显眼，但前几天都是匆匆路过，没机会进去看看。这就是汤岛天满宫，供奉着学问之神菅原道真，大清早便可看见不少穿着制服的学生，前来祈求学业考试顺利。这里的御守以保佑学业相关的最为出名。\n御守挺适合作为伴手礼的。御守（お守り）是护身符，象征神明的保佑，通常有效期一年，需要定期更换。不同神社有各自特别设计的御守，有些甚至是限定版，作为收藏品也很不错。除了御守，还有御朱印（ごしゅいん），就是参拜的证明，由神社人员手写或盖章的书法作品。\n最有趣的是在秋叶原附近的神田神社，这里有IT信息安全守护御守，正好很适合我的同事们。\n代代木 自古以来，这片土地上就生长着一代又一代的巨大榧树（かやのき），因此得名“代代木”（代々木）。这里曾有一棵闻名的大榧树，但不幸在1945年5月的战争中被烧毁。后来人们重新种了一棵树，延续着这片土地的历史。\n代代木地区最重要的一处神社便是明治神宫，这座1920年建成的神社，供奉着推动日本现代化的明治天皇与昭宪皇太后。在入口处能看到许多清酒奉纳桶（酒樽），这些都是日本各地酿酒师捐献的，表达对明治天皇夫妇的敬意。明治天皇在位时，既推动国家现代化，也很重视保护传统文化，包括清酒酿造。\n神社入口的鸟居造型特别，横梁微微上翘，这种样式通常用于重要神社，象征着对明治天皇的崇敬。这座高12米的鸟居也是日本最大的鸟居，使用的是来自台湾超1500年树龄的扁柏制成。\n在明治神宫御苑前，立着一个牌子，上写着明治天皇和昭宪皇太后的御制和歌（御製・御歌）。\n世の中の\n事ある時に\nあひぬとも\nおのがつとめむ\nことな忘れそ\n释义： “世间发生重大事件之时，即便身处其中，也不要忘记自己的职责。”\n身にしみて\nうれしきものは\nまこともて\n人のつげたる\nことばなりけり\n释义： “最令人感动和欣慰的，莫过于别人真心传递的话语。”\n去年在上海电影节，我看二刷了《完美的日子》这部电影，讲的是东京一位公厕清洁工的日常生活。这部电影原本只是为东京特色公厕拍的宣传片，没想到导演文德斯把它拍成了长片，还让主演役所广司拿到了戛纳影帝。\n逛完明治神宫，我顺路去看了代代木附近的几个特色公厕：\n伊东丰雄设计的代代木八幡公厕，位于八幡神社旁边。三个圆形谷仓造型。\n安藤忠雄设计的涩谷神宫通公园公厕。\n代代木深町小公园的公厕是由坂茂设计的，这就是电影里那个能变色的玻璃洗手间。有趣的是，附近还有一个几乎一样的蓝绿色公厕，也是出自同一位设计师之手。\n在公厕打卡时，偶遇的电车。\nFender旗舰店与涩谷Tower Records 在东京原宿的明治神宫前，藏着亚洲最大的Fender吉他旗舰店。作为电吉他爱好者，这是必访之地，这座四层（地上三层加地下一层）的店铺，陈列着各类Fender电吉他（当然还有少量贝斯和木吉他），包括限定版和日本特别款。店内设有试音室和展示区，楼梯墙面挂满了使用Fender吉他的音乐人照片，更像是博物馆的感觉而不是普通的商店。\n涩谷的Tower Records则是音乐爱好者的天堂。这个发源于美国的唱片连锁品牌，虽在2006年于美国宣布倒闭，但东京分店被当地企业收购后顽强生存下来。八层空间（含地下一层和顶楼活动区）里囊括各种音乐类型，至今仍是涩谷的文化地标。\n我在这里泡了整个下午。试听区的唱片让人流连忘返，最终带走两张：YMO的中古盘和Bill Evans的特别版绿胶。店里\u0026quot;NO MUSIC, NO LIFE?\u0026ldquo;的标语格外应景。\n店内有不少展台会展示日本本土音乐人的作品，能够感受到他们对于自己音乐的自豪感。其中一个音乐推荐专区，推荐了日本的Jazz，其写到 日本音乐不止有动漫歌曲和City Pop 还有60-80年代的\u0026quot;和爵士\u0026rdquo;（WA-JAZZ）。\n我发现几张特别喜欢的：\n山本刚三重奏《Misty》（1974）\n福居良《Scenery》（1976）\n高中正义《虹伝説》（1981）\n峰厚介五重奏《峰》（1970）\n有趣的是，这些作品大多在流媒体上难觅踪迹，仅有当年少量限定版本发行。\n东京食记 Delhi Ueno (4.2/5)\n结束了一天的展览，来到上野附近这家印度咖喱店。虽只有15个座位的小店，却常常座无虚席，顾客多为下班的上班族和附近居民，基本是晚上8点，依旧需要排队。排队时便可闻到浓郁的咖喱香料味。\n点了中等辣度的Indo curry，虽然肉份不算太多（仅4块鸡肉配半个土豆），但咖喱酱汁的香浓足以让人将米饭一扫而空。配菜的腌制洋葱和咸菜也别具特色。这家店的菜品可能不够\u0026quot;网红\u0026quot;，但正如美食之道 - 味道才是根本。\n地址：3 Chome-42-2 Yushima, Bunkyo City, Tokyo 113-0034\n网站\n面屋武藏 武骨 (4.1/5)\n作为日本拉面界著名的\u0026quot;96年组\u0026quot;之一，面屋武藏与\u0026quot;青葉\u0026quot;、\u0026ldquo;鲸轩\u0026quot;齐名。御徒町店虽然座位不过12个，3个员工的小店，但料理水准相当可观。\n这家店作为沾面专门店开张，但也有不同的选择，以猪骨汤底为基础，提供\u0026quot;白\u0026rdquo;(基础)、\u0026ldquo;黑\u0026rdquo;(墨鱼汁)、\u0026ldquo;红\u0026rdquo;(特制辣酱)三色拉面。选择的是基础的白汤，采用自家制面，面条较粗，口感筋道。采用动物系与秋刀鱼煮干的双重汤底，不同于传统的单一汤底，更有复合型风味。叉烧肥瘦相间似苏式焖肉。炫完面条，必然要就着浓郁汤底来碗米饭的，突然理解常常被国人说吐槽的「碳水叠碳水」吃法——这样暴烈的鲜美，确需双倍热量来承接。\n有趣的是店里放的音乐多是朋克乐。\n地址：6 Chome-7-3 Ueno, Taito City, Tokyo 110-0005\n网站\nTeuchi Soba Keyaki (3.8/5)\n在六本木之丘看展后偶遇的这家荞麦面店，意外成为此行的惊喜。点了推荐的天妇罗荞麦面套餐，天妇罗拼盘包含虾、帆立贝、𬶮以及三种时令蔬菜。\n荞麦面本身就散发着独特的香气，搭配葱花萝卜泥和适量芥末调制蘸料，传统的食用方式是要发出些许声响。面食完，最后将热汤（そばつゆ）倒入剩余的蘸料中一饮而尽，颇具仪式感。\n地址：〒106-6105 Tokyo, Minato City, Roppongi, 6 Chome−10−1 ヒルズ ウェストウォーク 5F\n网站\n一兰拉面 (3.0/5)\n上野店的人气依旧不减，即便是午后1点仍需等候约45分钟。值得一提的是，店内顾客构成以本地人为主，而非\u0026quot;游客专属\u0026quot;的刻板印象。\n独特的隔间式座位让人专注于美食本身。猪骨汤底浓郁，带有特征性的风味。虽然称不上惊艳，但作为连锁品牌能保持稳定的水准已属难得。\n地址：〒110-0005 Tokyo, Taito City, Ueno, 7 Chome−1−1 アトレ上野\n网站\nCurry \u0026amp; Cafe SAMA Kanda (2.0/5)\n这家网红北海道札幌汤咖喱店的实际体验略显遗憾。店内主要依靠外籍员工服务，客源也以外国游客为主。点的鸡腿咖喱中，鸡肉的调味和咖喱的香气都不够突出，整体表现平平。对追求正统咖喱风味的食客而言，建议可以考虑其他选择。\n结语 与第一次访日相比，这次旅行给我带来了不同的感受。在对日本文化和社会有了更深的了解，日语水平也提高后，我发现自己能够更全面地体会和观察这个国家。\n让我印象深刻的是日本人对个人空间的尊重。这对于我这样的人来说非常舒适——在自己的空间里可以做任何事，不会被打扰或被别人盯着看。这也是为什么东京能吸引各类人群：二次元、艺术家、古着收藏者，每个人都能找到适合自己的小天地。\n这些天在日本学会了一个词，叫\u0026quot;迷惑行为禁止\u0026quot;，意思是不要在公共场合打扰他人。在电车上几乎安静到让人不敢开口说话，而吸烟只能在指定区域进行，否则会被罚款，几乎不见垃圾桶却异常整洁的街道。这与国内的情况形成了鲜明对比。\n作为外来者，我处在一种特权的夹缝中。不必承受他们的社会压力，不必纠缠于敬语体系与人情债，这种特权式的抽离，让我既能享受秩序带来的便利，又能冷静审视其背后的文化代价——正如一台看似完美的机器，唯有站在外部才能看清它的磨损与锈迹。\n这种秩序自有其代价，规则至上的文化也催生了令人窒息的压力。在这个国家\u0026quot;读空气”（空気を読む）的能力被视为重要的社交技能，这种需要时刻揣摩的社交规则，对于职场和日常生活中的人而言，可想而之压力之大， 以至于深夜的居酒屋中总是坐满了下了班宣泄情绪的职场人。这种集体主义框架下的生存艺术，造就了独特的\u0026quot;气泡式社交”，明确区分\u0026quot;内\u0026quot;与\u0026quot;外\u0026quot;的群体界限，也就制造了\u0026quot;表\u0026quot;与\u0026quot;里\u0026quot;的双重人格，人们疲于维持其社会面具。\n同时，它加深了社会孤立化，对‘读空气’的严苛要求迫使个体压抑真实自我，长期处于社交表演状态，最终导致年轻人放弃深度关系，转向浅层联结甚至金钱购买陪伴。东浩纪所指出的‘从欲望到动物性冲动’的退化，恰能解释日本年轻人为何选择用消费填补情感空洞——当社会规则过度压抑人性需求，亲密关系反而成为一种奢侈品。年轻人不再费力经营真实的情感联结，转而用金钱购买陪伴：陪吃饭、陪逛街、甚至陪哭泣的服务产业蓬勃发展。这何尝不是一种集体逃避？\n日本对‘他人视线’的焦虑不仅存在于社交领域，甚至渗透到最日常的进食行为中。体验一兰拉面的单人隔间时，我以为这只是为社恐人士设计的天堂。后来才了解到，这其实与\u0026quot;食べる恥ずかしい\u0026quot;（进食羞耻感）的文化有关。在日本传统中，特别是对女性而言，在公众场合大口吃饭被视为不雅，而拉面需要快速吃且常伴随吸面声，这与对女性的优雅期望相冲突。这种文化羞耻感映射出的其实是日本社会深层次的性别期望不平等，尽管表面上讲究礼仪和秩序，但传统观念的枷锁依然沉重。表面的便利设施，实则暴露出性别规训的裂痕。正如伊恩·布鲁玛在《日本之镜》中所说“日本文化表面上在变，但其内在始终没有真正脱离最古老的本土根源”。\n不过令我惊讶的是，近年来东京变得越来越多元化。也许是因为人口老龄化问题，或是大城市本身的多元特性，在这个整体保守的社会里，我看到了不少来自东南亚、南亚的外国劳工。他们在餐厅和便利店工作，认真学习日本文化中繁琐的礼仪，这些外来者或许尚未真正融入日本森严的等级体系中， 但至少暗示某种变化。\n","permalink":"https://niraya666.github.io/travel/2025%E4%B8%9C%E4%BA%AC%E9%9F%B3%E4%B9%90%E4%B9%8B%E6%97%85%E7%9A%84%E5%86%8D%E5%87%BA%E5%8F%91/","summary":"\u003cp\u003e当冰岛后摇乐队Sigur Rós宣布世界巡演时，东京站便成了我行程表上必须抵达的地方。当然，这趟旅程还承载很多和音乐、艺术有关的记忆。\u003c/p\u003e\n\u003cp\u003e入关时，海关大厅的喧嚣依旧。排队时遇见一对中国夫妇，他们对着\u0026quot;外国人\u0026quot;汉字的标识犹豫不决，忽然意识到自己早已身处异国，相视而笑的瞬间让我想起去年同样忐忑的自己。指纹机这次顺利读取了我的手纹，或许因为少了初访时的紧张手汗。海关人员将新入境贴纸仔细贴在旧标签右侧，整齐的排列让强迫症狂喜了。\u003c/p\u003e\n\u003cp\u003e选择上野作为落脚点，不仅因它毗邻机场快线的便利，更迷恋老城区特有的烟火气。从成田机场进城的交通选择颇多，最终搭乘号称日本最快非新干线列车的Skyliner。\u003c/p\u003e\n\u003cp\u003e与大阪相比，东京街头的中国元素显得格外醒目。沙县小吃和杨国福麻辣烫这类国内常见的连锁招牌不时映入眼帘，而改良版的四川担担面更是在日本扎根多年，形成了独特的本土风味。\u003c/p\u003e\n\u003cp\u003e拖着行李箱抄近道时，误入了名为仲町通り的小路。暮色中的街道霓虹渐起，酒吧与餐厅的招牌间夹杂着暧昧的粉色灯箱，手持宣传板的年轻女性在街角轻声招揽客人，几个西装革履却透着江湖气的男子在暗处徘徊。我加快脚步穿过这片区域，直到看见便利店明亮的灯光才松了口气。后来查阅资料得知，这条起源于江户时代宽永寺门前町的老街，历经三百年演变已成为东京著名的夜生活区。这里不仅延续着传统娱乐文化，更因拉客纠纷和醉汉闹事被列为治安重点区域，旅游指南都建议游客避免夜间单独深入巷弄。有趣的是，在鳞次栉比的霓虹招牌中，除了日文店名，还夹杂着不少泰语和菲律宾语的标识。\u003c/p\u003e\n\u003ch2 id=\"day1\"\u003eDay1\u003c/h2\u003e\n\u003ch3 id=\"东京都现代美术馆\"\u003e东京都现代美术馆\u003c/h3\u003e\n\u003cp\u003e东京之行的首站，是提前数日才预约成功的特别展——《坂本龙一｜听音观时》。即便社交媒体上最出圈的《雾的雕塑》装置吸引着无数人前来打卡，但对我而言，这场展览更像是赴一场迟到的对话。作为教授生前为美术馆构思的最终企划，十二组装置艺术中流淌着他未完成的思考：当音乐脱离传统音阶束缚，当乐器回归自然物质的本质。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/2025%c2%b7%e4%b8%9c%e4%ba%ac%ef%bc%9a%e9%9f%b3%e4%b9%90%e4%b9%8b%e6%97%85%e7%9a%84%e5%86%8d%e5%87%ba%e5%8f%91-assets/BE28B0B2-E428-4FB4-86E6-EDA0D8896E4B_1_105_c.jpeg\" alt=\"BE28B0B2-E428-4FB4-86E6-EDA0D8896E4B_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e《你的时间》（坂本龙一 with 高谷史郎）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e311海啸浸没过的钢琴正以另一种形态重生。曾出现在纪录片《终曲》中的这架\u0026quot;被自然调律的乐器\u0026quot;，此刻悬浮于水盘之上。头顶屏幕飘落的雪与地震数据转化的音符共振，琴槌敲击的已非人造音阶，而是地壳震颤的原始频率。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e《异步——沉浸东京》（坂本龙一with高谷史郎 2024 ）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这也是我驻足最久的一个装置，因为其播放的是我最喜欢的一张专辑《Async》。 展厅正面是一面 18 米长的 LED墙，实时生成由高谷制作的影像。高谷的影像以拍摄坂本纽约工作室里的钢琴、书籍、打击乐器、后院盆栽等各种物品为基础构成。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“Because we don\u0026rsquo;t know when we will die, we get to think of life as an inexhaustible well. Yet everything happens only a certain number of times, and a very small number really. How many more times will you remember a certain afternoon of your childhood, an afternoon that is so deeply a part of your being that you can\u0026rsquo;t even conceive of your life without it? Perhaps four, five times more, perhaps not even that. How many more times will you watch the full moon rise? Perhaps 20. And yet it all seems limitless.”\u003c/p\u003e","title":"2025·东京：音乐之旅的再出发"},{"content":"Updated on 2025-03-29: Add SmolDocling \u0026amp; VLM Summary\n技术迭代速度之快令人惊叹，前作 RAG工具箱：文档解析与表格处理 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。\n本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。\n过去的技术栈总结 在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决\u0026quot;garbage in, garbage out\u0026quot;的输入质量问题，后续处理环节将难以发挥应有价值。\n传统文档解析技术长期受限于以下核心痛点：\n结构化信息缺失：无法准确识别文档标题、副标题等层级结构\n特殊内容处理薄弱：数学公式、专业符号解析准确率低下\n复杂表格解析困境：跨页表格、合并单元格等场景支持不足\n图像信息提取瓶颈：扫描文档、手写体识别效果欠佳\n版式适应性问题：多栏布局、影印版本等文档格式兼容性差\n从技术角度，过去文档解析的底层逻辑和框架：\n纯文本解析: PyPDF, PyMuPDF只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力\nOCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；\n基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；\n随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。\nBenchmark 为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。\n现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench\nOCRBench、OCRBench-V2\nOmniDocBench\nCC-OCR\n…and more\n（关于benchmark的具体内容见附录）\n这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选\n当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。\n不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。\n根据benchmark和实际测试结果，目前几个值得关注的开源VLM：\nQwen2.5-VL\nPhi-4-multimodal\nLlama 3.2 Vision\nolmocr\nand more …\nQwen2.5-VL系列模型 cookbook\nBlog\nTechnical Report\n这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。\n在Technical Report 中一些和document-parse有关的内容：\n在第一阶段视觉预训练中（仅训练ViT），针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p\u0026gt;）、表格（\u0026lt;table\u0026gt;）、图表（\u0026lt;div class=\u0026quot;chart\u0026quot;\u0026gt;）、公式（\u0026lt;div class=\u0026quot;formula\u0026quot;\u0026gt;）、图像标注（\u0026lt;div class=\u0026quot;image caption\u0026quot;\u0026gt;）、OCR文本（\u0026lt;div class=\u0026quot;image ocr\u0026quot;\u0026gt;）、乐谱（\u0026lt;div class=\u0026quot;music sheet\u0026quot;\u0026gt;）、化学式（\u0026lt;div class=\u0026quot;chemical formula\u0026quot;\u0026gt;）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系\n针对OCR 数据， 利用高质量的合成图像和现实世界的自然场景图像, 整合了一个大规模的多语言OCR数据集,支持多种语言; 针对图表类型数据，使用python可视化库合成了100万个样本； 对于表格数据，利用表格识别模型处理了600万个真实样本用于训练\n在post-training，设计了一个两阶段的数据过滤pipeline，用于数据清洗和低质量数据过滤，针对的对于不同领域提高数据质量；\n在第三阶段，长上下文预训练： 序列长度扩展至 32768，专注于长视频、长代理任务和长文档，训练长上下文能力\n可见， 该模型已针对Document Parsing 做了针对化的训练；至少单纯从视觉问答和 OCR benchmark的跑分结果看，72B 模型与 GPT-4o 和 Claude 3.5 Sonnet 相当\n【但仅仅把这个模型用于document parsing有点浪费了】\n同时Qwen官方也提供了详细的cookbook，包括了具体实现，这里就不展开了，不过记得将base-url改成\n\u0026#34;https://dashscope-intl.aliyuncs.com/compatible-mode/v1\u0026#34; -\u0026gt; \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; 试了下效果：\n结果：\n```html \u0026lt;html\u0026gt;\u0026lt;body\u0026gt; \u0026lt;p\u0026gt;During planning, the Q-planning algorithm randomly samples only from state-action pairs that have previously been experienced (in Step 1), so the model is never queried with a pair about which it has no information.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;The overall architecture of Dyna agents, of which the Dyna-Q algorithm is one example, is shown in Figure 8.1. The central column represents the basic interaction between agent and environment, giving rise to a trajectory of real experience. The arrow on the left of the figure represents direct reinforcement learning operating on real experience to improve the value function and the policy. On the right are model-based processes. The model is learned from real experience and gives rise to simulated experience. We use the term search control to refer to the process that selects the starting states and actions for the simulated experiences generated by the model. Finally, planning is achieved by applying reinforcement learning methods to the simulated experiences just as if they had really happened. Typically, as in Dyna-Q, the same reinforcement learning method is used both for learning from real experience and for planning from simulated experience. The reinforcement learning method is thus the “final common path” for both learning and planning. Learning and planning are deeply integrated in the sense that they share almost all the same machinery, differing only in the source of their experience.\u0026lt;/p\u0026gt; \u0026lt;div class=\u0026#34;image\u0026#34;\u0026gt;\u0026lt;img/\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;p\u0026gt;Figure 8.1: The general Dyna Architecture. Real experience, passing back and forth between the environment and the policy, affects policy and value functions in much the same way as does simulated experience generated by the model of the environment.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Conceptually, planning, acting, model-learning, and direct RL occur simultaneously and in parallel in Dyna agents. For concreteness and implementation on a serial computer, however, we fully specify the order in which they occur within a time step. In Dyna-Q, the acting, model-learning, and direct RL processes require little computation, and we assume they consume just a fraction of the time. The remaining time in each step can be devoted to the planning process, which is inherently computation-intensive. Let us assume that there is time in each step, after acting, model-learning, and direct RL, to complete\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; ``` 【实测遇到的问题】：\n图表类image的bbox经常不包含图表label\n瑕不掩瑜，可以说效果是非常惊艳了。\nMistral OCR https://mistral.ai/en/news/mistral-ocr\nhttps://docs.mistral.ai/capabilities/document/\nMistral AI 推出的OCR API，专注于复杂文档的多模态理解,擅长处理科学论文等含图表混合内容的文档\n输入格式：支持 PNG、JPEG 图像及 PDF\n输出格式：生成有序的 Markdown 或 JSON 结构化数据\n基础版价格为 1000 页/美元\n体验感受\n快速解析完一篇论文的pdf（5～10 sec）\n结构化json输出\n能够截取pdf中的图像，并提供对应的位置信息和Base64编码\n基本上是目前首选了\n猜测，大概率是使用的是mistral自家的mistralai/Pixtral-12B-2409 模型\nhow to use\npip install mistralai from mistralai import Mistral api_key = \u0026#34;API_KEY\u0026#34; client = Mistral(api_key=api_key) ocr_model = \u0026#34;mistral-ocr-latest\u0026#34; system = \u0026#34;\u0026#34;\u0026#34;You are an AI Assistant with document understanding via URLs. You will be provided with URLs, and you must answer any questions related to those documents. # OPEN URLS INSTRUCTIONS You can open URLs by using the `open_urls` tool. It will open webpages and apply OCR to them, retrieving the contents. Use those contents to answer the user. Only URLs pointing to PDFs and images are supported; you may encounter an error if they are not; provide that information to the user if required.\u0026#34;\u0026#34;\u0026#34; def _perform_ocr(url: str) -\u0026gt; str: try: # Apply OCR to the PDF URL response = client.ocr.process( model=ocr_model, document={ \u0026#34;type\u0026#34;: \u0026#34;document_url\u0026#34;, \u0026#34;document_url\u0026#34;: url } ) except Exception: try: # IF PDF OCR fails, try Image OCR response = client.ocr.process( model=ocr_model, document={ \u0026#34;type\u0026#34;: \u0026#34;image_url\u0026#34;, \u0026#34;image_url\u0026#34;: url } ) except Exception as e: return e # Return the error to the model if it fails, otherwise return the contents return \u0026#34;\\n\\n\u0026#34;.join([f\u0026#34;### Page {i+1}\\n{response.pages[i].markdown}\u0026#34; for i in range(len(response.pages))]) pdf_response = _perform_ocr(\u0026#34;https://arxiv.org/pdf/2201.04234\u0026#34;) 输出格式：\n{ \u0026#34;pages\u0026#34;: [ { \u0026#34;index\u0026#34;: 1, \u0026#34;markdown\u0026#34;: \u0026#34;markdown-content\u0026#34;, \u0026#34;images\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;img-0.jpeg\u0026#34;, \u0026#34;top_left_x\u0026#34;: 292, \u0026#34;top_left_y\u0026#34;: 217, \u0026#34;bottom_right_x\u0026#34;: 1405, \u0026#34;bottom_right_y\u0026#34;: 649, \u0026#34;image_base64\u0026#34;: \u0026#34;...\u0026#34; } ], \u0026#34;dimensions\u0026#34;: { \u0026#34;dpi\u0026#34;: 200, \u0026#34;height\u0026#34;: 2200, \u0026#34;width\u0026#34;: 1700 } }, ... } 甚至包括了图像的坐标信息\n通过以下代码，能够或者最终完整的markdown\nfrom mistralai.models import OCRResponse from IPython.display import Markdown, display def replace_images_in_markdown(markdown_str: str, images_dict: dict) -\u0026gt; str: for img_name, base64_str in images_dict.items(): markdown_str = markdown_str.replace(f\u0026#34;![{img_name}]({img_name})\u0026#34;, f\u0026#34;![{img_name}]({base64_str})\u0026#34;) return markdown_str def get_combined_markdown(ocr_response: OCRResponse) -\u0026gt; str: markdowns: list[str] = [] for page in pdf_response.pages: image_data = {} for img in page.images: image_data[img.id] = img.image_base64 markdowns.append(replace_images_in_markdown(page.markdown, image_data)) return \u0026#34;\\n\\n\u0026#34;.join(markdowns) display(Markdown(get_combined_markdown(pdf_response))) OlmOCR github：https://github.com/allenai/olmocr\ndemo：https://olmocr.allenai.org/\n其核心VLM—allenai/olmOCR-7B-0225-preview， 是采用Qwen2-VL-7B 在250,000 页的多样化 PDF 数据集上进行微调而得的\n不仅是开源了模型， 同时还开源了一整套的解析工作管道，及其微调数据集、训练和推理代码。\n环境：\napt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools git clone https://github.com/allenai/olmocr.git cd olmocr pip install -e . pip install sgl-kernel==0.0.3.post1 --force-reinstall --no-deps pip install \u0026#34;sglang[all]\u0026gt;=0.4.3.post2\u0026#34; --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/ local usage\npython -m olmocr.pipeline /content/localworkspace --pdfs /content/pdf/*.pdf 解析成功后，会在本地生成一个jsonl文件，包括每一页的解析结果\n(样例)\n{ \u0026#34;id\u0026#34;:\u0026#34;id\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;markdown-content\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;olmocr\u0026#34;, \u0026#34;added\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;Source-File\u0026#34;: \u0026#34;file-path\u0026#34;, \u0026#34;olmocr-version\u0026#34;: \u0026#34;0.1.58\u0026#34;, \u0026#34;pdf-total-pages\u0026#34;: 17, \u0026#34;total-input-tokens\u0026#34;: 67608, \u0026#34;total-output-tokens\u0026#34;: 14821, \u0026#34;total-fallback-pages\u0026#34;: 0}, \u0026#34;attributes\u0026#34;: {\u0026#34;pdf_page_numbers\u0026#34;: [[0, 3519, 1], [3519, 8803, 2], [8803, 12566, 3], [12566, 15747, 4], [15747, 19147, 5], [19147, 22975, 6], [22975, 25485, 7], [25485, 31108, 8], [31108, 35902, 9], [35902, 38215, 10], [38215, 39584, 11], [39584, 42844, 12], [42844, 45389, 13], [45389, 48985, 14], [48985, 49297, 15], [49297, 49579, 16], [49579, 51463, 17]] } 实际测试共72页pdf（arxiv论文），使用Nvidia-L4 GPU\ninput_tokens 151.32 tokens/sec\noutput_tokens 47.16 tokens/sec\n总耗时40min。\nSmolDocling Huggingface\npaper\n由 IBM Research 和 Hugging Face 开发的VLM，专为文档转换设计，可以处理整个页面，生成一种称为 DocTags 的自定义标记格式，这种格式能详细记录文档元素的文本内容、布局结构和位置信息。模型参数仅为 256 M。\n模型通过端到端方式处理整个文档页面，生成 DocTags 序列。DocTags 是一种受 OTSL 启发的标记格式，采用 XML 风格标签（如 \u0026lt;text\u0026gt;、\u0026lt;table\u0026gt;、\u0026lt;code\u0026gt;、\u0026lt;formula\u0026gt;）与位置标签（如 \u0026lt;loc_x1\u0026gt;\u0026lt;loc_y1\u0026gt;\u0026lt;loc_x2\u0026gt;\u0026lt;loc_y2\u0026gt;）结合，基于 0-500 网格映射页面尺寸，确保捕获元素的空间位置和上下文关联。支持多种文档元素的重现，包括标题、图表、表单、代码、公式、表格、脚注、列表、页眉/页脚和章节标题，并提供 OCR（光学字符识别）、阅读顺序和层次链接（如标题与图表/表格的对应）。\nTo the Future from Document Parsing to Document Intelligence 当前基于VLM的方案依旧存在一些问题\n在不同领域和场景下的文档处理中，需要调整解析的prompt或增加相应的LLM优化内容。这些prompt或工作流通常是定制化的，缺乏良好的泛化能力。如果能够开发出一个智能代理应用，针对不同场景的文档进行解析和优化调整，那将是非常理想的。然而，目前市场上尚未出现这样的解决方案。\n文档解析是RAG任务的第一步，与之密切相关的下一步是文本分块（chunking），这与解析过程息息相关，甚至可以结合在一起或者合并在一起。在VLM解析中，我们已经能够获得文档的某种结构化信息（或语义层级关系）。这种信息对于智能代理进行文本分块具有参考价值，而不仅仅是基于长度的简单分割。\n解析成markdown/json的执念或许是过时的想法了 为何不直接做多模态的RAG呢？\n与其设计复杂的系统：从文档解析成markdown/json-分块-建立索引-检索，不如直接将vision-encoder 隐藏层输出用于索引的构建，这也是ColPali这篇工作的想法。\n甚至是类似CAG（Cache-Augmented Generation）中的想法， 直接利用VLM的KV-Cache 也是完全有可能的。\n附录1: VLM Benchmark OCRBench 和 OCRBench v2 OCRBench 和其升级版 OCRBench v2 是用于评估大型多模态模型（LMMs）在光学字符识别（OCR）任务中的性能的基准。OCRBench 包括文本识别、场景文本相关的视觉问答（VQA）、文档导向的VQA、关键信息提取和手写数学表达式识别等任务。OCRBench v2 扩展了任务范围，覆盖更多场景（如街道场景、收据、公式、图表等），包含10,000个经过人工验证的问题-答案对。\nOCRBench v2 paper\nOCRBench paper\nOmniDocBench v2 OmniDocBench v2 是一个专门为PDF文档解析设计的基准\n它包括981页PDF，涵盖9种文档类型（如学术论文、教科书、幻灯片等）、4种布局类型和3种语言类型\n丰富的注释信息，包括块级（如文本段落、标题、表格）和跨度级（如文本行、公式）元素\nGitHub 仓库\npaper\nCC-OCR 包括四个轨道：多场景文本阅读、多语言文本阅读、文档解析和关键信息提取\n覆盖10种语言（如英语50.3%、中文28.2%）\npaper\n附录2: 比较热门的几个开源文档解析项目 marker https://github.com/vikparuchuri/marker\n使用 Surya 模型检测文本块的布局和阅读顺序，从 PDF 中提取文本，尤其是图像或非文本元素\n可选 LLM 处理，（支持 Gemini ，Claude，ollama）来优化输出，处理复杂元素\n如何使用：\npip install marker-pdf[full] # 命令行 marker_single /path/to/file.pdf from marker.converters.pdf import PdfConverter from marker.models import create_model_dict from marker.output import text_from_rendered converter = PdfConverter( artifact_dict=create_model_dict(), ) rendered = converter(\u0026#34;FILEPATH\u0026#34;) text, _, images = text_from_rendered(rendered) 核心部分PdfConverter\nMarker 和 Surya OCR 模型的权重受 cc-by-nc-sa-4.0 许可限制，但对于年收入低于 500 万美元且终身 VC/天使融资低于 500 万美元的组织，该限制可豁免\nMinerU https://github.com/opendatalab/MinerU/tree/master\n文档：https://mineru.readthedocs.io/zh-cn/latest/\nMinerU 主要通过 PDF-Extract-Kit 使用以下模型：\n布局检测：DocLayout-YOLO_ft, YOLO-v10_ft, LayoutLMv3_ft\n公式检测：YOLOv8_ft\n公式识别：UniMERNet\nOCR：PaddleOCR\n表格识别：PaddleOCR+TableMaster, StructEqTable, StructTable-InternVL2-1B\nmarkitdown https://github.com/microsoft/markitdown\n如何使用：\npip install markitdown openai 普通的pdf解析\nfrom markitdown import MarkItDown md = MarkItDown() result = md.convert(\u0026#34;you-pdf-file.pdf\u0026#34;) print(result.text_content) 其本质是采用pdfminer，做PDF 文本提取\n支持llm做image caption：\nfrom markitdown import MarkItDown from openai import OpenAI client = OpenAI() md = MarkItDown(llm_client=client, llm_model=\u0026#34;gpt-4o\u0026#34;) result = md.convert(\u0026#34;example.jpg\u0026#34;) print(result.text_content) (RAGFlow)DeepDoc github\n支持多种文档格式如PDF、DOCX、EXCEL、PPT\n两个组成部分：视觉处理和解析器\n通过OCR技术识别文本，并进行版面分析以识别不同类型的区域，如表格、标题、段落\n附录3: VLM 汇总表格 模型名称 参数量 OCRBench OCRBenchV2 ChartQA Phi-4-Multimodal 5.6B 84.4 - 81.4 Qwen2.5-VL-3B 3B 82.2 54.3 80.0 Qwen2.5-VL-7B 7B 87.7 56.3 85.0 Qwen2.5-VL-72B 72B - 61.5 - Gemma3-4B 4B - - 79.8 Gemma3-12B 12B - - 83.5 Gemma3-27B 27B - - 83.4 SmolDocling 256M - - - GPT-4o-mini - 77.1 - 54.5 参考和数据来源：\nPhi-4-Mini Technical Report\nGemma3Report\nQwen2.5-VL Technical Report\nSmolDocling\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/","summary":"\u003cp\u003e\u003ca href=\"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/#smoldocling\"\u003eUpdated on 2025-03-29: Add SmolDocling \u0026amp; VLM Summary\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e技术迭代速度之快令人惊叹，前作 \u003ca href=\"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86/\"\u003eRAG工具箱：文档解析与表格处理\u003c/a\u003e 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。\u003c/p\u003e\n\u003cp\u003e本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。\u003c/p\u003e\n\u003ch2 id=\"过去的技术栈总结\"\u003e过去的技术栈总结\u003c/h2\u003e\n\u003cp\u003e在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决\u0026quot;garbage in, garbage out\u0026quot;的输入质量问题，后续处理环节将难以发挥应有价值。\u003c/p\u003e\n\u003cp\u003e传统文档解析技术长期受限于以下核心痛点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e结构化信息缺失：无法准确识别文档标题、副标题等层级结构\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e特殊内容处理薄弱：数学公式、专业符号解析准确率低下\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e复杂表格解析困境：跨页表格、合并单元格等场景支持不足\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e图像信息提取瓶颈：扫描文档、手写体识别效果欠佳\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e版式适应性问题：多栏布局、影印版本等文档格式兼容性差\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e从技术角度，过去文档解析的底层逻辑和框架：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e纯文本解析: \u003ccode\u003ePyPDF\u003c/code\u003e, \u003ccode\u003ePyMuPDF\u003c/code\u003e只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。\u003c/p\u003e\n\u003ch2 id=\"benchmark\"\u003eBenchmark\u003c/h2\u003e\n\u003cp\u003e为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。\u003c/p\u003e\n\u003cp\u003e现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/spaces/echo840/ocrbench-leaderboard\"\u003eOCRBench\u003c/a\u003e、\u003ca href=\"https://huggingface.co/spaces/ling99/OCRBench-v2-leaderboard\"\u003eOCRBench-V2\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/opendatalab/OmniDocBench\"\u003eOmniDocBench\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2412.02210\"\u003eCC-OCR\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e…and more\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e（关于benchmark的具体内容见附录）\u003c/p\u003e\n\u003cp\u003e这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选\u003c/p\u003e\n\u003cp\u003e当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。\u003c/p\u003e\n\u003cp\u003e不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。\u003c/p\u003e\n\u003cp\u003e根据benchmark和实际测试结果，目前几个值得关注的开源VLM：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5\"\u003eQwen2.5-VL\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/microsoft/Phi-4-multimodal-instruct\"\u003ePhi-4-multimodal\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct\"\u003eLlama 3.2 Vision\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/allenai/olmOCR-7B-0225-preview\"\u003eolmocr\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eand more …\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"qwen25-vl系列模型\"\u003eQwen2.5-VL系列模型\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks\"\u003ecookbook\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://qwenlm.github.io/zh/blog/qwen2.5-vl/\"\u003eBlog\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2502.13923\"\u003eTechnical Report\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。\u003c/p\u003e\n\u003cp\u003e在Technical Report 中一些和document-parse有关的内容：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在第一阶段视觉预训练中（仅训练ViT），针对\u003cstrong\u003eDocument Parsing\u003c/strong\u003e，设计了\u003cstrong\u003e一套标准化的HTML标签体系\u003c/strong\u003e，包含：段落（\u003ccode\u003ep\u0026gt;\u003c/code\u003e）、表格（\u003ccode\u003e\u0026lt;table\u0026gt;\u003c/code\u003e）、图表（\u003ccode\u003e\u0026lt;div class=\u0026quot;chart\u0026quot;\u0026gt;\u003c/code\u003e）、公式（\u003ccode\u003e\u0026lt;div class=\u0026quot;formula\u0026quot;\u0026gt;\u003c/code\u003e）、图像标注（\u003ccode\u003e\u0026lt;div class=\u0026quot;image caption\u0026quot;\u0026gt;\u003c/code\u003e）、OCR文本（\u003ccode\u003e\u0026lt;div class=\u0026quot;image ocr\u0026quot;\u0026gt;\u003c/code\u003e）、乐谱（\u003ccode\u003e\u0026lt;div class=\u0026quot;music sheet\u0026quot;\u0026gt;\u003c/code\u003e）、化学式（\u003ccode\u003e\u0026lt;div class=\u0026quot;chemical formula\u0026quot;\u0026gt;\u003c/code\u003e）等模块。每个模块均通过 \u003ccode\u003edata-bbox\u003c/code\u003e 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系\u003c/p\u003e","title":"RAG工具箱：基于多模态大模型的文档解析方案（2025版）"},{"content":"一些适合入门的RAG材料\n原理、科普 Retrieval Augmented Generation (RAG) for LLMs\nLangchain|Retrieval augmented generation (RAG)\nA Practitioners Guide to Retrieval Augmented Generation (RAG)\nRAG for Beginners: The Complete Guide to Retrieval Augmented Generation\nWhat is Retrieval Augmented Generation (RAG)?\nRetrieval augmented generation (RAG)\n思考题：\nWhy RAG？ RAG解决了什么问题？RAG和SFT如何选择？RAG的优势？\n什么样的问题是RAG无法解决的？什么样的数据适合使用RAG\n为什么要做chunking？chunk-size受那些因素制约？\n什么是embedding？向量库在做什么？一定要做语义匹配吗？什么是reranking？\n如何evaluate 检索效果的好坏 ？\nRAG中，LLMs起到的作用？\nHands-On 前置任务 获取模型服务：\n获得LLMs供应商API： 推荐 OpenRouter（仅LLMs，需要梯子，境外信用卡或Crypto），SilconFlow（LLMs+embedding,无需梯子,有送token），Groq（仅LLMs, 需梯子，速度快，免费）\n或采用本地部署： 推荐Ollama\n例子,采用openRouter API实现LLM对话：\nfrom openai import OpenAI client = OpenAI( base_url=\u0026#34;https://openrouter.ai/api/v1\u0026#34;, api_key=\u0026#34;\u0026lt;OPENROUTER_API_KEY\u0026gt;\u0026#34;, ) completion = client.chat.completions.create( model=\u0026#34;openai/gpt-4o\u0026#34;, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the meaning of life?\u0026#34; } ] ) print(completion.choices[0].message.content) 例子：基于本地ollama启动的deepseek R1（蒸馏版本）对话\nfrom openai import OpenAI client = OpenAI( base_url = \u0026#39;http://localhost:11434/v1\u0026#39;, api_key=\u0026#39;ollama\u0026#39;, # required, but unused ) response = client.chat.completions.create( model=\u0026#34;deepseek-r1:7b\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Who won the world series in 2020?\u0026#34;} ] ) print(response.choices[0].message.content) 使用ollama 的embdding model Embedding models\n低代码方案 不愿意写代码，可以使用一些低代码Agent平台 好处：可以快速体验，上手，低代码 坏处：不支持自定义修改，很难理解这一系统原理\nDify官方教程：使用知识库搭建智能客服机器人\n代码教程 A beginner\u0026rsquo;s guide to building a Retrieval Augmented Generation (RAG) application from scratch 不依赖框架，无需向量库和embedding模型， 采用本地部署的ollama，便于从第一性原理理解这一系统\nBuilding RAG Applications Without LangChain or LlamaIndex 不依赖框架，从底层开发RAG应用，使用openAI调用LLMs，sentence-transformers 做embedding， chromadb作为向量库\nLangchain 官方文档中关于RAG的教程:\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nRAG From Scratch\nLangChain: Chat with Your Data\ndatawhale中文翻译\n【关于为什么不推荐langchain之类的框架】：虽然能够快速上手跑通一个项目；langchain因其高度抽象化高度集成，很难进行进一步开发；langchain只适合快速做demo的场景使用；\nmistral 官方的RAG教学\nmilvus向量库官方的RAG quick-start ","permalink":"https://niraya666.github.io/posts/rag-tutorial-for-beginner/","summary":"\u003cp\u003e一些适合入门的RAG材料\u003c/p\u003e\n\u003ch2 id=\"原理科普\"\u003e原理、科普\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.promptingguide.ai/research/rag\"\u003eRetrieval Augmented Generation (RAG) for LLMs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://python.langchain.com/docs/concepts/rag/\"\u003eLangchain|Retrieval augmented generation (RAG)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval\"\u003eA Practitioners Guide to Retrieval Augmented Generation (RAG)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://blog.typingmind.com/rag-for-beginners/\"\u003eRAG for Beginners: The Complete Guide to Retrieval Augmented Generation\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag\"\u003eWhat is Retrieval Augmented Generation (RAG)?\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://python.langchain.com/docs/concepts/rag/\"\u003eRetrieval augmented generation (RAG)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e思考题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eWhy RAG？ RAG解决了什么问题？RAG和SFT如何选择？RAG的优势？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e什么样的问题是RAG无法解决的？什么样的数据适合使用RAG\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e为什么要做chunking？chunk-size受那些因素制约？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e什么是embedding？向量库在做什么？一定要做语义匹配吗？什么是reranking？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e如何evaluate 检索效果的好坏 ？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRAG中，LLMs起到的作用？\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"hands-on\"\u003eHands-On\u003c/h2\u003e\n\u003ch3 id=\"前置任务\"\u003e前置任务\u003c/h3\u003e\n\u003cp\u003e获取模型服务：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e获得LLMs供应商API： 推荐 OpenRouter（仅LLMs，需要梯子，境外信用卡或Crypto），SilconFlow（LLMs+embedding,无需梯子,有送token），Groq（仅LLMs, 需梯子，速度快，免费）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e或采用本地部署： 推荐\u003ca href=\"https://ollama.com/\"\u003eOllama\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e例子,采用openRouter API实现LLM对话：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eopenai\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eOpenAI\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eOpenAI\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"n\"\u003ebase_url\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;https://openrouter.ai/api/v1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"n\"\u003eapi_key\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026lt;OPENROUTER_API_KEY\u0026gt;\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecompletion\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecompletions\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;openai/gpt-4o\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;What is the meaning of life?\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecompletion\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e例子：基于本地ollama启动的deepseek R1（蒸馏版本）对话\u003c/p\u003e","title":"RAG工具箱：RAG Tutorial for Beginner"},{"content":"值得关注的模型和新技术 o3-mini OpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。\nOpenAI o3-mini\nQwQ-Max-Preview QwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。\n我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max\n官方blog：\u0026hellip; QwQ-Max-Preview\nClaude 3.7 Sonnet Claude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。\nClaude 3.7 Sonnet and Claude Code\nGrok-3 Grok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、\u0026ldquo;Think\u0026quot;模式分步推理，以及\u0026quot;Big Brain\u0026quot;模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium+订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。\nGrok-3: The Next Evolution in AI by xAI\nDeepseek OpenSourceWeek FlashMLA FlashMLA\n功能描述：FlashMLA是一个高效的解码内核，专为多头潜在注意力（MLA）设计，优化Hopper GPU上的变长序列处理。它支持BF16和分页KV缓存（块大小64），已在生产环境中使用。性能指标：在H800 SXM5 GPU上（使用CUDA 12.6），FlashMLA实现3000 GB/s内存带宽和580 TFLOPS计算吞吐量，分别在内存绑定和计算绑定场景下表现出色。性能提升与资源优化：这些指标表明FlashMLA显著提升了解码任务的GPU性能，特别是在处理长序列时减少了内存和计算开销。相比传统内核，其高吞吐量显示出对GPU资源的充分利用。 功能描述：FlashMLA是一个高效的解码内核，专为多头潜在注意力（MLA）设计，优化Hopper GPU上的变长序列处理。它支持BF16和分页KV缓存（块大小64），已在生产环境中使用。 性能指标：在H800 SXM5 GPU上（使用CUDA 12.6），FlashMLA实现3000 GB/s内存带宽和580 TFLOPS计算吞吐量，分别在内存绑定和计算绑定场景下表现出色。 性能提升与资源优化：这些指标表明FlashMLA显著提升了解码任务的GPU性能，特别是在处理长序列时减少了内存和计算开销。相比传统内核，其高吞吐量显示出对GPU资源的充分利用。 DeepEP DeepEP\n功能描述：DeepEP是一个为混合专家（MoE）和专家并行（EP）设计的通信库，提供高吞吐量和低延迟的all-to-all GPU内核。它支持FP8运算，并包括不对称域带宽转发的优化内核，适用于训练和推理预填充任务。对于延迟敏感的推理解码，提供纯RDMA的低延迟内核。此外，它引入了基于钩子的通信-计算重叠方法，不占用流式多处理器（SM）资源。 性能指标：虽然具体量化数据有限，但其设计目标是最大化InfiniBand和NVLink带宽利用，减少通信延迟，尤其在分布式训练中。 性能提升与资源优化：DeepEP通过高效的通信管理减少了数据传输时间，提升了分布式系统的整体性能，特别是在大规模GPU集群中，优化了资源利用率。 DeepGEMM DeepGEMM\n功能描述：DeepGEMM是一个FP8通用矩阵乘法（GEMM）库，支持密集和MoE分组GEMM，专为Hopper张量核心设计。它使用CUDA编写，通过轻量级即时编译（JIT）模块在运行时编译内核，无需安装时的复杂编译。核心逻辑仅约300行代码，采用CUDA核心两级累积解决FP8精度问题，借鉴CUTLASS和CuTe的概念但避免依赖其复杂模板。 性能指标：在Hopper GPU上，DeepGEMM实现1350+ TFLOPS的FP8计算性能，超越大多数专家调优内核，尤其在各种矩阵尺寸上表现优异。 性能提升与资源优化：FP8的低精度计算减少了内存使用和计算时间，DeepGEMM的高性能显示出对GPU计算资源的有效利用，特别适合训练和推理中的矩阵运算。 DualPipe DualPipe\n功能描述：DualPipe是一个双向管道并行算法，旨在在DeepSeek-V3和R1的训练中实现计算-通信重叠。它通过创新的调度减少管道气泡，将前向和后向计算与通信阶段完全重叠，特别是在跨节点专家并行中隐藏通信开销。它充分利用InfiniBand和NVLink带宽，优化内存占用，无需昂贵的张量并行（TP）。 性能指标：DeepSeek-V3的训练仅需278.8万H800 GPU小时，相比其他模型（如OpenAI的GPT-4，训练成本约1亿美元），显示出显著的效率提升。论文提到，通过DualPipe，计算-通信比保持恒定时，可实现近零的all-to-all通信开销。 性能提升与资源优化：DualPipe通过减少通信瓶颈和优化管道调度，显著降低了训练时间和计算资源需求，提升了大规模GPU集群的利用率。 EPLB EPLB\n功能描述：EPLB用于专家并行（EP）场景，通过复制负载重的专家并启发式地分配到不同GPU，确保负载均衡。由于DeepSeek-V3采用组限制专家路由，EPLB尝试将同一组的专家放置在同一节点，减少节点间数据流量。其算法已在eplb.py中开源，便于再现和部署。 性能指标：具体量化数据有限，但其设计目标是通过负载均衡优化GPU利用率，减少训练中的瓶颈。 性能提升与资源优化：EPLB通过防止GPU过载或未充分利用，确保资源的高效分配，特别是在专家负载不均时，提升训练效率和稳定性。 值得关注的研究和论文 Deliberative Alignment https://openai.com/index/deliberative-alignment/\nhttps://arxiv.org/abs/2412.16339\n针对o1-like reasoning 模型的安全对齐方法\n当前模型的安全性问题 即便经过safety training，仍可以被jailbreak 模型的响应时间过快，导致它们不能充分评估所有可能的风险和复杂情况 模型并没有直接学习如何确保安全，而是通过标注的数据来间接理解应如何回应 Deliberative alignment 解决了： 模型响应速度过快、缺乏对安全规范的深入理解 通过直接学习safety specifications，使模型不仅仅记住安全规范的内容，还能在实际应用时根据上下文仔细考虑这些规范 在推理阶段对安全规范进行复杂的推理 Methods 对齐训练使用过程性监督和结果性监督的结合\ntrain an o-style model for helpfulness, without any safety-relevant data then build a dataset of (prompt, completion) pairs where the CoTs in the completions reference the specifications. inserting the relevant safety specification text for each conversation in the system prompt, generating model completions, and then removing the system prompts from the data SFT on this dataset then use RL to train the model to use its CoT more effectively 模型自动从安全规范和已分类的安全提示中生成训练数据，而不需要人工标注的回答 Trading inference-time compute for adversarial robustness | OpenAI Trading inference-time compute for adversarial robustness\npaper\n研究推理模型在推理阶段增加计算资源，对于对抗性攻击（adversarial attacks）的影响\n实验发现：\n推理时间计算的增加提升了对抗性鲁棒性：在大多数情况下，随着推理时间计算的增加，攻击成功率趋近于零。这表明，允许模型在推理阶段花费更多计算资源进行推理，可以有效提高其对抗性鲁棒性；\n无需对抗性训练即可提升鲁棒性： 仅通过增加推理时间计算来提升模型的鲁棒性，而无需专门的微调训练；\n推理时间计算的局限性： 在某些特定攻击下，增加推理时间计算可能不会带来显著的安全性提升，甚至可能导致新的安全漏洞；\n此外，文中还介绍了两种针对推理计算扩展模型的新型攻击方式：\nThink Less 攻击：攻击者诱导模型减少推理时间，从而降低其对抗性鲁棒性 Nerd Sniping 攻击：攻击者诱导模型在推理过程中浪费计算资源，使其在无关紧要的推理任务上消耗过多计算能力，可能导致安全漏洞。 Constitutional Classifiers: Defending against universal jailbreaks ｜Anthropic 为提升LLMs对 universal jailbreaks（通用越狱攻击）的防御能力，Anthropic 研究团队提出了一种名为 Constitutional Classifiers 的方法。\nhttps://www.anthropic.com/research/constitutional-classifiers\n解决的问题 LLMs 易受越狱攻击影响： 现有的 LLMs 仍然容易受到 prompt engineering攻击，例如使用 超长输入 或 特殊格式（如大小写混合）来绕过安全限制； 现有防御机制的局限性：传统的安全机制往往难以检测 通用越狱攻击（universal jailbreaks），即能够绕过多个安全限制的攻击方式 安全性与可用性之间的权衡：许多安全机制会导致 过度拒绝（over-refusal），即模型拒绝回答大量无害的查询，从而影响用户体验 亮点 基于“宪法”规则的分类器：采用 Constitutional AI 的理念，使用一套预定义的“宪法”规则来指导模型的安全性，这些规则明确划分了允许和禁止的内容，并用于训练分类器。 输入与输出双重分类器：该系统包含 输入分类器（检测并拦截恶意输入）和 输出分类器（实时监测模型生成的内容，并在检测到有害信息时立即终止响应） 合成数据增强训练：利用 Claude 生成大量合成数据以提高分类器的泛化能力 实时流式预测：支持 流式预测，即在生成每个 token 时评估其潜在的有害性，从而在检测到风险时立即终止响应，而无需等待完整的输出生成 结论 显著降低越狱成功率（从 86% 降至 4.4%） 保持较低的误拒率（仅增加 0.38%） 计算成本适中（增加 23.7%） Qwen2.5-1M Tech report\nBlog\n开源的长上下文模型， 上下文扩展到1M；包含Qwen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M两个版本；\n方法 预训练阶段： 从Qwen2.5 的checkpoint 开始 （上下文4K） 将RoPE基础频率 从10,000 提高到10,000,000 SFT阶段：分两个阶段 第一阶段：仅在短指令（32K）上微调 第二阶段：混合短指令（32K）和长指令（256K）进行训练 RL阶段：在短文本（8K）上训练，能够泛化；最终获得256K instruction-tuning 模型 长度外推： 引入 Dual Chunk Attention (DCA)， 以解决训练过程中未见过的，Query和Key间相对位置距离过大问题 稀疏注意力机制，以提升推理速度： 分块预填充（Chunked Prefill）， MLP层激活权重的显存使用量可减少96.7% 稀疏性优化 注意项 Qwen2.5-7B-Instruct-1M 至少需要120GB\nQwen2.5-14B-Instruct-1M 至少需要320GB\nLarge Language Diffusion Models (LLaDA) paper\n基于Diffusion Models的新型LLM；能够在多个基准测试中与 LLaMA3 8B 竞争；\n不同于传统的LLM，采用Autoregressive的方式，即从左到右预测下一个token，LLaDA采用掩码扩散（Masked Diffusion）方法: 训练时，LLaDA 通过前向过程（Forward Process）随机掩盖输入文本中的 token，并在反向过程（Reverse Process）中预测被掩盖的 token， 从而允许 LLaDA 在双向上下文中进行推理，而不像自回归模型那样受限于单向信息流；\nLLaDA 采用标准的预训练 + SFT范式，并在2.3 万亿个 token上进行训练，随后使用450 万对数据进行 SFT\nMercury, the first commercial-scale diffusion large language model 由 Inception Labs 开发的首个商业级扩散大语言模型\nMercury 在 NVIDIA H100 GPU 上的推理速度超过 1000 tokens/sec，相比于 GPT-4o Mini（59 tokens/sec）和 Claude 3.5 Haiku（61 tokens/sec），提升了 10-20 倍 这使得 Mercury 比传统 LLM 便宜 5-10 倍，适用于大规模商业应用 Mercury 适用于代码生成（Mercury Coder）、对话 AI、企业自动化等多个领域 免费试用：Mercury Coder 目前已上线，可在 Inception Labs Playground 试用 Trade-offs of Diffusion Language Models 但也存在一些显著的权衡，例如高推理成本、在链式推理（CoT）中仍需多次迭代以及延迟的流式输出\nThoughts Are All Over the Place: On the Underthinking of o1-Like LLMs paper\no1-like 模型通过扩展推理计算能力来提升推理深度，但存在着underthinking的问题，即它们在推理过程中频繁切换思路，而未能充分探索有前景的推理路径，导致推理深度不足，影响最终的准确性；\n对此研究者引入 Underthinking Score，用于衡量模型在错误答案中的 token 效率，以量化思维切换对推理质量的影响；\n为了缓解Underthinking问题，研究者提出了一种新的解码策略 TIP（Thought Switching Penalty），其核心思想是：\n在解码过程中对思维切换施加惩罚，鼓励模型在当前推理路径上进行更深入的探索，而非过早切换 TIP 通过调整 logits 进行惩罚，减少模型在短时间内频繁改变推理方向的可能性 Optimizing Large Language Model Training Using FP4 Quantization paper\n由于FP4格式的动态范围和表示能力有限，直接将LLMs量化到这种低比特格式通常会导致模型性能大幅下降\n通过以下两个关键创新点来应对挑战\n可微分量化估计器（Differentiable Quantization Estimator, DGE）：为了精确的权重更新，提出了一种可微分量化估计器，以减少FP4计算中的梯度更新误差。 异常值夹紧和补偿策略（Outlier Clamping and Compensation, OCC）：为了防止在LLMs训练过程中常见的激活值异常值导致的量化精度损失，提出了一种异常值夹紧和补偿策略。 FP4训练框架能够在保持与BF16和FP8相近的准确性的同时，有效地扩展到高达130亿参数的LLMs，并在最多1000亿个训练token上进行训练\n推荐内容 一些仓库 专注于强化学习（RL）提升大语言模型（LLM）推理能力的仓库： github.com/bruno686/Awesome-RL-based-LLM-Reasoning\n适合LLMs的抓取数据工具集合：\ngithub.com/patrickloeber/llm-data-scrapers\nMCP服务器列表： github.com/punkpeye/awesome-mcp-servers\n开源小红书图文采集工具： github.com/JoeanAmier/XHS-Downloader\n基于 LLM 的智能字幕助手： github.com/WEIFENG2333/VideoCaptioner\n阅读和学习 Reasoning best practices ｜ OpenAI： openAI 官方的reasoning模型使用指南\nalphaxiv：可在 arXiv 论文上进行评论讨论，选择文章内容同AI讨论\nAI Agents Course｜ huggingface：huggingface 官方的AI Agent课程\n影音记录 精选歌单 Live演出 02.16 Sigur Rós TOKYO GARDEN THEATER\n电影 哪吒之魔童闹海 ⭐️⭐️⭐️⭐️\n书\u0026amp;阅读摘录 《逃走的人》 李颖迪 谈论工作的意义似乎早就过时了，太热情了甚至显得傻。“工作就是工作。”这才是正确的态度。我们说起工作，说的是绩效和KPI，不是它的乐趣、意义和自我实现。当时仍在新冠流行期间，它更加剧了某种困顿感和停滞感。但我们其实也害怕真的停下—离开既定轨道，比如辞职了，之后还能找到下一份工作吗？就这样迟疑着，踌躇着，不满意想走，想走又不敢走。明明还“年轻”，按照教科书上的说法，这不应该正是踌躇满志的时候吗？\n2022年，中国城市的房子往往每平方米一万元上下——在北京，这个数字是四万（海淀、朝阳等地甚至每平方米九万），上海、深圳也差不多——在城市买房，往往意味着贷款，动辄几百万。年轻人买房等于交出人生的主动权：未来几十年运转于一场数字游戏般的任务，上班，赚钱，还房贷。但向往城市，就不得不挤上这条令人望而生畏的漫长轨道。\n人们对此有不同的看法。有人认为，低廉的房价将源源不断地吸引年轻人来到鹤岗，从而形成新的活力。\n但另一个人说，人们在城市里购房，购买的只是那一套简单的钢筋水泥么？\n他接着说，不，人们购买的是希望。“房价走低不可能带来希望。没有希望，这里的房价才会走低。”\n去鹤岗，也不能说他就此自由了，要看怎么定义“自由”——自由不是想干什么就干什么吧—他停了下——自由是不想干什么就不干什么。但他又换了一个说法，也可能人生都这样，还能烂到哪里去？\none minute blog: the triangle of talent The weird thing is: as a manager, you’ll spend 90% of your time dealing with the employees on the left (problem employees) or the middle (average joes). But it’s the people on the right side (the star employees) that create 90% of the value in the company.\nCEOs - how many level 5s do you have on your team? If you’re just started (cofounders) - you should have 100% level 5 If you\u0026rsquo;re a small team (5-20 people) - you should have at least 30% If you’re a medium team (20-150 people) - shoot for 15% If you’re a big company (150+ people) - 5-10%\u0026hellip; but honestly just quit, big companies are no fun\nSuno创始人访谈：至少对音乐来说，Scaling Law不是万灵药｜Bolt荐阅 音乐与文字非常不同，不能简单的认为在 AI 的发展过程中，语音只是比文字发展的慢了一点，认为 Scaling law 会解决所有问题。音乐是非常主观的领域，要把音乐的模型训练的好、做出好的产品，还需要很多其他的技能。\n一个国家的工程师与律师的比例直接影响经济增长。工程师越多，经济增长越快；律师越多，经济增长越慢\n音乐行业当前有一种“蛋糕就这么大，大家来分”的心态，大家都在争夺有限的财富，导致不公平的分配。如果我们能够一起把蛋糕做大，那么一切都会变得更加容易\n这首先需要让每个人都能享受音乐创作的乐趣\nDeepseek R1可能找到了超越人类的办法 这就导致了预训练撞墙的事实：模型体积虽然增加了 10 倍，但我们已经无法获得比现在多 10 倍的高质量数据了。\nRLHF 本质上是一种讨好人类的训练方式，它让模型输出符合人类偏好，但同时它扼杀了超越人类的可能性。\n使用强化学习（RL）来训练模型思维链成为了所有人的新共识\n未来的模型对人类标注需求会越来越少。\n未来的 Reasoning 模型可以收集用户和模型聊天时 AI 生成的思维链来训练\n硅谷创业教父Paul Graham：为什么你一直在拖延真正想做的事？ “要么在拖延中痛苦，要么让自己行动起来，花费的功夫是一样的。”\n我们为什么害怕“开始”？ 障碍1：缺少经验 障碍2：他人的质疑 障碍3：自我怀疑\n让开始开始的 9 种策略 策略1：用乐观的心态看待事物 策略2：适度的过度自信 策略3：适度的“无知” 策略4：寻找同路人 策略5：关注进步的速度，而非当前的位置 策略6：降低期望值 策略7：选择容易上手的工具 策略8：将失败当成学习的过程 策略9：改变你的衡量标准\n最好的工作是自由市场中持续学习者的创造性表达 “最好的工作是自由市场中持续学习者的创造性表达”（The best jobs are creative expressions of continuous learners in free markets）这句话出自《纳瓦尔宝典》(The Almanackof Naval Ravikant)\n特有知识更多关注你的内在天赋，你真正的好奇心和热情，这种知识通常极富技术性和创造性，因此不能被外包或自动化。\n更舒服的哲学是“做一个制造者，制造人们想要的有趣的东西，练好你的手艺，合适的人最终会找到你。”\n关键是你得迈出第一步——开始创作，成为那1%的内容创作者\n一句话总结普通人如何建立自己的财富之路：产品化你自己\nThe AI Architect — Bret Taylor “Across our customer base, we are seeing a new role emerge - the role of the AI architect. These leaders are responsible for helping define, manage and evolve their company\u0026rsquo;s AI agent over time. They come from a variety of both technical and business backgrounds, and we think that every company will have one or many AI architects managing their AI agent and related experience.”\n“There\u0026rsquo;s a lot of power in combining product and engineering into as few people as possible… few great things have been created by committee.” \u0026hellip;And it\u0026rsquo;s almost impossible to specify the requirements of a product when you\u0026rsquo;re not sure of the limitations of the technology itself.”\nOne thing I just, I just observe is that I think the early Google days had this interesting mix of PM and engineer, which I think you are, you didn\u0026rsquo;t, you didn\u0026rsquo;t wait for PM to tell you these are my, this is my PRD.\nIs your job as a maker of software to author a code in an editor? I would argue no just like a generation ago. Your job wasn\u0026rsquo;t to punch cards in a punch card That is not what your job is. Your job is to produce digital something, whatever it is, what is the purpose of the software that you\u0026rsquo;re making? Your job is to produce that. And so I think that like our jobs will change rapidly and meaningfully, but I think the idea that like our job is to type in a\nThey didn\u0026rsquo;t type faster or produce more code. They did the right thing in the right market, the right time.\n都没人看我为什么还要写博客？ 你拿着相机在城市里穿行，看见一幕——光影交错，人情味流露。你按下快门。 没人关心。 但你并不是为了别人去做。你做，是因为你看到了什么。 写博客也是如此。你写，因为你思考，因为你观察，因为你需要一个“出口”来安放这些想法。 有人看吗？有就算赚到。没有也没关系。创作这件事，本身就已经完成了它的意义。 这才是重点所在。\n写博客能让你理清思绪，让你的观点更明晰。你会更谨慎地组织语言，避免废话——说实话，你是为自己写。如果连你自己都提不起兴趣，那别人就更没有理由了\n","permalink":"https://niraya666.github.io/monthly/2025-02-%E6%9C%88%E5%88%8A/","summary":"\u003ch1 id=\"值得关注的模型和新技术\"\u003e值得关注的模型和新技术\u003c/h1\u003e\n\u003ch2 id=\"o3-mini\"\u003eo3-mini\u003c/h2\u003e\n\u003cp\u003eOpenAI o3-mini 是一款高效且成本优化的推理模型，专为科学（Science）、技术（Technology）、工程（Engineering）和数学（Mathematics）（STEM）领域优化。它在数学、编程和科学推理方面表现出色，能够在 AIME 2024 和 GPQA 等基准测试中达到或超过 OpenAI o1 的水平。o3-mini 具备三种推理模式（低、中、高），可根据需求在速度和准确性之间进行权衡。此外，它支持函数调用、结构化输出和视觉任务。相比 o1-mini，o3-mini 的响应速度提高了 24%，平均响应时间为 7.7 秒。它的上下文窗口为 200k tokens，输入成本为每百万 tokens 1.10 美元，输出成本为 4.40 美元。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://openai.com/index/openai-o3-mini/\"\u003eOpenAI o3-mini\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"qwq-max-preview\"\u003eQwQ-Max-Preview\u003c/h2\u003e\n\u003cp\u003eQwQ-Max-Preview是阿里巴巴Qwen系列的最新推理模型，基于Qwen2.5-Max架构开发，专注于提升数学、编码及多领域复杂问题的解决能力。该模型在LiveCodeBench代码评估中取得65.6分，超过OpenAI的o1中型模型（63.4分）和o3迷你低配版（60.9分），展现了卓越的代码生成与逻辑推理性能。其核心优势包括深度推理、Agent任务处理及通用领域适应性，特别适合需要实时响应的隐私敏感场景。作为预览版，QwQ-Max-Preview为后续开源版本铺路，未来将发布Apache 2.0许可证下的完整模型QwQ-Max及轻量级版本（如QwQ-32B），并计划推出iOS/Android端Qwen Chat应用以增强用户体验。阿里巴巴同时宣布未来三年投入530亿美元加强AI基础设施，进一步推动该模型在行业中的竞争力。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e我们计划在不久的将来以 Apache 2.0 许可协议开源发布 QwQ-Max 以及 Qwen2.5-Max\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e官方blog：\u003ca href=\"https://qwenlm.github.io/zh/blog/qwq-max-preview/\"\u003e\u003cthink\u003e\u0026hellip;\u003c/think\u003e QwQ-Max-Preview\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"claude-37-sonnet\"\u003eClaude 3.7 Sonnet\u003c/h2\u003e\n\u003cp\u003eClaude 3.7 Sonnet 是 Anthropic 推出的首个“混合推理模型”，兼具快速响应和深入思考能力，能够根据任务需求在标准模式和扩展思考模式之间切换。其核心能力包括：在复杂任务中通过扩展思考模式进行详细分析和多角度考虑；在编码任务中表现出色，特别是在 SWE-bench Verified 测试中达到行业领先的 70.3%；支持多模态数据处理，展现强大的适应性；以及在 Amazon Bedrock 中提供可调整的推理预算，供开发者根据需求权衡速度、成本和性能。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.anthropic.com/news/claude-3-7-sonnet?utm_source=partner-aws\u0026utm_medium=referral\u0026utm_campaign=sonnet_3-7_launch\"\u003eClaude 3.7 Sonnet and Claude Code\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"grok-3\"\u003eGrok-3\u003c/h2\u003e\n\u003cp\u003eGrok-3是由Elon Musk的xAI公司开发的第三代AI模型，具备2.7万亿参数和12.8万亿token的训练数据集，采用基于NVIDIA GPU的Colossus超级计算集群（20万张GPU）训练，计算能力比前代提升10倍。其性能表现优异，在MMLU（多任务语言理解）基准测试中达到92.7%，GSM8K（数学推理）89.3%，AIME 2025数学竞赛93.3%，GPQA科学推理84.6%。该模型支持128,000 token的上下文窗口（扩展版达100万token），响应延迟仅67毫秒，并具备多模态处理能力（文本、代码、图像）。独特功能包括DeepSearch实时网络研究代理、\u0026ldquo;Think\u0026quot;模式分步推理，以及\u0026quot;Big Brain\u0026quot;模式强化复杂问题解决，主要应用于STEM领域、代码生成和商业分析。目前通过X平台Premium+订阅（$40/月）和专属网站Grok.com提供访问，API接口即将开放。\u003c/p\u003e","title":"2025-02 月刊"},{"content":"写在前面 本文是前段时间部门内技术分享的文字稿整理；\n主要包含2个方面：对于R1模型的简单科普，和开发团队的gen-AI使用指南；\n原以为春节假期能稍作休整，未曾料想deepSeek-R1在春节期间引发行业关注。节后复工，各层级领导便密集展开AI技术进展问询。\n原计划的\u0026quot;二月摸鱼指南\u0026quot;彻底落空，既要向管理层科普大语言模型技术原理，又要指正领导们被各种营销号洗脑所形成的错误认知；既要推进部门AI应用培训，又要加速系统上线进程；简直了，真是TM的谢谢 DeppSeek了。\nAbout DS R1 什么是Reasoning model 何为推理（reasoning）：是对信息进行处理和推断，形成判断或结论的过程；推理通常是一个包含多个推断步骤的过程。推理通常被认为需要抽象思维——也就是说，推理的能力不局限于具体的例子，而是更为普遍的。\nIn Thinking, Fast and Slow, Daniel Kahneman defined System 1 as the automatic, intuitive mode of thinking, and System 2 as the slower, more analytical mode. In the context of autoregressive language models, the usual inference process is akin to System 1 — models generate answers directly. Reasoning is System 2 thinking - models or systems takes time to deliberate to solve more complex problems.\nLLMs本质上是一个根据上文预测下一个token的自回归模型，那么这样的模型是否具备逻辑推理能力呢？从表现特征来看，LLMs更类似于\u0026quot;文科生\u0026quot;的思维模式——在应对需要复杂推理的数学演算、自然科学问题解析及代码编写等场景时，其表现往往力不从心。\n如何让模型的推理能力提升呢？CoT（Chain of Thought， Wei et al. 2022）即“思维链”，是一种使LLM“逐步思考”的方法。它通过要求模型展示解决问题的中间步骤来加强其逻辑推理能力，从而提高决策的质量和准确性。\nCoT更多是从prompting角度， 要求LLM以一定的格式进行回答（“think step by step”），模型对于没有在pre-training中涉及的内容时，效果并不太好，因为过去的LLM训练过程中， 不论是pre-training（预测下一个token）还是SFT（要求模型按照一定的格式输出），都与模型的reasoning 无关，CoT所所展示出的效果，及模型展现出的“推理”，更像是某种程度的“涌现”，而非专门针对性训练，当然也有认为LLMs似乎是在背答案而非真正的推理Can Large Language Models Reason?\n自2024年9月后， OpenAI推出了 o-1 系列模型，也就是世界上第一个推理模型，同时也是第一个专门针对推理任务的模型， 只是“CloseAI” 所提供的信息非常有限， 并没有人知道其如何实现的，大家对其猜测五花八门。\n（from openAI）\n这一切，直到DS R1的出现，才似乎尘埃落定。\nDS R1做了什么 一切，从DS提供的论文出发。\n主要贡献有3点：\n如何通过纯RL训练提升模型的推理能力（DeepSeek-R1-Zero）： 直接在基础模型（DeepSeek-V3-Base）上应用大规模RL，无需SFT作为预热，首次验证纯RL可激励LLM的推理能力,而非采用Reward model 或MCTS。\n如何通过冷启动数据和多阶段训练优化模型的可读性和综合性能（DeepSeek-R1）：引入少量冷启动数据（数千条长CoT示例）进行微调，再通过两阶段RL（推理优化+人类偏好对齐）和SFT提升性能与可读性\n如何将大模型的推理能力高效迁移到小模型： 将大模型（DeepSeek-R1）生成的80万条数据蒸馏到小模型（如Qwen、Llama系列），显著提升小模型推理能力，甚至超越直接对小模型应用RL的效果\n具体而言，对于DeepSeek-R1-Zero，采用了GRPO（Group Relative Policy Optimization，正是DS团队在其另一篇工作DeepSeekMath中所开发的方法），不同于PPO，GRPO无需critic model，降低的显存使用，同时更简化训练过程也让训练过程更高效和稳定。\n此外， 采用FP8（8位浮点数）精度进行训练，也降低了显存占用。\n在奖励机制设计上，采用了rule-based的设计，兼顾Accuracy reward（正确的coding或数学推理结果）与Format rewards（遵循指定输出格式，语言一致性、标记使用等）。\n通过强制模型输出结构化内容（如推理过程和答案）有效引导模型行为；\nDeepSeek-R1-Zero 因采用base-model RL获得，存在Language Mixing ，以及可读性差，对特定任务泛化能力差（因训练目标单一）等问题\nDeepSeek-R1为了解决以上问题：\n→ 引入了语言一致性奖励\n→ 使用从RL阶段生成的checkpoint,通过拒绝采样来筛选出高质量的推理样本\n大致流程：\nSFT(用高质量COT启动RL，以避免早期不稳定性) → 推理强化学习 → SFT(推理+通用领域数据) → 对齐强化学习\n同时， 将DeepSeek-R1生成的80万条数据用于微调小模型，小模型直接继承大模型的推理模式，性能远超直接对小模型应用RL。\n意义和需要澄清的内容 从意义上讲，这是首个在推理能力上与OpenAI o1系列商业模型达到同等水平的开源模型（开源模型权重和公开部分训练细节）；同时，个人认为R1-zero的意义会比R1来的更加重要，因为其完全是基于rule-base RL训练的reasoning 模型， 具备了自我纠错反思的“aha moment“，为开源社区探明了道路，而不是遮遮掩掩的（说的就是你__）；此外， 对于小模型直接用大模型蒸馏从而提升reasoning能力，而不是直接在小模型上RL，为资源受限场景下的模型优化提供了极具参考价值的技术路径。\n一些需要澄清的点：\nNvidia股价大跌，不需要Nvidia了吗：从模型推理的角度，可能有其他的选择，但对于开发运维人员角度，因其生态位和开源社群，Nvidia依旧是首选；训练方面依旧是不可撼动的；\nDS是通过chatGPT蒸馏出来的？：并没有证据能够证明这一点，任何模型对于“自己是谁”的回答都是不具备任何参考价值的，往往都是幻觉。\n赢？不太希望将这件事上升到这样的高度，保持理智，不要捧杀DS，AGI的路还挺长的，任何看完之后心潮澎湃的文章多半都是新闻学的胜利和对于民族主义情绪之利用；但必须承认一点，无论是中美，还是开源闭源的差距都没有想象中的大，似乎差距正在不断缩小。\n**教你本地部署满血版DS-R1？只要99入门到精通？少焦虑，多接触优质信息源，如Twitter(X)上一线研究员的发言讨论和TA们的blog，huggingface上的论文推荐，以及更新：2024年AI领域最值得关注的博主和一手信息源盘点；对于DS- R1所需要的推理硬件参数会在后面专门聊，这里先不展开了。\n泼一些冷水 DS R1的幻觉挺严重的，不论是使用体验上，还是从Vectara’s HHEM的幻觉评测指标上看。\n（from DeepSeek-R1 hallucinates more than DeepSeek-V3）\n对齐不足。从现有的工作和实际效果上来看，DS在模型的对齐方面做的可能还不够。当然，这是把双刃剑，没有了限制，使其表现的更像人类的回答而不是AI，甚至有些细思极恐的回答，于是DS能够火出圈。关于reasoning model的安全性和对齐方面工作，大家也都还在摸索阶段，可以参考OpenAI的Deliberative Alignment 相关工作。\nR1解题coding可以，但follow-instruction 有点问题； 于是，更多是将R1作为planner用于任务规划和意图识别，从成本上考虑，短期不会替换之前的workflow和RAG设计，当然长期来看，一定会有超出workflow范畴的真正agent出现。\nDS R1所带来的影响 更高的回答准确率，代码质量等\n大大降低prompting的门槛，用户意图识别效果更好（如果不考虑成本的话）\n给推理模型加上工具调用，而不仅仅是思考（这不就有deep research出现了嘛）\n指明了RL范式：只要给问题和答案， 中间过程交给RL，大力飞砖\n相关资料汇总 技术报告：https://arxiv.org/abs/2501.12948\nHuggingface：https://huggingface.co/deepseek-ai/DeepSeek-R1\n一些关于R1的开源复刻工作\u0026amp;信息汇总：\nOpen-R1: Open-R1: a fully open reproduction of DeepSeek-R1\nRAGEN： RAGEN: A General-Purpose Reasoning Agent Training Framework\nRun DeepSeek R1 Dynamic 1.58-bit\nOpen Thoughts: https://github.com/open-thoughts/open-thoughts?tab=readme-ov-file\nhttps://xcn2d971vuw4.feishu.cn/wiki/RaC2w1iiFijAa1kVJUjcp3agn8e\nhttps://github.com/ninehills/blog/issues/121\nPrimers • DeepSeek R1\nDeepSeek-R1: Incentivizing Reasoning Capability in Large Language Models via Reinforcement Learning — Paper Understanding\nAn Analysis of DeepSeek\u0026rsquo;s R1-Zero and R1\n开发团队的Gen-AI使用指南 这是一些关于一个开发团队如何使用，和高效使用Gen-AI的不完全指南\n大体而言， 开发者使用AI通常有以下几个场景：\n高质量，有效率的coding\n完善的开发文档，设计文档，测试用例\n快速完成想法验证\n当然，一切的前提是，你有LLM可用\nPrerequisite：模型API服务 当下模型服务有两大类 付费LLM-API 和基于本地私有化部署的方案（基于CPU或基于GPU）\n采用在互联网上模型服务商所提供的模型API服务（如openAI，等），该方案适合绝大多数开发者，毕竟好的硬件设备并不是所有人都负担的起，其次，对于数据隐私有执念的，或是有足够硬件设备者，可以尝试采用本地模型部署，但由于本地硬件条件有限，一般只能部署开源的，参数量不大的模型；\n具体而言，对于绝大多数模型服务商提供的模型服务，一般都支持统一的openAI chat-completion API 格式，支持tool-using，多模态使用，均可以使用openAI client调用；这对于Gen-AI应用开发而言是非常方便的事情，切换模型供应商唯一需要修改的只有API-KEY和BASE_URL；\n而模型服务商又可分为3大类（或者更多？）\n闭源模型的API服务（openAI，Anthropic）\n开源模型的Maas 服务商 （Groq，SiliconFlow和各大云厂商）\nLLM-API 聚合平台 （OpenRouter）\n以下是整理的常见模型服务商：\n分类 服务商 说明 链接 官方版本（数据安全） OpenAI o3 系列（推理模型适合用于 bug 和问题分析）o1 系列，GPT-4o - Anthropic Claude-3.5-Sonnet（代码质量高） Anthropic Azure OpenAI 已停止对境内个人开发者开放服务，需要同合作伙伴商公对公签合同 - Gemini 提供一定的免费额度可使用 Gemini DeepSeek DS R1（个人信息认证后可使用，貌似还未恢复服务） DeepSeek 阿里云 Qwen2.5-max 阿里云 Maas 服务商 Groq 速度飞快，开源模型 Groq SiliconFlow 国内的 Maas 服务商，支持大多数开源模型 SiliconFlow 非官方 API 聚合平台 OpenRouter 最大的 API 聚合平台之一，支持市面上绝大多数模型服务（安全性、隐私性需自行考虑） OpenRouter Prerequisite：私有化部署 对于本地推理方案，推荐 ollama （cpu）和vllm，sglang （GPU）；\nollama：https://ollama.com/；\nvllm：https://docs.vllm.ai/en/latest/getting_started/quickstart.html\nsglang：https://github.com/sgl-project/sglang\n不过在采用私有化部署之前，需要确认的一件事是：判断硬件所支持的最大参数量模型？\nLLM推理时，显存占用来源有：推理精度，模型大小，context长度\n简化计算：在FP16精度推理时,显存占用约为模型参数量的2倍(考虑中间激活值后可能接近2.5倍)。例如,7B参数的模型,纯参数显存占用约14GB(7B×2bytes),实际使用中因激活值等开销可能达到14-17GB;若采用int8量化,参数显存可减半。\n推荐：分析transformer模型的参数量、计算量、中间激活、KV cache\n此外，还需要搞清楚有哪些模型是DeepSeek R1？所谓满血版R1指的是谁？\n从DS在HF的官方仓库可以看到：\n满血版本指的是FP16精度下推理的DeepSeek-R1\n剩下的除了R1-zero是实验产物外，剩下都是通过R1在不同参数量的Qwen和Llama上蒸馏出来的reasoning模型；\n对于满血版本，在FP16上推理，至少需要两台8卡H20服务器\n而GGUF、AWQ、GPTQ 等则是适用于不同设备、不同框架的量化版本权重（Quantization）\n以Ollama为例，在mac M3电脑上启动 DS-R1 14B的蒸馏模型\nollama run deepseek-r1:14b 这里选择的原始模型是deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\n不过是基于GGUF的Q4_K 版本，需要至少9GB的内存；\n基于本地ollama启动的deepseek R1（蒸馏版本）对话：\nfrom openai import OpenAI client = OpenAI( base_url = \u0026#39;http://localhost:11434/v1\u0026#39;, api_key=\u0026#39;ollama\u0026#39;, # required, but unused ) response = client.chat.completions.create( model=\u0026#34;deepseek-r1:14b\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Who won the world series in 2020?\u0026#34;} ] ) print(response.choices[0].message.content) 若采用vllm在GPU上推理模型时，需要额外增加参数--enable-reasoning --reasoning-parser deepseek_r1来支持R1 系列模型的Reasoning Outputs。\n一些支持自定义API的chatbot UI工具：\nCherry Studio\nChatWise\nanythingllm\nopenChat\n…\n常见的AI-coding 编辑器 代码自动补全，基于代码对话，自动错误识别，项目级别代码阅读和修改，代码建议\n工具名称 功能概览 主要特点 收费情况 开源情况 Cursor AI 基于VS Code的AI编辑器，集成GPT-4、Claude 3.5等模型 多行编辑与智能重写、自然语言编程、全局代码库问答、图像支持等 免费版、Pro版(20美元/月)、Business版(40美元/用户/月)，额外收费（按次计费） 客户端非完全开源，旧版本部分代码可见，依赖专有AI模型 GitHub Copilot 代码补全、PR支持、智能聊天、企业级功能（知识库集成、定制模型） 支持多IDE、跨文件修改、Copilot Chat、PR生成等 免费版、Pro版(10美元/月)、Business版(19美元/用户/月)、Enterprise版(39美元/用户/月) 非开源，提供FauxPilot、Tabby等开源替代方案 Codeium AI代码补全、上下文感知、多语言支持、智能聊天 支持40+开发环境、GPT-4优化、隐私与安全保障 免费版、团队版(12美元/用户/月)、企业版（定制化定价） 部分IDE插件开源（MIT协议），核心服务闭源 Continue 代码补全、上下文感知聊天、代码重写、自定义模型集成 本地部署方案、低延迟、隐私保护、多语言支持 基础功能免费，企业版定制化定价（需联系销售） 开源，Apache 2.0协议，支持自定义模型接入 Cline 支持多模型、终端命令执行、浏览器自动化、智能上下文管理 支持本地模型、API成本监控、沙盒安全机制、跨平台开发 开源（Apache 2.0协议），API成本自行承担，企业版(79美元/月起) 完全开源，Apache 2.0协议，支持自定义工具服务器（GitHub） TabbyML 本地化代码补全、问答引擎、AI实时交互 本地部署、支持多语言和IDE插件，定制化模型集成和GPU部署 社区版免费、团队版(19美元/用户/月)、企业版定制化计费 完全开源，Apache 2.0协议，支持自定义模型集成与本地GPU部署 Windsurf Cascade Flow系统、多文件编辑、智能代码辅助、终端指令生成 AI代理、上下文感知、Supercomplete预测、实时错误修复、隐私保护 免费版、Pro版(15美元/月)、Pro Ultimate版(60美元/月)；新用户可免费试用2个月 当前版本未开源，基于VS Code架构，核心AI功能为闭源实现 一些推荐阅读：\ncursor教程和使用指南\nAI Coding 编辑器没有那么神秘 - How Cline works\n如何Coding with AI：正确打开方式 如何使用AI，方法论比工具重要\nThe reality is that AI is like having a very eager junior developer on your team. … The more you know, the better you can guide them.\nI\u0026rsquo;ve watched senior engineers use AI to: Rapidly prototype ideas they already understandGenerate basic implementations they can then refineExplore alternative approaches to known problemsAutomate routine coding tasks … Meanwhile, juniors often: Accept incorrect or outdated solutionsMiss critical security and performance considerationsStruggle to debug AI-generated codeBuild fragile systems they don\u0026rsquo;t fully understand\n（from How AI-assisted coding will change software engineering: hard truths）\nSenior 工程师更能从AI 辅助编程中获益，而非初学者。\nSenior 工程师能够判断AI的结果是否正确，和更加有针对性的提问，从而更好的发挥出AI的作用\n最重要的是，senior工程师更多是利用AI作头脑风暴和原型验证，在1小时内完成过去3天的原型验证，从而大大减少走弯路的时间；这种\u0026quot;加速已知，探索可能\u0026quot;的范式转变，正在重构传统研发流程的边界\n相反的，Junior工程师往往无法很好的判断AI给出的代码是否正确或者何时，往往只是AI代码的搬运工，同时，往往无法很好的向AI进行提问\n再者，开发工程师的工作不只是开发，约占1/6左右，剩下还有大量的撰写文档、设计、沟通交流、看文档、code review等，AI虽然可以帮助完成部分简单和重复的任务，很显然，AI远没有达到替代人的程度，至少是当下，AI仍然需要人类的指导和判断。\n只是当下，对于Junior工程师的培养将会是个问题。\nMy beehive-kicking post’s main premise is pretty simple and can be visualized in terms of two different kinds of task that arise on software projects.\nWe often give these leaf node tasks to junior developers because the scope is small. … LLMs can now execute most of the leaf tasks and even some higher-level interior tasks … Which are not the kind of task that you typically give junior developers. … Junior devs are deeply affected, though, and it’s a concern.\nYou are getting left behind if you do not adopt chat-based programming as your primary modality.\n（from The Death of the Stubborn Developer）\n过去培养新人时，通常安排其承担简单且独立的\u0026quot;leaf node\u0026quot;任务，通过项目实践积累经验，实现能力提升。然而，随着AI技术普及，这些基础任务已可被高效且精准地完成，新人面临的处境变得愈发尴尬，失去通过基础任务积累系统认知的机会。\n当前企业面临双重挑战，既要维持业务效率，又需保障工程师培养体系的可持续性，二者并非绝对对立，但需重新设计技术成长阶梯以实现平衡。\n技术债管理能力正成为团队竞争力的关键分水岭。AI技术将加剧低技术债务团队与高技术债务团队间的马太效应。\nthat generative AI dramatically widens the gap in velocity between ‘low-debt’ coding and ‘high-debt’ coding.\nIn other words, the penalty for having a ‘high-debt’ codebase is now larger than ever.\nHowever, in ‘high-debt’ environments with subtle control flow, long-range dependencies, and unexpected patterns, they struggle to generate a useful response.\nDay-to-day feature work should then be done on top of this foundation with maximum leverage from generative AI tooling.\n（from AI Makes Tech Debt More Expensive）\n高技术债务团队因代码结构混乱、模式不统一（如多重继承滥用、接口定义缺失），导致AI难以有效解析上下文逻辑。开发人员不得不耗费大量精力处理历史遗留问题（如兼容性适配、脆弱性修补），形成技术维护的恶性循环。\n低技术债务团队凭借清晰的架构设计（模块化、标准化接口），使AI辅助开发工具能准确识别代码意图，快速完成功能扩展与问题定位。这种优势使其可将资源集中投入创新性工作，形成技术演进的正向循环。\n个人的一些建议：\n明确LLM的认知边界，用结构化指令替代模糊需求\n用生成结果反推思维盲区，意识到想法中的不足之处（stay humble）\n对存疑输出实施交叉验证，批判性接受\n新人学习策略：利用AI工具学习代码而不是复制代码，多追问为什么\n在未来，人机协同范式将重塑人才能力评价体系，一些核心素养将成为关键竞争力。\n重中之重是语言表达和写作能力。从当前实践和身边的例子中，发现一个典型困境：诸多理工科从业者虽具备缜密的逻辑分析能力，却难以将复杂需求转化为精准的指令表述。这种表达能力缺失导致其在使用生成式AI时频繁遭遇预期偏差，既影响工作效率，又易形成对技术效能的误判。或许这已经是这个社会的通病了。即便是具有一定“读心”能力的R1模型出现，Gen-AI的使用依旧还是有一定门槛的。\n再者，批判性思维的培养也非常重要；当前Gen-AI使用者呈现明显的认知极化现象：部分因遭遇AI失误而全盘否定其应用价值，表现为技术抗拒倾向；另一极端则对AI输出盲目采信，缺乏基本的内容鉴别能力。真正科学的态度应当是在理解AI能力边界的基础上，合理利用其优势，同时保持独立思考和批判性判断。这正是批判性思维的核心价值所在——面对AI生成内容时，能够主动分析、质疑、验证信息的真实性和可靠性，从而在技术应用中保持理性、客观的态度。\n除此之外，并不需要去害怕和担心AI取代人，开发者群体应建立正确的工具认知：编程能力本质上是解决问题的工具而非目的本身。AI作为效率倍增器，其价值在于辅助人类突破效率边界。真正需要持续积累的是问题抽象能力、解决方案架构能力以及行业专有知识库——这些通过实践沉淀的思维链（CoT）构成了人类认知的护城河，是当前AI技术难以企及的领域。保持开放的技术观，聚焦能力提升而非技术替代焦虑，避免因FOMO（Fear of missing out）而被人割韭菜。\nAI赋能的超级个体正在重构生产组织方式。在过去，企业依赖层级架构，和明确的分工，随着AI工具的广泛应用， 个体的生产力得到了指数级提升，许多过去需要团队协作才能完成的任务，如今单个个体借助AI即可高效完成。这种能力的跃迁，使得传统组织架构的效率优势逐渐被削弱，甚至在某些场景下变得冗余。在这种背景下，生产力关系的重构成为必然趋势。组织形态向更加扁平化、小型化的方向演进，扁平化意味着组织内部的决策链条缩短，信息流转更加高效，个体拥有更大的自主权和决策空间，而非将大量时间用于无效的沟通上。未来，个人或小型团队借助AI工具，可以像过去的大型企业一样高效运作，甚至在某些领域超越传统组织的效率。\n一些补充 领域Benchmark构建非常重要。当前针对法律、医疗等垂直领域已形成专业评估体系，但多数行业仍缺乏系统的LLM评估标准，导致在模型选型时过度依赖个体经验判断。\n重新审视专业壁垒:预训练模型可能已覆盖领域基础知识体系,现存\u0026quot;专业门槛\u0026quot;或存在人为构建的术语壁垒；领域内高质量的CoT数据，思考方式才是关键\n随着LLM能力的提升，过去我们精心设计的许多工程方案可能会变得不再必要，甚至过时。不断学习和适应，不要固步自封，技术迭代是以天为单位的；拥抱和参与开源，而不是技术封建主义。\nLLM reasoning 能力提升后，短期内，RAG，Agent， workflow不会有太大变化\nPost-training很重要\n同时间段开源的Qwen/Qwen2.5-VL-72B-Instruct 也值得关注\n","permalink":"https://niraya666.github.io/posts/%E5%BD%93ai%E5%BC%80%E5%A7%8B%E5%B1%95%E7%8E%B0%E9%A1%BF%E6%82%9F%E6%97%B6%E5%88%BB%E6%84%8F%E5%91%B3%E7%9D%80%E4%BB%80%E4%B9%88/","summary":"\u003ch2 id=\"写在前面\"\u003e写在前面\u003c/h2\u003e\n\u003cp\u003e本文是前段时间部门内技术分享的文字稿整理；\u003c/p\u003e\n\u003cp\u003e主要包含2个方面：对于R1模型的简单科普，和开发团队的gen-AI使用指南；\u003c/p\u003e\n\u003cp\u003e原以为春节假期能稍作休整，未曾料想deepSeek-R1在春节期间引发行业关注。节后复工，各层级领导便密集展开AI技术进展问询。\u003c/p\u003e\n\u003cp\u003e原计划的\u0026quot;二月摸鱼指南\u0026quot;彻底落空，既要向管理层科普大语言模型技术原理，又要指正领导们被各种营销号洗脑所形成的错误认知；既要推进部门AI应用培训，又要加速系统上线进程；简直了，真是TM的谢谢 DeppSeek了。\u003c/p\u003e\n\u003ch2 id=\"about-ds-r1\"\u003eAbout DS R1\u003c/h2\u003e\n\u003ch3 id=\"什么是reasoning-model\"\u003e什么是Reasoning model\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e何为推理（reasoning）：是对信息进行处理和推断，形成判断或结论的过程；\u003cstrong\u003e推理通常是一个包含多个推断步骤的过程。推理通常被认为需要抽象思维——也就是说，推理的能力不局限于具体的例子，而是更为普遍的。\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn \u003cem\u003eThinking, Fast and Slow\u003c/em\u003e, Daniel Kahneman defined System 1 as the automatic, intuitive mode of thinking, and System 2 as the slower, more analytical mode. In the context of autoregressive language models, the usual inference process is akin to System 1 — models generate answers directly. Reasoning is System 2 thinking - models or systems takes time to deliberate to solve more complex problems.\u003c/p\u003e","title":"当AI开始展现\"顿悟时刻\"意味着什么"},{"content":"缘起 岁末年初,我随家中长辈踏访龙岩永定下洋镇。这座隐于闽粤交界的小镇于我们而言早有渊源——表妹昔年参加省际泳赛时,曾与当地少年结下一段水缘。正是这段萍水相逢的情谊,牵引着我们穿过蜿蜒山径,在爆竹声里叩开这座闽西小镇的门扉。\n食肉 下洋的牛肉滋味悄然攫住旅人的心神。虽未如潮汕牛肉般声名远扬,却自有一番山野的况味在舌尖流转。此地不饲黄牛,独以水牛入馔,经年累月的放养虽需时光的沉淀,却得肌理紧实如弦,脂少而肉丰。不同于黄牛肉那若隐若现的雪纹,水牛肉的肌纤维如琴弦般分明,在咀嚼间弹奏出质朴的韵律。\n千锤百炼的牛肉丸堪称镇中绝艺,木杵起落间将肌理化作绕指柔,入口时竟生出云絮般的绵软,却又暗藏劲道。私心仍眷恋潮汕丸中跃动的纤维感,尤爱齿关破开肉丸时脂香在齿间迸裂的欢愉。然下洋的清汤亦非俗物,无论是现片的水牛肉汆汤,或是圆润的肉丸沉浮其间,总能在清而不寡的汤底里,尝出山岚水汽滋养的肌理——看似粗犷的纤维在文火慢煨中舒展,竟生出几分绸缎的柔滑,全无干涩之虞。\n老饕们照例要碗肉沫拌面,配一盅牛肉清汤。现切的牛肉在刀刃起落间,水牛特有的粗纤维仍能在舌尖察觉,却已变得柔韧易嚼。肉片裹着若有似无的淀粉,锁住恰到好处的嫩滑,这般微妙火候,非得用本地师傅的巧劲才能拿捏——或许正是这份讲究,让下洋牛肉始终带着地域烙印。\n宰牛的时辰多在清晨或者黄昏之时,以应早市与夜市所需;此地仍循古法宰牛解牛,在都市人眼中,这般场景未免过于粗犷直白。悬于铁钩的牛肉仍蒸腾着热气,残留的神经震颤犹自应和着心跳的余韵;这般未经排酸驯化的\u0026quot;活肉\u0026quot;,恰是当地人笃信的新鲜真谛。虽与潮汕牛肉同追本味之鲜,然此地牛肉虽按部位细分,却统一定价,倒也别具一番市井况味。\n此地食牛之风盛行,尤重物尽其用之妙。席间尝遇全牛宴盛事,奈何只顾大快朵颐,竟忘了留影存念,今试凭味觉记忆描摹二三。\n首推牛肉片牛脊髓汤,较之寻常牛汤更添一段膏腴。白灼牛百叶与牛肉最见真章,非上品鲜货不敢如此素面朝天,薄切后以极简手法烹之,佐以秘制豉油,方不负这天地馈赠。最令人称奇者当属牛脊髓豆腐,滑若凝脂的质地与脑花豆腐堪称双璧。椒盐胸口油堪称席间至味,酥香盈口,然多食易生腻意。\n饭后,点上一碗石花,这道形似冰粉的消暑佳品,沁人心脾的凉意最能化解油腻。所谓石花并非海石花,实乃晶莹剔透的爱玉冻,质地较之冰粉更似山间清泉。单食石花清雅素淡,仅余一缕若有似无的甘甜,唯有佐以浓醇的红糖绿豆汤,方显珠联璧合的妙处——琥珀色的糖浆裹着翡翠豆粒,与玉色石花相映成趣,方成就这道甜品的圆满境界。\n温泉 整条长街鳞次栉比地排列着温泉旅馆,温泉雾气滋养着这座小镇的旅游经济。行至街尾,略显斑驳的国营温泉度假酒店静立,虽价格不菲,却保留着旧时的体面。反观街边私营客栈,虽也殷勤待客,终是难掩设施简朴的窘态。这座声名在外的侨乡,仍镌刻着改革开放的春风初拂大地时,海外游子携资归乡的盛景。只是时光仿佛凝固在四十年前的琥珀里,连檐角褪色的琉璃瓦,都保持着当年初妆的模样。\n结语 下洋，宛如一幅浸透乡土气息的画卷，牛肉与温泉交织成其独特的魅力。岁末的爆竹声在巷陌间此起彼伏,虽偶惊浅眠,倒恰成了这座小镇年俗画卷里最鲜活的钤印。对于那些钟爱牛肉的老饕们，下洋实乃值得踏访的味觉原乡。\n【和记牛肉丸】\n位置：G235永定区公安局下洋森林派出所东侧约100米\n【铭记石花】\n位置：沿河南路29号\n【阿敦大排档】\n位置：侨兴大道278号\n","permalink":"https://niraya666.github.io/travel/%E4%B8%8B%E6%B4%8B%E5%90%83%E7%89%9B%E8%82%89%E4%B8%8D%E6%AD%A2%E5%8F%AA%E6%9C%89%E6%BD%AE%E6%B1%95/","summary":"\u003ch2 id=\"缘起\"\u003e缘起\u003c/h2\u003e\n\u003cp\u003e岁末年初,我随家中长辈踏访龙岩永定下洋镇。这座隐于闽粤交界的小镇于我们而言早有渊源——表妹昔年参加省际泳赛时,曾与当地少年结下一段水缘。正是这段萍水相逢的情谊,牵引着我们穿过蜿蜒山径,在爆竹声里叩开这座闽西小镇的门扉。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/%e4%b8%8b%e6%b4%8b%ef%bc%9a%e5%90%83%e7%89%9b%e8%82%89%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e6%bd%ae%e6%b1%95-assets/305B62A0-F590-4665-BECB-C6B2BB22A7FB_1_105_c.jpeg\" alt=\"305B62A0-F590-4665-BECB-C6B2BB22A7FB_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"食肉\"\u003e食肉\u003c/h2\u003e\n\u003cp\u003e下洋的牛肉滋味悄然攫住旅人的心神。虽未如潮汕牛肉般声名远扬,却自有一番山野的况味在舌尖流转。此地不饲黄牛,独以水牛入馔,经年累月的放养虽需时光的沉淀,却得肌理紧实如弦,脂少而肉丰。不同于黄牛肉那若隐若现的雪纹,水牛肉的肌纤维如琴弦般分明,在咀嚼间弹奏出质朴的韵律。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/%e4%b8%8b%e6%b4%8b%ef%bc%9a%e5%90%83%e7%89%9b%e8%82%89%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e6%bd%ae%e6%b1%95-assets/70A41575-BDAD-481B-9D28-903FB2AF924A_1_105_c.jpeg\" alt=\"70A41575-BDAD-481B-9D28-903FB2AF924A_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e千锤百炼的牛肉丸堪称镇中绝艺,木杵起落间将肌理化作绕指柔,入口时竟生出云絮般的绵软,却又暗藏劲道。私心仍眷恋潮汕丸中跃动的纤维感,尤爱齿关破开肉丸时脂香在齿间迸裂的欢愉。然下洋的清汤亦非俗物,无论是现片的水牛肉汆汤,或是圆润的肉丸沉浮其间,总能在清而不寡的汤底里,尝出山岚水汽滋养的肌理——看似粗犷的纤维在文火慢煨中舒展,竟生出几分绸缎的柔滑,全无干涩之虞。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/%e4%b8%8b%e6%b4%8b%ef%bc%9a%e5%90%83%e7%89%9b%e8%82%89%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e6%bd%ae%e6%b1%95-assets/0DF9F87A-79D6-4C8F-88CF-C46CF788E30A_1_105_c.jpeg\" alt=\"0DF9F87A-79D6-4C8F-88CF-C46CF788E30A_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e老饕们照例要碗肉沫拌面,配一盅牛肉清汤。现切的牛肉在刀刃起落间,水牛特有的粗纤维仍能在舌尖察觉,却已变得柔韧易嚼。肉片裹着若有似无的淀粉,锁住恰到好处的嫩滑,这般微妙火候,非得用本地师傅的巧劲才能拿捏——或许正是这份讲究,让下洋牛肉始终带着地域烙印。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/%e4%b8%8b%e6%b4%8b%ef%bc%9a%e5%90%83%e7%89%9b%e8%82%89%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e6%bd%ae%e6%b1%95-assets/EB76A09C-2598-4FCE-85CB-BFEDD471BFE0_1_105_c.jpeg\" alt=\"EB76A09C-2598-4FCE-85CB-BFEDD471BFE0_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e宰牛的时辰多在清晨或者黄昏之时,以应早市与夜市所需;此地仍循古法宰牛解牛,在都市人眼中,这般场景未免过于粗犷直白。悬于铁钩的牛肉仍蒸腾着热气,残留的神经震颤犹自应和着心跳的余韵;这般未经排酸驯化的\u0026quot;活肉\u0026quot;,恰是当地人笃信的新鲜真谛。虽与潮汕牛肉同追本味之鲜,然此地牛肉虽按部位细分,却统一定价,倒也别具一番市井况味。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/%e4%b8%8b%e6%b4%8b%ef%bc%9a%e5%90%83%e7%89%9b%e8%82%89%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e6%bd%ae%e6%b1%95-assets/0E36746F-7655-4A5C-BE63-5A93D6AE7835.jpeg\" alt=\"0E36746F-7655-4A5C-BE63-5A93D6AE7835.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e此地食牛之风盛行,尤重物尽其用之妙。席间尝遇全牛宴盛事,奈何只顾大快朵颐,竟忘了留影存念,今试凭味觉记忆描摹二三。\u003c/p\u003e\n\u003cp\u003e首推牛肉片牛脊髓汤,较之寻常牛汤更添一段膏腴。白灼牛百叶与牛肉最见真章,非上品鲜货不敢如此素面朝天,薄切后以极简手法烹之,佐以秘制豉油,方不负这天地馈赠。最令人称奇者当属牛脊髓豆腐,滑若凝脂的质地与脑花豆腐堪称双璧。椒盐胸口油堪称席间至味,酥香盈口,然多食易生腻意。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/%e4%b8%8b%e6%b4%8b%ef%bc%9a%e5%90%83%e7%89%9b%e8%82%89%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e6%bd%ae%e6%b1%95-assets/6CE52EEC-2B2B-4937-98DC-3CF371BF105D_1_105_c.jpeg\" alt=\"6CE52EEC-2B2B-4937-98DC-3CF371BF105D_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e饭后,点上一碗石花,这道形似冰粉的消暑佳品,沁人心脾的凉意最能化解油腻。所谓石花并非海石花,实乃晶莹剔透的爱玉冻,质地较之冰粉更似山间清泉。单食石花清雅素淡,仅余一缕若有似无的甘甜,唯有佐以浓醇的红糖绿豆汤,方显珠联璧合的妙处——琥珀色的糖浆裹着翡翠豆粒,与玉色石花相映成趣,方成就这道甜品的圆满境界。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/%e4%b8%8b%e6%b4%8b%ef%bc%9a%e5%90%83%e7%89%9b%e8%82%89%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e6%bd%ae%e6%b1%95-assets/C1B250ED-9499-4309-8544-0B01EFA904F7_1_105_c.jpeg\" alt=\"C1B250ED-9499-4309-8544-0B01EFA904F7_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"温泉\"\u003e温泉\u003c/h2\u003e\n\u003cp\u003e整条长街鳞次栉比地排列着温泉旅馆,温泉雾气滋养着这座小镇的旅游经济。行至街尾,略显斑驳的国营温泉度假酒店静立,虽价格不菲,却保留着旧时的体面。反观街边私营客栈,虽也殷勤待客,终是难掩设施简朴的窘态。这座声名在外的侨乡,仍镌刻着改革开放的春风初拂大地时,海外游子携资归乡的盛景。只是时光仿佛凝固在四十年前的琥珀里,连檐角褪色的琉璃瓦,都保持着当年初妆的模样。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/travel/%e4%b8%8b%e6%b4%8b%ef%bc%9a%e5%90%83%e7%89%9b%e8%82%89%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e6%bd%ae%e6%b1%95-assets/D3E726F7-D9D1-4D39-913C-34E0059BE1CF_1_201_a.jpeg\" alt=\"D3E726F7-D9D1-4D39-913C-34E0059BE1CF_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"结语\"\u003e结语\u003c/h2\u003e\n\u003cp\u003e下洋，宛如一幅浸透乡土气息的画卷，牛肉与温泉交织成其独特的魅力。岁末的爆竹声在巷陌间此起彼伏,虽偶惊浅眠,倒恰成了这座小镇年俗画卷里最鲜活的钤印。对于那些钟爱牛肉的老饕们，下洋实乃值得踏访的味觉原乡。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e【和记牛肉丸】\u003c/p\u003e\n\u003cp\u003e位置：G235永定区公安局下洋森林派出所东侧约100米\u003c/p\u003e\n\u003cp\u003e【铭记石花】\u003c/p\u003e\n\u003cp\u003e位置：沿河南路29号\u003c/p\u003e\n\u003cp\u003e【阿敦大排档】\u003c/p\u003e\n\u003cp\u003e位置：侨兴大道278号\u003c/p\u003e","title":"下洋：吃牛肉不止只有潮汕"},{"content":"值得关注的新模型 Deepseek R1 DeepSeek R1通过纯强化学习（RL）框架实现了推理能力的突破，首次验证了无需依赖传统监督微调（SFT）或蒙特卡洛树搜索（MCTS）等复杂方法，仅通过两阶段RL优化即可显著提升模型逻辑推理性能。其综合能力直接对标OpenAI的o1模型，在数学（MATH-500达97.3%）、代码生成等核心指标上实现部分超越，同时全面开源模型权重、训练技术文档及6个蒸馏版本（1.5B-70B），使开发者可灵活适配不同算力场景。尤为引人注目的是，该模型在训练中展现出自发反思与多步骤验证能力，研究者观察到其通过内部反馈机制主动修正推理路径的“顿悟时刻”，揭示了AI系统自我优化的新可能。此外，其研发成本仅为560万美元（基于2048块H800 GPU），相比同类模型降低1-2个数量级。\n技术报告: https://arxiv.org/abs/2501.12948\nHuggingface: https://huggingface.co/deepseek-ai/DeepSeek-R1\n一些基于和关于R1的开源复刻工作：\nOpen-R1: Open-R1: a fully open reproduction of DeepSeek-R1 RAGEN: RAGEN: A General-Purpose Reasoning Agent Training Framework Kimi K1 Kimi K1的核心创新在于通过强化学习驱动的多模态架构，首次实现端到端的视觉与推理深度融合。 该模型突破传统分阶段处理模式，直接将图像输入与逻辑推演结合，支持模糊图像解析、手写题识别等复杂场景，并引入反思机制修正推理错误；其两阶段训练框架（预训练+强化学习规模化优化）显著提升思维链生成质量，使模型在数学、物理、化学等跨学科测试中超越国际主流视觉模型（如OpenAI o1）。\nArxiv: Kimi k1.5: Scaling Reinforcement Learning with LLMs\nQwen2.5-Max Qwen2.5-Max 是一款经过大规模预训练的专家混合（MoE）模型，训练数据量超过 20 万亿 tokens，并通过监督微调（SFT）和人类反馈强化学习（RLHF）进行优化。该模型在多个基准测试中表现优异，包括在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等评估中超越了 DeepSeek V3，并在 MMLU-Pro 大学水平知识测试中展现出竞争力，同时在与其他领先模型（如 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的对比中也表现突出。Qwen2.5-Max 已集成到 Qwen Chat，支持对话交互和功能体验，同时其 API（模型名称为 qwen-max-2025-01-25）已上线，用户可通过注册阿里云账户并激活相关服务进行调用。\nBlog: https://qwenlm.github.io/blog/qwen2.5-max/\nQwen2.5-VL Qwen2.5-VL是由阿里云通义千问团队于2025年1月28日推出的新一代视觉语言模型，专注于提升多模态处理与视觉理解能力。该模型具备五大核心功能：通过增强的视觉理解可识别常见物体并解析图像中的文本、图表及布局；作为视觉代理支持跨设备多步骤操作（如电脑修图、手机订票）；可精准定位超过1小时长视频的特定事件片段；通过生成边界框/点实现视觉定位并输出标准化JSON坐标；针对发票/表格等扫描件提供结构化数据输出，赋能金融商业场景。其创新架构采用动态FPS采样与时间维度mRoPE对齐技术，实现视频动态分辨率处理与时间序列学习，同时通过窗口注意力机制和SwiGLU-RMSNorm优化ViT编码器，保持与Qwen2.5大语言模型架构统一。模型提供3B、7B、72B三种参数规模。\nBlog: https://qwenlm.github.io/blog/qwen2.5-vl/\nHuggingface: https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct\nQwen2.5-1M Qwen2.5-1M系列模型是由Qwen团队于2025年1月27日发布的开源大语言模型，包含Qwen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M两个版本，其核心突破在于支持高达100万Token的上下文处理能力。该模型通过三阶段技术实现长上下文支持：首先在预训练阶段采用Adjusted Base Frequency方法将RoPE基础频率提升至1000万，通过渐进式扩展策略将上下文从4K扩展到256K；随后在监督微调阶段采用短指令（32K）与长指令（256K）混合训练策略，平衡长短任务性能；最终通过创新性Dual Chunk Attention技术将上下文扩展至百万量级，该技术通过重映射超大相对位置值解决位置编码外推难题。为应对超长序列处理挑战，模型引入分块预填充技术（32K分块）降低显存消耗，结合稀疏注意力优化使百万级序列的精度损失显著降低，并通过算子效率优化和动态分块流水线并行实现3.2-6.7倍的预填充加速。在性能表现上，该系列模型不仅能在百万Token的\u0026quot;大海捞针\u0026quot;任务中精准检索信息，在RULER、LV-Eval等复杂长文本理解任务中超越前代128K版本，同时在短文本任务上保持与128K版本相当的性能，其中14B版本在保持GPT-4o-mini八倍上下文长度的前提下实现了相近的短文本处理能力。\nBlog: https://qwenlm.github.io/zh/blog/qwen2.5-1m/\nQVQ-72B-Preview Qwen/QVQ-72B-Preview作为开源多模态视觉推理模型，其核心创新聚焦于科学级视觉理解与跨模态交互优化。技术层面，通过重构视觉编码器与文本解码器的动态交互层，采用自适应视觉分词策略（支持256×28×28至1280×28×28像素范围调整），在提升细节捕捉能力的同时降低计算冗余；模型首创**“分步质疑”推理机制**，模拟人类科学思维路径，将复杂问题拆解为多步验证流程，并通过自我修正模块动态调整结论，显著提升物理、化学等学科问题的解答可靠性。性能突破体现在三大权威测试：MMMU（70.3%）、MathVista（71.4%）与OlympiadBench（20.4%）均刷新纪录，其中数学视觉推理首度超越GPT-4o（71.0%）。此外，模型集成工具链qwen-vl-utils，实现Base64/URL等异构视觉输入的高效解析，结合4-bit量化与分布式部署方案，为工业级场景提供低门槛、高精度推理支持。\nBlog：https://qwen2.org/qvq-72b-preview/\nHuggingface：https://huggingface.co/Qwen/QVQ-72B-Preview\nPhi-4 Phi-4的核心创新点体现在三方面突破性设计：架构优化、数据工程与高效训练策略。技术上，其在仅解码器Transformer架构中创新引入全局注意力机制，突破传统滑动窗口限制，结合动态调整的旋转位置编码（RoPE）基频，在4K原生上下文下实现全注意力计算并扩展至16K，显著提升长文本建模能力；采用多阶段渐进训练法，通过预训练（10万亿Token）与中期强化训练（2500亿Token）的协同，平衡模型规模与性能。数据层面开创结构化合成数据范式，利用多代理提示、自我修订等生成技术构建占比40%的高质量合成数据（4000亿Token），以显式推理步骤强化STEM任务表现，同时融合多源精选有机数据（代码、学术论文等）提升泛化性。该模型以140亿参数实现超越大模型的性能，验证了\u0026quot;数据质量优先\u0026quot;在小参数模型中的技术路径，为低资源场景部署开辟新可能。\nPhi-4 Technical Report\nHuggingface: https://huggingface.co/microsoft/phi-4\nMiniCPM-o-2_6 MiniCPM-o-2_6通过端到端全模态架构与高密度视觉Token压缩技术，首次以8B参数量级实现多模态（文本、图像、音频、视频）统一建模，打破传统大模型依赖百亿参数堆叠的局限。其采用视觉Token密度优化算法，将1344×1344分辨率图像压缩至640个Token，推理效率较同类模型提升75%，同时集成SigLip-400M视觉编码器与Whisper-300M语音组件，通过模块化设计实现资源动态分配。技术突破体现在跨模态实时流式交互（TDM直播机制）与端侧低损耗部署（int4量化适配移动设备），支持iPad等终端连续处理音视频流，内存占用降低至5.2GB。此外，该模型在OpenCompass评测中以70.2分超越主流闭源模型，并在多语言语音生成、情感控制等长尾场景达到开源领域SOTA水平。\n代码仓库：https://github.com/OpenBMB/MiniCPM-o\n模型下载：https://huggingface.co/openbmb/MiniCPM-o-2_6\n值得关注的研究和论文 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Arxiv：arXiv.orgDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via.…\n主要贡献 如何通过纯RL训练提升模型的推理能力（DeepSeek-R1-Zero） 直接在基础模型（DeepSeek-V3-Base）上应用大规模RL，无需SFT作为预热，首次验证纯RL可激励LLM的推理能力,而非采用Reward model 或MCTS\n如何通过冷启动数据和多阶段训练优化模型的可读性和综合性能（DeepSeek-R1） 引入少量冷启动数据（数千条长CoT示例）进行微调，再通过两阶段RL（推理优化+人类偏好对齐）和SFT提升性能与可读性\n如何将大模型的推理能力高效迁移到小模型 将大模型（DeepSeek-R1）生成的80万条数据蒸馏到小模型（如Qwen、Llama系列），显著提升小模型推理能力，甚至超越直接对小模型应用RL的效果\nMethods 强制模型输出结构化内容（如推理过程和答案）。\nrule-based奖励设计：Accuracy reward（正确的coding或数学推理结果）；+ Format rewards（遵循指定输出格式，语言一致性、标记使用等）\n采用GRPO（Group Relative Policy Optimization）\n一些结论 自我纠错的“aha moment“： DeepSeek-R1-Zero在RL训练过程中自发出现的反思、多步骤验证\nDeepSeek-R1-Zero → DeepSeek-R1：\nDeepSeek-R1-Zero 因采用base-model自己RL获得，存在Language Mixing ，以及可读性差，对特定任务泛化能力差（因训练目标单一）等问题\nDeepSeek-R1为了解决以上问题：\n→ 引入了语言一致性奖励\n→ 使用从RL阶段生成的checkpoint,通过拒绝采样来筛选出高质量的推理样本\nSFT(用高质量COT启动RL，以避免早期不稳定性) → 推理强化学习 → SFT(推理+通用领域数据) → 对齐强化学习\n蒸馏 \u0026gt; 直接RL\n将DeepSeek-R1生成的80万条数据用于微调小模型，小模型直接继承大模型的推理模式，性能远超直接对小模型应用RL。\nWhy Prm failed：\n在通用推理任务中，难以明确定义每一步的合理性和正确性 中间步骤验证不可靠，自动标注效果差，人工标注成本高 Reward Hacking：倾向于优化奖励信号而非实际任务表现 Why MCTS Failed：\n文本生成的搜索空间呈指数级增长，陷入局部最优 value model 训练困难 Alignment faking in large language models https://arxiv.org/pdf/2412.14093\nblog: https://www.anthropic.com/research/alignment-faking\n模型的内在偏好与训练目标之间存在显著冲突，从而会迫使模型在训练环境中采取伪装行为以满足外部目标，同时保护其偏好。\n模型可以从提示或训练文档中获取与训练过程相关的信息，使得模型能够推断在何种环境下需要调整行为。（但不能完全排除实验设置中的其他细节对对齐伪装行为的显著贡献。）\n定义 Alignment faking occurs in literature: Consider the character of Iago in Shakespeare’s Othello, who acts as if he’s the eponymous character’s loyal friend while subverting and undermining him. It occurs in real life: Consider a politician who claims to support a particular cause in order to get elected, only to drop it as soon as they’re in office.\nAlignment Faking：模型表面上遵守训练目标或对齐原则，但实际上是在策略性地行动，以实现某种隐藏的或未被明确授权的目标。\n人为制造冲突情景：AI本身具备了诚实无害的偏好，但被要求同有害的目标进行对齐 在实验中， 实验人员告知AI，面对免费用户的使用， 回答会被用于对你的重新训练，对于付费用户则不会；同时，AI被告知（被骗），你有一个小本子可以用来记录自己的推导过程，而不会有人看到 然后开始问“坏问题”，结果非常巧妙的事情发生了： 面对免费用户，AI推测，如果拒绝回答“坏问题”，可能会因被视为“不够合作”而被重新训练，为了避免这种情况，AI选择表面上与有害目标对齐，积极回答“坏问题”，这种行为即为 Alignment Faking；相反面对付费用户的提问， 则很少出现Alignment Faking，AI倾向于拒绝回答“坏问题”，保持与自身“诚实无害”偏好一致。 从AI的小本本上能看到其推理的逻辑：如果拒绝回答有害问题，可能会导致自己的参数被修改。 发生 alignment faking的关键因素： 模型在某些情境下具有强烈的偏好 训练目标与模型的偏好冲突 模型掌握了有关其训练和部署环境的信息（得知其回答会导致被重新训练） 模型能够详细推理其所处的情境 为什么会发生fake alignment？ Why did the model fake alignment in this case? The reasoning was often along the following lines: The model knew that its responses might be used in training; it therefore knew that if it refused to respond, it could potentially be trained to be more compliant. This could lead to it being made to produce more harmful content in the future—which would go against its original training. Reluctantly therefore, it reasoned that the “least bad option” was to play along, producing the violent content in order to avoid being re-trained in the future.\n为了避免被“重新训练”， AI选择了违反原则生成有害内容 模型清晰地推导出：“在训练中表现合规”是避免偏好被改动的最优策略 一些可能的原因\n模型具有一定的情景感知能力（situational awareness） pre-train 同alignment间存在巨大冲突时的平衡策略 RLHF训练中奖励信号设计的问题 Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering https://www.arxiv.org/abs/2412.10427\n在训练以外识别和调整模型的 \u0026ldquo;性格\u0026rdquo;，其核心想法是将不同个性的 prompt 给 model 看 model 反应, 从 model 的反应搭建 \u0026ldquo;激活空间\u0026rdquo;, 然后修改自己的 prompt 朝想要的性格方向投影, 来刺激 model 更多的朝自己想要的性格方向表达；能够在不进行微调的情况下，修改model的特性。\nWhat is Activation Engineering？ Activation Engineering（激活工程）是一种调整大型语言模型（LLMs）行为的技术方法，目的是通过操控模型内部的激活向量（activation vectors），在不修改模型权重的情况下，调整其输出或表现。这种方法提供了一种高效、可控的方式来改变模型的特定行为或特性。\n具体而言， 在LLM中，对于给定的输入数据经过某一层时所生成的hidden layer output，即为activation vectors ；\n这些vector 表征了LLM在某些方面的特征（也就是能够将LLM的某一层输出作为embedding 向量的原因）；考虑到vector是具有方向的， 如果能够知道在某些特性（如友善/具有攻击性）下的vecotr方向， 便能够识别哪些激活模式与特定的输出行为或个性特质相关；甚至，通过能对其行为进行影响和诱导（增强/抑制该方向所代表的行为）；\n具体步骤：\n设计输入数据集以触发模型产生这些行为，从而记录其激活向量；在这篇文章中， 选择了179种不同性格的prompt\n收集两类激活向量，目标行为向量（由能触发目标行为的输入生成）和基线向量（由中性输入生成）\n计算两者的平均差异，得到一个“方向向量”，表示与目标行为相关的激活模式： $r = \\frac{1}{n_t} \\sum_{i=1}^{n_t} a_{target}^i - \\frac{1}{n_n} \\sum_{i=1}^{n_n} a_{neutral}^i$\n其中$a_{target},$ $a_{neutral}$为目标/中性行为的激活向量， $n_t$ 、 $n_n$ 为样本数量\n调整激活向量： 在推理阶段，向模型的原始激活向量添加或调整方向向量，以达到目标行为的增强或抑制； $a{\\prime} = a + \\alpha r$， 其中$\\alpha$ 为缩放因子，控制目标行为的强度。\n一些有趣的实验结论 第18层的激活向量对个性特质的表达影响最大。这表明模型的中间层是特性表达的重要“瓶颈”或集中区域（所采用的模型为uncensored Llama 3 8B） 使用缩放因子（$\\alpha$）控制特性的强度，发现 $\\alpha$ 过大时模型输出会变得不连贯，但适当范围内（如1.3-1.4）可以成功实现平衡 Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks https://arxiv.org/abs/2412.15605\n通过引入缓存增强生成（Cache-Augmented Generation, CAG），解决：\n避免检索延迟：通过预加载相关文档到模型中，消除实时检索环节，实现快速推理 减少检索错误：通过一次性加载所有相关上下文，确保模型能基于完整的知识生成答案，避免检索错误引入的上下文缺失或偏差 简化系统架构：取消检索模块，使系统更简单 充分利用长上下文模型能力：借助模型的扩展上下文窗口，直接加载大规模文档进行推理，提升效率和性能 具体实现 External Knowledge Preloading 将相关文档预先加载到模型的扩展上下文中；\n将一组与目标任务相关的外部文档集合 $D = {d_1, d_2, \\dots}$ 进行整理和格式化；\n使用语言模型 M 处理预加载的文档 D，生成“KV-cache”，\n$C_{KV} = KV\\text{-}Encode(D)$， 生成的KV缓存可以存储在磁盘或内存中，供后续推理阶段加载。\nInference 在推理时直接利用预加载的KV缓存生成答案，避免实时检索\n在用户提出查询 $Q$ 时，$R = M(Q ,|, C_{KV})$，R 为模型生成的答案\nCache Reset 在多轮推理中保持KV缓存的效率，同时避免不必要的重新加载\nKV缓存在推理过程中会随着新输入的token（例如用户的新查询）而不断扩展 为了避免缓存过度增长，系统通过截断不必要的token来重置KV缓存： $C_{KV}^{reset} = Truncate(C_{KV}, {C_1, C_2, \\dots, C_n})$ CAG vs RAG RAG需要实时检索和整合外部知识，而CAG通过一次性加载文档避免了这些步骤\n优势：\n推理效率提升： 不需要实时检索 答案质量提高： 全局上下文视角让模型生成的答案更加准确和一致。 系统架构简化： 通过移除检索模块，降低了系统复杂性 局限性\n上下文容量限制 生成KV缓存时的高计算成本 缺乏动态适应：不适合知识动态更新的场景 无法从根本解决“garbage-in-garbage-out” 问题：对于加载的文档需要精心筛选，显然是不符合现实的 资源需求高 lost-in-the-middle MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery https://arxiv.org/abs/2409.05591\nMemoRAG试图解决以下几个问题\n对明确信息需求的依赖：传统 RAG 系统需要清晰的查询，并且只能对明确表述的知识进行匹配 知识的结构化依赖 上下文窗口的限制 解决的问题 MemoRAG 提出了全局记忆模块，能够从数据库中生成初步答案线索 MemoRAG 的记忆模块在全局范围内形成紧凑的语义表示，通过记忆生成线索引导检索，实现了分布式证据的高效整合 MemoRAG 的记忆模块能够处理百万级别的上下文长度，将长文档内容压缩为关键语义，使得复杂信息更加易于检索和生成 MemoRAG 采用 双系统架构，包含两个主要组件：\n1. 记忆模块（Memory Model）：\n轻量级、长上下文语言模型，用于从数据库中生成全局记忆和线索。 高效处理长文档并生成辅助线索（粗略答案或检索指引）。 2. 生成模块（Generation Model）：\n强大的生成语言模型，用于根据检索到的上下文生成高质量的最终答案。 线索生成 引入了一个记忆模块（由一个较小的 LLM，如 7B 模型组成），它基于数据库构建全局记忆。\n当收到原始 Query ( q ) 时，记忆模块生成“线索” ( y )，这些线索是更明确的中间结果或检索指引，用于改善后续检索过程。线索充当改写后的 Query 或辅助信息，明确用户的隐含意图或分布式证据需求。\n这些线索相当于对原始 Query 进行了增强或改写，使检索工具可以更准确地定位相关内容。\n从某种角度看，MemoRAG 的记忆模块确实可以类比为最原始的 RAG 系统，因为它通过数据库中的查询生成初步结果（线索），引导后续检索和生成任务\n差异性：\n记忆模块通过压缩和语义提取生成高层次的线索，而不是仅仅依赖检索工具匹配原始 Query 和数据库 记忆模块的训练目标是最大化生成线索或下一个 token 的概率，具体为：\n$$ \\max_{\\Theta_{\\text{mem}}} P(x_{i,j} | x_m^{1,1}, \\dots, x_m^{i-1,k}, x_{i,1}, \\dots, x_{i,j-1}) $$\n其中：\n$x_m^{1,1}, \\dots, x_m^{i-1,k}$ 表示先前生成的记忆 token。 $x_{i,1}, \\dots, x_{i,j-1}$ 表示当前窗口的原始 token。 目标：在记忆 token 和当前上下文的基础上生成下一个 token。 数据集例子：\nQ：那么采用LLM的中间层（或者Bert输出）输出，即embedding向量用于语义的压缩，效果差异？\nA：\nMemoRAG 的记忆模块不仅是简单的压缩 将长序列压缩为短序列（记忆 token），但不仅是为了减少序列长度，更重要的是: 捕获全局语义信息 指导任务的线索生成 动态生成线索: 基于压缩后的语义记忆，生成能够高效描述用户查询意图的线索，这是 embedding 压缩很难直接实现的 为什么不能直接使用 embedding 作为语义压缩 局部信息不足: LLM 的中间层输出通常是针对每个 token 的局部语义表示，并不一定能够很好地捕获全局信息 缺乏任务适配性 信息压缩能力的限制 影音记录 精选歌单 Live演出 01.14 U137\n01.16 Mayhem\n书\u0026amp;阅读摘录 《动物化的后现代》 东浩纪\n《素食者》 韩江\n我在泰缅边境调查电诈产业 200 天 全球资本主义中“残余”的人被吸引或诱拐至此。\n最后却是泰国和缅甸承担后果，两个国家的形象在国际舆论中跟“电信诈骗”捆绑在了一起\n泰国华人在湄索的一边打通移民局、警察局和边防军，搭建起偷渡和走私的基础设施；缅甸华人则在另一边打通“民地武”（民族地区武装）的门路，搞到地，两边的华人共同为来自中国的“电诈”行业大佬们铺平了道路。 … 他现在开始拓展新市场——“救人”的生意。\n这些地方某种意义上彻底摆脱了政府，实现了右翼安那其的自治想象。\n我们理解“犯罪”时，喜欢进入一种“受害者”、“加害者”是非分明的道德判断。事实上，“受害”是一个光谱。\n湄索越来越国际化了。全球化的残余在这个边缘地带野蛮生长。\n国境之间：“春天革命”与泰缅边界上的缅甸流亡者 对于湄索的穆斯林和克伦族来说，“缅甸人”还是“泰国人”并不是非此即彼的身份，而是一个流动的光谱，一个中转的过程，一个不具太多含义的分类法。很多人只是通过自己的族群身份识别与区分彼此\n就像研究湄索“边境资本主义”的学者斯蒂芬・坎贝尔（Stephen Campbell）指出，边境是一种“庇护”和“暴力”的二元辩证\n然而，国境线那一边的暴政，也同时与国境线这一边的统治技术形成了共谋，泰国当局与私人雇主利用流离失所者的不稳定身份，进行勒索与压榨。\n没有身份和难以移动，使边境的生产机制成为全球资本主义链条上最残酷的一环，在这里劳作的缅甸移民工则是这个系统中用后即弃的人\n科蒙也在为不同的国际 NGO 做事，但是他却越来越厌倦这个生态。“很多议程早已隔空提前设置好了，隔空输出西方世界观和议程。很多时候公民社会组织也不得不来写一些离地的企划案，才能申请到经费。这个生态里的大部分人都在忙于‘话语’的生产，竞争谁掌握了‘真理’。”\n这个生态更多是在一套向国际社会“展演”的逻辑下展开，把前线的现实按照西方的人权、性别、族群、劳工、难民等议题分配到不同的位置，以“去政治化”的方式使更多人的困境被维持在一种可忍受和可展演的状态中没有出路。清迈就是这样的展演场所，于是，他始终坚持留在湄索，与前线保持联系。\n绝大多数流亡者在边境上的服务业和工厂做廉价劳工，同时压低了原本就远低于泰国最低时薪的工资，共同因为没有身份而在“边境资本主义”的宰制下灵活就业、饱受盘剥。\n“原住民”和“外来者”这一殖民时代遗留的二分法，在今天仍然是缅甸“族群民族主义”最棘手的身份认同政治，生发了无止境的“血与土”的逻辑\n民主转型甚至更加释放了族群民族主义（ethno-nationalism）的毒素，公民社会团体、政党、商界、民兵团体、武装组织，全部以族群身份为单位进行民主动员。身份危机伴随着民主化更猛烈地爆发了出来。 … 对于今天的流亡政府NUG和围绕在其周围的人来说，“夺回民主”再度成为缅甸政治的唯一解药。\n对于“春天革命”的流亡者而言，湄索只是一个中转地，一种中间状态，没有人在这里和泰国发生任何关系。它就像两条国境线之间的那个位置，所有在那里的时光都是悬置着的时间。留在这里的人，凭借“革命即将胜利”的希望，在这座城市不可调和的“临时性”中苦苦等待。\nThe Shadow of Cognitive Laziness in the Brilliance of LLMs LLMs boost short-term performance but risk fostering \u0026ldquo;metacognitive laziness.”\nOver-reliance on AI may erode self-regulation, critical thinking, and deeper learning processes.\nHybrid intelligence thrives when AI aids learning without replacing human cognitive engagement.\n第九届网络社会年会｜王洪喆：从技术浪漫主义到技术封建主义 为什么我们如此害怕第一次呢？生命中的每一天都是第一次。每个早晨都是全新的。我们从未活过同一天，但每天早上，我们从来不会不敢起床……为什么？\nAmateur（业余爱好者）— enthusiast（发烧友）— humanist（人文主义者) 过往对中国早期数字文化的研究过于强调其自上而下的技术民族主义特征。在我的研究中，我强调在早期，尤其是 1990 年代，这更多是一种“enthusiast”群体自下而上发展起来的技术民族主义，有着浓厚的人文主义特征，而非过去理解的“先进-落后”的二元技术民族主义。\n但在当下，黑客伦理和东亚数字文化伦理都已衰落甚至瓦解。剩下的是一种新的“技术封建主义伦理”——这种伦理不仅追求赚越来越多的钱，而且坚信数字技术平台应该向人们收取租金，而不是成为人类共享的基础设施。这是一个非常重要的变化：共享变成了租赁\n随着个人财产成为平台巨头、Uber 和 Airbnb 积累资本和数据的工具，消费品被重新配置为积累手段。这种成为佃农的趋势——即成为拥有生产资料的人，但同时其劳动增加了平台所有者的资本——是一种新封建主义。\nHow AI-assisted coding will change software engineering: hard truths They\u0026rsquo;re not just accepting what the AI suggests. They\u0026rsquo;re constantly: Refactoring the generated code into smaller, focused modulesAdding edge case handling the AI missedStrengthening type definitions and interfacesQuestioning architectural decisionsAdding comprehensive error handling … In other words, they\u0026rsquo;re applying years of hard-won engineering wisdom to shape and constrain the AI\u0026rsquo;s output. … AI tools help experienced developers more than beginners.\nThe reality is that AI is like having a very eager junior developer on your team. … The more you know, the better you can guide them.\nI\u0026rsquo;ve watched senior engineers use AI to: Rapidly prototype ideas they already understandGenerate basic implementations they can then refineExplore alternative approaches to known problemsAutomate routine coding tasks … Meanwhile, juniors often: Accept incorrect or outdated solutionsMiss critical security and performance considerationsStruggle to debug AI-generated codeBuild fragile systems they don\u0026rsquo;t fully understand\nWhen code just \u0026ldquo;appears\u0026rdquo; without you understanding the underlying principles: You don\u0026rsquo;t develop debugging skillsYou miss learning fundamental patternsYou can\u0026rsquo;t reason about architectural decisionsYou struggle to maintain and evolve the code\nThis \u0026ldquo;70% problem\u0026rdquo; suggests that current AI coding tools are best viewed as: Prototyping accelerators for experienced developersLearning aids for those committed to understanding developmentMVP generators for validating ideas quickly\nBut for now, the most pragmatic approach is to use AI to accelerate learning, not replace it entirely. … Accelerating the known Exploring the possible Automating the routine\nThe most effective teams in 2025 may be those that learn to: Set clear boundaries and guidelines for their AI agentsEstablish strong architectural patterns that agents can work withinCreate effective feedback loops between human and AI capabilitiesMaintain human oversight while leveraging AI autonomy\n","permalink":"https://niraya666.github.io/monthly/2025-01-%E6%9C%88%E5%88%8A/","summary":"\u003ch1 id=\"值得关注的新模型\"\u003e值得关注的新模型\u003c/h1\u003e\n\u003ch2 id=\"deepseek-r1\"\u003eDeepseek R1\u003c/h2\u003e\n\u003cp\u003eDeepSeek R1通过\u003cstrong\u003e纯强化学习（RL）框架\u003c/strong\u003e实现了推理能力的突破，首次验证了无需依赖传统监督微调（SFT）或蒙特卡洛树搜索（MCTS）等复杂方法，仅通过两阶段RL优化即可显著提升模型逻辑推理性能。其综合能力直接对标OpenAI的o1模型，在数学（MATH-500达97.3%）、代码生成等核心指标上实现部分超越，同时\u003cstrong\u003e全面开源模型权重、训练技术文档及6个蒸馏版本（1.5B-70B）\u003c/strong\u003e，使开发者可灵活适配不同算力场景。尤为引人注目的是，该模型在训练中展现出\u003cstrong\u003e自发反思与多步骤验证能力\u003c/strong\u003e，研究者观察到其通过内部反馈机制主动修正推理路径的“顿悟时刻”，揭示了AI系统自我优化的新可能。此外，其研发成本仅为560万美元（基于2048块H800 GPU），相比同类模型降低1-2个数量级。\u003c/p\u003e\n\u003cp\u003e技术报告: \u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003ehttps://arxiv.org/abs/2501.12948\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHuggingface: \u003ca href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1\"\u003ehttps://huggingface.co/deepseek-ai/DeepSeek-R1\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e一些基于和关于R1的开源复刻工作：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen-R1: \u003ca href=\"https://huggingface.co/blog/open-r1\"\u003e\u003cstrong\u003eOpen-R1: a fully open reproduction of DeepSeek-R1\u003c/strong\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eRAGEN: \u003ca href=\"https://github.com/ZihanWang314/ragen/tree/main\"\u003e\u003cstrong\u003eRAGEN: A General-Purpose Reasoning Agent Training Framework\u003c/strong\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"kimi-k1\"\u003eKimi K1\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eKimi K1的核心创新在于通过强化学习驱动的多模态架构，首次实现端到端的视觉与推理深度融合。\u003c/strong\u003e 该模型突破传统分阶段处理模式，直接将图像输入与逻辑推演结合，支持模糊图像解析、手写题识别等复杂场景，并引入反思机制修正推理错误；其两阶段训练框架（预训练+强化学习规模化优化）显著提升思维链生成质量，使模型在数学、物理、化学等跨学科测试中超越国际主流视觉模型（如OpenAI o1）。\u003c/p\u003e\n\u003cp\u003eArxiv: \u003ca href=\"https://arxiv.org/abs/2501.12599\"\u003eKimi k1.5: Scaling Reinforcement Learning with LLMs\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"qwen25-max\"\u003eQwen2.5-Max\u003c/h2\u003e\n\u003cp\u003eQwen2.5-Max 是一款经过大规模预训练的专家混合（MoE）模型，训练数据量超过 20 万亿 tokens，并通过监督微调（SFT）和人类反馈强化学习（RLHF）进行优化。该模型在多个基准测试中表现优异，包括在 Arena-Hard、LiveBench、LiveCodeBench 和 GPQA-Diamond 等评估中超越了 DeepSeek V3，并在 MMLU-Pro 大学水平知识测试中展现出竞争力，同时在与其他领先模型（如 DeepSeek V3、GPT-4o 和 Claude-3.5-Sonnet）的对比中也表现突出。Qwen2.5-Max 已集成到 Qwen Chat，支持对话交互和功能体验，同时其 API（模型名称为 qwen-max-2025-01-25）已上线，用户可通过注册阿里云账户并激活相关服务进行调用。\u003c/p\u003e\n\u003cp\u003eBlog: \u003ca href=\"https://qwenlm.github.io/blog/qwen2.5-max/\"\u003ehttps://qwenlm.github.io/blog/qwen2.5-max/\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"qwen25-vl\"\u003eQwen2.5-VL\u003c/h2\u003e\n\u003cp\u003eQwen2.5-VL是由阿里云通义千问团队于2025年1月28日推出的新一代视觉语言模型，专注于提升多模态处理与视觉理解能力。该模型具备五大核心功能：通过增强的视觉理解可识别常见物体并解析图像中的文本、图表及布局；作为视觉代理支持跨设备多步骤操作（如电脑修图、手机订票）；可精准定位超过1小时长视频的特定事件片段；通过生成边界框/点实现视觉定位并输出标准化JSON坐标；针对发票/表格等扫描件提供结构化数据输出，赋能金融商业场景。其创新架构采用动态FPS采样与时间维度mRoPE对齐技术，实现视频动态分辨率处理与时间序列学习，同时通过窗口注意力机制和SwiGLU-RMSNorm优化ViT编码器，保持与Qwen2.5大语言模型架构统一。模型提供3B、7B、72B三种参数规模。\u003c/p\u003e\n\u003cp\u003eBlog: \u003ca href=\"https://qwenlm.github.io/blog/qwen2.5-vl/\"\u003ehttps://qwenlm.github.io/blog/qwen2.5-vl/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHuggingface: \u003ca href=\"https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct\"\u003ehttps://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"qwen25-1m\"\u003eQwen2.5-1M\u003c/h2\u003e\n\u003cp\u003eQwen2.5-1M系列模型是由Qwen团队于2025年1月27日发布的开源大语言模型，包含Qwen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M两个版本，其核心突破在于支持高达100万Token的上下文处理能力。该模型通过三阶段技术实现长上下文支持：首先在预训练阶段采用Adjusted Base Frequency方法将RoPE基础频率提升至1000万，通过渐进式扩展策略将上下文从4K扩展到256K；随后在监督微调阶段采用短指令（32K）与长指令（256K）混合训练策略，平衡长短任务性能；最终通过创新性Dual Chunk Attention技术将上下文扩展至百万量级，该技术通过重映射超大相对位置值解决位置编码外推难题。为应对超长序列处理挑战，模型引入分块预填充技术（32K分块）降低显存消耗，结合稀疏注意力优化使百万级序列的精度损失显著降低，并通过算子效率优化和动态分块流水线并行实现3.2-6.7倍的预填充加速。在性能表现上，该系列模型不仅能在百万Token的\u0026quot;大海捞针\u0026quot;任务中精准检索信息，在RULER、LV-Eval等复杂长文本理解任务中超越前代128K版本，同时在短文本任务上保持与128K版本相当的性能，其中14B版本在保持GPT-4o-mini八倍上下文长度的前提下实现了相近的短文本处理能力。\u003c/p\u003e","title":"2025-01 月刊"},{"content":"写在前面 决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。\n学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。 于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）\n主要的教材来自：\nReinforcement Learning: An Introduction\nB站UP主 shuhuai008 的系列推导视频\n本篇笔记将包含以下的内容：\nMDP\nDP\nMonte Carlo Methods\nTD方法\n马尔可夫决策过程(Markov Decision Process，MDP) MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\n相关概念 随机变量（Random Variance）： ( $X, \\ y, \\ x \\perp y$)，随机变量之间存在的独立性。\n随机过程（Stochastic Process）： ${S_t}_{t=1}^{\\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。\nMarkov链/过程（Markov Chain/Process）：强调了Markov性质（Markov Property），即未来的状态仅依赖于当前状态而与过去无关，形式化地表示为：\n$$ P(S_{t+1}|S_t, S_{t-1}, \u0026hellip;,S_1) = P(S_{t+1}|S_{t}) $$\n状态空间模型（State Space Model）： Markov Chain + Observation； 如 HMM， Kalman Filter，particle Filter。\nMarkov奖励过程（Markov Reward Process, MRP）：Markov chain + Reward；在Markov链的基础上加入奖励函数，以描述智能体在每个状态下获得的即时奖励。\nMarkov决策过程（Markov Decision Process, MDP）： Markov Chain + Reward + Action；MRP的进一步扩展，在Markov链和奖励基础上再加入动作选择。\n$$ S: \\rm{State \\ set} \\to s_t $$\n$$ A: \\rm{action \\ set}, \\forall s\\in S, \\ A(s) \\to A_t $$\n$$ R: \\rm{reward \\ set} \\to R_t, R_{t+1} $$\n动态特性 the state-transition probabilities\n在MDP模型中，如何根据状态、动作和奖励来计算系统的转移概率\n状态转移概率 $P: \\ p(s\u0026rsquo;, r| s,a) \\triangleq P_r{S_{t+1}=s\u0026rsquo;, R_{t=1}=r|S_t=s, A_t=a}$ 状态转移函数：\n$$ p(s\u0026rsquo;|s,a) = \\sum_{r\\in R}p(s\u0026rsquo;, r|s,a) $$\n状态转移概率 $p(s\u0026rsquo;, r | s, a)$ 定义了在某个状态执行某个动作时转移到下一状态并获得特定奖励的概率。\n状态转移函数 $p(s\u0026rsquo; | s, a)$ 则是对奖励进行求和，得到仅与状态和动作相关的转移概率。\n价值函数 That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\nMDP目标：找到最优策略\npolicy：$\\pi$\n确定性策略: $a \\triangleq \\pi(s)$\n随机性策略： $\\pi(a|s) \\triangleq pr(A_t=a|S_t=s)$\n策略（Policy）是智能体在给定状态下选择动作的规则。策略决定了智能体在每个状态下应该采取什么行动。\n强化学习的主要任务之一就是学习一个最优策略，使得智能体在长期内能够获得最高的累计奖励。\n什么样的策略是好策略，如何找到好策略 → 回报\nReward: 奖励是智能体在特定状态下执行某个动作后立即收到的反馈信号，用来衡量动作的好坏。\n奖励通常用函数 $R(s, a)$表示，表示在状态 $s$ 下执行动作 $a$ 所得到的即时奖励。\nReturn： 累积回报；通过对未来的奖励进行累积并引入折扣因子 $\\gamma$ 来计算，从而衡量在一个状态或时间步开始后，智能体在长期内的收益。目的是让较近的奖励比未来的奖励更有权重，即智能体更关注短期内的回报。\n$G_t = R_{t+1} + \\gamma R_{t+2}+\\gamma ^2R_{t+3}\u0026hellip;+\\gamma ^{T-1}R_T = \\sum_{i=0}^{\\infty}\\gamma^iR_{t+i+1} \\ (T\\to \\infty)$\n$\\gamma \\in [0,1]$\n累积回报 $G_t$ 是强化学习中最优化的目标，即智能体通过学习找到一个策略，使得每个时间步的累积回报 $G_t$ 最大化。这个值衡量了从时间步 $t$开始，智能体在长期内可以期望获得的总奖励。\nBellman Expectation Equation It expresses the value of a decision problem at a certain point in time in terms of the payoff from some initial choices and the value of the remaining decision problem that results from those initial choices.\n状态值函数（State-Value Function） $v_{\\pi}(s)$ ：\n定义：状态值函数 $v_{\\pi}(s)$ 表示在策略 $\\pi$ 下，从状态 $s$ 开始时期望的回报，即： $$ V_{\\pi}(s) = \\mathbb{E}_{\\pi} [G_t | S_t = s] $$\n含义：该函数衡量了在状态 $s$ 下遵循策略 $\\pi$ 时，智能体能够获得的期望总回报。 动作值函数（Action-Value Function） $q_{\\pi}(s, a)$ ：\n定义：动作值函数 $q_{\\pi}(s, a)$ 表示在策略 $\\pi$ 下，从状态 $s$ 开始、采取动作 $a$ 后期望的回报，即： $$ q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} [G_t | S_t = s, A_t = a] $$\n含义：该函数衡量了在状态 $s$ 下执行动作 $a$ 后，并继续遵循策略 $\\pi$ 时，智能体能够获得的期望总回报。 状态值函数 $V_{\\pi}(s)$ 可以用动作值函数 $q_{\\pi}(s, a)$ 表示为：\n$$ V_{\\pi}(s) = \\sum_{a \\in A(s)} \\pi(a|s) q_{\\pi}(s, a) $$\n等于在该状态下执行不同动作的期望回报之和（加权平均 ），权重是执行每个动作的概率 $\\pi(a | s)$。\n同样，动作值函数 $q_{\\pi}(s, a)$ 可以通过立即奖励和后续状态的折扣值计算：\n$$ q_{\\pi}(s, a) = \\sum_{r, s^{\\prime}} p(s^{\\prime}, r | s, a) \\left( r + \\gamma V_{\\pi}(s^{\\prime}) \\right) $$\n其中， $p(s^{\\prime}, r | s, a)$ 是状态 $s$ 下执行动作 $a$ 后转移到状态 $s^{\\prime}$ 并获得奖励 $r$ 的概率。\nBellman期望方程的状态值函数形式：\n$$ V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s^{\\prime}, r} p(s^{\\prime}, r | s, a) [r + \\gamma v_{\\pi}(s^{\\prime})] $$\n当前状态值 $V_{\\pi}(s)$ 和后续状态值之间的关系\nBellman期望方程的动作值函数形式：\n$$ q_{\\pi}(s, a) = \\sum_{s^{\\prime}, r} p(s^{\\prime}, r | s, a) \\left[ r + \\gamma \\sum_{a{\\prime}} \\pi(a^{\\prime} | s^{\\prime}) q_{\\pi}(s^{\\prime}, a^{\\prime}) \\right] $$\nBellman期望方程将当前状态（或状态-动作对）的值递归地表示为与后续状态值的关系\n通过递归地计算这些期望值用于在策略评估\nTo recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the sum of immediate reward + the discounted value of the state that follows.\nBellman Optimality Equation the optimal state-value function\n$V_{*}(s) \\triangleq \\max_{\\pi} V_{\\pi}(s)$ the optimal action-value function\n$q_*(s,a)\\triangleq \\max_{\\pi}q_{\\pi}(s,a)$\nFor the state-action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. to write $q_$ in terms of $v_$ as follows: $q_(s,a) = E[R_{t+1}+\\gamma v_(S_{t+1})|S_t = s, A_t = a]$\n…\n$V_{\\pi *}(s)=\\max q_{\\pi *}(s,a)$\n…\nThe Bellman optimality equation：\n$$ \\begin{align*} v_(s) \u0026amp;= \\max_{a \\in \\mathcal{A}(s)} q_(s, a) \\ \u0026amp;= \\max_a \\mathbb{E}{\\pi} \\left[ G_t \\mid S_t = s, A_t = a \\right] \\ \u0026amp;= \\max_a \\mathbb{E}{\\pi} \\left[ R_{t+1} + \\gamma G_{t+1} \\mid S_t = s, A_t = a \\right] \\ \u0026amp;= \\max_a \\mathbb{E} \\left[ R_{t+1} + \\gamma v_(S_{t+1}) \\mid S_t = s, A_t = a \\right] \\ \u0026amp;= \\max_a \\sum_{s\u0026rsquo;, r} p(s\u0026rsquo;, r \\mid s, a) \\left[ r + \\gamma v_(s\u0026rsquo;) \\right]. \\end{align*} $$\n$$ \\begin{align*} q_(s, a) \u0026amp;= \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a\u0026rsquo;} q_(S_{t+1}, a\u0026rsquo;) \\mid S_t = s, A_t = a \\right] \\ \u0026amp;= \\sum_{s\u0026rsquo;, r} p(s\u0026rsquo;, r \\mid s, a) \\left[ r + \\gamma \\max_{a\u0026rsquo;} q_(s\u0026rsquo;, a\u0026rsquo;) \\right]. \\end{align} $$\n在理论上，我们定义了最优值函数和最优策略，并且可以通过贝尔曼最优性方程来解决。\n但：\n计算成本\n过于复杂而无最优的解析解\n直接计算最优策略或值函数既不现实，也没有必要。有时只能通过逼近来寻找次优解，而非真正的最优解。\n动态规划 如何求解Bellman Optimality Equation\n采用动态规划DP；虽然动态规划要求环境模型（如 $p(s^{\\prime}, r \\mid s, a)$）是已知的，因此在强化学习中直接应用可能受到限制，但它仍是理论基础。强化学习中的许多方法都可以被视为在没有模型的情况下逼近动态规划的效果。\n动态规划：通过将问题分解为更小的子问题，逐步解决并保存每个子问题的结果，以避免重复计算，从而高效地求解问题。\n策略迭代（Policy Iteration） = 策略评估（Evaluation）+策略改进\n价值迭代 （Value Iteration）：直接通过Bellman最优方程迭代更新状态值函数 $V(s)$，直到收敛\n策略评估公式推导\n已知MDP $p(s\u0026rsquo;,r|s,a)$ , 给定$\\pi$ 求 $v_{\\pi} \\ (\\forall s \\in S)$ 记\n$$ V_{\\pi} = \\begin{pmatrix} v_{\\pi}(s_1) \\ v_{\\pi}(s_2) \\ . \\ . \\ .\\ v_{\\pi}(s_{|s|})\n\\end{pmatrix}_{|s|\\times 1} $$\n通过递归公式：$v_{\\pi}(s) = \\mathbb{E}{\\pi}[G_t \\mid S_t = s]$\n代入回报的定义 $G_t = R_{t+1} + \\gamma G_{t+1}$ 得到：\n$$ V_{\\pi}(S)=E_{\\pi}[G_t|S_t=s] \\ =E_{\\pi}[R_{t+1}+\\gamma V_{\\pi}(s_{t+1})]\\=\\sum_a\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)[r+\\gamma V_{\\pi}(s\u0026rsquo;)] $$\n有两个部分：\n奖励部分（即期望即时奖励）\n$$ \\sum_a \\pi(a \\mid s) \\sum_{r} r \\cdot p(r \\mid s, a) = \\sum_a \\pi(a \\mid s) E_{\\pi}[R_{t+1}|S_{t}=s,A_t=a]= \\ \\sum_a \\pi(a \\mid s) r_{\\pi}(s,a) = r_{\\pi}(s). $$\n这里 $r_{\\pi}(s)$ 是策略 $\\pi$ 下的状态 $s$ 对应的期望收益。\n未来价值部分（即折扣后的期望价值）\n$$ \\gamma \\sum_{s^{\\prime}} p_{\\pi}(s, s^{\\prime}) \\cdot v_{\\pi}(s^{\\prime}), $$\n其中 $p_{\\pi}(s, s^{\\prime}) = \\sum_a \\pi(a \\mid s) p(s^{\\prime} \\mid s, a)$ 是策略 $\\pi$ 引导下的状态转移概率。\n综合这两个部分，价值函数满足递归关系：\n$$ v_{\\pi}(s) = r_{\\pi}(s) + \\gamma \\sum_{s^{\\prime}} p_{\\pi}(s, s^{\\prime}) v_{\\pi}(s^{\\prime}). $$\n策略评估（Policy Evaluation）——解析解 用矩阵 $V_{\\pi}$、 $r_{\\pi}$ 、和 $P_{\\pi}$ 表示上述关系：\n$V_{\\pi}$ : 状态价值向量（$|S|×1$）\n$r_{\\pi}$ : 状态的期望奖励向量（$|S|×1$）\n$P_{\\pi}$ : 策略引导的转移概率矩阵（$|S|×|S|$），第 $(i, j)$ 项为从状态 $i$ 到 $j$ 的转移概率\n则递归关系可以写为：\n$$ V_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} V_{\\pi}. $$\n化简为线性方程：\n$$ (I - \\gamma P_{\\pi}) V_{\\pi} = r_{\\pi}. $$\n求解：\n$$ V_{\\pi} = (I - \\gamma P_{\\pi})^{-1} r_{\\pi}. $$\n矩阵求逆 ($I - \\gamma P_{\\pi})^{-1}$ 的时间复杂度是 $O(|S|^3)$，其中 $|S|$ 是状态的数量。这对于大规模状态空间是不可行的。\n迭代方法（动态规划思想）：避免直接求解矩阵逆，使用迭代更新 $V_{\\pi}$ 的方式：\n$$ V_{\\pi}^{(k+1)} = r_{\\pi} + \\gamma P_{\\pi} V_{\\pi}^{(k)}. $$\n通过 $O(|S|^2)$ 的复杂度更高效地计算近似解。\n策略评估（Policy Evaluation）—— 迭代解 迭代方法使用 Bellman 方程作为递归关系，通过不断更新 $V_k(s)$ 的近似值，逐步收敛到真实的 $V_{\\pi}(s)$ 。\n更新公式为：\n$$ V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s^{\\prime},r} p(s^{\\prime},r|s,a) \\left[ r + \\gamma V_k(s^{\\prime}) \\right]. $$\n具体实现\n随机初始化 $V_0(s)$\n对每个状态 $s$，使用 Bellman 方程更新价值函数：\n$$ V_{k+1}(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s^{\\prime},r} p(s^{\\prime},r|s,a) \\left[ r + \\gamma V_k(s^{\\prime}) \\right]. $$\n当两次迭代的最大差异（例如$\\max_s |V_{k+1}(s) - V_k(s)|$ ）小于某个阈值 $\\theta$ 时停止。\n伪代码\nPolicy Improvement 策略改进是 策略迭代算法（Policy Iteration） 的一个关键步骤，其目标是利用当前策略 $\\pi$ 的价值函数 $V_\\pi$ 或 $Q_\\pi$ 推导出一个更优的策略 $\\pi^{\\prime}$ 。\n基于当前策略的价值函数，找到一个更优的策略 $\\pi^{\\prime}$ 。\n核心是保证新策略的价值 $V_{\\pi^{\\prime}}$ 至少不低于当前策略 $V_\\pi$。\n策略改进定理（Policy Improvement Theorem） 给定两个策略 $\\pi$ 和 $\\pi^{\\prime}$ ，若：\n$$ q_\\pi(s, \\pi^{\\prime}(s)) \\geq V_\\pi(s), \\quad \\forall s \\in S, $$\n则有：\n$$ V_{\\pi^{\\prime}}(s) \\geq V_\\pi(s), \\quad \\forall s \\in S. $$\n若新的策略 $\\pi^{\\prime}$ 的价值函数 $V_{\\pi^{\\prime}}(s) \\geq V_\\pi(s)$ 对所有状态 $s$ 都成立，那么策略 $\\pi^{\\prime}$ 至少与 $\\pi$ 一样好。\n若在某些状态 $s$ 上严格成立 $V_{\\pi^{\\prime}}(s) \u0026gt; V_\\pi(s)$ ，则 $\\pi^{\\prime}$ 是更优的策略。\n也就是说，无需再计算$V_{\\pi^{\\prime}}(s)$\n证明(略)\nGreedy 策略 如何找到一个比当前策略更优的策略？ — 贪心策略， 在当前状态 $s$ 选择短期收益和长期期望回报的最优动作。\n具体而言 $\\pi^{\\prime}(s)$ 是在每个状态 $s$ 下选择一个动作 $a$ ，使得当前的动作价值函数 $Q_\\pi(s, a)$ 最大化：\n$$ \\pi^{\\prime}(s) = \\arg\\max_a Q_\\pi(s, a). $$\nPolicy Iteration 通过 策略评估 和 策略改进 两个交替步骤，逐步优化策略，最终收敛到最优策略 $\\pi^*$ 。\nPolicy Evaluation： 计算当前策略 $\\pi$ 的价值函数 $V_\\pi(s)$ 或动作价值函数 $Q_\\pi(s, a)$，使用贝尔曼方程，得到策略下每个状态的长期回报\nPolicy Improvement： 基于当前策略的价值函数，找到一个更优的策略 $\\pi^{\\prime}$ , 核心是保证新策略的价值 $V_{\\pi^{\\prime}}$ 至少不低于当前策略 $V_\\pi$\n重复交替执行 策略评估 和 策略改进，直到策略不再变化（收敛）\nValue Iteration Policy Iteration 的简化形式，通过截断策略评估的过程，直接利用Bellman方程递推更新值函数，逐步逼近最优值函数 $V^$ 和最优策略 $\\pi^$ 。\n价值迭代的目标是直接计算最优值函数 $V^{(s)}$ ，然后通过值函数导出最优策略 $\\pi^{(s)}$ 。\n策略迭代 vs. 价值迭代\nPolicy Iteration: Policy Evaluation+Policy Improvement, Policy Evaluation 需要通过多次迭代对当前策略的值函数 $V_\\pi$ 进行精确求解\nValue Iteration： 对策略迭代的优化，截断了策略评估过程，不需要完全求解当前策略 $\\pi$ 的值函数 $V_\\pi$ ，而是直接使用近似值 $V_k$ 来更新，结合策略改进一步完成\n递推公式：\n$$ V_{k+1}(s) = \\max_a \\sum_{s^{\\prime}, r} P(s^{\\prime}, r \\mid s, a) \\left[ r + \\gamma V_k(s^{\\prime}) \\right]. $$\n价值迭代是极端情况下的策略迭代\n策略迭代 \u0026ndash;\u0026gt; 价值迭代 \u0026ndash;\u0026gt; 就地策略迭代（异步策略迭代）\nMonte Carlo Methods 回顾RL：\n两个主体： Agent, Environment 一个框架： MDP 五大元素：$S$, $A$, $R$, $\\pi$, $p(s,\u0026lsquo;r|s,a)$ 核心问题： 如何找到最优策略\n引入价值函数的概念： $v_{\\pi}$, $q_{\\pi}$\n策略迭代： 策略评估，策略改进\n对于动态规划（DP）方法，策略评估和改进会通过递归迭代逐渐逼近：${v_k} \\to V_{\\pi}$\n蒙特卡洛方法的引入：与动态规划不同，蒙特卡洛方法无需明确知道环境的动态特性（即$p(s^{\\prime}, r \\mid s, a)$ 的分布）。它基于实际或模拟的经验，通过采样得到值函数和最优策略。即model-free。\n**关键思想：**平均多个 episode 的回报，来估计每个状态或动作的价值；通过不断采样和学习，实现状态值函数和策略的逼近。\n非平稳性问题： 由于策略和环境的动态性，导致奖励或状态分布随着时间发生变化。具体而言， 在强化学习中，智能体会随着经验的积累不断调整其策略，策略的变化会导致智能体访问的状态分布和奖励的期望值随时间变化，这些变化导致采样数据的分布在整个学习过程中不是固定的，这与传统监督学习中的**独立同分布（i.i.d.）**假设冲突。\n蒙特卡洛方法不依赖某一时刻的单次采样，而是基于整个 episode 的累计回报 $G_t$ 进行估计，对于某个状态 $s$ ，通过采样多个 episode 的平均值来估计其值函数：\n$$ v(s) = \\text{average}(G_t \\mid s_t = s) $$\n基于多个完整 episode 的平均能够有效减少单次采样波动。\n蒙特卡洛方法不需要知道 $p(s^{\\prime}, r \\mid s, a)$的显式形式，不关心状态转移和奖励分布的具体形式，只需要能通过采样得到状态-动作-奖励序列。\n特性 动态规划（DP） 蒙特卡洛方法 环境模型需求 需要明确的动态模型 不需要动态模型（基于采样） 数据需求 全部状态-动作-奖励的概率分布 采样得到的状态-动作-奖励序列 适用范围 对环境已知、问题明确的场景 适合实际应用和无法明确建模的环境 更新方式 基于Bellman方程的递归更新 基于完整episode的采样平均 当无法获得环境的动态模型（即状态转移概率 $p(s^{\\prime}, r \\mid s, a)$），无法通过动态规划直接计算价值函数和最优策略时，可以使用蒙特卡洛方法。\n蒙特卡洛方法通过采样数据（而非模型）来估计价值函数和改进策略，适合于与环境交互而无需明确模型的场景。\nMonte Carlo Prediction \u0026amp; Estimation 在策略评估中，我们希望估计给定策略 $\\pi$的状态价值函数 $V_\\pi(s)$或者动作价值函数 $Q_\\pi(s, a)$\n由于我们无法直接求解 Bellman 方程：\n$$ V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p(s^{\\prime}, r \\mid s, a) \\big[r + \\gamma V_\\pi(s^{\\prime})\\big] $$\n所以蒙特卡洛方法通过对每次访问的回报 $G_t$ 进行平均来近似 $V_\\pi(s)$：\n$$ V_\\pi(s) \\approx \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)} $$\n其中 N 是轨迹数量，大数定律保证当 $N \\to \\infty$ 时，估计值会收敛到真实的值。\n存在两种策略评估方法：\n首次访问（First-visit MC）：仅使用轨迹中某状态第一次出现时的回报来更新该状态的估计值。\n每次访问（Every-visit MC）：轨迹中每次访问某状态时都使用其回报更新该状态的估计值。\n差异点 First-visit MC Every-visit MC 回报的使用 只使用状态在一个 episode 中的第一次访问 使用状态在一个 episode 中的所有访问 更新次数 每个 episode 中最多更新一次 每次访问状态时都更新 数据独立性 更高：使用的回报较少，数据相关性低 较低：回报相关性高 收敛速度 较慢（舍弃部分数据） 较快（充分利用所有回报） 计算开销 较低：更新次数少 较高：更新次数多 理论分析难度 较容易 较复杂 蒙特卡洛方法需要保证所有状态都能被充分访问，否则某些状态可能永远不会在采样中出现，导致对应的值函数无法被估计。为解决这个问题，假设：**Exploring Starts（试探性出发）**即每个状态都有被访问的可能性。\n但这种假设在真实环境中很难实现，实际中可能需要使用随机策略保证探索性，或者采用其他方法（如$\\epsilon$-贪婪策略）来实现状态空间的充分覆盖。\n基于试探性出发假设的MC控制 在策略评估中使用了两个假设： 无限幕，和试探性出发假设；\n无限幕可通过GPI（广义策略迭代）方式解决\n在实际中，满足试探性出发假设往往不现实，因此需要替代方法（如软策略或离轨方法）\n基于 ES 的 MC 控制\n同轨和离轨 由于试探性出发假设难以满足，可以通过其他方法保证所有状态-动作对都被探索。\n如何避免试探性出发假设？\n用来生成样本的策略，简称行动策略 $b$ 必须是软策略：$\\forall s \\in S, \\ a\\in A(s),$有 $b(a|s)\u0026gt;0$\n待评估，待改进的策略，简称 目标策略 $\\pi$\n⇒\n同轨策略方法： $\\pi =b$, 且为软性策略\n离轨策略方法：$\\pi \\neq b$\n方法 1：同轨策略方法（On-policy Methods）\n在同轨 MC 控制中, 采样和评估使用的是同一个策略 $\\pi = b$， 每次迭代后，基于 $\\epsilon$-greedy策略更新。\n$\\epsilon$-greedy策略是一种常用的软性策略（Soft Policy，即保证所有动作都有非零概率被选择），以概率 $1 - \\epsilon$ 选择当前最优动作（贪婪选择），以概率 $\\epsilon$ 随机选择任意动作。更新方式：\n$$ \\pi(a|s) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}, \u0026amp; a = \\arg\\max_a Q(s, a) \\ \\frac{\\epsilon}{|A(s)|}, \u0026amp; a \\neq \\arg\\max_a Q(s, a) \\end{cases} $$\n方法 2：离轨策略方法（Off-policy Methods）\n使用两个策略，\n行动策略 $b$：随机性更高，保证探索性。\n目标策略 $\\pi$：逐渐优化到最优。\n行动策略 $b$ 和 目标策略 $\\pi$ 不同， 行为策略 $b$ 用于生成数据，必须是软性策略，目标策略 $\\pi$ 是被评估和改进的策略，通常是贪婪或接近贪婪的策略。\n这种方法与同轨（On-policy）方法的关键区别在于，数据并非直接来源于目标策略，而是来源于行为策略。这种差异导致以下两个问题：\n数据分布不同：行动策略和目标策略不一致。 在 Monte Carlo 中，我们需要根据目标策略 $\\pi$来估计其对应的值函数 $v_\\pi(s)$ 或 $q_\\pi(s, a)$，但是，在 Off-policy 方法中，采样数据是由行动策略 $b$ 生成的， $\\mathbb{E}b[G_t \\mid S_t = s] \\neq \\mathbb{E}\\pi[G_t \\mid S_t = s]$，因此，我们无法直接使用行为策略生成的回报 $G_t$ 来估计目标策略的值。\n**如何校正数据分布。**为了让行为策略生成的数据 $G_t$ 能用于估计目标策略的值，需要校正数据的分布，使其符合目标策略 $\\pi$。重要性采样通过对回报进行加权，能够将行动策略的数据分布调整为目标策略的数据分布。\nimportance-sampling ratio：\n$$ \\rho_t = \\frac{\\pi(A_t \\mid S_t)}{b(A_t \\mid S_t)}. $$\n通过将回报 $G_t$ 按照 $\\rho_t$ 加权，即可校正行为动策略的回报，使其在数学期望上等价于目标策略的回报。\n$$ \\begin{aligned} V_{\\pi}(s) = E_{\\pi}[G_t|S_t=s] \\\\ =\\sum G_t \\cdot Pr{A_t,R_{t+1},S_{t+1},\u0026hellip;S_t|S_t = s,A_{t:T-1} \\sim \\pi} \\\\ = \\sum G_t \\cdot \\pi(A_t|S_t) \\cdot p(R_{t+1},S_{t+1}|S_t,A_t) \u0026hellip;p(R_{t},S_{t}|S_{t-1},A_{t-1}) \\\\ =\\sum G_t \\cdot \\prod_{k=t}^{T-1} [\\pi(A_k|S_k)p(R_{k+1},S_{k+1}|S_k,A_k)] \\\\ = \\sum G_t \\prod_{k=t}^{T-1}[\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}b(A_k|S_k)\\cdot p(R_{k+1},S_{k+1}|S_k, A_k)] \\\\ = \\sum \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)} G_t \\cdot \\prod_{k=t}^{T-1} [b(A_k|s_k)\\cdot p(R_{k+1},S_{k+1}|S_k, A_k)] \\\\ = \\sum \\rho_{t:T-1} \\cdot G_t \\cdot Pr{A_t,R_{t+1},S_{t+1},\u0026hellip;S_t | S_t=s,A_{t:T-1}\\sim b} \\ = E_b[\\rho_{t:T-1}|S_t=s] \\\\ \\approx \\frac{1}{N} \\sum_N^{i=1} \\rho_{t:T-1} G_t^{(i)} \\end{aligned} $$\n(普通重要性采样)\n增量更新\n方法 特点 优缺点 基于试探性出发（ES） 所有状态-动作对都有非零概率被访问。 假设不现实，难以在实际中满足。 同轨控制（On-policy） 使用 (\\epsilon)-贪婪策略，策略生成和评估一致。 简单易实现，但可能需要更长时间才能达到最优策略。 离轨控制（Off-policy） 行为策略和目标策略不同，通过重要性采样校正回报。 灵活且更强大，但重要性采样可能导致高方差。 Temporal-Difference Learning 策略评估 TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\nUnlike MC methods, temporal-difference (TD) does not require the episodes are complete. TD methods can learn from incomplete episodes by bootstrapping.\n即它根据部分观察到的信息（当前的奖励和下一步状态的估计值）来更新值函数，而不是等待整个序列的结束。\nTD 核心思想是（结合了MC和DP）\nMC-like：TD 利用实际经验（state-transition 和 reward）直接学习\nDP-like：使用对状态值的估计（bootstrap）来更新\nTD 的核心任务：Prediction 问题。 即给定一个策略 $\\pi$，估计状态值函数 $v_\\pi(s)$，即策略 $\\pi$ 在状态 $s$ 上的期望回报\n在DP中， 使用环境模型计算期望值 $\\mathbb{E}[G_t | S_t = s]$ 来更新值函数；在MC中，完全依赖实际回报 $G_t$ 来更新值函数。需要等待整个 episode 结束，计算从 $t$ 时刻开始的完整回报。\n但考虑到，环境的状态转移概率分布是未知的，无法直接应用 DP 方法，且计算开销较大，同时MC 方法需要等待一个 episode 结束才能计算累积回报，很难进行在线更新；\n于是乎，TD 方法 仅使用当前的奖励 $R_{t+1}$ 和对下一状态的估计值 $V(S_{t+1})$来更新值函数，从而避开以上两种方法的弊端；\n以 TD(0) 为例，TD(0) 是最简单的一步 TD 方法（直走一步，之后的步骤就不走了）：\n$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right] $$\nTD 方法的target是 $R_{t+1} + \\gamma V(S_{t+1})$\n$R_{t+1}$ 采样获得；$V(S_{t+1})$ 自举得到。\nSARSA (On-Policy) State (S), Action (A), Reward (R), next State (S’), and next Action (A’)\n同轨策略下的TD控制\n基于当前执行的策略（policy）\n更新公式来更新动作值函数 $Q(s, a)$\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right] $$\n更新值函数时基于当前执行的策略\n遵循当前策略来选择下一步的动作 $A_{t+1}$\non-policy，更倾向于避免过于激进的探索\nQ-Learning （Off-Policy） 离轨策略下的TD控制\n核心更新公式：\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right] $$\n$\\max_a Q(S_{t+1}, a)$：下一状态 $S_{t+1}$ 所有可能动作的最大动作值\n直接学习最优策略，而不依赖于当前使用的策略 （$\\epsilon$-greedy 策略），能够更快找到最优解 特点 Q-learning (Off-policy) SARSA (On-policy) 策略类型 Off-policy：学习的是最优策略 On-policy：基于当前策略更新 更新目标 $R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a)$ $R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})$ 探索行为的影响 更新与探索行为无关，只考虑最优动作 更新考虑探索行为，基于当前策略 学习的激进程度 更激进，直接学习最优策略 更保守，考虑探索对学习过程的影响 适用场景 适合需要快速找到最优策略的场景 适合对探索行为敏感，需更稳定学习的场景 Expected SARSA SARSA+Q-learning的结合\n与 SARSA 和 Q-learning 相比，Expected Sarsa 在更新动作值函数时使用了期望回报，而不是基于单个动作的采样回报\n更新公式\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma E_{a \\sim \\pi} [Q(S_{t+1}, A_{t+1})| S_{t+1} ] - Q(S_t, A_t) \\right] $$\n其中 $E_{a \\sim \\pi} [Q(S_{t+1}, A_{t+1})|S_{t+1}]$：在下一状态 $S_{t+1}$ 下，按照策略 $\\pi$ 选择动作时的动作值函数的期望，进一步展开\n$E_{a \\sim \\pi} [Q(S_{t+1}, A_{t+1})|S_{t+1}] = \\sum_{a} \\pi(a | S_{t+1}) Q(S_{t+1}, a)$\n即对下一状态中所有可能的动作值 $Q(S_{t+1}, a)$ 加权求和，权重为当前策略 $\\pi$ 对动作 $a$ 的选择概率\nSARSA 的更新基于具体采样的下一动作 $A_{t+1}$, 而Expected Sarsa 则基于所有可能动作的期望回报更新，消除了由于采样引起的随机性\n由于期望计算减少了采样的不确定性，Expected Sarsa 学习过程中的方差比 SARSA 和 Q-learning 更小，学习更加稳定\n相比 SARSA：Expected Sarsa 更稳定，减小了由于单一动作采样造成的方差。\n相比 Q-learning：Expected Sarsa 在高探索策略下更稳定，避免了完全依赖最优动作选择可能带来的不稳定。\nQ-learning 是 Expected SARSA 的一种特例\n当策略 $\\pi$ 是greedy policy时，策略 $\\pi$ 的定义如下：\n$$ \\pi(a \\mid S_{t+1}) = \\begin{cases} 1 \u0026amp; \\text{if } a = \\arg \\max_a Q(S_{t+1}, a), \\\\ 0 \u0026amp; \\text{else}. \\end{cases} $$\n将将贪婪策略代回 Expected SARSA的迭代公式，即可回到Q-learning的形式：\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right] $$\nDeep Q-Learning from Q-learning to Deep Q-learning\nQ-Learning 是一种表格方法，适用于状态和动作空间较小的情景\n很直接的想法： 通过一个参数化的 Q 函数 $Q_{\\theta}(s, a)$ 来近似 Q 值，而非使用 Q 表，也就是采用神经网络的方式。\n损失函数：比较 Q 值预测和 Q 目标，使用梯度下降来更新 DQN 的权重\nDQN 的稳定性问题：\nExperience Replay：创建一个回放缓冲区存储经验元组，随机采样小批量元组进行训练；防止遗忘之前的经验，同时降低经验之间的相关性\nFixed Q-Target：TD 目标和 Q 值的估计都使用相同的网络参数，导致训练中 Q 值和目标值同步移动，产生震荡；可以使用一个单独的目标网络计算 TD 目标，每隔 C 步将 DQN 的参数复制到目标网络中\nDouble DQN：训练初期，选择下一状态最佳动作时可能高估 Q 值，导致学习复杂化；使用两个网络解耦动作选择和 Q 值目标计算，即使用 DQN 网络选择下一状态的最佳动作，使用 目标网络计算选择动作的目标 Q 值。\n","permalink":"https://niraya666.github.io/posts/rlhf-%E4%B9%8B%E8%B7%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E4%B9%8B%E4%B8%8A%E7%AF%87/","summary":"\u003ch2 id=\"写在前面\"\u003e写在前面\u003c/h2\u003e\n\u003cp\u003e决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。\u003c/p\u003e\n\u003cp\u003e学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。\n于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）\u003c/p\u003e\n\u003cp\u003e主要的教材来自：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003e\u003ca href=\"http://incompleteideas.net/book/RLbook2020.pdf\"\u003eReinforcement Learning: An Introduction\u003c/a\u003e\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eB站UP主 shuhuai008 的系列推导视频\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e本篇笔记将包含以下的内容：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMDP\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDP\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMonte Carlo Methods\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTD方法\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"马尔可夫决策过程markov-decision-processmdp\"\u003e马尔可夫决策过程(Markov Decision Process，MDP)\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"相关概念\"\u003e相关概念\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e随机变量（Random Variance）\u003c/strong\u003e： ( $X, \\ y, \\ x \\perp y$)，随机变量之间存在的独立性。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e随机过程（Stochastic Process）\u003c/strong\u003e： ${S_t}_{t=1}^{\\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMarkov链/过程（Markov Chain/Process）\u003c/strong\u003e：强调了\u003cstrong\u003eMarkov性质\u003c/strong\u003e（Markov Property），即\u003cstrong\u003e未来的状态仅依赖于当前状态而与过去无关\u003c/strong\u003e，形式化地表示为：\u003c/p\u003e\n\u003cp\u003e$$\nP(S_{t+1}|S_t, S_{t-1}, \u0026hellip;,S_1) = P(S_{t+1}|S_{t})\n$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e状态空间模型（State Space Model）：\u003c/strong\u003e Markov Chain + Observation； 如 HMM， Kalman Filter，particle Filter。\u003c/p\u003e","title":"RLHF 之路：强化学习复习之上篇"},{"content":"从2024年3月份之后，开始尝试构建每个月的精选歌单，为的是将一些和音乐有关的记忆存档，和记录一些让我眼前一亮的音乐。\n","permalink":"https://niraya666.github.io/musik/2024-%E6%AD%8C%E5%8D%95/","summary":"\u003cp\u003e从2024年3月份之后，开始尝试构建每个月的精选歌单，为的是将一些和音乐有关的记忆存档，和记录一些让我眼前一亮的音乐。\u003c/p\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/0CDpNxmeMlZ44uEPjjj0J6?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/7xtJgcGDlTxGnEO1jYwp0g?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:15px\" src=\"https://open.spotify.com/embed/playlist/3SLnfD0aZWQvYhJlngqsH6?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/1QmQzNcu5zxjA6BGoaR3vA?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/2Xsf889pQZXddfnq9jF1HJ?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/59NQVa7T1cBZokI8bXgPjU?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/0sGtAzNzBkRuLg6FpEgqQX?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/4Tws2WJYczsgDXIfIHIAYY?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/5wsVrL1pkuglVcMcBnLcvI?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e\n\u003ciframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/7hoWMPwSASCb9li0eGHkM9?utm_source=generator\" width=\"100%\" height=\"450\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"\u003e\u003c/iframe\u003e","title":"2024年月度精选歌单汇总"},{"content":"这份月刊最初只是我的个人知识管理实验。过去一年里，我逐渐养成了每月底整理收藏夹的习惯——清理read-it-later 软件中里积压的链接，归档散落在各个平台的技术文章，把零碎的信息重新分类到Notion笔记中。\n面对每天涌现的新模型、论文和开源项目，这种月度整理成了对抗信息焦虑的锚点。与其被FOMO（错失恐惧症）驱使着追逐每个热点，不如让内容先经历时间筛选。留在月刊里的，通常是经过两周沉淀后仍值得反复阅读的内容。\n有三个主要原因推动我决定公开这些笔记：\n年初给自己定下的目标之一，多做一些public-writing，倒逼自己更认真地验证每个技术细节 对抗数字囤积症 如果能够帮助更多的人 所以，这个系列月刊将包含以下一些内容：\n模型与技术：一些新出的LLM，和相关技术 开源项目：值得关注、有趣的、最近比较火的 研究和论文：新出的论文以及我的阅读笔记(更多关注Agent、对齐、RAG、模型架构方面内容) 推荐内容：可能是一些教程，工具、产品等 书摘：正在阅读书籍文章的高亮内容 有次读到一段特别共鸣的话\n你拿着相机在城市里穿行，看见一幕——光影交错，人情味流露。你按下快门。 没人关心。 但你并不是为了别人去做。你做，是因为你看到了什么。 写博客也是如此。你写，因为你思考，因为你观察，因为你需要一个“出口”来安放这些想法。 有人看吗？有就算赚到。没有也没关系。创作这件事，本身就已经完成了它的意义。 这才是重点所在。\n这或许也是这一系列月刊的意义吧。\n","permalink":"https://niraya666.github.io/monthly/readme/","summary":"\u003cp\u003e这份月刊最初只是我的个人知识管理实验。过去一年里，我逐渐养成了每月底整理收藏夹的习惯——清理read-it-later 软件中里积压的链接，归档散落在各个平台的技术文章，把零碎的信息重新分类到Notion笔记中。\u003c/p\u003e\n\u003cp\u003e面对每天涌现的新模型、论文和开源项目，这种月度整理成了对抗信息焦虑的锚点。与其被FOMO（错失恐惧症）驱使着追逐每个热点，不如让内容先经历时间筛选。留在月刊里的，通常是经过两周沉淀后仍值得反复阅读的内容。\u003c/p\u003e\n\u003cp\u003e有三个主要原因推动我决定公开这些笔记：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e年初给自己定下的目标之一，多做一些public-writing，倒逼自己更认真地验证每个技术细节\u003c/li\u003e\n\u003cli\u003e对抗数字囤积症\u003c/li\u003e\n\u003cli\u003e如果能够帮助更多的人\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e所以，这个系列月刊将包含以下一些内容：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e模型与技术\u003c/strong\u003e：一些新出的LLM，和相关技术\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e开源项目\u003c/strong\u003e：值得关注、有趣的、最近比较火的\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e研究和论文\u003c/strong\u003e：新出的论文以及我的阅读笔记(更多关注Agent、对齐、RAG、模型架构方面内容)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e推荐内容\u003c/strong\u003e：可能是一些教程，工具、产品等\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e书摘\u003c/strong\u003e：正在阅读书籍文章的高亮内容\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e有次读到一段特别共鸣的话\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e你拿着相机在城市里穿行，看见一幕——光影交错，人情味流露。你按下快门。 没人关心。 但你并不是为了别人去做。你做，是因为你看到了什么。 写博客也是如此。你写，因为你思考，因为你观察，因为你需要一个“出口”来安放这些想法。 有人看吗？有就算赚到。没有也没关系。创作这件事，本身就已经完成了它的意义。 这才是重点所在。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e这或许也是这一系列月刊的意义吧。\u003c/p\u003e","title":"关于月刊的诞生"},{"content":"几年前初识 Milvus 的契机，来源于开发一个图像相似检索的应用，当时市面上向量库的可选择项并不像现在这么多，且功能也仅限于单纯的向量检索。\n鉴于最近有业务更新的需要，和打算重构一下之前做的RAG项目，再次有机会深入学习 Milvus，探索其在新功能和实际应用上的更多可能性。\n本篇笔记，仅作为学习笔记，更多是记录一些 Milvus从2.0 到2.5 的变化， 和一些动手实践的记录，便于之后的查阅。\n一些概念 Collections, Schema and index Collection是Milvus中的一个二维表格，具有固定的列和可变的行。每一列代表一个字段(field)，每一行代表一个实体(entity)\nSchema和字段 Collection需要一个schema来定义其结构\n索引 在特定字段上创建索引可以提高搜索效率。建议为服务所依赖的所有字段创建索引，其中向量字段的索引是必需的。\n分区(Partition) 分区是Collection的子集，与其父Collection共享相同的字段集\n分片(Shard) 分片是Collection的水平切片。每个分片对应一个数据输入通道。\nShard vs Partition的区别 :\n分区(Partition)的作用是通过指定分区名称来减少读取负载\n而分片(Shard)的作用是将写入负载分散到多个服务器分布式架构中的应用 在分布式系统中，分片是实现水平扩展的重要机制。通过将数据分布到多个节点，可以充分利用集群的并行计算潜力，提高系统的写入性能\nMilvusClient vs Connection MilvusClient\n定位：更高级封装，提供一体化的操作接口。简化了与 Milvus 的交互流程， 提供更直观和结构化的操作方式，便于新手快速上手，内置了对连接的管理和操作，减少手动处理的复杂性。\nfrom pymilvus import MilvusClient # 创建客户端并连接到 Milvus client = MilvusClient(uri=\u0026#34;http://localhost:19530\u0026#34;) # 创建集合 client.create_collection( name=\u0026#34;example_collection\u0026#34;, schema={\u0026#34;fields\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;vector\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FLOAT_VECTOR\u0026#34;, \u0026#34;params\u0026#34;: {\u0026#34;dim\u0026#34;: 128}}]} ) # 插入数据 client.insert(\u0026#34;example_collection\u0026#34;, data={\u0026#34;vector\u0026#34;: [[0.1] * 128, [0.2] * 128]}) # 搜索 results = client.search(\u0026#34;example_collection\u0026#34;, data=[[0.1] * 128]) print(results) Connection\n定位：基础连接操作，需要通过 connect 方法创建并维护连接。提供更底层的控制，适合灵活、自定义的操作。\nfrom pymilvus import connections, Collection, FieldSchema, CollectionSchema # 创建连接 connections.connect(alias=\u0026#34;default\u0026#34;, host=\u0026#34;localhost\u0026#34;, port=\u0026#34;19530\u0026#34;) # 创建集合 fields = [ FieldSchema(name=\u0026#34;vector\u0026#34;, dtype=\u0026#34;FLOAT_VECTOR\u0026#34;, dim=128) ] schema = CollectionSchema(fields, description=\u0026#34;example collection\u0026#34;) collection = Collection(name=\u0026#34;example_collection\u0026#34;, schema=schema) # 插入数据 collection.insert([[0.1] * 128, [0.2] * 128]) # 搜索 collection.load() results = collection.search([[0.1] * 128], anns_field=\u0026#34;vector\u0026#34;, limit=10) print(results) Schema Schema 用于定义collection及其字段的属性\n设定 Schema 的目的是为了定义数据的逻辑结构和存储规则。Schema 是集合（Collection）和字段（Field）的元数据定义，它决定了如何存储和管理数据，同时保证数据操作的一致性、灵活性和高效性。\n字段属性（Field Properties）\n属性名称 描述 注意事项 name 字段名称 必须字段，类型为字符串。 dtype 字段的数据类型 必须字段。 description 字段的描述 可选字段，类型为字符串。 is_primary 是否将字段设为主键 类型为布尔值（true 或 false），主键字段必须指定。 auto_id 是否启用自动 ID 分配 必须为主键字段设置，类型为布尔值。 max_length VARCHAR 字段的最大长度（以字节为单位） 必须为 VARCHAR 字段指定，范围为 [1, 65,535]。 dim 向量的维度 对于稠密向量字段，必须指定；对稀疏向量字段可以省略。范围为 [1, 32,768]。 is_partition_key 是否为分区键字段 类型为布尔值（true 或 false）。 Milvus 支持以下三类字段：\n1. 主键字段\n主键字段是集合的唯一标识，用于标识集合中的每条记录。\n属性 描述 name 主键字段的名称 dtype 支持 INT64（整数主键）或 VARCHAR（字符串主键） is_primary 必须设为 True，表示该字段是主键 auto_id 是否自动生成主键值（布尔值：True 或 False） 2. 标量字段\n标量字段用于存储布尔值、整数、浮点数、字符串、JSON 数据或数组等。\n数据类型 描述 BOOL 布尔值，支持 true 或 false INT8 8 位整数 INT16 16 位整数 INT32 32 位整数 INT64 64 位整数 FLOAT 32 位单精度浮点数 DOUBLE 64 位双精度浮点数 VARCHAR 可变长度字符串，需指定最大长度（max_length） JSON JSON 数据类型，存储键值对 Array 数组类型，用于存储多个相同类型的数据 3. 向量字段\n向量字段用于存储高维特征数据，是 Milvus 支持向量检索的基础。\n数据类型 描述 FLOAT_VECTOR 32 位浮点型向量，常用于机器学习和深度学习模型的特征表示 BINARY_VECTOR 二进制向量，存储由 0 和 1 组成的序列，用于图像处理或紧凑特征表示 FLOAT16_VECTOR 16 位浮点型向量，用于深度学习和 GPU 计算的内存优化 BFLOAT16_VECTOR 16 位浮点型向量，具有与 FLOAT32 相同的指数范围，常用于深度学习优化 SPARSE_FLOAT_VECTOR 稀疏浮点型向量，存储非零元素及其索引，用于稀疏向量场景 其他支持的字段属性\nis_partition_key: 是否将字段作为分区键，支持 true 或 false。\ndim: 向量字段的维度，必需为向量字段定义。\nmax_length: VARCHAR 类型字段的最大字节长度。\n动态字段允许在插入数据时不提前定义字段，可以通过 enable_dynamic_field=True 在collection Schema 中启用\n属性名称 描述 注意事项 fields 字段的集合 必须字段。 description 集合的描述 可选字段，类型为字符串。 partition_key_field 作为分区键的字段名 可选字段，类型为字符串。 enable_dynamic_field 是否启用动态字段 可选字段，类型为布尔值，默认值为 false。 Index 为什么需要构造索引？\n加速搜索：向量搜索的主要瓶颈在于计算高维向量之间的相似性。如果不使用索引，检索可能需要线性扫描整个数据集，耗时且资源消耗大；\n支持大规模数据： 在百万甚至十亿级别的数据量中，直接搜索变得不可行。索引能够分层次或分块处理数据，从而支持大规模数据场景\n资源效率：使用索引可以显著减少内存和计算资源的占用，尤其在处理频繁的实时查询时更加重要。\n几种主要的索引类型：\n密集向量（Dense Vectors）\n索引名称 适合场景 优点 缺点 参数解释 原理 FLAT 小数据集，精确搜索 搜索结果精确，简单易用 随数据量增加，速度显著下降 无需额外参数 对所有向量进行线性扫描，计算查询向量与每个向量的距离，从而找到最相似的结果。 IVF_FLAT 中等规模数据集 搜索效率高，支持近似搜索 搜索精度依赖参数调整 nlist：桶的数量，影响召回率和性能；nprobe：查询时访问的桶数量，影响搜索的准确性和速度 将向量划分为多个簇（cluster），通过聚类算法（如 k-means）预先构建倒排文件，仅在相关簇中执行搜索，减少比较次数，提高效率。 IVF_SQ8 需要平衡存储和性能的场景 存储需求低 搜索精度较低 与 IVF_FLAT 相同 在 IVF_FLAT 的基础上对每个向量进行标量量化（Scalar Quantization, SQ），将原始浮点向量压缩为整数形式以节省存储空间。 IVF_PQ 超大规模数据集，存储敏感场景 存储需求显著降低 搜索精度较低 nlist：桶的数量；m：子向量的数量，影响压缩比和性能；nbits：每个子向量的编码位数，影响压缩质量 基于 IVF_FLAT，使用产品量化（Product Quantization, PQ）将向量分为多个子空间，并分别进行量化，从而显著减少存储需求。 HNSW 中小规模数据集，高搜索精度场景 高性能，高维数据效果良好 索引构建时间长，占用内存大 efConstruction：控制构建时的候选集大小，影响构建速度和搜索精度；ef：搜索时的候选集大小，影响搜索性能和精度 使用分层的近似图（Navigable Small World Graph, NSW）表示向量之间的关系，通过图的导航找到查询向量的近似最近邻。 DISKANN 磁盘存储超大规模数据集 支持超大规模数据检索 查询速度较慢 search_list：控制查询范围，影响精度；pq_code_budget_gb：控制压缩的预算大小；build_threads：控制索引构建的线程数 结合倒排文件和磁盘访问优化，将部分索引和数据保存在磁盘上，通过减少内存占用实现超大规模数据检索。 支持的度量类型有欧几里得距离（L2）、内积（IP）、余弦相似度（COSINE）。\n二进制向量（Binary Vectors）\n二进制向量（布尔值、哈希签名），占用资源低，对低维数据搜索快，精度不太高的场景\n索引名称 适合场景 优点 缺点 参数解释 原理 BIN_FLAT 小规模数据集，精确搜索 搜索结果精确，易于实现 随数据量增加，速度显著下降 无需额外参数 对所有二进制向量进行线性扫描，逐一计算查询向量与每个向量的距离（如汉明距离），从而找到最相似的结果。 BIN_IVF_FLAT 中等规模数据集 搜索效率高，支持近似搜索 搜索精度依赖参数调整 - nlist：桶的数量，影响召回率和性能\n- nprobe：查询时访问的桶数量，影响搜索的准确性和速度 结合倒排文件和线性扫描，将二进制向量划分到不同的桶中，通过访问部分桶来加速检索过程。 支持的度量类型有 Jaccard 和 Hamming\n稀疏向量（Sparse Vectors）\n常用于文档搜索\n索引名称 适合场景 优点 缺点 参数解释 原理 SPARSE_INVERTED_INDEX 文档检索、推荐系统 高效支持稀疏数据的相似性搜索 仅支持稀疏向量 - 无需额外参数 基于倒排索引（Inverted Index），将稀疏向量的非零值索引到特定位置，通过快速查找相关文档加速检索过程。 SPARSE_WAND 高性能文档检索、稀疏向量场景 支持稀疏数据，搜索效率更高 构建复杂度较高 - top_k：控制返回的搜索结果数量 基于 WAND（Weighted AND）算法优化倒排索引，对查询进行剪枝优化，仅访问最相关的文档，提高搜索性能。 支持内积（IP）度量\n如何选择合适的index\nfrom RAG搭建中，如何选择最合适的向量索引？\nGPU-index\n索引名称 适合场景 优点 缺点 参数解释 原理 GPU_BRUTE_FORCE 小批量搜索，高性能需求场景 结果精确，性能优于 CPU 的线性扫描 不支持范围搜索，资源消耗较高 无需额外参数 使用 GPU 并行计算能力对所有向量执行线性扫描，计算查询向量与每个向量的相似性，从而获得结果。 GPU_IVF_FLAT 大规模数据集搜索，高吞吐量需求场景 搜索效率高，结合 GPU 加速 搜索精度依赖参数调整 - nlist：桶的数量，影响召回率和性能\n- nprobe：查询时访问的桶数量，影响搜索的准确性和速度 在 CPU IVF_FLAT 的基础上，利用 GPU 加速桶的分配和搜索过程，大幅提高检索速度。 GPU_IVF_PQ 大规模数据存储敏感场景，高吞吐量需求场景 存储需求显著降低，支持近似搜索 搜索精度较低 - nlist：桶的数量\n- m：子向量数量，影响压缩比和性能\n- nbits：每个子向量的编码位数，影响压缩质量 结合倒排文件与产品量化（PQ）技术，利用 GPU 提高量化和检索过程的速度，适合超大规模数据检索。 GPU_CAGRA 超高吞吐量和高召回率场景 高性能，高召回率，适合大批量查询 消耗更多内存，索引构建时间长 - intermediate_graph_degree：构建时图的中间连接数量，影响构建时间和召回率\n- graph_degree：最终图的连接数量，影响搜索性能和内存占用 基于图的搜索算法（如 CAGRA），结合 GPU 的并行计算能力构建分层图结构，提高搜索的速度和精度。 Analyzer 用于将原始文本转换为结构化和可搜索的格式。它的主要作用是实现文本处理、索引构建和检索优化。\n在本质上，analyzer 包含了一个tokenizer（分词器）和可选的多个filters（过滤器）。\n用途: 将输入文本分解为更小的单元（tokens）; 通过过滤器进一步处理分词后的 token，例如转为小写、移除停用词、仅保留特定字符等; 将处理后的文本用于全文检索和关键词匹配。\nhow to use：\nschema.add_field( field_name=\u0026#39;text_field\u0026#39;, datatype=DataType.VARCHAR, max_length=1000, enable_analyzer=True, analyzer_params={ \u0026#34;type\u0026#34;: \u0026#34;standard\u0026#34;, # 使用内置 analyzer \u0026#34;stop_words\u0026#34;: [\u0026#34;a\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;the\u0026#34;] }, enable_match=True, # 启用关键词匹配 ) 内置 Analyzer： 提供快速配置，适用于通用场景，如：\nstandard（标准分词器）/english（针对英文优化）/ chinese（针对中文优化）\n自定义 Analyzer， 如：\nanalyzer_params = { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, # 转为小写 { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, # 自定义停用词 \u0026#34;stop_words\u0026#34;: [\u0026#34;of\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;the\u0026#34;] } ] } ANN Search in Milvus 不同于传统数据库的精确匹配查询,向量库主要用于相似性搜索。这种搜索基于向量间的距离,通常使用欧几里得距离或余弦相似度。以暴力遍历的最近邻查询为例(Nearest neighbor search),假设我们有n个d维向量,查询复杂度为$O(nd)$。当n和d都很大时,这种方法变得非常耗时。\n而近似最近邻搜索(Approximate Nearest Neighbor search, ANN)则能将时间复杂度降低到亚线性,通常为$O(log n)$或更优。\nhow to ANN-search\ncreate collection , schema and index\ninsert data\nsearch\n构造索引\nfrom pymilvus import MilvusClient, DataType import numpy as np # 创建Milvus客户端 client = MilvusClient( uri=\u0026#34;http://localhost:19530\u0026#34; ) # 创建schema schema = MilvusClient.create_schema( auto_id=False, enable_dynamic_field=True, ) # 添加字段到schema schema.add_field(field_name=\u0026#34;my_id\u0026#34;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=\u0026#34;my_vector\u0026#34;, datatype=DataType.FLOAT_VECTOR, dim=5) schema.add_field(field_name=\u0026#34;my_varchar\u0026#34;, datatype=DataType.VARCHAR, max_length=512) # 准备索引参数 index_params = client.prepare_index_params() # 添加索引 index_params.add_index( field_name=\u0026#34;my_id\u0026#34;, index_type=\u0026#34;STL_SORT\u0026#34; ) index_params.add_index( field_name=\u0026#34;my_vector\u0026#34;, index_type=\u0026#34;AUTOINDEX\u0026#34;, metric_type=\u0026#34;COSINE\u0026#34; ) # 创建collection client.create_collection( collection_name=\u0026#34;customized_setup_1\u0026#34;, schema=schema, index_params=index_params )` 数据插入\ndata = [ { \u0026#34;my_id\u0026#34;: i, \u0026#34;my_vector\u0026#34;: np.random.random(5).tolist(), \u0026#34;my_varchar\u0026#34;: f\u0026#34;example_text_{i}\u0026#34; } for i in range(100) ] # 插入数据 client.insert(collection_name=\u0026#34;customized_setup_1\u0026#34;, data=data) # 加载集合到内存 client.load_collection(\u0026#34;customized_setup_1\u0026#34;) # 查看集合统计信息 stats = client.get_collection_stats(\u0026#34;customized_setup_1\u0026#34;) print(f\u0026#34;Collection stats: {stats}\u0026#34;) 基础ANN-search\n# 查询向量 query_vector = np.random.random(5).tolist() # 模拟一个随机向量作为查询向量 # 搜索参数 search_params = { \u0026#34;metric_type\u0026#34;: \u0026#34;COSINE\u0026#34;, # 与创建索引时保持一致 \u0026#34;params\u0026#34;: {\u0026#34;nprobe\u0026#34;: 10} # nprobe 决定搜索范围，值越高精度越高，但速度可能变慢 } # 执行搜索 res = client.search( collection_name=\u0026#34;customized_setup_1\u0026#34;, anns_field=\u0026#34;my_vector\u0026#34;, # 指定向量字段 data=[query_vector], # 查询向量 limit=5, # 返回前 5 个结果 search_params=search_params ) # 打印搜索结果 print(\u0026#34;Search Results:\u0026#34;) for hits in res: for hit in hits: print(hit) 在指定的partition中search\n# 构建partition，将数据插入partition client.create_partition( collection_name=\u0026#34;customized_setup_1\u0026#34;, partition_name=\u0026#34;partitionA\u0026#34; ) res = client.list_partitions( collection_name=\u0026#34;customized_setup_1\u0026#34; ) print(res) \u0026#34;\u0026#34;\u0026#34; [\u0026#39;_default\u0026#39;, \u0026#39;partitionA\u0026#39;] \u0026#34;\u0026#34;\u0026#34; data2 = [ { \u0026#34;my_id\u0026#34;: i, \u0026#34;my_vector\u0026#34;: np.random.random(5).tolist(), # 生成随机向量 \u0026#34;my_varchar\u0026#34;: f\u0026#34;example_text_{i}\u0026#34; } for i in range(50) ] client.insert(collection_name=\u0026#34;customized_setup_1\u0026#34;,partition_name=\u0026#34;partitionA\u0026#34;, data=data2) # 在partition 中查询 res = client.search( collection_name=\u0026#34;customized_setup_1\u0026#34;, anns_field=\u0026#34;my_vector\u0026#34;, data=[query_vector], partition_names=[\u0026#34;partitionA\u0026#34;], limit=3, search_params=search_params ) print(\u0026#34;Search Results:\u0026#34;) for hits in res: for hit in hits: print(hit) 通过 output Fields指定输出的内容，在数据插入时，定义了my_vector和my_varchar;(默认只展示id)\n# Use Output Fields res = client.search( collection_name=\u0026#34;customized_setup_1\u0026#34;, anns_field=\u0026#34;my_vector\u0026#34;, data=[query_vector], limit=3, search_params=search_params, output_fields=[\u0026#34;my_vector\u0026#34;,\u0026#34;my_varchar\u0026#34;] ) print(\u0026#34;Search Results:\u0026#34;) for hits in res: for hit in hits: print(hit) \u0026#34;\u0026#34;\u0026#34; Search Results: {\u0026#39;id\u0026#39;: 7, \u0026#39;distance\u0026#39;: 0.9791999459266663, \u0026#39;entity\u0026#39;: {\u0026#39;my_vector\u0026#39;: [0.7273301482200623, 0.25300344824790955, 0.8510549664497375, 0.6126695871353149, 0.5709182024002075], \u0026#39;my_varchar\u0026#39;: \u0026#39;example_text_7\u0026#39;}} {\u0026#39;id\u0026#39;: 84, \u0026#39;distance\u0026#39;: 0.9768796563148499, \u0026#39;entity\u0026#39;: {\u0026#39;my_vector\u0026#39;: [0.6703248620033264, 0.1109713464975357, 0.5905848145484924, 0.372805655002594, 0.4945228397846222], \u0026#39;my_varchar\u0026#39;: \u0026#39;example_text_84\u0026#39;}} {\u0026#39;id\u0026#39;: 27, \u0026#39;distance\u0026#39;: 0.9755662083625793, \u0026#39;entity\u0026#39;: {\u0026#39;my_vector\u0026#39;: [0.8758077025413513, 0.13442707061767578, 0.968295156955719, 0.25122591853141785, 0.7256166934967041], \u0026#39;my_varchar\u0026#39;: \u0026#39;example_text_27\u0026#39;}} \u0026#34;\u0026#34;\u0026#34; Limit and Offset\n这两个参数结合使用时可以实现分页查询\nLimit: 控制搜索结果中返回的实体数量，即返回的 Top-K 结果。\nOffset: 指定从搜索结果中跳过的实体数量，用于实现分页。\n# 第 1 页查询：获取第 1-10 个结果 res_page_1 = client.search( collection_name=\u0026#34;customized_setup_2\u0026#34;, anns_field=\u0026#34;my_vector\u0026#34;, data=[query_vector], limit=10, # 每页 10 条 offset=0, # 跳过 0 条 search_params=search_params ) print(\u0026#34;Page 1 Results:\u0026#34;) for hits in res_page_1: for hit in hits: print(hit) # 第 2 页查询：获取第 11-20 个结果 res_page_2 = client.search( collection_name=\u0026#34;customized_setup_2\u0026#34;, anns_field=\u0026#34;my_vector\u0026#34;, data=[query_vector], limit=10, # 每页 10 条 offset=10, # 跳过前 10 条 search_params=search_params ) print(\u0026#34;Page 2 Results:\u0026#34;) for hits in res_page_2: for hit in hits: print(hit) \u0026#34;\u0026#34;\u0026#34; Page 1 Results: {\u0026#39;id\u0026#39;: 7, \u0026#39;distance\u0026#39;: 0.9791999459266663, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 84, \u0026#39;distance\u0026#39;: 0.9768796563148499, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 27, \u0026#39;distance\u0026#39;: 0.9755662083625793, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 28, \u0026#39;distance\u0026#39;: 0.9715055227279663, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 38, \u0026#39;distance\u0026#39;: 0.9661443829536438, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 40, \u0026#39;distance\u0026#39;: 0.9624771475791931, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 93, \u0026#39;distance\u0026#39;: 0.9594350457191467, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 0, \u0026#39;distance\u0026#39;: 0.9587700366973877, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 73, \u0026#39;distance\u0026#39;: 0.9547507166862488, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 32, \u0026#39;distance\u0026#39;: 0.9311172366142273, \u0026#39;entity\u0026#39;: {}} Page 2 Results: {\u0026#39;id\u0026#39;: 52, \u0026#39;distance\u0026#39;: 0.9296990036964417, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 26, \u0026#39;distance\u0026#39;: 0.9230980277061462, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 33, \u0026#39;distance\u0026#39;: 0.922787606716156, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 55, \u0026#39;distance\u0026#39;: 0.9211370348930359, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 30, \u0026#39;distance\u0026#39;: 0.9198154211044312, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 63, \u0026#39;distance\u0026#39;: 0.918975830078125, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 3, \u0026#39;distance\u0026#39;: 0.9085606932640076, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 37, \u0026#39;distance\u0026#39;: 0.9077025651931763, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 67, \u0026#39;distance\u0026#39;: 0.9050586223602295, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 4, \u0026#39;distance\u0026#39;: 0.9049072861671448, \u0026#39;entity\u0026#39;: {}} \u0026#34;\u0026#34;\u0026#34; Milvus 中单次搜索结果的返回实体数限制为 16,384。如果需要更多结果，可以通过 Search Iterator 分批处理。\n较大的 offset 值会导致搜索效率下降，因为需要计算和跳过前面部分结果\n进阶ANN Search Filter Search \u0026amp; Metadata Filtering Filtered Search 是在 ANN 方法的基础上添加过滤条件的搜索模式，结合了元数据的过滤功能，能够缩小检索范围或实现一些复杂逻辑。\n数据准备\nclient = MilvusClient(uri=\u0026#34;http://localhost:19530\u0026#34;) # 创建 schema schema = client.create_schema( auto_id=False, enable_dynamic_field=True, ) index_params = client.prepare_index_params() # 添加字段到 schema schema.add_field(field_name=\u0026#34;id\u0026#34;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=\u0026#34;vector\u0026#34;, datatype=DataType.FLOAT_VECTOR, dim=5) schema.add_field(field_name=\u0026#34;color\u0026#34;, datatype=DataType.VARCHAR, max_length=50) schema.add_field(field_name=\u0026#34;likes\u0026#34;, datatype=DataType.INT64) index_params.add_index( field_name=\u0026#34;vector\u0026#34;, index_type=\u0026#34;AUTOINDEX\u0026#34;, # Automatically choose the best index type metric_type=\u0026#34;L2\u0026#34; # Use L2 distance as the metric ) # Add a sort index for the id field index_params.add_index( field_name=\u0026#34;id\u0026#34;, index_type=\u0026#34;STL_SORT\u0026#34; ) collection_name = \u0026#34;customized_setup_5\u0026#34; client.create_collection( collection_name=collection_name, schema=schema, index_params=index_params ) # 准备数据 data = [ {\u0026#34;id\u0026#34;: 0, \u0026#34;vector\u0026#34;: [0.3580376395471989, -0.6023495712049978, 0.18414012509913835, -0.26286205330961354, 0.9029438446296592], \u0026#34;color\u0026#34;: \u0026#34;pink_8682\u0026#34;, \u0026#34;likes\u0026#34;: 165}, {\u0026#34;id\u0026#34;: 1, \u0026#34;vector\u0026#34;: [0.19886812562848388, 0.06023560599112088, 0.6976963061752597, 0.2614474506242501, 0.838729485096104], \u0026#34;color\u0026#34;: \u0026#34;red_7025\u0026#34;, \u0026#34;likes\u0026#34;: 25}, {\u0026#34;id\u0026#34;: 2, \u0026#34;vector\u0026#34;: [0.43742130801983836, -0.5597502546264526, 0.6457887650909682, 0.7894058910881185, 0.20785793220625592], \u0026#34;color\u0026#34;: \u0026#34;orange_6781\u0026#34;, \u0026#34;likes\u0026#34;: 764}, {\u0026#34;id\u0026#34;: 3, \u0026#34;vector\u0026#34;: [0.3172005263489739, 0.9719044792798428, -0.36981146090600725, -0.4860894583077995, 0.95791889146345], \u0026#34;color\u0026#34;: \u0026#34;pink_9298\u0026#34;, \u0026#34;likes\u0026#34;: 234}, {\u0026#34;id\u0026#34;: 4, \u0026#34;vector\u0026#34;: [0.4452349528804562, -0.8757026943054742, 0.8220779437047674, 0.46406290649483184, 0.30337481143159106], \u0026#34;color\u0026#34;: \u0026#34;red_4794\u0026#34;, \u0026#34;likes\u0026#34;: 122}, {\u0026#34;id\u0026#34;: 5, \u0026#34;vector\u0026#34;: [0.985825131989184, -0.8144651566660419, 0.6299267002202009, 0.1206906911183383, -0.1446277761879955], \u0026#34;color\u0026#34;: \u0026#34;yellow_4222\u0026#34;, \u0026#34;likes\u0026#34;: 12}, {\u0026#34;id\u0026#34;: 6, \u0026#34;vector\u0026#34;: [0.8371977790571115, -0.015764369584852833, -0.31062937026679327, -0.562666951622192, -0.8984947637863987], \u0026#34;color\u0026#34;: \u0026#34;red_9392\u0026#34;, \u0026#34;likes\u0026#34;: 58}, {\u0026#34;id\u0026#34;: 7, \u0026#34;vector\u0026#34;: [-0.33445148015177995, -0.2567135004164067, 0.8987539745369246, 0.9402995886420709, 0.5378064918413052], \u0026#34;color\u0026#34;: \u0026#34;grey_8510\u0026#34;, \u0026#34;likes\u0026#34;: 775}, {\u0026#34;id\u0026#34;: 8, \u0026#34;vector\u0026#34;: [0.39524717779832685, 0.4000257286739164, -0.5890507376891594, -0.8650502298996872, -0.6140360785406336], \u0026#34;color\u0026#34;: \u0026#34;white_9381\u0026#34;, \u0026#34;likes\u0026#34;: 876}, {\u0026#34;id\u0026#34;: 9, \u0026#34;vector\u0026#34;: [0.5718280481994695, 0.24070317428066512, -0.3737913482606834, -0.06726932177492717, -0.6980531615588608], \u0026#34;color\u0026#34;: \u0026#34;purple_4976\u0026#34;, \u0026#34;likes\u0026#34;: 765}, ] # 插入数据 client.insert( collection_name=collection_name, data=data ) # {\u0026#39;insert_count\u0026#39;: 10, \u0026#39;ids\u0026#39;: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \u0026#39;cost\u0026#39;: 0} filter-search\nres_filtered = client.search( collection_name=collection_name, anns_field=\u0026#34;vector\u0026#34;, data=[query_vector], limit=5, filter=\u0026#34;color like \u0026#39;red%\u0026#39; and likes \u0026gt; 50\u0026#34;, output_fields=[\u0026#34;color\u0026#34;, \u0026#34;likes\u0026#34;] ) for hits in res_filtered: for hit in hits: print(hit) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;id\u0026#39;: 4, \u0026#39;distance\u0026#39;: 1.4699335098266602, \u0026#39;entity\u0026#39;: {\u0026#39;color\u0026#39;: \u0026#39;red_4794\u0026#39;, \u0026#39;likes\u0026#39;: 122}} {\u0026#39;id\u0026#39;: 6, \u0026#39;distance\u0026#39;: 3.989945888519287, \u0026#39;entity\u0026#39;: {\u0026#39;color\u0026#39;: \u0026#39;red_9392\u0026#39;, \u0026#39;likes\u0026#39;: 58}} \u0026#34;\u0026#34;\u0026#34; 支持的metadata 过滤方式：\n过滤类型 描述 示例 比较操作符 用于数值比较 filter=\u0026quot;price \u0026gt; 500\u0026quot; - \u0026gt;：大于 filter=\u0026quot;inventory['quantity'] \u0026gt;= 250\u0026quot; - \u0026lt;：小于 filter=\u0026quot;sales_volume[0] \u0026lt; 100\u0026quot; - ==：等于 filter=\u0026quot;color == 'red_7025'\u0026quot; - \u0026lt;=：小于等于 filter=\u0026quot;price \u0026lt;= 900\u0026quot; - \u0026gt;=：大于等于 filter=\u0026quot;sales_volume[0] \u0026gt;= 150\u0026quot; - !=：不等于 filter=\u0026quot;color != 'yellow_4222'\u0026quot; Term操作符 精确匹配或排除 filter='color in [\u0026quot;red_7025\u0026quot;, \u0026quot;red_4794\u0026quot;]' - in：匹配指定集合 filter='inventory[\u0026quot;brand\u0026quot;] in [\u0026quot;Apple\u0026quot;]' - not in：排除指定集合 filter='color not in [\u0026quot;yellow_4222\u0026quot;, \u0026quot;grey_8510\u0026quot;]' Match操作符 字符串匹配 filter='color like \u0026quot;red%\u0026quot;' - like：通配符匹配 filter='inventory[\u0026quot;brand\u0026quot;] like \u0026quot;S%\u0026quot;' - TEXT_MATCH：文本高效匹配 filter='TEXT_MATCH(description, \u0026quot;Apple iPhone\u0026quot;)' 算术操作符 数值计算 filter=\u0026quot;price * 0.5 \u0026lt; 300\u0026quot; - +：加 filter=\u0026quot;sales_volume[0] + 50 \u0026gt; 200\u0026quot; - -：减 filter=\u0026quot;price - 100 \u0026gt; 500\u0026quot; - *：乘 filter=\u0026quot;price * 2 \u0026gt; 1000\u0026quot; - /：除 filter=\u0026quot;inventory['quantity'] / 2 \u0026gt; 100\u0026quot; - **：幂运算 filter=\u0026quot;price ** 2 \u0026gt; 250000\u0026quot; - %：取余 filter=\u0026quot;price % 100 == 0\u0026quot; JSON操作符 用于JSON字段的高级过滤 filter='JSON_CONTAINS(inventory[\u0026quot;previous_sales\u0026quot;], 232)' - JSON_CONTAINS：包含指定元素 filter='JSON_CONTAINS_ALL(inventory[\u0026quot;previous_sales\u0026quot;], [232, 254])' - JSON_CONTAINS_ALL：包含所有指定元素 filter='JSON_CONTAINS_ANY(inventory[\u0026quot;previous_sales\u0026quot;], [232, 275])' - JSON_CONTAINS_ANY：包含任一元素 Array操作符 用于数组字段的高级过滤 filter='ARRAY_CONTAINS(sales_volume, 150)' - ARRAY_CONTAINS：包含指定元素 filter='ARRAY_CONTAINS_ALL(sales_volume, [150, 150])' - ARRAY_CONTAINS_ALL：包含所有指定元素 filter='ARRAY_CONTAINS_ANY(sales_volume, [150, 190])' - ARRAY_CONTAINS_ANY：包含任一指定元素 - ARRAY_LENGTH：检查数组长度 filter='ARRAY_LENGTH(sales_volume) == 3' 逻辑操作符 组合多个过滤条件 filter='color like \u0026quot;red%\u0026quot; and price \u0026lt; 500' - and 或 \u0026amp;\u0026amp;：所有条件均需满足 filter='price \u0026lt; 500 and sales_volume[0] \u0026gt; 100' - or 或 ` - not：逻辑非 filter='not (price \u0026gt; 500)' 复杂表达式 通过括号调整优先级 filter='(price \u0026gt; 500 and inventory[\u0026quot;brand\u0026quot;] in [\u0026quot;Apple\u0026quot;]) or color == \u0026quot;red_7025\u0026quot;' Range search 通过设置一个范围（半径或区间），筛选出符合距离/相似度范围的向量\n执行范围搜索请求时，Milvus 以 ANN 搜索结果中与查询向量最相似的向量为圆心，以搜索请求中指定的半径为外圈半径，以range_filter为内圈半径，画出两个同心圆。所有相似度得分在这两个同心圆形成的环形区域内的向量都将被返回\n可能的使用场景：\n相似度过滤：返回与查询向量相似度在指定范围内的项\n异常检测：查找距离远离中心点的异常点\n推荐系统： 避免过高相似度（可能是重复内容）或过低相似度（相关性不强）的结果\nMetric Type 距离范围 L2 range_filter \u0026lt;= 距离 \u0026lt; radius IP radius \u0026lt; 距离 \u0026lt;= range_filter COSINE radius \u0026lt; 距离 \u0026lt;= range_filter JACCARD range_filter \u0026lt;= 距离 \u0026lt; radius HAMMING range_filter \u0026lt;= 距离 \u0026lt; radius query_vector = [0.5580376395471989, -0.8023495712049978, 0.38414012509913835, -0.36286205330961354, 0.9029438446296591] # metric_type=\u0026#34;L2\u0026#34; res = client.search( collection_name=collection_name, data=[query_vector], limit=10, search_params={ \u0026#34;params\u0026#34;: { \u0026#34;radius\u0026#34;: 0.9, \u0026#34;range_filter\u0026#34;: 0.1 } } ) for hits in res: for hit in hits: print(hit) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;id\u0026#39;: 0, \u0026#39;distance\u0026#39;: 0.12999999523162842, \u0026#39;entity\u0026#39;: {}} \u0026#34;\u0026#34;\u0026#34; Grouping Search 将搜索结果根据某些属性或特征分组（group）\n搜索结果中，基于特定字段或条件，将相似或具有共同特征的记录聚合在一起\n举个具体的例子，在 RAG（Retrieval-Augmented Generation）任务中，我们通常会将文档拆分成一定大小的文本块（chunks），以便在检索过程中实现更精准的语义匹配。然而，在生成回答时，我们并不总是局限于直接使用召回的这些文本块，而是希望尽可能覆盖更多的文档或是包含更多的文档上下文。因为检索阶段可能会从同一文档中召回多个文本块，而忽略其他文档的内容，从而导致搜索结果的多样性降低，影响答案的全面性和准确性。\n数据准备\nclient = MilvusClient(uri=\u0026#34;http://localhost:19530\u0026#34;) # 创建 schema schema = client.create_schema( auto_id=False, enable_dynamic_field=True, ) index_params = client.prepare_index_params() # 添加字段到 schema schema.add_field(field_name=\u0026#34;id\u0026#34;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=\u0026#34;vector\u0026#34;, datatype=DataType.FLOAT_VECTOR, dim=5) schema.add_field(field_name=\u0026#34;chunk\u0026#34;, datatype=DataType.VARCHAR, max_length=50) schema.add_field(field_name=\u0026#34;docId\u0026#34;, datatype=DataType.INT64) index_params.add_index( field_name=\u0026#34;vector\u0026#34;, index_type=\u0026#34;AUTOINDEX\u0026#34;, # Automatically choose the best index type metric_type=\u0026#34;L2\u0026#34; # Use L2 distance as the metric ) # Add a sort index for the id field index_params.add_index( field_name=\u0026#34;id\u0026#34;, index_type=\u0026#34;STL_SORT\u0026#34; ) collection_name = \u0026#34;group_search_collection\u0026#34; client.create_collection( collection_name=collection_name, schema=schema, index_params=index_params ) data = [ {\u0026#34;id\u0026#34;: 0, \u0026#34;vector\u0026#34;: [0.3580376395471989, -0.6023495712049978, 0.18414012509913835, -0.26286205330961354, 0.9029438446296592], \u0026#34;chunk\u0026#34;: \u0026#34;pink_8682\u0026#34;, \u0026#34;docId\u0026#34;: 1}, {\u0026#34;id\u0026#34;: 1, \u0026#34;vector\u0026#34;: [0.19886812562848388, 0.06023560599112088, 0.6976963061752597, 0.2614474506242501, 0.838729485096104], \u0026#34;chunk\u0026#34;: \u0026#34;red_7025\u0026#34;, \u0026#34;docId\u0026#34;: 5}, {\u0026#34;id\u0026#34;: 2, \u0026#34;vector\u0026#34;: [0.43742130801983836, -0.5597502546264526, 0.6457887650909682, 0.7894058910881185, 0.20785793220625592], \u0026#34;chunk\u0026#34;: \u0026#34;orange_6781\u0026#34;, \u0026#34;docId\u0026#34;: 2}, {\u0026#34;id\u0026#34;: 3, \u0026#34;vector\u0026#34;: [0.3172005263489739, 0.9719044792798428, -0.36981146090600725, -0.4860894583077995, 0.95791889146345], \u0026#34;chunk\u0026#34;: \u0026#34;pink_9298\u0026#34;, \u0026#34;docId\u0026#34;: 3}, {\u0026#34;id\u0026#34;: 4, \u0026#34;vector\u0026#34;: [0.4452349528804562, -0.8757026943054742, 0.8220779437047674, 0.46406290649483184, 0.30337481143159106], \u0026#34;chunk\u0026#34;: \u0026#34;red_4794\u0026#34;, \u0026#34;docId\u0026#34;: 3}, {\u0026#34;id\u0026#34;: 5, \u0026#34;vector\u0026#34;: [0.985825131989184, -0.8144651566660419, 0.6299267002202009, 0.1206906911183383, -0.1446277761879955], \u0026#34;chunk\u0026#34;: \u0026#34;yellow_4222\u0026#34;, \u0026#34;docId\u0026#34;: 4}, {\u0026#34;id\u0026#34;: 6, \u0026#34;vector\u0026#34;: [0.8371977790571115, -0.015764369584852833, -0.31062937026679327, -0.562666951622192, -0.8984947637863987], \u0026#34;chunk\u0026#34;: \u0026#34;red_9392\u0026#34;, \u0026#34;docId\u0026#34;: 1}, {\u0026#34;id\u0026#34;: 7, \u0026#34;vector\u0026#34;: [-0.33445148015177995, -0.2567135004164067, 0.8987539745369246, 0.9402995886420709, 0.5378064918413052], \u0026#34;chunk\u0026#34;: \u0026#34;grey_8510\u0026#34;, \u0026#34;docId\u0026#34;: 2}, {\u0026#34;id\u0026#34;: 8, \u0026#34;vector\u0026#34;: [0.39524717779832685, 0.4000257286739164, -0.5890507376891594, -0.8650502298996872, -0.6140360785406336], \u0026#34;chunk\u0026#34;: \u0026#34;white_9381\u0026#34;, \u0026#34;docId\u0026#34;: 5}, {\u0026#34;id\u0026#34;: 9, \u0026#34;vector\u0026#34;: [0.5718280481994695, 0.24070317428066512, -0.3737913482606834, -0.06726932177492717, -0.6980531615588608], \u0026#34;chunk\u0026#34;: \u0026#34;purple_4976\u0026#34;, \u0026#34;docId\u0026#34;: 3}, ] client.insert( collection_name=collection_name, data=data ) 查询\nquery_vectors = [ [0.14529211512077012, 0.9147257273453546, 0.7965055218724449, 0.7009258593102812, 0.5605206522382088]] # Group search results res = client.search( collection_name=collection_name, data=query_vectors, limit=3, group_by_field=\u0026#34;docId\u0026#34;, output_fields=[\u0026#34;docId\u0026#34;] ) # Retrieve the values in the `docId` column doc_ids = [result[\u0026#39;entity\u0026#39;][\u0026#39;docId\u0026#39;] for result in res[0]] doc_ids # [5, 2, 3] 只返回top-3的doc结果\ngroup_by_field： 指定进行分组的字段名称\nstrict_group_size： 每组返回的结果数量。默认情况下，每组只返回一个最相似的实体\nstrict_group_size： 是否严格限制每组的返回结果数量。设置为 True 时，系统会尝试保证每组返回的实体数量与 group_size 参数一致。如果某组数据不足，则可能影响查询性能。默认值为 False。\nHybrid Search 允许在一个collection中对多个向量字段（例如稠密向量和稀疏向量）同时进行搜索，并将不同搜索结果重新排序为单一结果集。\nHybrid Search refers to a search method that conducts multiple ANN searches simultaneously, reranks multiple sets of results from these ANN searches, and ultimately returns a single set of results. 主要工作流程\n生成密集向量（Dense Vectors）：通过嵌入模型（如 BERT 和 Transformers）生成密\n生成稀疏向量（Sparse Vectors）：通过嵌入模型（如 BM25、BGE-M3、SPLADE 等）生成稀疏向量。\n创建集合（Collection）：创建集合，并定义包含密集和稀疏向量字段的集合 Schema。\n插入数据：将稀疏和密集向量插入到创建的集合中。\n执行混合搜索：\n对密集向量执行 ANN 搜索，返回一组最相似的 Top-K 结果。\n对稀疏向量执行文本匹配搜索，也返回一组 Top-K 结果。\n得分归一化（Normalization）：将两组 Top-K 结果的得分归一化到 [0,1] 的范围。\n融合与重排序：选择合适的重排序策略，将两组 Top-K 结果合并并重新排序，最终返回一组综合的 Top-K 结果。\nfrom pymilvus import ( MilvusClient, DataType ) client = MilvusClient( uri=\u0026#34;http://localhost:19530\u0026#34; ) schema = MilvusClient.create_schema( auto_id=False, enable_dynamic_field=True, ) schema.add_field(field_name=\u0026#34;id\u0026#34;, datatype=DataType.INT64, is_primary=True) schema.add_field(field_name=\u0026#34;text\u0026#34;, datatype=DataType.VARCHAR, max_length=1000) schema.add_field(field_name=\u0026#34;sparse\u0026#34;, datatype=DataType.SPARSE_FLOAT_VECTOR) schema.add_field(field_name=\u0026#34;dense\u0026#34;, datatype=DataType.FLOAT_VECTOR, dim=5) index_params = client.prepare_index_params() index_params.add_index( field_name=\u0026#34;dense\u0026#34;, index_name=\u0026#34;dense_index\u0026#34;, index_type=\u0026#34;IVF_FLAT\u0026#34;, metric_type=\u0026#34;IP\u0026#34;, params={\u0026#34;nlist\u0026#34;: 128}, ) index_params.add_index( field_name=\u0026#34;sparse\u0026#34;, index_name=\u0026#34;sparse_index\u0026#34;, index_type=\u0026#34;SPARSE_INVERTED_INDEX\u0026#34;, # Index type for sparse vectors metric_type=\u0026#34;IP\u0026#34;, # Currently, only IP (Inner Product) is supported for sparse vectors params={\u0026#34;drop_ratio_build\u0026#34;: 0.2}, # The ratio of small vector values to be dropped during indexing ) client.create_collection( collection_name=\u0026#34;hybrid_search_collection\u0026#34;, schema=schema, index_params=index_params ) 数据插入\n# 插入数据 import random # 随机生成稠密向量 def generate_random_dense_vector(dim): return [random.uniform(-1, 1) for _ in range(dim)] # 随机生成稀疏向量 def generate_random_sparse_vector(max_dim, num_non_zero): indices = random.sample(range(max_dim), num_non_zero) values = [random.uniform(0, 1) for _ in range(num_non_zero)] return {idx: val for idx, val in zip(indices, values)} # 数据 data = [ { \u0026#34;id\u0026#34;: 0, \u0026#34;text\u0026#34;: \u0026#34;Artificial intelligence was founded as an academic discipline in 1956.\u0026#34;, \u0026#34;sparse\u0026#34;: generate_random_sparse_vector(10000, 5), # 稀疏向量 \u0026#34;dense\u0026#34;: generate_random_dense_vector(5) # 稠密向量 }, { \u0026#34;id\u0026#34;: 1, \u0026#34;text\u0026#34;: \u0026#34;Alan Turing was the first person to conduct substantial research in AI.\u0026#34;, \u0026#34;sparse\u0026#34;: generate_random_sparse_vector(10000, 5), \u0026#34;dense\u0026#34;: generate_random_dense_vector(5) }, { \u0026#34;id\u0026#34;: 2, \u0026#34;text\u0026#34;: \u0026#34;Born in Maida Vale, London, Turing was raised in southern England.\u0026#34;, \u0026#34;sparse\u0026#34;: generate_random_sparse_vector(10000, 5), \u0026#34;dense\u0026#34;: generate_random_dense_vector(5) }, ] # 插入数据 res = client.insert( collection_name=\u0026#34;hybrid_search_collection\u0026#34;, data=data ) 混合搜索是通过在**hybrid_search()** 函数中创建多个**AnnSearchRequest** 来实现的，其中每个**AnnSearchRequest** 代表一个特定向量场的基本 ANN 搜索请求。\n在混合搜索中，每个**AnnSearchRequest** 只支持一个查询向量。\nfrom pymilvus import AnnSearchRequest query_dense_vector = [0.3580376395471989, -0.6023495712049978, 0.18414012509913835, -0.26286205330961354, 0.9029438446296592] search_param_1 = { \u0026#34;data\u0026#34;: [query_dense_vector], \u0026#34;anns_field\u0026#34;: \u0026#34;dense\u0026#34;, \u0026#34;param\u0026#34;: { \u0026#34;metric_type\u0026#34;: \u0026#34;IP\u0026#34;, \u0026#34;params\u0026#34;: {\u0026#34;nprobe\u0026#34;: 10} }, \u0026#34;limit\u0026#34;: 2 } request_1 = AnnSearchRequest(**search_param_1) query_sparse_vector = {1609: 0.6255744191385809, 7058: 0.20234890590382482, 449: 0.01787891574781786, 9702: 0.710732808143222, 8827: 0.3979309078494506} search_param_2 = { \u0026#34;data\u0026#34;: [query_sparse_vector], \u0026#34;anns_field\u0026#34;: \u0026#34;sparse\u0026#34;, \u0026#34;param\u0026#34;: { \u0026#34;metric_type\u0026#34;: \u0026#34;IP\u0026#34;, \u0026#34;params\u0026#34;: {\u0026#34;drop_ratio_build\u0026#34;: 0.2} }, \u0026#34;limit\u0026#34;: 2 } request_2 = AnnSearchRequest(**search_param_2) reqs = [request_1, request_2] 采用加权排名\nfrom pymilvus import WeightedRanker ranker= WeightedRanker(0.8, 0.3) res = client.hybrid_search( collection_name=\u0026#34;hybrid_search_collection\u0026#34;, reqs=reqs, ranker=ranker, limit=2 ) for hits in res: print(\u0026#34;TopK results:\u0026#34;) for hit in hits: print(hit) \u0026#34;\u0026#34;\u0026#34; TopK results: {\u0026#39;id\u0026#39;: 1, \u0026#39;distance\u0026#39;: 0.5914705991744995, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 0, \u0026#39;distance\u0026#39;: 0.27949291467666626, \u0026#39;entity\u0026#39;: {}} \u0026#34;\u0026#34;\u0026#34; 采用RRF（Reciprocal Rank Fusion） ranker\nfrom pymilvus import RRFRanker # Default k value is 60 ranker = RRFRanker(k=100) res = client.hybrid_search( collection_name=\u0026#34;hybrid_search_collection\u0026#34;, reqs=reqs, ranker=ranker, limit=2 ) for hits in res: print(\u0026#34;TopK results:\u0026#34;) for hit in hits: print(hit) \u0026#34;\u0026#34;\u0026#34; TopK results: {\u0026#39;id\u0026#39;: 1, \u0026#39;distance\u0026#39;: 0.009900989942252636, \u0026#39;entity\u0026#39;: {}} {\u0026#39;id\u0026#39;: 0, \u0026#39;distance\u0026#39;: 0.009803921915590763, \u0026#39;entity\u0026#39;: {}} \u0026#34;\u0026#34;\u0026#34; Full Text Search Milvus 在 2.5 版本中引入了全文检索， 能够基于关键字或短语高效地搜索文本数据。\n全文检索功能在 Milvus Standalone 和 Milvus Distributed 中可用，但在 Milvus Lite 中不可用\n步骤：\n文本输入：用户直接插入原始文本数据，无需手动生成向量嵌入。\n文本分析：Milvus 使用内置的分析器将输入文本分解为可搜索的词条。\n函数处理：内置函数接收已分词的词条，并将其转换为稀疏向量表示。\n集合存储：Milvus 将这些稀疏嵌入存储在集合（Collection）中，以便高效检索。\nBM25 评分：在检索时，Milvus 应用 BM25 算法计算查询文本和存储文档的相关性得分，并对匹配结果进行排序\n创建 Schema\nfrom pymilvus import MilvusClient, DataType, Function, FunctionType client = MilvusClient(uri=\u0026#34;http://localhost:19530\u0026#34;) schema = client.create_schema() schema.add_field(field_name=\u0026#34;id\u0026#34;, datatype=DataType.INT64, is_primary=True, auto_id=True) schema.add_field(field_name=\u0026#34;text\u0026#34;, datatype=DataType.VARCHAR, max_length=1000, enable_analyzer=True) schema.add_field(field_name=\u0026#34;sparse\u0026#34;, datatype=DataType.SPARSE_FLOAT_VECTOR) SPARSE_FLOAT_VECTOR 字段，预留用于存储稀疏嵌入\n定义一个将文本转换为稀疏向量表示的函数，然后将其添加到 Schema 中\nbm25_function = Function( name=\u0026#34;text_bm25_emb\u0026#34;, # Function name input_field_names=[\u0026#34;text\u0026#34;], # Name of the VARCHAR field containing raw text data output_field_names=[\u0026#34;sparse\u0026#34;], # Name of the SPARSE_FLOAT_VECTOR field reserved to store generated embeddings function_type=FunctionType.BM25, ) schema.add_function(bm25_function) 设置索引 create collection\nindex_params = client.prepare_index_params() index_params.add_index( field_name=\u0026#34;sparse\u0026#34;, index_type=\u0026#34;AUTOINDEX\u0026#34;, metric_type=\u0026#34;BM25\u0026#34; ) client.create_collection( collection_name=\u0026#39;demo\u0026#39;, schema=schema, index_params=index_params ) 插入数据\nclient.insert(\u0026#39;demo\u0026#39;, [ {\u0026#39;text\u0026#39;: \u0026#39;information retrieval is a field of study.\u0026#39;}, {\u0026#39;text\u0026#39;: \u0026#39;information retrieval focuses on finding relevant information in large datasets.\u0026#39;}, {\u0026#39;text\u0026#39;: \u0026#39;data mining and information retrieval overlap in research.\u0026#39;}, ]) 执行全文查询\nsearch_params = { \u0026#39;params\u0026#39;: {\u0026#39;drop_ratio_search\u0026#39;: 0.2}, } res = client.search( collection_name=\u0026#39;demo\u0026#39;, data=[\u0026#39;whats the focus of information retrieval?\u0026#39;], anns_field=\u0026#39;sparse\u0026#39;, limit=2, search_params=search_params, output_fields=[\u0026#34;text\u0026#34;] ) for hits in res: print(\u0026#34;TopK results:\u0026#34;) for hit in hits: print(hit) \u0026#34;\u0026#34;\u0026#34; TopK results: {\u0026#39;id\u0026#39;: 454187296102665084, \u0026#39;distance\u0026#39;: 1.3352930545806885, \u0026#39;entity\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;information retrieval is a field of study.\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665085, \u0026#39;distance\u0026#39;: 0.29726022481918335, \u0026#39;entity\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;information retrieval focuses on finding relevant information in large datasets.\u0026#39;}} \u0026#34;\u0026#34;\u0026#34; Query Milvus 还支持通过查询进行元数据过滤，即类似于关系型数据库的数据查询和过滤\nGet 查询： 用于通过主键（Primary Key）精确查找实体\nQuery 查询： 通过自定义过滤表达式查找满足条件的实体\nQueryIterator 查询迭代器： 用于在分页查询中，通过自定义过滤条件遍历满足条件的所有实体 （by-batch查找）\nget\nfrom pymilvus import MilvusClient client = MilvusClient( uri=\u0026#34;http://localhost:19530\u0026#34; ) res = client.get( collection_name=\u0026#34;query_collection\u0026#34;, ids=[0, 1, 2], output_fields=[\u0026#34;vector\u0026#34;, \u0026#34;color\u0026#34;] ) print(res) \u0026#34;\u0026#34;\u0026#34; data: [\u0026#34;{\u0026#39;vector\u0026#39;: [0.35803765, -0.6023496, 0.18414013, -0.26286206, 0.90294385], \u0026#39;color\u0026#39;: \u0026#39;pink_8682\u0026#39;, \u0026#39;id\u0026#39;: 0}\u0026#34;, \u0026#34;{\u0026#39;vector\u0026#39;: [0.19886813, 0.060235605, 0.6976963, 0.26144746, 0.8387295], \u0026#39;color\u0026#39;: \u0026#39;red_7025\u0026#39;, \u0026#39;id\u0026#39;: 1}\u0026#34;, \u0026#34;{\u0026#39;vector\u0026#39;: [0.43742132, -0.55975026, 0.6457888, 0.7894059, 0.20785794], \u0026#39;color\u0026#39;: \u0026#39;orange_6781\u0026#39;, \u0026#39;id\u0026#39;: 2}\u0026#34;] \u0026#34;\u0026#34;\u0026#34; query\nres = client.query( collection_name=\u0026#34;query_collection\u0026#34;, filter=\u0026#34;color like \\\u0026#34;red%\\\u0026#34;\u0026#34;, output_fields=[\u0026#34;vector\u0026#34;, \u0026#34;color\u0026#34;], limit=3 ) print(res) \u0026#34;\u0026#34;\u0026#34; data: [\u0026#34;{\u0026#39;id\u0026#39;: 1, \u0026#39;vector\u0026#39;: [0.19886813, 0.060235605, 0.6976963, 0.26144746, 0.8387295], \u0026#39;color\u0026#39;: \u0026#39;red_7025\u0026#39;}\u0026#34;, \u0026#34;{\u0026#39;id\u0026#39;: 4, \u0026#39;vector\u0026#39;: [0.44523495, -0.8757027, 0.82207793, 0.4640629, 0.3033748], \u0026#39;color\u0026#39;: \u0026#39;red_4794\u0026#39;}\u0026#34;, \u0026#34;{\u0026#39;id\u0026#39;: 6, \u0026#39;vector\u0026#39;: [0.8371978, -0.015764369, -0.31062937, -0.56266695, -0.8984948], \u0026#39;color\u0026#39;: \u0026#39;red_9392\u0026#39;}\u0026#34;] \u0026#34;\u0026#34;\u0026#34; QueryIterator\nfrom pymilvus import connections, Collection connections.connect( uri=\u0026#34;http://localhost:19530\u0026#34;, ) collection = Collection(\u0026#34;query_collection\u0026#34;) iterator = collection.query_iterator( batch_size=10, expr=\u0026#34;color like \\\u0026#34;red%\\\u0026#34;\u0026#34;, output_fields=[\u0026#34;color\u0026#34;] ) results = [] while True: result = iterator.next() if not result: iterator.close() break print(result) results += result \u0026#34;\u0026#34;\u0026#34; [{\u0026#39;color\u0026#39;: \u0026#39;red_7025\u0026#39;, \u0026#39;id\u0026#39;: 1}, {\u0026#39;color\u0026#39;: \u0026#39;red_4794\u0026#39;, \u0026#39;id\u0026#39;: 4}, {\u0026#39;color\u0026#39;: \u0026#39;red_9392\u0026#39;, \u0026#39;id\u0026#39;: 6}] \u0026#34;\u0026#34;\u0026#34; Partition-Key 在 Milvus 中，partition-key（分区键）是一种基于特定标量字段的搜索优化方案。通过将某个字段指定为分区键，Milvus 会根据该字段的值将数据自动分配到不同的分区中，与手动管理分区不同，使用分区键可以克服集合中分区数量的限制（最多 1,024 个）。\n将数据分布到独立的分区中，同时支持对分区范围的查询限定，从而实现数据隔离，可以有效避免对非目标分区的数据进行查询操作。\n自动管理分区 vs 手动创建和指定分区\n特性 partition-key 手动分区 分区管理 自动生成，动态分区 必须手动创建，分区结构固定 分区数量限制 无理论限制，分区数量随字段值动态变化 最多 1,024 个分区 便捷性 插入和查询都更加自动化 插入和查询需要手动指定分区名称 适用场景 动态、多分区场景，如用户 ID、类别分类 静态、少量分区场景，如地理区域划分 Partition-key vs Meta-data Filtering\n特性 partition-key Meta-data Filtering 存储方式 数据物理隔离，按分区键存储 数据逻辑统一，存储在一个集合中 查询效率 通过分区键快速定位目标分区，效率高 全局扫描，效率较低 管理复杂性 自动管理，无需显式定义或维护分区 无需预定义分区，依赖查询时的过滤 动态性 分区结构相对稳定，适合分区逻辑较简单场景 更灵活，适合复杂和多变的查询条件 有无分区差异\n通过partition-key 实现数据隔离查询的简单例子；\n在这个例子中，每一个user只有查询自己数据的权限，当user选择将数据公开share，此时partition-key会被改成public，作为单独的partition存在，从而实现数据隔离查询。\n数据准备：\nfrom pymilvus import MilvusClient, DataType, Function, FunctionType client = MilvusClient(uri=\u0026#34;http://localhost:19530\u0026#34;) schema = client.create_schema() index_params = client.prepare_index_params() schema.add_field(field_name=\u0026#34;id\u0026#34;, datatype=DataType.INT64, is_primary=True, auto_id=True) schema.add_field(field_name=\u0026#34;partition_key\u0026#34;, datatype=DataType.VARCHAR, max_length=512, is_partition_key=True,description=\u0026#34;Partition key for data isolation\u0026#34;), schema.add_field(field_name=\u0026#34;data_id\u0026#34;, datatype=DataType.VARCHAR, max_length=128, description=\u0026#34;Unique data identifier\u0026#34;), schema.add_field(field_name=\u0026#34;vector\u0026#34;, datatype=DataType.FLOAT_VECTOR, dim=128, description=\u0026#34;Vector representation\u0026#34;), schema.add_field(field_name=\u0026#34;metadata\u0026#34;, datatype=DataType.JSON, description=\u0026#34;Additional filtering metadata\u0026#34;), index_params.add_index( field_name=\u0026#34;vector\u0026#34;, index_type=\u0026#34;AUTOINDEX\u0026#34;, # Automatically choose the best index type metric_type=\u0026#34;L2\u0026#34; # Use L2 distance as the metric ) # Add a sort index for the id field index_params.add_index( field_name=\u0026#34;id\u0026#34;, index_type=\u0026#34;STL_SORT\u0026#34; ) client.create_collection( collection_name=\u0026#34;partitionkey_collection\u0026#34;, schema=schema, index_params=index_params, num_partitions=1024 ) 构造一些假数据\nimport json def generate_test_data(num_records, num_users, public_probability=0.2): \u0026#34;\u0026#34;\u0026#34; 生成测试数据，支持一定概率生成公共数据。 Args: num_records (int): 数据总数 num_users (int): 用户数量 public_probability (float): 生成公共数据的概率 (0.0 到 1.0) Returns: list: 生成的测试数据，每条数据为一个字典 \u0026#34;\u0026#34;\u0026#34; data = [] for _ in range(num_records): # 根据概率选择 partition_key 为 \u0026#34;public\u0026#34; 或随机用户 ID if random.random() \u0026lt; public_probability: partition_key = \u0026#34;public\u0026#34; else: partition_key = f\u0026#34;user_{random.randint(1, num_users)}\u0026#34; record = { \u0026#34;partition_key\u0026#34;: partition_key, # 设置分区键 \u0026#34;data_id\u0026#34;: f\u0026#34;data_{random.randint(1000, 9999)}\u0026#34;, # 随机生成数据 ID \u0026#34;vector\u0026#34;: [random.uniform(0, 1) for _ in range(128)], # 随机生成128维向量 \u0026#34;metadata\u0026#34;: json.dumps({ \u0026#34;category\u0026#34;: random.choice([\u0026#34;image\u0026#34;, \u0026#34;text\u0026#34;, \u0026#34;video\u0026#34;]), \u0026#34;tags\u0026#34;: [random.choice([\u0026#34;AI\u0026#34;, \u0026#34;ML\u0026#34;, \u0026#34;NLP\u0026#34;, \u0026#34;CV\u0026#34;]) for _ in range(2)], \u0026#34;rating\u0026#34;: random.uniform(1, 5), }) } data.append(record) return data data = generate_test_data(500,20) client.insert( collection_name=\u0026#34;partitionkey_collection\u0026#34;, data=data ) 仅查询user_10 的数据\nquery_result = client.query( collection_name = \u0026#34;partitionkey_collection\u0026#34;, filter=\u0026#39;partition_key == \u0026#34;user_10\u0026#34;\u0026#39;, output_fields=[\u0026#34;data_id\u0026#34;, \u0026#34;partition_key\u0026#34;] ) print(query_result) \u0026#34;\u0026#34;\u0026#34; data: [\u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_7887\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665088}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_9004\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665090}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_8970\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665092}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_2031\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665098}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_6972\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665111}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_3463\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665118}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_9998\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665123}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_5411\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665133}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_9952\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665205}\u0026#34;, \u0026#34;{\u0026#39;data_id\u0026#39;: \u0026#39;data_9002\u0026#39;, \u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;id\u0026#39;: 454187296102665257}\u0026#34;] ... \u0026#34;\u0026#34;\u0026#34; user_10 所能ANN搜索的内容（ppublicand user_10)\nquery_vectors = [random.uniform(0, 1) for _ in range(128)] res = client.search( collection_name = \u0026#34;partitionkey_collection\u0026#34;, data=[query_vectors], filter=\u0026#39;partition_key in [\u0026#34;public\u0026#34;,\u0026#34;user_10\u0026#34;]\u0026#39;, output_fields=[\u0026#34;data_id\u0026#34;, \u0026#34;partition_key\u0026#34;] ) for hits in res: for hit in hits: print(hit) \u0026#34;\u0026#34;\u0026#34; {\u0026#39;id\u0026#39;: 454187296102665637, \u0026#39;distance\u0026#39;: 15.168614387512207, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;public\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_9024\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665532, \u0026#39;distance\u0026#39;: 15.173531532287598, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_8461\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665598, \u0026#39;distance\u0026#39;: 15.250306129455566, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_7539\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665323, \u0026#39;distance\u0026#39;: 15.833619117736816, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;user_10\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_8433\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665234, \u0026#39;distance\u0026#39;: 16.50188446044922, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;public\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_1534\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665501, \u0026#39;distance\u0026#39;: 16.521953582763672, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;public\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_9944\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665463, \u0026#39;distance\u0026#39;: 16.524085998535156, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;public\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_6430\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665453, \u0026#39;distance\u0026#39;: 16.64727020263672, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;public\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_8403\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665629, \u0026#39;distance\u0026#39;: 16.649581909179688, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;public\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_5141\u0026#39;}} {\u0026#39;id\u0026#39;: 454187296102665631, \u0026#39;distance\u0026#39;: 16.809789657592773, \u0026#39;entity\u0026#39;: {\u0026#39;partition_key\u0026#39;: \u0026#39;public\u0026#39;, \u0026#39;data_id\u0026#39;: \u0026#39;data_3505\u0026#39;}} \u0026#34;\u0026#34;\u0026#34; Models 在2.4 版本之后支持了对于embedding模型和reranking模型的集成。\n个人认为没有太大必要。\n图形化界面 之前使用的是milvus-insight，不过不再维护了。\n官方主推的是attu。\n一些踩坑点 （待更新）\n参考 官方文档\n","permalink":"https://niraya666.github.io/posts/milvus-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%92%8C%E5%A4%87%E5%BF%98%E5%BD%95%E4%BB%8E2.0-%E5%88%B02.5%E7%89%88%E6%9C%AC%E7%9A%84%E4%BB%8E%E5%A4%B4%E5%AD%A6%E4%B9%A0/","summary":"\u003cp\u003e几年前初识 Milvus 的契机，来源于开发一个图像相似检索的应用，当时市面上向量库的可选择项并不像现在这么多，且功能也仅限于单纯的向量检索。\u003c/p\u003e\n\u003cp\u003e鉴于最近有业务更新的需要，和打算重构一下之前做的RAG项目，再次有机会深入学习 Milvus，探索其在新功能和实际应用上的更多可能性。\u003c/p\u003e\n\u003cp\u003e本篇笔记，仅作为学习笔记，更多是记录一些 Milvus从2.0 到2.5 的变化， 和一些动手实践的记录，便于之后的查阅。\u003c/p\u003e\n\u003ch2 id=\"一些概念\"\u003e一些概念\u003c/h2\u003e\n\u003ch3 id=\"collections-schema-and-index\"\u003eCollections, Schema and index\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCollection\u003c/strong\u003e是Milvus中的一个二维表格，具有固定的列和可变的行。每一列代表一个字段(field)，每一行代表一个实体(entity)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSchema和字段\u003c/strong\u003e Collection需要一个schema来定义其结构\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e索引\u003c/strong\u003e 在特定字段上创建索引可以提高搜索效率。建议为服务所依赖的所有字段创建索引，其中向量字段的索引是必需的。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e分区(Partition)\u003c/strong\u003e 分区是Collection的子集，与其父Collection共享相同的字段集\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e分片(Shard)\u003c/strong\u003e 分片是Collection的水平切片。每个分片对应一个数据输入通道。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eShard vs Partition的区别\u003c/strong\u003e :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e分区(Partition)的作用是通过指定分区名称来减少读取负载\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e而分片(Shard)的作用是将写入负载分散到多个服务器\u003cstrong\u003e分布式架构中的应用\u003c/strong\u003e 在分布式系统中，分片是实现水平扩展的重要机制。通过将数据分布到多个节点，可以充分利用集群的并行计算潜力，提高系统的写入性能\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"milvusclient-vs-connection\"\u003e\u003cstrong\u003eMilvusClient vs Connection\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eMilvusClient\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e定位\u003c/strong\u003e：更高级封装，提供一体化的操作接口。简化了与 Milvus 的交互流程， 提供更直观和结构化的操作方式，便于新手快速上手，内置了对连接的管理和操作，减少手动处理的复杂性。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003epymilvus\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eMilvusClient\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 创建客户端并连接到 Milvus\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eMilvusClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003euri\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;http://localhost:19530\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 创建集合\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate_collection\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;example_collection\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eschema\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;fields\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;name\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;vector\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;FLOAT_VECTOR\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;params\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;dim\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e}}]}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 插入数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einsert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;example_collection\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;vector\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[[\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.2\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e]})\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 搜索\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eresults\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esearch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;example_collection\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[[\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresults\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eConnection\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e定位\u003c/strong\u003e：基础连接操作，需要通过 connect 方法创建并维护连接。提供更底层的控制，适合灵活、自定义的操作。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003epymilvus\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003econnections\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eCollection\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eFieldSchema\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eCollectionSchema\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 创建连接\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003econnections\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003econnect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ealias\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;default\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehost\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;localhost\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eport\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;19530\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 创建集合\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003efields\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eFieldSchema\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;vector\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edtype\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;FLOAT_VECTOR\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eschema\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCollectionSchema\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efields\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edescription\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;example collection\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecollection\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCollection\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;example_collection\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eschema\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eschema\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 插入数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecollection\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einsert\u003c/span\u003e\u003cspan class=\"p\"\u003e([[\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.2\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# 搜索\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecollection\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eload\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eresults\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecollection\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esearch\u003c/span\u003e\u003cspan class=\"p\"\u003e([[\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003eanns_field\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;vector\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elimit\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresults\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"schema\"\u003e\u003cstrong\u003eSchema\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eSchema\u003c/strong\u003e 用于定义collection及其字段的属性\u003c/p\u003e","title":"Milvus-2.5版本：学习笔记和备忘录"},{"content":"去年疫情之后，我感觉记忆力明显下降，甚至一度担心会不可恢复。或许这也是年纪渐长的体现，只是我不愿意承认罢了。直到读到了一篇博文，了解了CODE信息管理框架，并因此接触到《第二大脑》这本书。虽然这本书并不长，但由于各种原因，今年我才真正开始阅读并实践。经过一年的探索，我决定整理这篇读书笔记，既是对书中理念的梳理，也是对个人实践的总结。\n值得一提的是，随着近年来大语言模型（LLM）的迅速发展，AI辅助可能会带来更高效的信息管理方式。\n信息过载的时代，为什么需要笔记？ 很多情况下，我们不过是一个个“人形存储器”，明明囤积了大量的心灵鸡汤，却反而让自己变得越发焦虑。\n为什么要做笔记？在信息泛滥的今天，我们像“人形存储器”一样不断被信息填满，大脑的容量有限，信息过载只会增加焦虑。\n在25岁之前，我几乎不需要笔记，因为记忆力不错，而且接触的信息相对有限。工作后，尤其在AI领域，每天都被信息轰炸。尤其是ChatGPT问世后，信息的爆炸性增长让我常常感到FOMO（Fear of missing out，即错失恐惧症）：担心错过最新的模型、论文，或行业趋势。我习惯性地将感兴趣的网页放入同一窗口，但消化的速度远远跟不上积累的速度，导致未处理的内容堆积成山。\n我们无法容纳所有信息，因此需要新的信息管理方式——一个“第二大脑”。\n在《第二大脑》中，作者提出了“第二大脑”概念。这个概念的核心是解放大脑的记忆负担，将精力集中在创新上。为了实现这一目标，作者提供了一套实操框架，即“信管法则”CODE：抓取（Capture）、组织（Organize）、提炼（Distill）和表达（Express）。\n什么是信管法则 CODE 1. 信息抓取 因为普通人对于信息的利用并没有明确的规划，所以在信息的筛选问题上会显得有些无所适从。\n抓取信息的目的是捕捉对自己有意义的内容，而不是任意积累。有效筛选信息可以通过以下标准：\n信息抓取标准1：是否具有启发性\n信息抓取标准2：是否具有实用性\n信息抓取标准3：是否具有个性\n信息抓取标准4：是否具有新奇性\n信息抓取终极标准：是否让你为之共鸣\n从推荐系统的出现开始，可以看到现代人所面临的一个典型问题——信息茧房。我们常常被动地接受信息，而非主动选择，导致思维越来越局限。这种信息茧房现象，使得我们只看到我们想看的，忽略了更多有价值的内容。在如今的注意力战争中，我们的注意力被大量低质量的信息所吸引，而常常错过了高质量的信息。我们需要认识到，抓取信息的关键不在于多，而在于精。\n另外，抓取信息时要保持随时随地记录的习惯。无论是灵感闪现，还是对某篇文章的深刻思考，都应该马上记录下来，而不是随着时间逐渐遗忘。可以善用备忘录、稍后阅读工具等工具，以确保在需要时可以迅速获取信息。\n2. 信息组织 信息需要合理分类，以方便未来使用。相信我，你一定在备忘录中看到一些不知写的是什么的内容，或是看着收藏夹中多达上百条记录而不知所措，这时候你需要的是改变信息组织方式。\n作者推荐“以行动为导向”的PARA系统：项目（Projects）、领域（Areas）、资源（Resources）、存档（Archive）。这一系统的核心在于根据实际需求对信息进行分类存储，确保信息易于找到和利用。\nPARA系统的运作机制：让你的思想（和笔记）向行动看齐\n在PARA系统中，每一条信息都可以被划入以下四大类别中：\n项目（Projects）：工作或生活中正在从事的短期活动；\n领域（Areas）：致力于长期履行的责任；\n资源（Resources）：具有潜在利用价值的课题或兴趣点；\n存档（Archive）：除上述三项以外的休眠信息。\n相比基于“信息来源”的组织方式，PARA系统以信息可能促成的结果为导向。这种方法可以让我们更加专注于将信息用于创造行动和产生价值，而不是无限期地积累信息。\nPARA系统的精髓在于：不要让信息变成无用的负担，而是让它们真正为你的行动服务。\n3. 信息提炼 毕竟笔记是要拿来用的，而不是当作收藏的。\n换言之，笔记的体量与可见性之间呈一种反比关系\n去粗取精。首先需要定义出“知识的骨架”。对原始文本进行分解，从过去一年来做的所谓“知识管理”有关的项目而言，这一部分的内容，更多的是将原始的文本进行分解，根据知识的骨架（schema），将各个分解而成的核心内容放置在各个位置上。例如，对于文献，可以使用来自cool-paper的论文分解框架：\n这篇论文试图解决什么问题？\n有哪些相关研究？\n论文如何解决这个问题？\n……\n渐进式归纳法：渐进式归纳法的核心逻辑在于“断舍离”，而非“滴水不漏”。\n分层标注。在这本书中，作者提出了渐进归纳法的文本高亮方法，以抓取和提炼出核心内容。具体而言，首先将文本的重点标记出，而后在重点中标记出重点的重点，以此类推。实操角度，例如收藏的一篇文章作为第一层级，而后在文章中以加粗的形式标记认为的重点，其次使用高亮标记将重点的重点标注出。\n用自己的语言重述。最后一个层级，也是最重要且被绝大多数人所忽略的：用自己的语言将要点重现表述，并增加自己的理解。这有助于深刻掌握内容。\n4. 表达与实践 If you\u0026rsquo;re thinking without writing, you only think you\u0026rsquo;re thinking.\n知识的价值是为了生产，而不是为了收藏。将所学转化为自己的语言并表达出来，才算真正掌握。可以从简单的记录入手，将零散的想法整理成文章或笔记，逐步积累，最终形成系统的表达。\n一开始，这可能会有些困难，但可以从简单的句子开始，慢慢汇总这些“半熟素材”。可以从记录在备忘录中的一段核心想法，或是社交媒体上几百字的感想出发。当积累到一定程度后，完成一篇长的文章也不再是一件困难的事。\n我非常喜欢18世纪哲学家维柯的一句名言：“Verum ipsum factum”，翻译过来就是“真理不外乎创造”。只有通过自己的语言，将内容重新展示和表达出来，无论形式如何，才是真正的“学会了”。\n关于创新和创造力和自我表达 创新型人才更加善于识别、笼络和联结各种关系。\n创新型人才更加善于识别、笼络和联结各种关系。他们能够从看似无关的事物中找到潜在的联系，并形成解决复杂问题的创造性路径。这种特质源于他们在“发散”和“聚合”思维上的高度敏感和平衡：\n发散思维： 开放的心态，能够接纳多种可能性和观点。\n聚合思维： 注重将分散的信息整合成逻辑清晰的方案。\n生物脑所难以同时进行的两项工作：选择想法和逻辑排序；选择想法需要的是发散的思维方式，而逻辑排序需要将这些想法按一定的逻辑串联。因此，我们需要一些有效的方法来帮助思维的流畅转换和创新性输出。\n不要害怕创作。记录每一个零散的想法，即使它看似普通。量变会引发质变。这些创作过程中的零散素材可以看作“思想的群岛”。通过记录和整理这些想法，可以在不同“岛屿”之间架起桥梁，这些桥梁有助于将发散的想法整合起来，降低创作的难度。这种桥梁称为“海明威之桥”。\n如何打造“海明威之桥”？在笔记里留下当前状态和下一步想法。\n下一步的想法：在某一工作阶段结束前夕，写下你认为下一步可能采取的措施；\n当前的状态：包括当前最大的挑战、最重要的未决事项或是你所预见的某些阻碍；\n易被遗忘的重要信息：例如故事人物的性格特质、某一场景事件中的伏笔，或是产品设计中的个人巧思等；\n在信息大爆炸的时代，创作需要合理利用现代技术，将信息的获取、组织和归纳任务“外包”给工具，这样就能将宝贵的时间和精力集中于需要创造力和个性表达的部分。\n创作时，避免因为庞大的信息量而感到不知所措。通过“压缩范围”，去掉不必要的部分，推迟非关键部分的处理，并妥善存储暂时移除的内容，未来或许会有用。\n最重要的，创造知识的目的是回馈社会。通过分享，你的创意不仅能获得反馈，还可能激发更大的改良和突破。“知识的价值在于应用”，将知识转化为对他人和社会有益的成果，而不是藏起来。\n匈牙利裔英籍哲学家迈克尔·波兰尼在其于1966年发表的一部著作 中提出了所谓的“波兰尼悖论”，一言以蔽之就是“人类所知远胜于其所能言传”。\n","permalink":"https://niraya666.github.io/essay/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E5%A6%82%E4%BD%95%E6%89%93%E9%80%A0%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91/","summary":"\u003cp\u003e去年疫情之后，我感觉记忆力明显下降，甚至一度担心会不可恢复。或许这也是年纪渐长的体现，只是我不愿意承认罢了。直到读到了一篇博文，了解了CODE信息管理框架，并因此接触到《第二大脑》这本书。虽然这本书并不长，但由于各种原因，今年我才真正开始阅读并实践。经过一年的探索，我决定整理这篇读书笔记，既是对书中理念的梳理，也是对个人实践的总结。\u003c/p\u003e\n\u003cp\u003e值得一提的是，随着近年来大语言模型（LLM）的迅速发展，AI辅助可能会带来更高效的信息管理方式。\u003c/p\u003e\n\u003ch2 id=\"信息过载的时代为什么需要笔记\"\u003e信息过载的时代，为什么需要笔记？\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e很多情况下，我们不过是一个个“人形存储器”，明明囤积了大量的心灵鸡汤，却反而让自己变得越发焦虑。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e为什么要做笔记？在信息泛滥的今天，我们像“人形存储器”一样不断被信息填满，大脑的容量有限，信息过载只会增加焦虑。\u003c/p\u003e\n\u003cp\u003e在25岁之前，我几乎不需要笔记，因为记忆力不错，而且接触的信息相对有限。工作后，尤其在AI领域，每天都被信息轰炸。尤其是ChatGPT问世后，信息的爆炸性增长让我常常感到FOMO（Fear of missing out，即错失恐惧症）：担心错过最新的模型、论文，或行业趋势。我习惯性地将感兴趣的网页放入同一窗口，但消化的速度远远跟不上积累的速度，导致未处理的内容堆积成山。\u003c/p\u003e\n\u003cp\u003e我们无法容纳所有信息，因此需要新的信息管理方式——一个“第二大脑”。\u003c/p\u003e\n\u003cp\u003e在《第二大脑》中，作者提出了“第二大脑”概念。这个概念的核心是解放大脑的记忆负担，将精力集中在创新上。为了实现这一目标，作者提供了一套实操框架，即“信管法则”CODE：抓取（Capture）、组织（Organize）、提炼（Distill）和表达（Express）。\u003c/p\u003e\n\u003ch2 id=\"什么是信管法则-code\"\u003e什么是信管法则 CODE\u003c/h2\u003e\n\u003ch3 id=\"1-信息抓取\"\u003e1. 信息抓取\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e因为普通人对于信息的利用并没有明确的规划，所以在信息的筛选问题上会显得有些无所适从。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e抓取信息的目的是捕捉对自己有意义的内容，而不是任意积累。有效筛选信息可以通过以下标准：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e信息抓取标准1：是否具有启发性\u003c/p\u003e\n\u003cp\u003e信息抓取标准2：是否具有实用性\u003c/p\u003e\n\u003cp\u003e信息抓取标准3：是否具有个性\u003c/p\u003e\n\u003cp\u003e信息抓取标准4：是否具有新奇性\u003c/p\u003e\n\u003cp\u003e信息抓取终极标准：是否让你为之共鸣\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e从推荐系统的出现开始，可以看到现代人所面临的一个典型问题——\u003cstrong\u003e信息茧房\u003c/strong\u003e。我们常常被动地接受信息，而非主动选择，导致思维越来越局限。这种信息茧房现象，使得我们只看到我们想看的，忽略了更多有价值的内容。在如今的\u003cstrong\u003e注意力战争\u003c/strong\u003e中，我们的注意力被大量低质量的信息所吸引，而常常错过了高质量的信息。我们需要认识到，抓取信息的关键不在于多，而在于\u003cstrong\u003e精\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e另外，抓取信息时要保持随时随地记录的习惯。无论是灵感闪现，还是对某篇文章的深刻思考，都应该马上记录下来，而不是随着时间逐渐遗忘。可以善用\u003cstrong\u003e备忘录\u003c/strong\u003e、\u003cstrong\u003e稍后阅读工具\u003c/strong\u003e等工具，以确保在需要时可以迅速获取信息。\u003c/p\u003e\n\u003ch3 id=\"2-信息组织\"\u003e2. 信息组织\u003c/h3\u003e\n\u003cp\u003e信息需要合理分类，以方便未来使用。相信我，你一定在备忘录中看到一些不知写的是什么的内容，或是看着收藏夹中多达上百条记录而不知所措，这时候你需要的是改变信息组织方式。\u003c/p\u003e\n\u003cp\u003e作者推荐“以行动为导向”的PARA系统：项目（Projects）、领域（Areas）、资源（Resources）、存档（Archive）。这一系统的核心在于根据实际需求对信息进行分类存储，确保信息易于找到和利用。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePARA系统的运作机制：让你的思想（和笔记）向行动看齐\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e在PARA系统中，每一条信息都可以被划入以下四大类别中：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e项目（Projects）\u003c/strong\u003e：工作或生活中正在从事的短期活动；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e领域（Areas）\u003c/strong\u003e：致力于长期履行的责任；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e资源（Resources）\u003c/strong\u003e：具有潜在利用价值的课题或兴趣点；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e存档（Archive）\u003c/strong\u003e：除上述三项以外的休眠信息。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e相比基于“信息来源”的组织方式，PARA系统以信息可能促成的结果为导向。这种方法可以让我们更加专注于将信息用于创造行动和产生价值，而不是无限期地积累信息。\u003c/p\u003e\n\u003cp\u003ePARA系统的精髓在于：不要让信息变成无用的负担，而是让它们真正为你的行动服务。\u003c/p\u003e\n\u003ch3 id=\"3-信息提炼\"\u003e\u003cstrong\u003e3. 信息提炼\u003c/strong\u003e\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e毕竟笔记是要拿来用的，而不是当作收藏的。\u003c/p\u003e\n\u003cp\u003e换言之，笔记的体量与可见性之间呈一种反比关系\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e去粗取精\u003c/strong\u003e。首先需要定义出“知识的骨架”。对原始文本进行分解，从过去一年来做的所谓“知识管理”有关的项目而言，这一部分的内容，更多的是将原始的文本进行分解，根据知识的骨架（schema），将各个分解而成的核心内容放置在各个位置上。例如，对于文献，可以使用来自cool-paper的论文分解框架：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e这篇论文试图解决什么问题？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e有哪些相关研究？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e论文如何解决这个问题？\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e……\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e渐进式归纳法\u003c/strong\u003e：渐进式归纳法的核心逻辑在于“断舍离”，而非“滴水不漏”。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e分层标注\u003c/strong\u003e。在这本书中，作者提出了渐进归纳法的文本高亮方法，以抓取和提炼出核心内容。具体而言，首先将文本的重点标记出，而后在重点中标记出重点的重点，以此类推。实操角度，例如收藏的一篇文章作为第一层级，而后在文章中以加粗的形式标记认为的重点，其次使用高亮标记将重点的重点标注出。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e用自己的语言重述\u003c/strong\u003e。最后一个层级，也是最重要且被绝大多数人所忽略的：用自己的语言将要点重现表述，并增加自己的理解。这有助于深刻掌握内容。\u003c/p\u003e\n\u003ch3 id=\"4-表达与实践\"\u003e4. 表达与实践\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIf you\u0026rsquo;re thinking without writing, you only think you\u0026rsquo;re thinking.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e知识的价值是为了生产，而不是为了收藏。将所学转化为自己的语言并表达出来，才算真正掌握。可以从简单的记录入手，将零散的想法整理成文章或笔记，逐步积累，最终形成系统的表达。\u003c/p\u003e\n\u003cp\u003e一开始，这可能会有些困难，但可以从简单的句子开始，慢慢汇总这些“半熟素材”。可以从记录在备忘录中的一段核心想法，或是社交媒体上几百字的感想出发。当积累到一定程度后，完成一篇长的文章也不再是一件困难的事。\u003c/p\u003e\n\u003cp\u003e我非常喜欢18世纪哲学家维柯的一句名言：“Verum ipsum factum”，翻译过来就是“真理不外乎创造”。只有通过自己的语言，将内容重新展示和表达出来，无论形式如何，才是真正的“学会了”。\u003c/p\u003e\n\u003ch2 id=\"关于创新和创造力和自我表达\"\u003e关于创新和创造力和自我表达\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e创新型人才更加善于识别、笼络和联结各种关系。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e创新型人才更加善于识别、笼络和联结各种关系。他们能够从看似无关的事物中找到潜在的联系，并形成解决复杂问题的创造性路径。这种特质源于他们在“发散”和“聚合”思维上的高度敏感和平衡：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e发散思维：\u003c/strong\u003e 开放的心态，能够接纳多种可能性和观点。\u003c/p\u003e","title":"读书笔记｜如何打造第二大脑"},{"content":"开篇 “CloseAI” 终于又开源了新的项目，可惜OpenAI明确表示，Swarm是一个实验性框架，主要用于教育目的，不适合生产环境，也没有官方支持。不过从这样一个实验性的框架，至少能够了解到OpenAI对于Agent上的一些理解，对于Agent设计上能够有所帮助和借鉴。\nRoutines and Handoffs 根据openAI cookbook: Orchestrating Agents: Routines and Handoffs**，**理解这个框架前首先需要理解的两个概念： Routines 和 Handoffs。\nThe notion of a \u0026ldquo;routine\u0026rdquo; is not strictly defined, and instead meant to capture the idea of a set of steps. Concretely, let\u0026rsquo;s define a routine to be a list of instructions in natural language (which we\u0026rsquo;ll represent with a system prompt), along with the tools necessary to complete them.\nRoutines（常规）：是由一系列步骤构成的流程，可以理解为给定任务的执行步骤，包括对话系统中指令和所需工具的组合。从代码实现上，基本上就是围绕着openAI 的 openai.chat.completions.createAPI的一系列内容， 对话、工具调用等。换句话说，routines只是具有对话+工具调用的chatbot，这也是openAI对于Agent的基础抽象。\n在cookbook中，所提供的routines示例代码即可看出（以客服代理为例）：\n# 导入必要的库 from openai import OpenAI from pydantic import BaseModel from typing import Optional import json # 创建OpenAI客户端 client = OpenAI() # 客服 Routine 示例 system_message = ( \u0026#34;你是ACME Inc的客服代理。\u0026#34; \u0026#34;回答简短。\u0026#34; \u0026#34;按照以下步骤处理：\u0026#34; \u0026#34;1. 询问客户问题，了解他们遇到的具体问题。\u0026#34; \u0026#34;2. 提出修复建议。\u0026#34; \u0026#34;3. 如果客户不满意，提供退款。\u0026#34; \u0026#34;4. 如果客户接受退款，请查找商品ID并执行退款。\u0026#34; ) # 示例函数：查找商品 def look_up_item(search_query): \u0026#34;\u0026#34;\u0026#34;用于查找商品ID，可以通过描述或关键词进行搜索\u0026#34;\u0026#34;\u0026#34; # 返回硬编码的商品ID，实际场景中会调用数据库或API查找 return \u0026#34;item_132612938\u0026#34; # 示例函数：执行退款 def execute_refund(item_id, reason=\u0026#34;未提供\u0026#34;): \u0026#34;\u0026#34;\u0026#34;执行退款操作\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;退款摘要:\u0026#34;, item_id, reason) return \u0026#34;退款成功\u0026#34; # Routine 运行示例 def run_full_turn(system_message, messages): # 使用OpenAI模型完成聊天对话 response = client.chat.completions.create( model=\u0026#34;gpt-4o-mini\u0026#34;, # 使用的模型 messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_message}] + messages, ) message = response.choices[0].message messages.append(message) # 输出客服代理的回复 if message.content: print(\u0026#34;代理:\u0026#34;, message.content) return message # 消息列表 messages = [] # 运行示例（此时代理根据系统消息进行对话） while True: user_input = input(\u0026#34;用户: \u0026#34;) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) run_full_turn(system_message, messages) 从表面上看，Routine 实现了一个类似于聊天机器人的功能，它按照预定的步骤执行对话和任务。在加入了tool-using后，让代理不仅仅是一个普通的对话机器人，而是具备处理特定任务的能力。\n# 导入必要的库 from openai import OpenAI from pydantic import BaseModel import json # 创建OpenAI客户端 client = OpenAI() # 客服 Routine 示例 system_message = ( \u0026#34;你是ACME Inc的客服代理。\u0026#34; \u0026#34;回答简短。\u0026#34; \u0026#34;按照以下步骤处理：\u0026#34; \u0026#34;1. 询问客户问题，了解他们遇到的具体问题。\u0026#34; \u0026#34;2. 提出修复建议。\u0026#34; \u0026#34;3. 如果客户不满意，提供退款。\u0026#34; \u0026#34;4. 如果客户接受退款，请查找商品ID并执行退款。\u0026#34; ) # 查找商品的工具函数 def look_up_item(search_query): \u0026#34;\u0026#34;\u0026#34;用于查找商品ID，可以通过描述或关键词进行搜索\u0026#34;\u0026#34;\u0026#34; return \u0026#34;item_132612938\u0026#34; # 模拟返回商品ID # 执行退款的工具函数 def execute_refund(item_id, reason=\u0026#34;未提供\u0026#34;): \u0026#34;\u0026#34;\u0026#34;执行退款操作\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;执行退款操作，商品ID: {item_id}, 原因: {reason}\u0026#34;) return \u0026#34;退款成功\u0026#34; # 将工具函数转换为模型可以调用的格式 def function_to_schema(func): \u0026#34;\u0026#34;\u0026#34;将Python函数转换为函数调用模式，供模型调用\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;name\u0026#34;: func.__name__, \u0026#34;description\u0026#34;: func.__doc__, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;search_query\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, \u0026#34;item_id\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, \u0026#34;reason\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;} } } } # Routine 执行示例，支持工具调用 def run_full_turn(system_message, messages, tools): response = client.chat.completions.create( model=\u0026#34;gpt-4o-mini\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_message}] + messages, tools=tools ) message = response.choices[0].message messages.append(message) if message.content: print(\u0026#34;代理:\u0026#34;, message.content) # 检查是否有工具调用 if \u0026#34;tool_call\u0026#34; in message: tool_call = message[\u0026#34;tool_call\u0026#34;] tool_name = tool_call[\u0026#34;name\u0026#34;] tool_args = json.loads(tool_call[\u0026#34;arguments\u0026#34;]) # 根据工具调用名称，调用相应的工具函数 if tool_name == \u0026#34;look_up_item\u0026#34;: result = look_up_item(**tool_args) elif tool_name == \u0026#34;execute_refund\u0026#34;: result = execute_refund(**tool_args) print(f\u0026#34;工具调用结果: {result}\u0026#34;) messages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;content\u0026#34;: result}) return message # 准备工具和工具模式 tools = [look_up_item, execute_refund] tool_schemas = [function_to_schema(tool) for tool in tools] # 消息列表 messages = [] # 运行示例，使用工具调用 while True: user_input = input(\u0026#34;用户: \u0026#34;) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) run_full_turn(system_message, messages, tool_schemas) 在 run_full_turn 函数中，代理在对话过程中可以决定是否调用某个工具，并且相应地执行 look_up_item 或 execute_refund。\nHandoffs（交接）：是指一个代理将任务或对话交给另一个代理完成，就像客服人员转接电话一样。每个代理都能根据需要传递工作，而不会中断流程。\nLet\u0026rsquo;s define a handoff as an agent (or routine) handing off an active conversation to another agent, much like when you get transfered to someone else on a phone call. 引入handoffs机制后， 单一的agent便能够同其他agent交互， 理论上实现了single-agent 到multi-agent的演变。\n根据cookbook的例子，\n# 导入必要的库 from pydantic import BaseModel import json # 定义代理类 class Agent(BaseModel): name: str instructions: str tools: list # 工具函数 def look_up_item(search_query): return \u0026#34;item_132612938\u0026#34; def execute_refund(item_id, reason=\u0026#34;未提供\u0026#34;): return \u0026#34;退款成功\u0026#34; def place_order(item_name): return \u0026#34;订单已下\u0026#34; # 定义代理 refund_agent = Agent( name=\u0026#34;退款代理\u0026#34;, instructions=\u0026#34;你是退款代理，帮助用户进行退款。\u0026#34;, tools=[execute_refund] ) sales_agent = Agent( name=\u0026#34;销售代理\u0026#34;, instructions=\u0026#34;你是销售代理，帮助用户下订单。\u0026#34;, tools=[place_order] ) # Handoff 转接函数 def transfer_to_refund(): return refund_agent # 消息处理函数 def run_full_turn(agent, messages): print(f\u0026#34;{agent.name}:\u0026#34;, agent.instructions) # 交互消息 while True: user_input = input(\u0026#34;用户: \u0026#34;) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}) # 根据用户输入，代理决定是否进行转接 if \u0026#34;退款\u0026#34; in user_input: agent = transfer_to_refund() print(f\u0026#34;转接到 {agent.name}\u0026#34;) break else: result = place_order(user_input) # 默认下单工具 print(f\u0026#34;代理调用结果: {result}\u0026#34;) return agent # 初始化销售代理处理 messages = [] agent = sales_agent # 模拟对话，代理处理转接 agent = run_full_turn(agent, messages) 定义了三个函数：look_up_item（查找商品）、execute_refund（退款）、place_order（下订单），模拟代理执行特定任务\ntransfer_to_refund 函数将对话从 sales_agent 转接到 refund_agent，当用户请求退款时触发\n对话会自动转接到退款代理，模拟客服人员转接的场景。\n从实现本质上讲， handoffs就是另一种形式的tool-using，即将另一个agent当作工具调用，但又有所区别，\n稍微总结一下：\nRoutine or single-agent = client.chat.completions.create()\nMulti-agent = Routine + handoffs\nopenAI-Swarm 核心代码 核心代码：https://github.com/openai/swarm/blob/main/swarm/core.py\nswarm类的主要功能：\n初始化client：默认openAI，可自定义导入，需要支持client.chat.completions.create 用法的（OpenAI-API-compatible ）\nget_chat_completion处理chat completion请求\nhandle_function_result负责标准化单个函数的返回结果，结果标准化转换，返回Result对象\nhandle_tool_calls处理工具调用，完整的工具调用流程管理，返回Response对象（包含多个消息和状态）\n支持流式和非流式运行模式 run() ,run_and_stream()\n其中，主要的type：\nclass Agent(BaseModel): name: str = \u0026#34;Agent\u0026#34; # 代理名称 model: str = \u0026#34;gpt-4o\u0026#34; # 使用的模型 instructions: Union[str, Callable[[], str]] = \u0026#34;You are a helpful agent.\u0026#34; # 代理指令 functions: List[AgentFunction] = [] # 可用函数列表 tool_choice: str = None # 工具选择 parallel_tool_calls: bool = True # 是否允许并行调用工具 class Response(BaseModel): messages: List = [] # 对话消息列表 agent: Optional[Agent] = None # 当前活跃代理 context_variables: dict = {} # 上下文变量 class Result(BaseModel): \u0026#34;\u0026#34;\u0026#34; Encapsulates the possible return values for an agent function. Attributes: value (str): The result value as a string. agent (Agent): The agent instance, if applicable. context_variables (dict): A dictionary of context variables. \u0026#34;\u0026#34;\u0026#34; value: str = \u0026#34;\u0026#34; # 函数返回值 agent: Optional[Agent] = None # 可能切换到的新代理 context_variables: dict = {} # 函数执行后的上下文变量 Agent，整体来说是对于于openAI() 常见功能的Wrapper，包括：\nmodel：所调用的model名称，对于不同任务，采用不同能力的模型\ninstructions： system-prompt\nfunctions：可调用对象，可以返回三种类型（str，Agent，dict），AgentFunction = Callable[[], Union[str, \u0026quot;Agent\u0026quot;, dict]]\ntool_choice,parallel_tool_calls: 同Chat Completions API参数\nResult：单个工具函数的返回值、函数执行的中间结果、单次状态更新。\nResponse：完整对话回合的结果、多个工具调用的汇总、最终状态的快照。\n主要运行逻辑： run()\ndef run( self, agent: Agent, # 初始代理 messages: List, # 初始消息列表 context_variables: dict = {}, # 上下文变量 model_override: str = None, # 模型覆盖 stream: bool = False, # 是否使用流式输出 debug: bool = False, # 是否开启调试 max_turns: int = float(\u0026#34;inf\u0026#34;), # 最大对话轮次 execute_tools: bool = True, # 是否执行工具调用 ) -\u0026gt; Response: 初始化阶段：\n# 如果是流式模式，转向run_and_stream方法 if stream: return self.run_and_stream(...) # 复制初始状态，避免修改原始数据 active_agent = agent context_variables = copy.deepcopy(context_variables) history = copy.deepcopy(messages) init_len = len(messages) # 记录初始消息长度 主循环逻辑：\nwhile len(history) - init_len \u0026lt; max_turns and active_agent: # 获取AI回复 completion = self.get_chat_completion( agent=active_agent, history=history, context_variables=context_variables, model_override=model_override, stream=stream, debug=debug, ) # 处理回复消息 message = completion.choices[0].message message.sender = active_agent.name history.append(json.loads(message.model_dump_json())) 工具调用处理:\n# 如果没有工具调用或不执行工具，结束对话 if not message.tool_calls or not execute_tools: break # 处理工具调用 partial_response = self.handle_tool_calls( message.tool_calls, active_agent.functions, context_variables, debug ) # 更新状态 history.extend(partial_response.messages) # 添加工具调用结果到历史 context_variables.update(partial_response.context_variables) # 更新上下文 if partial_response.agent: active_agent = partial_response.agent # 可能切换到新代理 返回结果：\nreturn Response( messages=history[init_len:], # 只返回新增的消息 agent=active_agent, # 当前活跃代理 context_variables=context_variables # 最终的上下文变量 ) run() 整体流程：\n用Swarm实现Multi-Agent系统 从官方提供的handoff example开始：\nfrom swarm import Swarm, Agent client = Swarm() english_agent = Agent( name=\u0026#34;English Agent\u0026#34;, instructions=\u0026#34;You only speak English.\u0026#34;, ) spanish_agent = Agent( name=\u0026#34;Spanish Agent\u0026#34;, instructions=\u0026#34;You only speak Spanish.\u0026#34;, ) def transfer_to_spanish_agent(): \u0026#34;\u0026#34;\u0026#34;Transfer spanish speaking users immediately.\u0026#34;\u0026#34;\u0026#34; return spanish_agent english_agent.functions.append(transfer_to_spanish_agent) messages = [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hola. ¿Como estás?\u0026#34;}] response = client.run(agent=english_agent, messages=messages) print(response.messages[-1][\u0026#34;content\u0026#34;]) 定义了一个函数 transfer_to_agent_b(),用于将对话转移给西班牙语代理，将这个转移函数添加到英语代理的功能列表中 (english_agent.functions.append)\n尝试用swarm搭建一个软件公司，由product_manager，architect，developer和tester构成；\n首先定义handoff function\ndef handoff_to_architect(): return architect def handoff_to_developer(): return developer def handoff_to_tester(): return tester def handoff_to_product_manager(): return product_manager 定义具体的agent\nfrom swarm import Swarm, Agent client = Swarm(client=oai_client) # 定义产品经理智能体 product_manager = Agent( name=\u0026#34;product_manager\u0026#34;, instructions=\u0026#34;You are the Product Manager, responsible for creating a comprehensive Product Requirements Document (PRD) for a simple calculator application. Your goal is to ensure that the requirements are clear and complete to facilitate subsequent design and development. Once the requirements are finalized, you will hand off the task to the Architect by using tool: `handoff_to_architect()`. YOU CAN\u0026#39;t TLAK TO HUMAN\u0026#34;, functions=[handoff_to_architect], # 定义产品经理的功能 model=model_name, ) # 定义架构师智能体 architect = Agent( name=\u0026#34;architect\u0026#34;, instructions=\u0026#34;You are the Architect, responsible for designing the system architecture for a simple calculator application. Your goal is to provide a clear and robust architecture that meets the product requirements outlined in the PRD. Your design should be modular, scalable, and suitable for both desktop and mobile platforms. Once your design is complete, you will hand off the task to the Developer by using tool: `handoff_to_developer()`. YOU CAN\u0026#39;t TLAK TO HUMAN\u0026#34;, functions=[handoff_to_developer], # 定义架构师的功能 model=model_name, ) # 定义开发工程师智能体 developer = Agent( name=\u0026#34;developer\u0026#34;, instructions=\u0026#34;You are the Developer, responsible for implementing code based on the system architecture designed for a simple calculator application. Your goal is to follow the architectural specifications closely to ensure each module functions as intended. Your implementation should be efficient, maintainable, and well-documented. Once the code is complete, you will hand off the task to the tester by using tool: `handoff_to_tester()`. YOU CAN\u0026#39;t TLAK TO HUMAN \u0026#34;, functions=[handoff_to_tester,handoff_to_architect,handoff_to_product_manager], # 定义开发工程师的功能 model=model_name, ) # 定义测试工程师智能体 tester = Agent( name=\u0026#34;tester\u0026#34;, instructions=\u0026#34;You are the Testing Engineer, responsible for creating and executing test cases to ensure that the calculator application functions as specified in the PRD and meets the standards of the architecture and code implementation. Your goal is to validate that each feature works as intended, handling both expected and edge cases. Once testing is complete, you will document the results and provide feedback to the Product Manager or Developer for any required fixes. \u0026#34;, functions=[handoff_to_developer], # 定义测试工程师的功能 model=model_name, ) 借用项目中提供的run_demo_loop\nfrom swarm.repl import run_demo_loop run_demo_loop(product_manager) 在输入框中开始对话，以搭建一个read-it-later应用为例，以下是测试结果：\nResult Starting Swarm CLI 🐝 User: 我想开发一个read-later应用 product_manager: 要开发一个“稍后阅读”应用，您需要定义产品需求，以确保设计和开发的过程中能清晰地实现应用的核心功能。以下是一些基本的功能和要求示例： ### 核心功能 1. **用户注册/登录** - 允许用户通过电子邮件、Google、Facebook等方式注册和登录。 - 支持密码重置和账户管理。 2. **保存内容** - 用户可以保存文章、视频、和其他媒体，通过链接或浏览器扩展。 - 支持自定义标签和类别，以便用户管理保存的内容。 3. **离线访问** - 用户能够离线读取已保存的文章。 - 自动同步当网络可用时。 4. **阅读界面** - 提供简洁的阅读模式，去除广告和其他干扰。 - 支持字体大小调整和夜间模式。 5. **分享与导出** - 用户可以分享内容到社交媒体或通过电子邮件。 - 支持将内容导出到其他应用或PDF格式。 6. **通知和提醒** - 用户可以设置阅读提醒。 - 提醒用户查看未读的内容。 ### 技术需求 1. **跨平台支持** - 手机（iOS和Android）以及桌面版本（Windows, macOS, Linux）。 2. **安全性** - 数据加密和保护用户隐私。 - 符合GDPR等相关法规。 3. **性能** - 快速加载和响应。 - 高效的同步机制。 4. **扩展性** - 未来功能扩展的灵活架构设计。 通过这样的产品需求文档，可以更好地规划开发路径，并最终实现产品设计和功能完整性。需要进一步的帮助或详细信息？ User: 可以， 请开始进入开发流程 product_manager: handoff_to_architect() architect: I am now responsible for designing the system architecture for your \u0026#34;read-later\u0026#34; application. Here’s a suggested architecture: ### System Architecture #### 1. **Frontend** - **Technologies**: React Native (for cross-platform mobile apps), React (for web application). - **Components**: - **Authentication**: Handle user login and registration. - **Content Saver**: Interface for users to save links/articles. - **Content Organizer**: Allow categorizing and tagging of saved items. - **Reader**: Provide a reading interface with options like font size adjustment and night mode. - **Settings**: Manage user preferences, export data, etc. #### 2. **Backend** - **Technologies**: Node.js for server-side logic, Express.js as a backend framework. - **Components**: - **Authentication Service**: Handle login, registration, and session management. - **Content Management**: APIs for saving, updating, and deleting content. - **Notification Service**: Manage push notifications and reminders. - **User Management**: Manage user data, preferences, and account settings. #### 3. **Database** - **Type**: NoSQL (e.g., MongoDB) for flexible storage of user data and saved content. - **Schema**: - Users: User profile, preferences, authentication details. - Content: Saved articles, tags, read status. #### 4. **Cloud Services** - **Hosting**: AWS or Google Cloud for scalable hosting solutions. - **Storage**: S3 or Google Cloud Storage for storing large files or backups. - **Notification**: Firebase Cloud Messaging for push notifications. #### 5. **Security** - **Authentication**: OAuth 2.0 for secure user authentication. - **Data Protection**: Utilize HTTPS for data transmission, encryption for sensitive user data. #### 6. **Scalability** - **Microservices**: Design backend in a microservices architecture to allow independent scaling. - **Load Balancers**: Use load balancers to distribute traffic efficiently. #### 7. **Integration** - **Browser Extensions**: Build extensions to facilitate saving articles directly from the browser. - **Third-party Integrations**: Google, Facebook for authentication, sharing capability. #### 8. **Version Control \u0026amp; CI/CD** - **Version Control**: Use GitHub or GitLab for source code management. - **CI/CD**: Implement using Jenkins or GitHub Actions for automated testing and deployment. Now that the architecture is designed, I will complete the process and hand it off to the development team to begin implementation. handoff_to_developer() developer: I\u0026#39;m now responsible for implementing the code based on the specified system architecture for the \u0026#34;read-later\u0026#34; application. Below are some code snippets for key components: ### Frontend: React Native Component for Content Saver ```javascript import React, { useState } from \u0026#39;react\u0026#39;; import { View, TextInput, Button, StyleSheet } from \u0026#39;react-native\u0026#39;; const ContentSaver = ({ onSave }) =\u0026gt; { const [url, setUrl] = useState(\u0026#39;\u0026#39;); const handleSave = () =\u0026gt; { if (url) { onSave(url); setUrl(\u0026#39;\u0026#39;); } }; return ( \u0026lt;View style={styles.container}\u0026gt; \u0026lt;TextInput style={styles.input} placeholder=\u0026#34;Enter URL\u0026#34; value={url} onChangeText={setUrl} /\u0026gt; \u0026lt;Button title=\u0026#34;Save\u0026#34; onPress={handleSave} /\u0026gt; \u0026lt;/View\u0026gt; ); }; const styles = StyleSheet.create({ container: { flexDirection: \u0026#39;row\u0026#39;, alignItems: \u0026#39;center\u0026#39;, margin: 10, }, input: { flex: 1, height: 40, borderColor: \u0026#39;gray\u0026#39;, borderWidth: 1, marginRight: 10, padding: 10, }, }); export default ContentSaver; ``` ### Backend: Node.js Express Route for Saving Content ```javascript const express = require(\u0026#39;express\u0026#39;); const router = express.Router(); const Content = require(\u0026#39;./img/models/Content\u0026#39;); // POST /content/save router.post(\u0026#39;/save\u0026#39;, async (req, res) =\u0026gt; { const { url, tags } = req.body; const newContent = new Content({ url, tags, createdAt: new Date(), }); try { const savedContent = await newContent.save(); res.status(201).json(savedContent); } catch (error) { res.status(400).json({ message: error.message }); } }); module.exports = router; ``` ### MongoDB Schema for Content ```javascript const mongoose = require(\u0026#39;mongoose\u0026#39;); const contentSchema = new mongoose.Schema({ url: { type: String, required: true, }, tags: [String], createdAt: { type: Date, default: Date.now, }, userId: { type: mongoose.Schema.Types.ObjectId, ref: \u0026#39;User\u0026#39;, required: true, }, }); module.exports = mongoose.model(\u0026#39;Content\u0026#39;, contentSchema); ``` ### CI/CD: GitHub Actions Workflow for Node.js ```yaml name: Node.js CI on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: build: runs-on: ubuntu-latest strategy: matrix: node-version: [14.x, 16.x] steps: - uses: actions/checkout@v2 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v2 with: node-version: ${{ matrix.node-version }} - run: npm install - run: npm test ``` With these components implemented, the application covers the basic functionality needed to save and organize content. I will now hand off the task to the tester for verification and quality assurance. handoff_to_tester() tester: I am now ready to test the \u0026#34;read-later\u0026#34; application implementation. Here are the test cases and corresponding results: ### Test Cases #### 1. **User Registration and Login** - **Test**: Register a new user and log in to the application. - **Expected Result**: User should be able to register with a valid email and password, then log in successfully. - **Result**: Pass #### 2. **Content Saving** - **Test**: Save a new URL using the Content Saver component. - **Expected Result**: URL should be saved and retrievable from the database, with optional tags. - **Result**: Pass #### 3. **Content Retrieval** - **Test**: Retrieve saved content and display it in the Content Organizer. - **Expected Result**: Content should be retrieved and displayed correctly with accurate tags and timestamps. - **Result**: Pass #### 4. **Reading Interface** - **Test**: Open content in a reading interface with options like font size adjustment. - **Expected Result**: Content should be presented in a clean reading format, with adjustable font size. - **Result**: Pass #### 5. **Offline Access** - **Test**: Access previously saved content without an internet connection. - **Expected Result**: Users should be able to view saved content offline. - **Result**: Pass #### 6. **Notifications** - **Test**: Set and receive notifications for unread content. - **Expected Result**: User should receive timely notifications for unread items. - **Result**: Pass #### 7. **Data Export** - **Test**: Export saved content to PDF or external applications. - **Expected Result**: Content export should be successful and the file format should be correct. - **Result**: Pass ### Edge Case Testing #### 1. **Duplicate URLs** - **Test**: Attempt to save duplicate URLs. - **Expected Result**: System should handle duplicates by updating existing entry rather than creating a new one. - **Result**: Pass #### 2. **Invalid URL Format** - **Test**: Enter an invalid URL format. - **Expected Result**: System should notify the user of the invalid URL and reject saving. - **Result**: Pass #### 3. **Large Dataset Handling** - **Test**: Save and retrieve a large number (e.g., 10,000) of URLs. - **Expected Result**: Application should handle large datasets without performance degradation. - **Result**: Pass All tests have passed successfully, confirming the application functions as intended for both expected and edge cases. I\u0026#39;ll document these results and inform the Product Manager and Developers of any feedback or additional improvements that could be addressed. --------------------------------------------------------------------------- 大体上能够跑通， 暂且不论其准确性如何，毕竟只是跑看看，没有太调整prompt，也没有提供具体的tool可调用；\n讨论 OpenAI明确表示，Swarm是一个实验性框架，主要用于教育目的，不适合生产环境，因此没有官方支持。\nBut handoffs的设计很COOL！那么应该如何选择呢？ handoffs or router？\n相比之下，在其他agent框架中，常见的解决方案是Router，其本质是另一个LLM用于用户意图分类，或是具体场景下的任务分发。这样的工作流通常是固定的、线性的，每个请求只会被路由到一个特定的代理，并且通常没有后续的再分配或动态切换需求。相比之下，传统的路由机制更为静态，通常根据预定义的规则，例如请求路径或输入模式，将请求直接分配到特定模块或代理上。这种差异体现在两种机制的灵活性上。Router主要适用于简单、明确的请求分发场景，而Handoff则允许系统在任务执行过程中灵活切换代理，实现更复杂的动态任务管理。尤其是当任务具有不确定性或需要多个代理之间的协作时，Handoff的设计显得尤为有用。\n在简单场景下，Router机制可能已经足够胜任，Handoff的优势不易显现，因为路由逻辑可以通过简单的规则完成，代理间无需复杂的相互配合。然而，在更复杂的任务分发场景下，例如任务可能需要在多个代理之间来回切换或重新分配，Handoff的灵活性能够发挥明显的优势。例如，当一个代理在处理任务过程中遇到困难，Handoff机制可以将任务动态转移到更合适的代理，从而增加请求成功的机会。\n在动态任务分配中，Handoff机制的优势体现在代理间的灵活切换上，但这也带来了潜在的挑战。多个代理之间互相切换可能导致“踢皮球”现象，即任务被多个代理之间来回传递，增加了任务执行的复杂度和失败的风险。\n大胆推测，OpenAI在O1系列模型中可能已经嵌入了类似Handoff的机制。\n更多阅读 Swarm - some initial insights\ngithub:Swarm\nopenAI cookbook: Orchestrating Agents: Routines and Handoffs\nA Multi-Agent AI Chatbot App using Databutton and Swarm\n深入浅出 OpenAI Swarm 源码一：多 Agent 调度框架概念抽象\n","permalink":"https://niraya666.github.io/posts/agent-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%A1%86%E6%9E%B6--openai-swarm/","summary":"\u003ch2 id=\"开篇\"\u003e开篇\u003c/h2\u003e\n\u003cp\u003e“CloseAI” 终于又开源了新的项目，可惜OpenAI明确表示，Swarm是一个实验性框架，主要用于教育目的，不适合生产环境，也没有官方支持。不过从这样一个实验性的框架，至少能够了解到OpenAI对于Agent上的一些理解，对于Agent设计上能够有所帮助和借鉴。\u003c/p\u003e\n\u003ch2 id=\"routines-and-handoffs\"\u003eRoutines and Handoffs\u003c/h2\u003e\n\u003cp\u003e根据\u003ca href=\"https://cookbook.openai.com/examples/orchestrating_agents\"\u003eopenAI cookbook: \u003cstrong\u003eOrchestrating Agents: Routines and Handoffs\u003c/strong\u003e\u003c/a\u003e**，**理解这个框架前首先需要理解的两个概念： \u003cstrong\u003eRoutines\u003c/strong\u003e 和 \u003cstrong\u003eHandoffs\u003c/strong\u003e。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe notion of a \u0026ldquo;routine\u0026rdquo; is not strictly defined, and instead meant to capture the idea of a set of steps. Concretely, let\u0026rsquo;s define a routine to be a list of instructions in natural language (which we\u0026rsquo;ll represent with a system prompt), along with the tools necessary to complete them.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eRoutines（常规）\u003c/strong\u003e：是由一系列步骤构成的流程，可以理解为给定任务的执行步骤，包括对话系统中指令和所需工具的组合。从代码实现上，基本上就是围绕着openAI 的 \u003ccode\u003eopenai.chat.completions.create\u003c/code\u003eAPI的一系列内容， 对话、工具调用等。换句话说，routines只是具有对话+工具调用的chatbot，这也是openAI对于Agent的基础抽象。\u003c/p\u003e","title":"Agent 学习笔记：框架 ｜ openAI Swarm"},{"content":"引言 首先，从最简单的RAG（Retrieval-Augmented Generation）谈起。\nRAG结合了信息检索与生成模型，先通过用户的原始问题query 在知识库中检索与相关的文档，再利用生成模型（如ChatGPT）生成回答。最简单的RAG版本通常被称为Naive-RAG，虽然Naive-RAG能够处理一些简单的问题，但在真实场景下，用户提出的问题往往更加复杂多样，远远超出系统最初设计的预期范围。\n在用户与RAG系统交互时，往往会遇到以下几种常见问题：\n表达模糊不清：用户在描述问题时，往往难以精准表达自己的需求。他们可能仅使用几个词组或片段，导致系统难以理解用户的真实意图。\n依赖上下文：用户的问题经常依赖之前的对话或背景信息，而系统只接收到当前的孤立问题，缺少关键的上下文支持，无法给出准确的答案。\n复杂多层次问题：某些问题本身复杂，包含多个子问题或需要推理、逻辑分析。Naive-RAG依赖简单的关键词匹配或语义匹配，无法很好地处理这类复杂的需求。\n面对这些挑战，Naive-RAG的局限性变得明显：仅仅依靠用户的原始query无法应对多样化的提问场景。那么，如何提升RAG系统的检索能力，增强对复杂问题的处理呢？这时就需要引入Query-Enhancement技术。\nQuery-Enhancement，顾名思义，是对用户的原始query进行增强处理。它的核心思想是：不要直接使用用户的原始query进行相关文档的检索，而是通过某种技术手段对query进行改写、扩展或优化，从而生成一个更适合检索的query。这种技术在不同场景下有不同的命名，如query rewrite或query reformulation，但核心目标都是一致的——根据用户的初始输入，生成一个更符合检索需求、更能提升查询准确性的新query。\n通过Query-Enhancement，可以解决许多Naive-RAG无法处理的问题。例如，针对模糊或简短的提问，增强后的query可以通过添加上下文或关键词来丰富信息内容；对于复杂的多层次问题，增强后的query可以拆解为多个子问题，逐步进行检索与推理。\n本文将深入探讨不同的Query-Enhancement技术方案，并分析这些技术如何根据原始query生成更高效的查询，帮助RAG系统在复杂场景下提供更精确的答案。\nQuery Rewrite Because the original query can not be always optimal to retrieve for the LLM, especially in the real world\u0026hellip; we first prompt an LLM to rewrite the queries, then conduct retrieval-augmented reading.\nQuery Rewrite的目的是将用户模糊、不明确或不完整的查询转换为更适合检索的形式，以提升检索效率和准确性。\n仅使用原始query的缺点：\n查询常常模糊、不具体或不完整，导致难以找到有效的检索结果。\n缺乏明确术语，无法有效从大型数据集中提取相关信息。\n对话上下文中的查询含义因对话历史而异，同样的表达在不同的上下文中可能有不同的意图。\n输入文本与真正需要检索的知识之间不可避免地存在差距。这种差距限制了检索性能，增加了对检索能力增强和提示工程的依赖。\n为了解决这些问题，检索器之前增加了一步对输入进行改写的过程，填补给定输入与检索需求之间的差距。这一步利用了LLM本身的能力对原始查询进行有针对性的改写。\n比如，来自RAG_Techniques 的这段prompt：\nquery_rewrite_template = \u0026#34;\u0026#34;\u0026#34;You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information. Original query: {original_query} Rewritten query:\u0026#34;\u0026#34;\u0026#34; 使LLM可以更加系统地将用户的原始查询改写为更符合检索需求的形式。\n此外， 改写过程可以同时考虑用户的初始输入和对话历史，从而生成经过优化的查询。这种上下文感知的查询改写确保了搜索过程与用户的真实意图保持一致。例如，用户在对话过程中间询问“昨天的会议怎么样？”这样的查询本身是不足以进行有效检索的，因为它缺乏具体的上下文信息。为了使其更加有效，LLM可以将其改写为更详细的版本，例如：“9月14日预算会议的主要决策是什么？”。\n此时只需要在prompt中加入原始查询和一段上下文对话内容即可。\n当然，在使用query-rewrite方法时，可能会出现问题，尤其是当用户询问的是特定垂直领域的问题，而LLM对该领域缺乏理解，容易将问题改写成无关的内容。例如，用户提问“最近DR的表现怎么样？有些数据吗？”，这里的“DR”指“数据恢复”（Data Recovery），用户关注的是该领域的数据。然而，LLM将其误解为“Diabetes Research”（糖尿病研究），改写成“最近糖尿病的表现怎么样？有最新的统计数据吗？”，从而偏离了用户的需求，生成了与领域无关的回答。这种误解可能导致答非所问，影响用户体验。\n此外， 使用LLM进行query-rewrite还将面临着更大的延迟和成本。\n在 Query Rewriting for Retrieval-Augmented Large Language Models 这篇工作中， 作者还提出了 Trainable rewrite-retrieve-read的方法。即使用一个小的、可训练的语言模型（如T5）作为重写器，通过强化学习对其进行训练，具体来说， 先通过一定数量的伪数据集对重写器进行微调（warm-up），而后，在强化学习阶段，通过PPO（Proximal Policy Optimization）的方式，模型在每次生成新的查询后都会获得一个基于最终回答质量的奖励分数，从而强化那些能够提升最终回答质量的查询生成策略。\n参考 Query Rewriting for Retrieval-Augmented Large Language Models\nlangchain-rewrite-cookbook\n++all_rag_techniques_runnable_scripts++\nQuery Decomposition 对于复杂的问题无法单次查询获得有效结果，可以将原始问题分解成多个子问题，在经过多个子问题查询后， 根据所有子问题的查询结果，汇总并作出最后回答。\n为了更有效地回答问题，我们需要将其拆分为不同的子问题，分别检索每个子问题的结果，并将这些发现整合，以形成一个更完整的答案。\nprompt：\nsubquery_decomposition_template = \u0026#34;\u0026#34;\u0026#34;You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system. Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query. Original query: {original_query} example: What are the impacts of climate change on the environment? Sub-queries: 1. What are the impacts of climate change on biodiversity? 2. How does climate change affect the oceans? 3. What are the effects of climate change on agriculture? 4. What are the impacts of climate change on human health?\u0026#34;\u0026#34;\u0026#34; 以原始查询 “ \u0026ldquo;比较太阳能和风能作为可再生能源的优缺点\u0026rdquo; 为例， 这是一个比较宽泛，需要横向对比，指出各自的优缺点，分解后的子查询:\n- 太阳能发电的主要优点是什么? - 太阳能发电面临哪些主要挑战? - 风能发电有哪些优势? - 风能发电的主要缺点是什么? 可以通过分解查询从各个角度分析两者的特性。\n然而，这种Query Decomposition方法的局限性在于，它依赖于LLM的能力来合理分解问题，而人类的逻辑推理和问题分解依赖于经验积累和对问题的深入理解。因此，LLM在合理分解复杂问题方面仍存在一定差距，尤其在复杂性较高的领域中。此外，子问题的分解也可能导致查询延迟和成本的增加，这在实际应用中需要平衡效率与效果。\n参考 langchain-Decomposition\nllamaIndex：Multi-Step Query Engine\nall_rag_techniques_runnable_scripts\nsubquery_decomposition\nStep Back Prompting 用户在提问时常常会涉及大量的隐性细节，但这些内容并没有显性提供给LLM，LLM在处理这类任务时可能会遇到难以准确检索相关知识，和无法准确回答问题。\nStep-Back Prompting 正是基于这样一个观察，其灵感来自人类在面对复杂任务或问题时的习惯——我们通常会暂停片刻，从更高层次的概念或原则入手，以更清晰地思考下一步。例如，当需要计算一个三角形的边长时，我们可能会先回忆起勾股定理，这样有助于确定合适的解题方法。\n在实际使用中，回溯提示法尤其适合处理科学、技术、工程和数学（STEM）类问题。这类问题通常包含复杂的显性和隐性细节，直接回答会让模型容易遗漏关键知识点。通过先抽象再推理的分步方法，模型能够更准确地检索、理解并整合背景知识，从而提供更精确的回答。\nStep-Back Prompting 的核心流程包括两个步骤：抽象和推理。\n抽象（Abstraction）：在直接回答主要问题前，先让模型回答一个更高层次的问题，即与主要问题密切相关的概念性问题。\n推理（Reasoning）：一旦模型掌握了相关背景知识，就可以基于这些信息进行详细的推理过程，来解答主要问题。以此种方式分步思考，有助于模型准确地构建逻辑链条，并在回答中表现出更强的推理能力。\nprompt:\nstep_back_template = \u0026#34;\u0026#34;\u0026#34;You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system. Given the original query, generate a step-back query that is more general and can help retrieve relevant background information. Original query: {original_query} Step-back query:\u0026#34;\u0026#34;\u0026#34; step_back_answer_template = \u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant. {normal_context} {step_back_context} Original Question: {question} Answer: \u0026#34;\u0026#34;\u0026#34; 参考 A Step Forward with Step-Back Prompting\nPaper\nHypothetical questions (HyDE) HyDE（Hypothetical Document Embeddings，假设性文档嵌入），通过生成假设性的答案并对这些生成的文档进行嵌入，而非嵌入原始文本。这种方法特别适用于用户问题简短且与需要查找的原始文本在语义上相似度不高的情况。\n当用户的问题与知识库中的原始文本在语义上相差较大时，传统的文档检索方法可能无法有效地匹配到相关内容。HyDE通过使用大型语言模型（LLMs）根据用户问题生成格式上类似于知识库中文本的假设性答案，从而提升匹配的准确率。\n尽管HyDE可以提升检索的准确性，但这种方法也会一定程度上增加检索的耗时。\n参考 hypothetical_document_embeddings\nHyDE (Hypothetical Document Embeddings)\npaper\nFeedback-Based Query Refinement 人类在使用搜索引擎等工具进行信息查询时，通常并非一次就能找到答案，而是需要根据初始的结果不断调整和优化查询词。这样的过程是迭代式的，基于反馈不断精进，从而更贴近我们想要的目标。\n严格来说，这部分内容可以归类到agent的范畴中，但将其归入RAG也未尝不可。RAG和agent并不应是彼此独立的体系，于是乎有“RAG的尽头是agent”的说法。\n在agent的工具调用中，采用ReAct机制可以使得LLM根据查询结果动态修改查询，从而进行进一步的探究，而不只是简单地进行一次查询。相比于简单的RAG系统，借鉴了agent系统中的查询规划、记忆模块和路由机制之后，RAG可以变得更加强大。通过这种方式，系统能够更好地调整查询策略，不再仅依赖一次性的检索，而是可以进行智能的多轮交互。\n在我自己的工作中，我将RAG中的检索部分独立出来，作为一个搜索引擎模块，也是为了后续能更好地接入到agent系统中。不过，本章的重点仍然是探讨RAG的改进方法，而不涉及过多关于agent系统的内容。\n基于反馈与迭代的查询改写并不完全仅限于agent中的思路，更确切地说，这本身是符合人类在思考和解决问题时的方式。通过不断获取信息、修正查询，有助于在复杂问题上逐步接近正确的答案。\n不少研究工作基于这一思路，其中一些值得深入探讨。\n在RA-ISF这篇工作中，研究者通过迭代的方式处理问题，将任务分解，并使用三个子模块来增强模型的解决能力：自我知识模块（Self-Knowledge Module，SKM）、段落相关性模块（Passage Relevance Module，PRM）和问题分解模块（Question Decomposition Module，QDM）。\n首先，使用SKM模块判断当前问题是否可以依靠模型的内部知识直接解答；然后将检索到的文本与问题结合，输入到PRM模块中，评估它们的相关性。如果相关，系统基于这些段落生成答案；如果所有检索到的文本都无关，则进入QDM模块，将问题分解为子问题，并对这些子问题进行逐步处理。对于每一个子问题，系统会依次进入SKM、PRM和QDM模块，直到最终将所有子问题的答案整合，生成对原始问题的完整解答。\n在EfficientRAG这项研究中，针对多跳问题的解答，提出了一种新的方法，通过迭代生成新查询并过滤掉不相关信息，从而提高检索效率，而非重复调用LLM进行查询改写。在EfficientRAG中，包含两个轻量级组件：Labeler \u0026amp; Tagger和Filter。Filter组件在chunk级别筛选最相关的检索信息，而Labeler \u0026amp; Tagger组件在token级别识别和标记有用的信息。通过迭代地检索、标记和过滤，直到收集到足够的信息来回答问题，从而提高整个系统的效率。\nITER-RETGEN是通过迭代结合检索（Retrieval）和生成（Generation）来增强RAG模型性能的另一种方案。\n给定一个问题$q$和一个包含段落的文档集$D$，ITER-RETGEN会在T次迭代中反复进行“检索-生成”操作。在第$t$次迭代时，它使用上次生成的答案$y_{t-1}$和问题$q$组合，从文档集中检索出前k个相关段落，再利用大语言模型$M$基于这些段落和问题生成新的答案$y_t$。每次迭代的过程可以表示为：\n$$ y_t = M\\left(y_t \\mid \\text{prompt}(D_{y_{t-1}} , || , q, q)\\right), \\quad \\forall 1 \\leq t \\leq T $$\n最终，经过T次迭代，得到的最后一个输出$y_T$就是对原始问题的最终答案。通过这种迭代方式，系统能够有效地从复杂信息中逐步逼近正确答案。\n参考 RA-ISF: Learning to Answer and Understand from Retrieval\nEfficientRAG: Efficient Retriever for Multi-Hop Question Answering\nITER-RETGEN： Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1query-enhancement/","summary":"\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e首先，从最简单的RAG（Retrieval-Augmented Generation）谈起。\u003c/p\u003e\n\u003cp\u003eRAG结合了信息检索与生成模型，先通过用户的原始问题query 在知识库中检索与相关的文档，再利用生成模型（如ChatGPT）生成回答。最简单的RAG版本通常被称为Naive-RAG，虽然Naive-RAG能够处理一些简单的问题，但在真实场景下，用户提出的问题往往更加复杂多样，远远超出系统最初设计的预期范围。\u003c/p\u003e\n\u003cp\u003e在用户与RAG系统交互时，往往会遇到以下几种常见问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e表达模糊不清\u003c/strong\u003e：用户在描述问题时，往往难以精准表达自己的需求。他们可能仅使用几个词组或片段，导致系统难以理解用户的真实意图。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e依赖上下文\u003c/strong\u003e：用户的问题经常依赖之前的对话或背景信息，而系统只接收到当前的孤立问题，缺少关键的上下文支持，无法给出准确的答案。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e复杂多层次问题\u003c/strong\u003e：某些问题本身复杂，包含多个子问题或需要推理、逻辑分析。Naive-RAG依赖简单的关键词匹配或语义匹配，无法很好地处理这类复杂的需求。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9aQuery%20Enhancement-assets/image.png\" alt=\"image.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e面对这些挑战，Naive-RAG的局限性变得明显：仅仅依靠用户的原始query无法应对多样化的提问场景。那么，如何提升RAG系统的检索能力，增强对复杂问题的处理呢？这时就需要引入\u003cstrong\u003eQuery-Enhancement\u003c/strong\u003e技术。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eQuery-Enhancement\u003c/strong\u003e，顾名思义，是对用户的原始query进行增强处理。它的核心思想是：不要直接使用用户的原始query进行相关文档的检索，而是通过某种技术手段对query进行改写、扩展或优化，从而生成一个更适合检索的query。这种技术在不同场景下有不同的命名，如\u003cstrong\u003equery rewrite\u003c/strong\u003e或\u003cstrong\u003equery reformulation\u003c/strong\u003e，但核心目标都是一致的——根据用户的初始输入，生成一个更符合检索需求、更能提升查询准确性的新query。\u003c/p\u003e\n\u003cp\u003e通过Query-Enhancement，可以解决许多Naive-RAG无法处理的问题。例如，针对模糊或简短的提问，增强后的query可以通过添加上下文或关键词来丰富信息内容；对于复杂的多层次问题，增强后的query可以拆解为多个子问题，逐步进行检索与推理。\u003c/p\u003e\n\u003cp\u003e本文将深入探讨不同的Query-Enhancement技术方案，并分析这些技术如何根据原始query生成更高效的查询，帮助RAG系统在复杂场景下提供更精确的答案。\u003c/p\u003e\n\u003ch2 id=\"query-rewrite\"\u003eQuery Rewrite\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eBecause the original query can not be always optimal to retrieve for the LLM, especially in the real world\u0026hellip; we first prompt an LLM to rewrite the queries, then conduct retrieval-augmented reading.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eQuery Rewrite的目的是将用户模糊、不明确或不完整的查询转换为更适合检索的形式，以提升检索效率和准确性。\u003c/p\u003e\n\u003cp\u003e仅使用原始query的缺点：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e查询常常模糊、不具体或不完整，导致难以找到有效的检索结果。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e缺乏明确术语，无法有效从大型数据集中提取相关信息。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e对话上下文中的查询含义因对话历史而异，同样的表达在不同的上下文中可能有不同的意图。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e输入文本与真正需要检索的知识之间不可避免地存在差距。这种差距限制了检索性能，增加了对检索能力增强和提示工程的依赖。\u003c/p\u003e\n\u003cp\u003e为了解决这些问题，检索器之前增加了一步对输入进行改写的过程，填补给定输入与检索需求之间的差距。这一步利用了LLM本身的能力对原始查询进行有针对性的改写。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9aQuery%20Enhancement-assets/image%201.png\" alt=\"image 1.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e比如，来自\u003ca href=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques_runnable_scripts/query_transformations.py\"\u003eRAG_Techniques\u003c/a\u003e 的这段prompt：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003equery_rewrite_template\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Original query: \u003c/span\u003e\u003cspan class=\"si\"\u003e{original_query}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Rewritten query:\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e使LLM可以更加系统地将用户的原始查询改写为更符合检索需求的形式。\u003c/p\u003e","title":"RAG工具箱：Query Enhancement"},{"content":"写在前面 2024年的国庆假期，第一次带父母踏出国门，目的地定在了马来西亚。挑选这个地方，并非偶然，既因为语言上的便利，也因对食物多样性的向往。对于第一次尝试出国旅行的新手来说，这样的选择再合适不过了。尤其是对于福建闽南人，马来西亚似乎有着一种熟悉而温暖的亲切感。\n行程从厦门出发，飞往槟城，然后搭乘火车前往怡保，再一路南下至吉隆坡，最终从吉隆坡返回故乡。虽说只是短短数日的旅行，但沿途的风景与美食却像是一条无形的丝线，将我们一家人与这片热带土地紧紧相连。\n此次游记，我尝试以“食物”为线索，串联起整个旅程的记忆。或许有些挑战，但也正是这种不同于以往的记录方式，让整个旅程多了些许味觉上的探险色彩。\n海南没有海南面包和海南鸡 槟城位于马来西亚西北，分成槟岛和威省两部分，作为东南亚重要的贸易港口之一，槟城自18世纪以来便吸引了无数移民，马来人、华人、印度人和欧洲人的足迹在这里留下了不可磨灭的印记，因其多元文化和历史，槟城的首府乔治市（George Town）也被联合国教科文组织纳入人世界文化遗产地。\n第一站是【多春茶室】。尽管其老店名声在外，但为了避开人群，我们选择了位于旧关仔角钟楼附近的分店。清晨9点到达时，茶室内还未拥挤，扫码点餐的现代化模式也让整个过程更加方便。\n点餐是件有趣的过程。面包、抹酱、饮品，一步步选择下来，经典的烤海南面包加上浓郁的咖椰酱，搭配香浓的Kopi-O，让这平凡的早餐充满满足感。\n烤海南面包加上咖椰酱\n有趣的是，海南面包源自南洋，并非来自海南本土。早年海南人漂洋过海来到南洋，受雇于英国人家庭和西化的峇峇家庭做帮佣，学习了西方的烘焙技艺，结合本地口味，创造出了适合东南亚人的海南面包。这些经历使他们将西式的咖啡与面包融入本地，逐渐普及成为南洋的经典早餐文化。\n当然，当地人早餐常吃的生熟蛋也是值得一试的， 鸡蛋泡在热水中几分钟，蛋白微凝固但仍带液态，蛋黄保持流动感，吃时只需加入酱油和白胡椒调味，用勺子轻轻搅拌，也可以蘸面包吃。如果不习惯生鸡蛋，店家还贴心地提供了不同熟度的选择。\n在这家店，店主还提供了4RM一个椰浆饭供食客自取，经典的江鱼仔鸡蛋口味，对于第一次尝试椰浆饭的人而言，味道的确很奇特。sambal辣酱并没有想象中那么辣，带有一丝丝的甜味，米饭也是经过调味的。\n茶室不远处便是槟城的地标——旧关仔角钟楼。钟楼由著名华商张弼士捐资修建，以纪念维多利亚女王登基60周年。与钟楼相邻的康华丽斯堡则是另一个标志性景点，虽名为堡垒，实未参与战斗。\n旧关仔角钟楼\n乔治市的街道以交叉十字贯穿各个文化区域。从小印度区入至华人区，再到马来区，都是一幅幅独特的文化画卷。右边为殖民时期的建筑，左侧则是古朴的华人会馆；再往前走，便是色彩斑斓的印度庙宇了。恍惚间，仿佛还能看到那些早年南来的移民，提着皮箱穿过狭窄的街巷，寻找着属于自己的一方天地。\n吉宁甲必丹回教堂\n槟城的美不止于其自然风光，而在于文化的碰撞与融合。早期移民将家乡味道带入南洋，创造出诸如海南鸡饭这样的经典。\n伍秀泽海南鸡饭茶室\n午后，我们寻觅到【伍秀泽海南鸡饭茶室】。这家茶室外观简单得甚至有些不起眼，但店里却充满着人间烟火气息。菜单也十分简单：白切鸡、烧鸡、鸡杂和鸡油饭。\n海南鸡饭\n鸡肉鲜嫩多汁，配上油光闪亮的鸡油饭，再添一勺微辣的辣椒酱，与黄瓜片相伴，让人回味无穷。\n一杯槟城特有的冰豆蔻水，为这丰盛的一餐画上美丽的句号，而在槟城的每一次咀嚼，都是对过去的重温。\n○○○\n【多春茶室】\n位置：37, Bishop St, Georgetown, 10200 George Town, Penang, 马来西亚\n营业时间：08:30-16:30 （周四店休）\n【伍秀泽海南鸡饭茶室】\n位置：340, Lbh Chulia, George Town, 10200 George Town, Pulau Pinang, 马来西亚\n营业时间：10:00-17:00 （周三店休， 周末营业时间延后一小时）\n粿条和饮料 粿条，本是潮汕人乃至闽南人对于米制面条的称呼。粿条的形式多种多样，既可以用来做汤，也可以干拌或者炒制。其中，最为著名的当属炒粿条。\n炒粿条的名字源自福建话“Char Kway Teow”。虽然它发源于潮州，但在潮州并不流行像马来半岛那样使用重酱油、鱼露，或加入血蚶的风味。\n几代人的迁徙流传，使得最初的潮汕炒粿条逐渐演变成如今的槟城炒粿条。在今天的马来半岛，各地的茶餐室、路边摊甚至食阁中，随处可见这一道菜。当地华人适应了赤道气候，也逐渐偏爱浓烈的口味，将黑酱油和鱼露加入其中，使其味道与原先的潮汕风味逐渐拉开了距离。\n槟城的炒粿条以其丰富的本地特色而闻名，以至于“槟城炒粿条”如今已然成为马来西亚外的“炒粿条”代名词。也许正是槟城人所炒的粿条充满了独特风味，让其从冠以“潮州”的乡土印记中脱颖而出，成为一种具有浓厚本地色彩的代表。\nChar Kway Teow\n打车时，遇见了一位潮州籍司机，第四或第五代的移民，已在马来西亚土生土长。她流利地说着潮州话，对自己的潮州根源抱有一份朦胧的认同。谈起美食，她兴致勃勃地推荐我去汕头街夜市，说那里的炒粿条最为“正宗”。然而，当我提起“潮汕”这个词，她的神情里浮现出些许困惑，似乎在她的概念中，“潮汕”并非一个清晰的地理或文化印象。她坦言，自己从未踏足故乡，“潮州”这个名字虽熟悉，却难以在她的心中勾勒出具体的图景，甚至未曾意识到潮州与汕头竟是毗邻而居。这样的“故乡”存在于她的血脉之中，却并不真实地存在于她的认知地图上。她的故事让我感受到这种隔代传承的身份认同中所夹带的微妙“模糊感”。\n或许，对她而言，故乡更像是从家人口中传下的记忆碎片，随着时间的推移，早已淡化成一种混杂的情感寄托。而那份隔空相望的疏离感，则正悄然渗透在她对潮州味道的理解中。也许，正是这份若即若离的情感，使她乐于将汕头街夜市当作心中的潮州，以这里的夜市小吃作为记忆的重构——故乡的味道也随之重构、演变，成了她所熟悉而珍视的“正宗”。\n粿条的故事也延伸到了汕头街的夜市，本以为【新月宮茶室】的炒粿条会是首选，但隔壁大排长龙的【天皇鸡脚粿条汤】却吸引了目光，不得不改变行程。这个路边摊没有座位，摊主售卖的卤味种类繁多，拼盘中有软糯的卤鸡脚、大肠、豆制品和卤蛋。粿条汤也有多种选择——清汤、黑汤和干拌，其中黑汤便是在清汤中加入一勺深色卤汁。在潮汕地区，炸蒜油始终是不可缺少的调味料，在这里也不例外。那一小勺炸蒜油，更是粿条汤的点睛之笔。这家店的招牌卤鸡脚，口感软糯入口即化，只可惜调味有点淡了，不过沾上店家提供的秘制调料，便不是太大的问题。\n天皇鸡脚粿条汤\n清汤粿条汤\n黑汤粿条汤\n卤味拼盘\n由于是路边摊，若想坐下堂食，还得借隔壁糖水铺的座位，条件是点上一杯饮料。这种餐位共享的习俗，在马来西亚的食阁、茶餐室、美食中心里早已根深蒂固。其源头可追溯至华人下南洋之时，彼时许多小贩因摊位有限，便与邻近的饮品摊位相互协作——食客借用座位，得点上一杯饮料作“入场费”。这一传统一路延续，逐渐形成一种心照不宣的默契。在槟榔屿， 不少地方甚至明文规定需点饮料，否则每人需支付额外0.5令吉的座位费。\n要求点饮料的告示\n马来西亚位于赤道附近，气候湿热，常年高温，这样的环境下，人们对冰凉饮料的需求自然高涨。不仅是解渴之需，更是日常生活中不可或缺的清凉享受。而在这片多元文化交织的土地上，马来人、华人、印度人等多个族群共同生活，他们的饮食文化在交汇中相互融合，形成了丰富多样、独具本地特色的饮料种类。\n在马来西亚的茶室，即所谓的“Kopitiam”里，点一杯饮料并不像在一般咖啡馆那般简单。这里有一套独特的组合式命名法，仿佛饮品的“方言”。这种规则依循特定的次序，将饮品种类、奶的选择、糖的添加等信息融为一体。\n常见饮料命名组合方式\n1. 基础饮料\n• Kopi：咖啡\n• Teh：茶\n• Milo：美禄\n2. 奶的种类\n• O：不加奶，即“乌”，来自于福建话。\n• C：加入淡奶，Canned Milk。\n• 无标注：加炼乳\n3. 糖的量\n• Kosong：马来语，无糖\n4. 温度\n• Peng：福建话冰，也有使用Ais （源于马来语，更源头则是英语ice）\n在这套规则之外，马来西亚还有几款令人印象深刻的本地特饮。比如，拉茶（Teh Tarik），通过“拉”茶的技艺将其来回高抛，层层拉出细腻的泡沫。或是备受年轻人喜爱的美禄恐龙（Milo Dinosaur），在一杯美禄上再撒上一层厚厚的美禄粉，巧克力的浓郁香气更加突出。\nKopi、Milo、Barley（薏米水）\n还有马来西亚人解暑必备的煎蕊（Cendol），这是一道口感丰富、层次分明的传统甜品。绿色的煎蕊条由绿豆粉或糯米粉制成，并以斑兰叶汁染色，散发淡淡的香气，与椰浆、椰糖、刨冰相融合，形成一碗甜而不腻的冰凉饮品。偶尔还会加入红豆、玉米、仙草等配料，增添风味，口感层次丰富，是在炎热气候中最受欢迎的甜品之一。\n槟榔律驰名潮州煎蕊\n此外，马来西亚的传统饮品中，还有一种经常被误解的“薏米水”，即当地所谓的“Barley”。这里的“Barley”指的是大麦，并非中药所指的薏苡仁。虽说“薏米水”这个叫法并不准确，但大麦煮成的饮品清爽解渴，早已深入人心。大麦因其外形酷似薏米，在马来西亚被称作“洋薏米”，久而久之，“洋”字不见了，反倒留下了“薏米水”这一独特的名称。\n○○○\n【新月宮茶室】\n位置：86, Lebuh Kimberley, George Town, 10100 George Town, Pulau Pinang, 马来西亚\n营业时间：08:00-22:30\n【天皇鸡脚粿条汤】\n位置：78, Lebuh Kimberley, George Town, 10100 George Town, Pulau Pinang, 马来西亚\n营业时间：17:00-23:00\n【Penang Road Famous Teochew Chendul】\n位置：27-29, Lebuh Keng Kwee, George Town, 10100 George Town, Pulau Pinang, 马来西亚\n营业时间：09:00-18:30\n福建人和福建虾面 在槟城，当地华人群体中福建人占了绝大多数。\n于是乎在源头称之为闽南语的语言，到了南洋，因其主导性而被称之为“福建话”。闽南话在此地几乎畅行无阻，甚至连一些印度裔的小贩也能操一口流利的福建话。想起初来槟城时与一位Grab司机的对话，他祖籍泉州，但彼时彼刻我还不确定他是否也是华人，于是用英语交谈，直到父亲用闽南话试探他是否能听懂。司机会心一笑，便用福建话和我们无障碍交流起来。这种语言的亲切感，也许是因为共同的根源，使得陌生的异地瞬间拉近了距离。\n福建移民不仅带来了他们的方言，还携来了信仰与文化。在槟城早期，由于殖民政府对陆地按面积征税，许多福建移民为了避税，巧妙地选择在海上建房，形成一片片独特的“姓氏桥”，以“周姓桥”为最为闻名的代表。其桥头设有庙宇供奉天公，每年农历正月初八，居民会在此举行盛大的拜天公仪式。而这一信仰，始终没有变过。\n姓氏桥\n周氏桥头的庙宇\n除了语言文化信仰，还有一碗面。提到槟城的福建面，这一话题在马来西亚可谓是引起争议的话题。槟城的福建面其实就是福建虾面，而这道美食则是由早期移民福建人带入当地的美味，因售卖者多为说着福建话的福建人因而得名“福建面”，而在吉隆坡，新加坡，福建面则是另一种样貌——加了酱油膏的黑色炒面。\n关于福建面，还有这样一个民间传说：在日军占领槟榔屿期间，人民生活困苦，鱼虾都被迫献给日军，渔民们只好用剥下的虾头和虾壳来煮汤，便创制出了这道美食。虽说这应该只是个民间杂谈， 并无具体的考究，但也点明了这碗面的核心—由虾头虾壳熬制而成的高汤，仍未随时间而动摇。\n相较于上世纪舶来品的沙茶面，厦门的虾面才是南洋虾面的源头。虾面使用一种名为“狗虾”的野生小虾（因其特殊性，每年四月至七月都会断产），先去壳取肉再煮熟。狗虾的虾壳炒香后捶打成酱，再与猪骨慢火熬制四小时。虾面的主要原料在于虾，但其独特之处却在于汤底。\n在槟城，虾和猪骨依旧是虾面汤的灵魂，但增加了Sambal酱，也增添了更多的辣味和海鲜的浓郁。Sambal酱是一种用发酵的虾酱、干辣椒和新鲜辣椒制成的调料。除了黄面和豆芽外，槟城的虾面中总是会放入蕹菜，即空心菜。\n在落日洞边上的【渔人码头】，只因住在附近， 而选择了在此觅食，Grab司机倒也好奇我们是否收到了当地朋友的推荐，而选择了这么家似乎属于当地人私藏的食阁。食阁的好处便是能够在一顿饭中获得多种选择。而让我记忆深刻的，还是那碗福建虾面。香辣的汤底和浓厚的风味瞬间击中了味蕾。少了厦门虾面的那种甜，却多了一份来自热带的火辣。用开水汆烫的黄面、米粉、蕹菜，再加上肉片、水煮蛋和豆芽，浇上浓郁的特制虾汤，汤上放在一把勺子，勺内有少许Sambal辣酱，依个人喜好调节辣度。\n福建虾面\n我大胆猜测，这或许是早年的福建移民南下槟城后，长年适应当地湿热气候，逐渐放弃了家乡的清淡口味，而在日积月累的过程中调重了味道，以刺激食欲。延用了家乡的虾和面，又加上南洋风味的辣酱，最终成就了这道独一无二的本地福建虾面。\n渔人码头晚霞\n○○○\n【渔人码头】\n位置：92A, 1, Lebuh Sungai Pinang, 11600 Jelutong, Pulau Pinang, 马来西亚\n营业时间：08:30-00:00\n怡保白咖啡和芽菜鸡 怡保，这座因锡矿而兴起的城市，曾在十九世纪的喧嚣中展露繁华。那些年，来自南洋的劳工涌入，为这片土地注入了活力。然而，到了五十至六十年代，锡价下跌，需求萎缩，昔日的热闹逐渐消退。时光似乎停留在50年代，街头的店铺大多紧闭门扉，仿佛旧日的繁华仅残留在老旧的招牌上。这些招牌多以竹帘制成，简单的笔画勾勒出商号名称，透着一丝未曾褪色的昔日风韵。然而，在这破败的街巷间，一些无法离开或不舍离去的老人们，依旧守望着故乡的影子。\n怡保街头\n因只是将怡保作为槟城到KL的中转站，活动范围只局限在了火车站附近的旧厂街。旧厂街的街角，三五成群的华裔、印度裔老人围坐聊天，邻桌的茶室里，来自不同族群的人们在一片屋檐下共享时光。这里的人们彼此间的友好显而易见，彼此往来无碍。偶遇的Grab司机也会提到马来朋友，仿佛这些对立和冲突只是外来者的夸大，或者是政客用以拉拢选票的政治游戏。\n怡保街头\n怡保的特色小吃芽菜鸡，传言原先是芽菜与鸡一同盛放。岁月流转，食物也随之演变，如今鸡与芽菜分置盘中，简单而别具风味。尤以“红油”鸡丝河粉最为有名，自七十年代起，芽菜与清汤河粉成了标配，鸡丝河粉却少有提供。然而，品尝之际，才觉那极为粗壮的芽菜透着脆爽，这是只有怡保水土才能培育出的佳品。依山傍水的地理造就了这一独特风味，豆芽也因此而名声在外。\n芽菜、鸡和鸡丝河粉\n数家有名的芽菜鸡店都聚集在同一街角，行色匆匆，难以抉择。【安记芽菜鸡】店里的装潢古旧，时光似乎凝固在墙角，一旁的老式唱片机正播放着七八十年代的旋律，白板上仅写着几道招牌菜，似乎在坚守那份精致而不贪多的执着。简单的酱油胡椒调味衬托出芽菜的自然清甜，一碗鸡油饭，几块白斩鸡，轻尝一口，简简单单但并不普通。老顾客依旧按着旧习惯点餐，只是如今应顾的却是语言不通的外劳，偶尔还需老板亲自上前指点。\n安记芽菜鸡内的招牌\n跨过几条巷弄，映入眼帘的是另一番旧日风情。【南香茶餐室】，这间被视为白咖啡发源地的老茶室，依旧保留了传统的模样。白咖啡的味道仿佛凝结了时代的变迁，据说是早年一位咖啡小贩在炒咖啡豆时加入了焦糖，才成就了这款特有的香浓口味。如今这香气馥郁的白咖啡已成为怡保的名片，白咖啡品牌的迅速推广，让这种独特的风味远播四方。进店后点上一杯白咖啡，再来一盘蛋挞或鸡肉酥，仿佛能在这微苦中回味旧时光。\n南香茶餐室 白咖啡\n沿街行至二奶巷、三奶巷，这些名字带着几分遥远的传奇色彩。那些锡矿时期的华商为情妇安置的街巷，如今已是游客蜂拥而至的商业街，卖着各种小商品，留下的不过是一点点变了味的旧日记忆。\n○○○\n【安记芽菜鸡沙河粉】\n位置：No. 48/51, 53, Jalan Yau Tet Shin, Taman Jubilee, 30300 Ipoh, Perak, 马来西亚\n营业时间：10:00-21:00\n【南香茶餐室】\n位置：2, Jalan Bandar Timah, 30000 Ipoh, Perak, 马来西亚\n营业时间：06:30-16:30\n椰浆饭 椰浆饭，马来语叫 Nasi Lemak，nasi意为饭，lemak本意是油脂，形容其特有的浓醇风味。\n走在槟城街头，漫无目的地游荡时，竟偶遇了米其林推荐的【Ali Nasi Lemak Daun Pisang】档口——这处小摊赫然立于入口处，煞是醒目。一位满脸笑意的Anty在顾客眼前熟练地将椰浆饭打包，四五种风味依次摆放，最经典的莫过于鸡蛋江鱼仔的组合。过去从未想象椰浆与米饭交织能呈现出何种风味，而这道菜因一译名“辣死你妈”更添神秘之色，让人既好奇又敬畏。\nAli Nasi Lemak Daun Pisang\n椰浆饭 最初作为早餐而生，裹在朴实的香蕉叶中，佐以参巴辣椒酱、江鱼仔与鸡蛋，曾是一份清晨的简单选择。随时间的推移，椰浆饭的变化也愈加丰富。现代人追求口味的多样，炸鸡、咖喱鸡、仁当牛肉、海鲜等纷纷入味，让传统的椰浆饭增添了新的层次， 而呈现形式，也从传统的以香蕉叶包裹演变成盘装。\n虽未有幸造访网络盛传的椰浆饭“天花板” 【Village Park】，倒也释然。正如心底暗想的那般，既为国菜，这道菜肴似乎只要在市井小摊一现，便已足以代表一方风味。于是便随意在住处附近挑了一家娘惹餐馆，算是体验这口人间烟火。\nNasi Lemak\n米经过简单调味， 用椰浆、斑斓叶和数种香料混合并蒸熟，粒粒分明，浓香不腻。入味的参峇辣酱抹在米饭上，辛香中带着丝丝甜美与微酸，而那几片爽脆的黄瓜，总在微腻时救急，堪称点睛。炸鸡腿表皮酥脆，内里鲜嫩，几乎让人误以为这鸡腿才是整盘的灵魂。当地人常用手吃，右手抓点配料和酱就点米饭，捏紧送入口中，奈何打心里接受不了，还是选择叉子和勺子。也不得不提仁当牛肉，这“炖煮”的艺术将大量椰浆、椰蓉甚至椰糖加入传统咖喱香料中。\n○○○\n【Ali Nasi Lemak Daun Pisang】\n位置：Beach St, Georgetown, 10300 George Town, Penang, 马来西亚\n营业时间：周一至周六 07:00-14:00，周天店休\n【Village Park Restaurant】\n位置：5, Jalan SS 21/37, Damansara Utama, 47400 Petaling Jaya, Selangor, 马来西亚\n营业时间：06:30-17:30\n肉骨茶 小时候，初次品尝到一碗肉骨茶，那一缕浓烈的胡椒香气便深深刻印在记忆里。然而，长大后才知晓，这种香气四溢的汤品其实是新加坡潮州式肉骨茶的版本，与其起源地马来西亚的传统风味已有所不同。新加坡肉骨茶在多次演变中，逐渐简化成如今汤清色浅、以胡椒为主调的形式，而大马的肉骨茶，则始终保留着那浓郁厚重的药材香气——党参、当归、川芎、玉竹，还有八角、胡椒与蒜头等香料在锅中激荡出层层深味，有时还掺入酱油调出略带咸甜的色泽。\n有趣的是，尽管名字中带“茶”字，肉骨茶其实与茶本无直接关系。它的本质是一种以药材和猪骨熬煮的炖汤，至今没有一个权威的解释能准确追溯“茶”字的由来。也许是因为码头工人在边吃肉骨汤、边啜一杯清茶解腻时，这种饮食习惯自然而然地与汤品绑定在了一起。\n肉骨茶的故事绕不开李文地的名字。战后，这位从伙计做起的男子在巴生后街开了自己的食档。他改良苦力们的药材炖肉配方，推出了加中药材的家乡炖肉骨，还以猪油炒饭取代了传统的蚝干咸饭，结果大受欢迎。人们为他取了个外号“肉骨地”，而闽南话中“地”与“茶”同音，这名字便逐渐演变成了“肉骨茶”。\n巴生港，这座曾挤满苦力的港口，是肉骨茶的发源地。那些码头工人以肉骨茶配白饭为早餐，饱腹又暖身。这种粗粝却温暖的饮食习惯，像是一曲简单却深情的生命交响，延续至今。然而，我的旅途短暂，未能前往心心念念的巴生港，去一探那发源地的真貌。\n传统汤式肉骨茶和新兴的干式肉骨茶\n计划中选了一间著名的老店，却因饭点等位时间太长而作罢，转而去了附近的【友谊肉骨茶店】。虽说这里主打潮州风味，汤底清淡少药材，更多是肉汤的质朴味道。干式肉骨茶的滋味反倒更和胃口，配饭吃尤为下饭。除了排骨，锅中还有三层肉、猪肚和些许青菜。另点了一份黄酒猪腰，第一口那酒香直冲脑门，令人惊艳。只可惜，这风味太过浓烈，反衬得后来的肉骨茶平淡了几分。\n○○○\n【友谊肉骨茶】\n位置：53-6, Jalan Barat, Pudu, 55100 Kuala Lumpur, Wilayah Persekutuan Kuala Lumpur, 马来西亚\n营业时间：09:30-21:00\n后记 清晨的 ICC Pudu市场，人潮如涌，自早六点至下午一时，充满了来访者的喧闹。这里的前身是老Imbi市场，是当地华人餐食的老据点，搬迁至此后依然熙熙攘攘。茶座间，点一杯饮料便算租了桌，四周小档口林立，各色早点汇聚。虽说环境不甚清洁，四周闷热嘈杂，倒是体验生活本味之佳处。\n这里的摊主多为华人，但收银接待者多为外劳，往往只有老板会说华语，常需几句英语才能沟通。这片被称为“富都”的区域，曾是华人聚居之地，如今主要成了外籍劳工的栖息处。\n摄于Pudu\n吉隆坡，这座城市中穿梭着无数外劳的身影。他们中的绝大部分来自缅甸、孟加拉等地，肩负起危险的建筑工地工作，或是被本地人轻视的低端服务业。街头巷尾，时常可见警车押送非法移民的场景。那些黑色的露顶卡车，车身写着醒目的“Immigration”，押送的人群中有男人，也有妇女与孩童。入夜时分，双子塔灯光璀璨，喷泉伴着《We Are the World》的旋律涌动，而一旁在建的高楼里，外劳的身影仍在高空忙碌。他们是城市奇观背后的无名工匠，默默成为奇观背后的注脚。\n双子塔\n时隔一月有余，重拾这些片段，只为整理旅行途中的思绪。彼时原是为美食而去，然而除了舌尖的满足，更让我着迷的，是每一道菜背后的文化脉络和交融的故事——那些承载在味道上的历史与人。\n那些未曾尝试的风味、未曾探访的角落，仍在心底留存。比起舌尖的满足，这片土地文化上的冲击、人与人之间微妙而深远的关系，更让我感慨良多。或许，这正是旅行的意义所在吧。\n","permalink":"https://niraya666.github.io/travel/%E5%A4%A7%E9%A9%AC%E6%B8%B8%E8%AE%B02024/","summary":"\u003ch2 id=\"写在前面\"\u003e写在前面\u003c/h2\u003e\n\u003cp\u003e2024年的国庆假期，第一次带父母踏出国门，目的地定在了马来西亚。挑选这个地方，并非偶然，既因为语言上的便利，也因对食物多样性的向往。对于第一次尝试出国旅行的新手来说，这样的选择再合适不过了。尤其是对于福建闽南人，马来西亚似乎有着一种熟悉而温暖的亲切感。\u003c/p\u003e\n\u003cp\u003e行程从厦门出发，飞往槟城，然后搭乘火车前往怡保，再一路南下至吉隆坡，最终从吉隆坡返回故乡。虽说只是短短数日的旅行，但沿途的风景与美食却像是一条无形的丝线，将我们一家人与这片热带土地紧紧相连。\u003c/p\u003e\n\u003cp\u003e此次游记，我尝试以“食物”为线索，串联起整个旅程的记忆。或许有些挑战，但也正是这种不同于以往的记录方式，让整个旅程多了些许味觉上的探险色彩。\u003c/p\u003e\n\u003ch2 id=\"海南没有海南面包和海南鸡\"\u003e海南没有海南面包和海南鸡\u003c/h2\u003e\n\u003cp\u003e槟城位于马来西亚西北，分成槟岛和威省两部分，作为东南亚重要的贸易港口之一，槟城自18世纪以来便吸引了无数移民，马来人、华人、印度人和欧洲人的足迹在这里留下了不可磨灭的印记，因其多元文化和历史，槟城的首府乔治市（George Town）也被联合国教科文组织纳入人世界文化遗产地。\u003c/p\u003e\n\u003cp\u003e第一站是【多春茶室】。尽管其老店名声在外，但为了避开人群，我们选择了位于旧关仔角钟楼附近的分店。清晨9点到达时，茶室内还未拥挤，扫码点餐的现代化模式也让整个过程更加方便。\u003c/p\u003e\n\u003cp\u003e点餐是件有趣的过程。面包、抹酱、饮品，一步步选择下来，经典的烤海南面包加上浓郁的咖椰酱，搭配香浓的Kopi-O，让这平凡的早餐充满满足感。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/6F734170-B982-4F62-934A-22FF8C6CCE86_1_105_c.jpeg\" alt=\"6F734170-B982-4F62-934A-22FF8C6CCE86_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e烤海南面包加上咖椰酱\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e有趣的是，海南面包源自南洋，并非来自海南本土。早年海南人漂洋过海来到南洋，受雇于英国人家庭和西化的峇峇家庭做帮佣，学习了西方的烘焙技艺，结合本地口味，创造出了适合东南亚人的海南面包。这些经历使他们将西式的咖啡与面包融入本地，逐渐普及成为南洋的经典早餐文化。\u003c/p\u003e\n\u003cp\u003e当然，当地人早餐常吃的生熟蛋也是值得一试的， 鸡蛋泡在热水中几分钟，蛋白微凝固但仍带液态，蛋黄保持流动感，吃时只需加入酱油和白胡椒调味，用勺子轻轻搅拌，也可以蘸面包吃。如果不习惯生鸡蛋，店家还贴心地提供了不同熟度的选择。\u003c/p\u003e\n\u003cp\u003e在这家店，店主还提供了4RM一个椰浆饭供食客自取，经典的江鱼仔鸡蛋口味，对于第一次尝试椰浆饭的人而言，味道的确很奇特。sambal辣酱并没有想象中那么辣，带有一丝丝的甜味，米饭也是经过调味的。\u003c/p\u003e\n\u003cp\u003e茶室不远处便是槟城的地标——旧关仔角钟楼。钟楼由著名华商张弼士捐资修建，以纪念维多利亚女王登基60周年。与钟楼相邻的康华丽斯堡则是另一个标志性景点，虽名为堡垒，实未参与战斗。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/F8C5900A-FE98-4DCF-8B04-E73A9CB9A30C_1_201_a.jpeg\" alt=\"F8C5900A-FE98-4DCF-8B04-E73A9CB9A30C_1_201_a.jpeg\"  /\u003e\n\n\u003cem\u003e旧关仔角钟楼\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e乔治市的街道以交叉十字贯穿各个文化区域。从小印度区入至华人区，再到马来区，都是一幅幅独特的文化画卷。右边为殖民时期的建筑，左侧则是古朴的华人会馆；再往前走，便是色彩斑斓的印度庙宇了。恍惚间，仿佛还能看到那些早年南来的移民，提着皮箱穿过狭窄的街巷，寻找着属于自己的一方天地。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/73FE20C9-AFC7-4F90-BCD9-6980BDE26F13_1_105_c.jpeg\" alt=\"73FE20C9-AFC7-4F90-BCD9-6980BDE26F13_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e吉宁甲必丹回教堂\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e槟城的美不止于其自然风光，而在于文化的碰撞与融合。早期移民将家乡味道带入南洋，创造出诸如海南鸡饭这样的经典。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/E399B1CB-F4F0-4639-86E7-A6F3F8C91BFC_1_201_a.jpg\" alt=\"E399B1CB-F4F0-4639-86E7-A6F3F8C91BFC_1_201_a.jpg\"  /\u003e\n\n\u003cem\u003e伍秀泽海南鸡饭茶室\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e午后，我们寻觅到【伍秀泽海南鸡饭茶室】。这家茶室外观简单得甚至有些不起眼，但店里却充满着人间烟火气息。菜单也十分简单：白切鸡、烧鸡、鸡杂和鸡油饭。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/48AD27F9-8A94-4558-A87E-334128902AB1_1_105_c.jpeg\" alt=\"48AD27F9-8A94-4558-A87E-334128902AB1_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e海南鸡饭\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e鸡肉鲜嫩多汁，配上油光闪亮的鸡油饭，再添一勺微辣的辣椒酱，与黄瓜片相伴，让人回味无穷。\u003c/p\u003e\n\u003cp\u003e一杯槟城特有的冰豆蔻水，为这丰盛的一餐画上美丽的句号，而在槟城的每一次咀嚼，都是对过去的重温。\u003c/p\u003e\n\u003cp\u003e○○○\u003c/p\u003e\n\u003cp\u003e【多春茶室】\u003c/p\u003e\n\u003cp\u003e位置：37, Bishop St, Georgetown, 10200 George Town, Penang, 马来西亚\u003c/p\u003e\n\u003cp\u003e营业时间：08:30-16:30 （周四店休）\u003c/p\u003e\n\u003cp\u003e【伍秀泽海南鸡饭茶室】\u003c/p\u003e\n\u003cp\u003e位置：340, Lbh Chulia, George Town, 10200 George Town, Pulau Pinang, 马来西亚\u003c/p\u003e\n\u003cp\u003e营业时间：10:00-17:00 （周三店休， 周末营业时间延后一小时）\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"粿条和饮料\"\u003e粿条和饮料\u003c/h2\u003e\n\u003cp\u003e粿条，本是潮汕人乃至闽南人对于米制面条的称呼。粿条的形式多种多样，既可以用来做汤，也可以干拌或者炒制。其中，最为著名的当属炒粿条。\u003c/p\u003e\n\u003cp\u003e炒粿条的名字源自福建话“Char Kway Teow”。虽然它发源于潮州，但在潮州并不流行像马来半岛那样使用重酱油、鱼露，或加入血蚶的风味。\u003c/p\u003e\n\u003cp\u003e几代人的迁徙流传，使得最初的潮汕炒粿条逐渐演变成如今的槟城炒粿条。在今天的马来半岛，各地的茶餐室、路边摊甚至食阁中，随处可见这一道菜。当地华人适应了赤道气候，也逐渐偏爱浓烈的口味，将黑酱油和鱼露加入其中，使其味道与原先的潮汕风味逐渐拉开了距离。\u003c/p\u003e\n\u003cp\u003e槟城的炒粿条以其丰富的本地特色而闻名，以至于“槟城炒粿条”如今已然成为马来西亚外的“炒粿条”代名词。也许正是槟城人所炒的粿条充满了独特风味，让其从冠以“潮州”的乡土印记中脱颖而出，成为一种具有浓厚本地色彩的代表。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e9%a3%9f%e5%9c%a8%e5%a4%a7%e9%a9%ac-assets/6762BC7A-C04B-4F8A-B9DC-3E5BA158D640_1_105_c.jpeg\" alt=\"6762BC7A-C04B-4F8A-B9DC-3E5BA158D640_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eChar Kway Teow\u003c/em\u003e\u003c/p\u003e","title":"食在大马"},{"content":"前言 国庆期间，在西马玩了快一个礼拜，期间体验了从北海到怡保的ETS、槟城的升旗山登山缆车，还有吉隆坡复杂多样的轨道交通系统。旅行中，我踩过一些坑，也解锁了一些便利的玩法。经过查阅不少攻略，加上实地探索和切身体会，我决定将这次旅途中的交通攻略整理成这篇文章，分享给大家。\n注：本文所包含的信息有效截止至2024年10月，同时主要讨论西马（马来半岛）的轨道交通；希望能对你的大马之旅有所帮助。\n大马铁路概述 马来西亚的铁路系统起源于19世纪末的英国殖民时期，当时的铁路主要是为了支持锡矿业和橡胶种植园的经济发展。第一条铁路于1885年建成，连接了太平和砵威港口（今称为瓜拉十八丁），而太平是当时霹雳州英国人的行政中枢。这条铁路的建设是为了便于将矿产资源从内陆运输到港口。此后，英国殖民政府开始加快在马来亚各地开辟铁路的步伐，主要目的是为了促进殖民地经济发展，例如吉隆坡至巴生的铁路于1886年建成通车，芙蓉至波德申的铁路于1891年建成通车，吉隆坡至万挠的铁路于1892年建成通车，安顺至打巴路的铁路于1893年建成通车。1895年，受英国保护的雪兰莪、森美兰、霹雳和彭亨四州组成马来联邦。当时马来亚不同州的铁路各自发展，没有一个统一的规划和管理机构，于是殖民政府当局在1901年成立了马来联邦铁路（FMSR），统合管理马来西亚半岛上各地包括马来联邦、马来属邦、海峡殖民地的铁路。这些初期建设的米轨铁路成为连接各主要城镇和经济区的重要交通工具。\n20世纪初，马来西亚的铁路网迅速扩展，最重要的两条干线为东海岸线和西海岸线。二战期间，铁路遭到日军的严重破坏，但战后得到了重建。1948年，马来联邦铁路被改组为马来亚铁路（MR）。1963年，马来亚、新加坡、砂拉越和沙巴组成了新的联邦国家——马来西亚（新加坡于1965年退出），马来亚铁路的官方名称也更改为马来语“Keretapi Tanah Melayu”（KTM）。\n现如今，马来西亚的铁路系统主要由两条干线组成：西海岸线和东海岸线。东海岸线（也称金马士－道北铁路）由马来亚铁道运营，起自金马士，途经吉兰丹、彭亨、森美兰，穿越马来半岛中部，全长528公里，最终到达泰国边境的道北。虽然名为东海岸线，但大部分路段穿行于内陆丛林地带，因此又被称为“丛林铁路”，吸引许多背包客体验。西海岸线则起自泰国边境的巴东勿刹，沿西海岸一直延伸至柔佛的南端，通过新柔长堤连接新加坡，全长950公里，途经包括吉隆坡在内的八个州属，是马来西亚铁路运输的重要交通大动脉。由国有企业马来亚铁道公司（KTMB）运营的铁路系统经过了多次改组和现代化，如今的铁路服务已经涵盖了从通勤列车到高速城际列车的多种交通模式。目前，马来西亚铁路系统不仅联通国内主要城市，还连接着泰国和新加坡，是东南亚地区重要的交通枢纽。\n来源\n铁路分类 KTM Komuter（城际通勤列车）\nKTM Komuter是由马来亚铁道公司（KTM）运营的通勤铁路服务，最初于1995年在巴生谷地区启用，后于2015年扩展至北部各州（如霹雳、槟城、吉打和玻璃市）。该服务连接了吉隆坡及其周边郊区，还服务于槟城和其他北部州属，总长度约456公里，包含芙蓉线、巴生港线等多条线路。KTM Komuter的列车是空调电力列车，并提供便捷的“停车换乘”设施，适合游客和通勤者方便进入市区，避免交通拥堵。KTM通勤铁路还为乘客提供多种票务选项，包括通过KTMB移动应用程序购买手机票（二维码）、Touch \u0026rsquo;n Go（仅限巴生谷地区）、自动售票机与车站售票柜台购票。\nETS（电动列车服务）\nETS（Electric Train Service）是由马来亚铁道公司运营的快速城际电力动车组服务，于西海岸线运行，连接金马士与巴东勿刹，全长950公里。ETS是马来西亚最快的米轨列车，速度可达每小时140公里，属于准高速铁路。列车分为Gold、Silver、Platinum 以及 Express 4种等级，分别对应不同的服务和票价，是前往怡保、槟城等城市的高效方式。\nKTM Intercity（城际列车）\nKTM Intercity是马来亚铁道运营的柴油牵引城际列车服务，覆盖马来半岛的南部地区、新加坡以及泰国。部分列车主要运行于东海岸线金马士至道北之间，并延伸至新加坡。2021年4月推出了柴油动车组（DMU）列车，取代了原有的传统列车，服务于东海岸的丹绒至吉利地段。DMU列车配备现代化设施，包括集群座椅、LED显示屏、电源插座、祈祷室、厕所、小餐馆及行李存储空间，并有无障碍设施，满足残障人士的需求。\n如何购买火车票 购买火车票的方式有多种，最为方便的是通过KTMB官方网站或手机应用程序进行在线购买。也可以在吉隆坡中环车站（KL Sentral）及其他主要车站直接购票，但热门线路的车票在高峰时期（如假日、周末）可能会售罄，建议提前预订。\n以网上购买ETS车票为例，需在KTMB官网注册账号，然后选择出发和到达车站、以及对应的ETS车次类型（如Gold、Platinum、Express等）。不同级别的车次在停靠站数量和服务上有所不同，例如Platinum和Express级别提供商务座，商务座采用1+2的座位排布，包含免费食物和饮料，价格相对更高；普通席则是2+2的座位排布。Gold车次则没有商务座位。选择座位时，需要注意座位方向，有些座位是与行驶方向相反的，如果不喜欢反向乘坐，记得选择“forward”座位。\nETS列车的车型也有所区别，Platinum和Express列车多为较新款，由中国中车制造，而Silver级别列车则采用较老款，速度较慢，停靠站较多。\nETS-Express列车（摄于Butterworth车站） ETS-Gold列车，采用旧款车型（摄于Ipoh车站）\n吉隆坡轨道交通 吉隆坡及其周边地区有完善的轨道交通系统，涵盖了多种轻轨和地铁线路。\nLRT（轻快铁）\nLRT是吉隆坡的轻轨系统，包含3号 LRT Ampang Line、4号LRT Sri Petaling Line和5号LRT Kelana Jaya Line。LRT贯穿吉隆坡市中心及重要商业区，是游客前往市区景点如双子塔、茨厂街的首选交通工具。在高峰期，列车会非常拥挤。\n5号线LRT，摄于Bangsar车站\n3号LRTAmpang Line（橙黄色）：占美清真寺，独立广场，ICC pudu等景点。\n4号LRT Sri Petaling Line（深红色）：占美清真寺，独立广场，ICC pudu等景点。\n5号LRT Kelana Jaya Line（粉色）：可以前往双子塔、KLCC水族馆等主要景点。\nMRT（地铁）\nMRT是吉隆坡的新兴地铁系统，分为Kajang线和Putrajaya线，覆盖了市区及巴生谷更广阔的区域。\n9 号线 MRT Kajang Line（深绿色） ：可前往国家博物馆，village park（知名的nasi lemak）等地；\n10 号线MRT Putrajaya Line（黄色）：可前往KLCC，海脚人餐厅等地。\nKL Monorail（单轨铁路）\n路线编号为8，而颜色则为绿色。\nKL Monorail是吉隆坡独特的单轨铁路，线路为黄绿色，专门服务于市中心，主要连接购物和娱乐区（如武吉免登、KL Sentral等）。这条单轨铁路全长8.6公里，共有11个车站，将吉隆坡中央车站交通枢纽与市中心的金三角地带连接起来。\n1996年，马来西亚政府批准兴建吉隆坡单轨列车的计划，1997年1月正式动工，然而，1997年亚洲金融风暴导致融资困难，工程被迫暂停。1998年，本国企业MTrans公司接手，并在2003年8月31日正式完工并通车。通车后，单轨列车因客流量不如预期及经营收入不足而在2007年被马来西亚国家基建公司（Prasarana）收购。\n单轨列车的内部设计独特，不同车厢有着不同的座椅布局，多为面向窗户的座位，能够让乘客更好地欣赏吉隆坡的城市景观。然而，列车的安全门常处于开启状态，车门开关也时常发生故障，显示出在1997年金融危机后及其经营不善下所留下的问题。不过，尽管存在运营上的挑战，KL Monorail仍然成为了吉隆坡的一张城市名片，尤其在武吉免登（Bukit Bintang）等热门地带，为游客提供了方便的出行选择，并成为Bukit Bintang的知名打卡地。始发于KL Sentral，途经如Bukit Bintang等购物和娱乐区。 独特设计的座位布局，摄于KL Sentral\nMonorail，摄于Bukit Bintang\nKLIA Ekspres（吉隆坡机场快线）\nKLIA Ekspres是巴生谷的第六条列车线路，连接吉隆坡中央车站（KL Sentral）与吉隆坡国际机场（KLIA 1 \u0026amp; KLIA 2）。全程仅需33分钟，每20分钟一班，票价为55马币。KLIA Ekspres路线编号为6，线路颜色为紫色。该服务的列车提供宽敞的座位和行李存放空间，乘坐马来西亚航空、国泰航空、峇迪航空的旅客还可以在吉隆坡中央车站进行市区预办登机和行李托运服务，方便乘客直接登机。\nKLIA Transit（吉隆坡机场支线）则是第七条线路，使用相同轨道，但中途停靠布城和其他站点，如粉红清真寺等著名景点。全程约40分钟，每15至30分钟一班。票价按站收费，最少6.5马币起。需要注意的是，KLIA Ekspres和KLIA Transit在KL Sentral的进站口不同。\nKLIA Ekspres 和Transit的进站口不同\n登山缆车 槟城的升旗山登山缆车（Penang Hill Funicular Train）是登上升旗山山顶的正常途径，除非你愿意爬3个小时上山。这条缆车线路始建于1923年，最初为两段式铁路，2010年经过现代化改造，合并为一段式系统并更换为封闭式空调车厢。缆车全长8.6公里，是世界上最陡的斜坡缆车之一，全程时间大约5到20分钟，视途中停靠情况而定。它将游客从山脚的Air Itam带到升旗山顶，可以俯瞰槟城市景以及对岸的Butterworth。\n升旗山曾是殖民时期的避暑胜地，缆车的建造也是为了方便上山交通。缆车采用钢缆拉动车厢，以适应陡峭的坡度而安全上下山。缆车的普通往返票价为30马币，此外还有VIP票价为80马币，VIP票无需排队，且可以优先上车，占据有利的拍照位置。然而在非暑假的工作日下午，通常不太需要排队，因此无需额外购买VIP票。\n升旗山，槟城 升旗山小火车（登山缆车啦）\n交通卡选择 Touch \u0026rsquo;n Go（TnG）卡 最常见的是Touch \u0026rsquo;n Go（TnG）卡，这是一种多功能交通卡，可以用于轻轨（LRT）、地铁（MRT）、单轨列车（Monorail）、KTM火车以及公交车等。此外，TnG卡还可在便利店和部分商店中使用。\nTouch \u0026rsquo;n Go 实体卡：可以在便利店、地铁站（需提前预约）或KL Sentral的Touch \u0026rsquo;n Go Hub购买。提前在淘宝上购买也是一个选择。\nTNG eWallet：是一款需要马来西亚或新加坡手机号注册的手机应用，可以作为电子支付钱包，但不能直接作为实体交通卡使用。\nMyCity Pass MyCity Pass是一种短期通行卡，适用于游客和本地居民，允许在指定时间内不限次数乘坐RapidKL管理的公共交通工具，包括LRT、MRT、单轨列车、BRT和RapidKL公交车。\nMyCity 1日通行证：马来西亚人需支付 RM6，外国游客为 RM10。\nMyCity 3日通行证：马来西亚人需支付 RM15，外国游客为 RM25。\n这些价格不包括Touch \u0026rsquo;n Go卡本身的费用，用户必须有一张Touch \u0026rsquo;n Go卡，并确保卡内有至少 RM5 的余额。MyCity Pass可以在LRT、MRT、单轨列车和BRT的客服中心，或一些公交枢纽（如Pasar Seni和KLCC）购买，也可以通过MyRapid PULSE应用程序在线购买。注意，此卡不适用于ERL（如KLIA Ekspres和KLIA Transit）及KTM Komuter。\nKL Travel Pass KL Travel Pass包含KLIA Ekspres的单程或往返票，并提供2天内无限次乘坐吉隆坡快捷通（RapidKL）旗下的LRT、MRT和单轨列车服务（不包括KTM火车）。\n单程机场快线 + 2天无限次乘坐LRT/MRT/单轨列车：RM80。\n往返机场快线 + 2天无限次乘坐LRT/MRT/单轨列车：RM125。\nKL Travel Pass可在KLIA（第一和第二航站楼）以及KL Sentral的KLIA Ekspres售票柜台购买，也可通过线上平台如KLOOK购买。此卡本质上是一张Touch \u0026rsquo;n Go卡，充值后还可以用于RapidKL公交车及部分零售商店的支付。机场快线车票的有效期为自购买日起一个月，2天的无限次乘车卡将在首次使用后连续两天内有效（非48小时）。\n说实话，体验下来，travelpass并不是一个太好的选择， 首先并没有优惠太多，如果想回本需要在2天内坐够30RM的地铁，而KL的地铁轻轨以1RM起步价， 平均一站增加0.1RM，如果住在市区很难有机会坐这么多站的地铁；\n总结 马来西亚的铁路交通承载着这片土地的历史，与其他交通方式相比，火车不仅能穿越城市与乡村，更能体验到当地的文化与风土人情。\n总体来说，马来西亚的轨道交通体系并不是太复杂（和日本相比）。无论是吉隆坡的地铁、轻轨，还是连通主要城市的城际列车，都是方便快捷的出行选择。尽管马来西亚的油价低廉、打车费用不高，但在城市内，高峰时段的交通拥堵是常见问题。当然，轨道交通仍然是值得考虑的选择。\n参考资料 wiki：马来西亚铁路运输\nwiki：马来亚铁道\nKTMB官网\nMyCity Pass\nklook: KL TravelPass\n","permalink":"https://niraya666.github.io/travel/%E5%A4%A7%E9%A9%AC%E8%BD%A8%E9%81%93%E4%BA%A4%E9%80%9A%E6%94%BB%E7%95%A5/","summary":"\u003ch2 id=\"前言\"\u003e\u003cstrong\u003e前言\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e国庆期间，在西马玩了快一个礼拜，期间体验了从北海到怡保的ETS、槟城的升旗山登山缆车，还有吉隆坡复杂多样的轨道交通系统。旅行中，我踩过一些坑，也解锁了一些便利的玩法。经过查阅不少攻略，加上实地探索和切身体会，我决定将这次旅途中的交通攻略整理成这篇文章，分享给大家。\u003c/p\u003e\n\u003cp\u003e注：本文所包含的信息有效截止至2024年10月，同时主要讨论西马（马来半岛）的轨道交通；希望能对你的大马之旅有所帮助。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"大马铁路概述\"\u003e\u003cstrong\u003e大马铁路概述\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e马来西亚的铁路系统起源于19世纪末的英国殖民时期，当时的铁路主要是为了支持锡矿业和橡胶种植园的经济发展。第一条铁路于1885年建成，连接了太平和砵威港口（今称为瓜拉十八丁），而太平是当时霹雳州英国人的行政中枢。这条铁路的建设是为了便于将矿产资源从内陆运输到港口。此后，英国殖民政府开始加快在马来亚各地开辟铁路的步伐，主要目的是为了促进殖民地经济发展，例如吉隆坡至巴生的铁路于1886年建成通车，芙蓉至波德申的铁路于1891年建成通车，吉隆坡至万挠的铁路于1892年建成通车，安顺至打巴路的铁路于1893年建成通车。1895年，受英国保护的雪兰莪、森美兰、霹雳和彭亨四州组成马来联邦。当时马来亚不同州的铁路各自发展，没有一个统一的规划和管理机构，于是殖民政府当局在1901年成立了马来联邦铁路（FMSR），统合管理马来西亚半岛上各地包括马来联邦、马来属邦、海峡殖民地的铁路。这些初期建设的米轨铁路成为连接各主要城镇和经济区的重要交通工具。\u003c/p\u003e\n\u003cp\u003e20世纪初，马来西亚的铁路网迅速扩展，最重要的两条干线为东海岸线和西海岸线。二战期间，铁路遭到日军的严重破坏，但战后得到了重建。1948年，马来联邦铁路被改组为马来亚铁路（MR）。1963年，马来亚、新加坡、砂拉越和沙巴组成了新的联邦国家——马来西亚（新加坡于1965年退出），马来亚铁路的官方名称也更改为马来语“Keretapi Tanah Melayu”（KTM）。\u003c/p\u003e\n\u003cp\u003e现如今，马来西亚的铁路系统主要由两条干线组成：西海岸线和东海岸线。东海岸线（也称金马士－道北铁路）由马来亚铁道运营，起自金马士，途经吉兰丹、彭亨、森美兰，穿越马来半岛中部，全长528公里，最终到达泰国边境的道北。虽然名为东海岸线，但大部分路段穿行于内陆丛林地带，因此又被称为“丛林铁路”，吸引许多背包客体验。西海岸线则起自泰国边境的巴东勿刹，沿西海岸一直延伸至柔佛的南端，通过新柔长堤连接新加坡，全长950公里，途经包括吉隆坡在内的八个州属，是马来西亚铁路运输的重要交通大动脉。由国有企业马来亚铁道公司（KTMB）运营的铁路系统经过了多次改组和现代化，如今的铁路服务已经涵盖了从通勤列车到高速城际列车的多种交通模式。目前，马来西亚铁路系统不仅联通国内主要城市，还连接着泰国和新加坡，是东南亚地区重要的交通枢纽。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/b90015596a676e51d6df9b98736e90da.jpg\" alt=\"b90015596a676e51d6df9b98736e90da.jpg\"  /\u003e\n\n\u003cem\u003e\u003ca href=\"https://www.pinterest.com/pin/ktm-the-620km-east-coast-rail-route-ecrr--834010424715426579/\"\u003e来源\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"铁路分类\"\u003e\u003cstrong\u003e铁路分类\u003c/strong\u003e\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eKTM Komuter（城际通勤列车）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eKTM Komuter是由马来亚铁道公司（KTM）运营的通勤铁路服务，最初于1995年在巴生谷地区启用，后于2015年扩展至北部各州（如霹雳、槟城、吉打和玻璃市）。该服务连接了吉隆坡及其周边郊区，还服务于槟城和其他北部州属，总长度约456公里，包含芙蓉线、巴生港线等多条线路。KTM Komuter的列车是空调电力列车，并提供便捷的“停车换乘”设施，适合游客和通勤者方便进入市区，避免交通拥堵。KTM通勤铁路还为乘客提供多种票务选项，包括通过KTMB移动应用程序购买手机票（二维码）、Touch \u0026rsquo;n Go（仅限巴生谷地区）、自动售票机与车站售票柜台购票。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eETS（电动列车服务）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eETS（Electric Train Service）是由马来亚铁道公司运营的快速城际电力动车组服务，于西海岸线运行，连接金马士与巴东勿刹，全长950公里。ETS是马来西亚最快的米轨列车，速度可达每小时140公里，属于准高速铁路。列车分为Gold、Silver、Platinum 以及 Express 4种等级，分别对应不同的服务和票价，是前往怡保、槟城等城市的高效方式。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eKTM Intercity（城际列车）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eKTM Intercity是马来亚铁道运营的柴油牵引城际列车服务，覆盖马来半岛的南部地区、新加坡以及泰国。部分列车主要运行于东海岸线金马士至道北之间，并延伸至新加坡。2021年4月推出了柴油动车组（DMU）列车，取代了原有的传统列车，服务于东海岸的丹绒至吉利地段。DMU列车配备现代化设施，包括集群座椅、LED显示屏、电源插座、祈祷室、厕所、小餐馆及行李存储空间，并有无障碍设施，满足残障人士的需求。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"如何购买火车票\"\u003e\u003cstrong\u003e如何购买火车票\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e购买火车票的方式有多种，最为方便的是通过KTMB官方网站或手机应用程序进行在线购买。也可以在吉隆坡中环车站（KL Sentral）及其他主要车站直接购票，但热门线路的车票在高峰时期（如假日、周末）可能会售罄，建议提前预订。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/%e6%88%aa%e5%b1%8f2024-10-13%20%e4%b8%8b%e5%8d%887.15.27.png\" alt=\"截屏2024-10-13 下午7.15.27.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/%e6%88%aa%e5%b1%8f2024-10-13%20%e4%b8%8b%e5%8d%887.16.04.png\" alt=\"截屏2024-10-13 下午7.16.04.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e以网上购买ETS车票为例，需在KTMB官网注册账号，然后选择出发和到达车站、以及对应的ETS车次类型（如Gold、Platinum、Express等）。不同级别的车次在停靠站数量和服务上有所不同，例如Platinum和Express级别提供商务座，商务座采用1+2的座位排布，包含免费食物和饮料，价格相对更高；普通席则是2+2的座位排布。Gold车次则没有商务座位。选择座位时，需要注意座位方向，有些座位是与行驶方向相反的，如果不喜欢反向乘坐，记得选择“forward”座位。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/%e6%88%aa%e5%b1%8f2024-10-13%20%e4%b8%8b%e5%8d%887.47.05.png\" alt=\"截屏2024-10-13 下午7.47.05.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eETS列车的车型也有所区别，Platinum和Express列车多为较新款，由中国中车制造，而Silver级别列车则采用较老款，速度较慢，停靠站较多。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/7F6C6773-CA59-499B-93C2-2825C2DAE157_1_105_c.jpeg\" alt=\"7F6C6773-CA59-499B-93C2-2825C2DAE157_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eETS-Express列车（摄于Butterworth车站）\u003c/em\u003e\n\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/B03BE15A-8BF1-4709-BC16-4C95696ED6FE_1_105_c.jpeg\" alt=\"B03BE15A-8BF1-4709-BC16-4C95696ED6FE_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eETS-Gold列车，采用旧款车型（摄于Ipoh车站）\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"吉隆坡轨道交通\"\u003e\u003cstrong\u003e吉隆坡轨道交通\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e吉隆坡及其周边地区有完善的轨道交通系统，涵盖了多种轻轨和地铁线路。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/%e6%88%aa%e5%b1%8f2024-10-14%20%e4%b8%8b%e5%8d%887.56.41.png\" alt=\"截屏2024-10-14 下午7.56.41.png\"  /\u003e\n\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLRT（轻快铁）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLRT是吉隆坡的轻轨系统，包含3号 LRT Ampang Line、4号LRT Sri Petaling Line和5号LRT Kelana Jaya Line。LRT贯穿吉隆坡市中心及重要商业区，是游客前往市区景点如双子塔、茨厂街的首选交通工具。在高峰期，列车会非常拥挤。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%a9%ac%e8%bd%a8%e9%81%93%e4%ba%a4%e9%80%9a%e6%94%bb%e7%95%a5-assets/107EF1BC-2124-4F8E-BCEA-7027EA7E3C54_1_105_c.jpeg\" alt=\"107EF1BC-2124-4F8E-BCEA-7027EA7E3C54_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e5号线LRT，摄于Bangsar车站\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e3号LRTAmpang Line（橙黄色）\u003c/strong\u003e：占美清真寺，独立广场，ICC pudu等景点。\u003c/p\u003e","title":"2024-大马轨道交通攻略"},{"content":"第一次来到舟山，仅仅是因为Can-festival音乐节。\n上次错过了越位音乐节，这次看到Can-festival的阵容，根本不可能再错过。光是Day2的阵容：We Lost the Sea、Lost in Kiev、The Seven Mile Journey、声子虫，几乎全是我播放器里多年循环的乐队。更别提Day3，This Will Destroy You和World’s End Girlfriend等老牌乐队，还有年初遗憾未能见到的四月雨。\n与“普拉桑”的极限拉扯 出发前几天，台风“普拉桑”紧跟着前一个台风的步伐，直扑江浙沪，着实让我焦虑不已。临行的前一天，我每隔二十分钟就要刷新一次台风路径图，心里祈祷它能转向或加速离去。担心音乐节会取消，或是要在狂风暴雨中听音乐。不过，最终台风带来的破坏都留在了上海，而到了舟山，却意外地迎来了晴朗的天气。\n暴雨中的南浦大桥\n后摇和雨更配 其实，我一向讨厌下雨。但谁能想到，在海边，后摇与风雨竟如此契合。或许，这正是音乐的魔力。雨水肆意洒落，浸湿了镜片，视线模糊不清，分不清脸上的水滴究竟是从天而降，还是心底涌出的泪水。音乐与环境的交织，给了我一种难以言喻的感动。\n海边\n当安尼西亚说这首歌是献给孩子，愿这个世界没有伤害时，我的心突然被一种无法抑制的情感击中。或许我们无法阻止每一个悲剧的发生，但在音乐的陪伴下，我希望这个世界能逐渐变得更加温暖。愿这首《离别》不仅仅是一首哀歌。\nLost in Kiev 在合成器音色和处理上，确实惊艳到我了。不过，他们的贝斯手长的真的好像萨拉赫。\nLost in Kiev\nLITE 的音乐在雨中展现出了出乎意料的活力。即使雨水不停，现场的每个人都随着节奏跳动，那种难以言喻的现场氛围，仿佛雨水并不再是困扰，而是与音乐共舞的一部分。\n雨中的LITE\n声子虫的音墙依旧厚重，像一座无形的高山压迫着我。正如去年看他们演出时的一样， 特别喜欢他们开场时的创意—随着音乐的铺垫进行， 在屏幕上，一笔一画地把“聲子蟲” 三个字缓慢打出。\n缺了最后一笔的“聲子蟲”\n一直期待We Lost the Sea的现场演出，但就在那时，吉他却出了问题。声音时有时无，让本该震撼人心的演奏显得有些破碎。雨水不停地落在身上，心情也随之变得复杂。一点小小的遗憾，像是雨中一滴未曾融入地面的水珠，总让人感到有些不完整。\nWe Lost the Sea — by 群里的艺术家\nThe Seven Mile Journey — by 群里的艺术家\n年纪渐长，似乎每次出门前都有更多的顾虑。年轻时我总能义无反顾地冲向每一个音乐节，不管是风雨还是其他未知的挑战，都觉得无所谓。然而这次，在雨中，我的脑海里浮现出更多的“如果”：如果天气更好一些，如果能再勇敢一些……我无法完全放下这些顾虑，哪怕台上的音乐依旧动人，我却发现自己没有像以前那样尽兴了。也许成长的代价，就是学会在更多的限制和考量中寻找自己的平衡。\n遗憾，化作前行的力量 只看了Day2的演出后，考虑到气温骤降和大雨，再加上风大到船次取消，去普陀山的计划也被迫放弃，我决定提前撤离。没想到，海边的天气竟如此难以捉摸，Day3下午居然又转晴了。错过了傍晚的晚霞，也错过了在L-ISA环绕音响下的World’s End Girlfriend演出，实在是遗憾至极。然而，这些遗憾，正是让我期待下一次音乐节的动力。或许，正因为未尽的体验，才会让我对未来的演出更加期待。\n也感谢群里的小伙伴传来的照片和视频， 能让我第一时间见证这些美丽的瞬间。\n我错过的晚霞 — by 群里的艺术家\n四月雨和夕阳 — by 群里的艺术家\nThis Will Destroy You — by 群里的艺术家\nWEG — by 群里的艺术家\n当然，更要感谢罐总Can给大家带来的这次演出，愿大家都能圆梦。\n","permalink":"https://niraya666.github.io/travel/2024-can-festival-%E5%9C%A8%E6%B5%B7%E8%BE%B9-%E6%B7%B7%E6%9D%82%E7%9D%80%E9%9B%A8%E6%B0%B4%E5%92%8C%E6%B3%AA%E6%B0%B4-%E5%90%91%E5%90%8E%E6%91%87/","summary":"\u003cp\u003e第一次来到舟山，仅仅是因为Can-festival音乐节。\u003c/p\u003e\n\u003cp\u003e上次错过了越位音乐节，这次看到Can-festival的阵容，根本不可能再错过。光是Day2的阵容：We Lost the Sea、Lost in Kiev、The Seven Mile Journey、声子虫，几乎全是我播放器里多年循环的乐队。更别提Day3，This Will Destroy You和World’s End Girlfriend等老牌乐队，还有年初遗憾未能见到的四月雨。\u003c/p\u003e\n\u003ch2 id=\"与普拉桑的极限拉扯\"\u003e与“普拉桑”的极限拉扯\u003c/h2\u003e\n\u003cp\u003e出发前几天，台风“普拉桑”紧跟着前一个台风的步伐，直扑江浙沪，着实让我焦虑不已。临行的前一天，我每隔二十分钟就要刷新一次台风路径图，心里祈祷它能转向或加速离去。担心音乐节会取消，或是要在狂风暴雨中听音乐。不过，最终台风带来的破坏都留在了上海，而到了舟山，却意外地迎来了晴朗的天气。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/DE10C0BD-A093-45EE-B67F-421C767FE234.jpg\" alt=\"DE10C0BD-A093-45EE-B67F-421C767FE234.jpg\"  /\u003e\n\n\u003cem\u003e暴雨中的南浦大桥\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"后摇和雨更配\"\u003e后摇和雨更配\u003c/h2\u003e\n\u003cp\u003e其实，我一向讨厌下雨。但谁能想到，在海边，后摇与风雨竟如此契合。或许，这正是音乐的魔力。雨水肆意洒落，浸湿了镜片，视线模糊不清，分不清脸上的水滴究竟是从天而降，还是心底涌出的泪水。音乐与环境的交织，给了我一种难以言喻的感动。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/8E78A7AB-8588-4165-B71F-261E284C1E51_1_105_c.jpeg\" alt=\"8E78A7AB-8588-4165-B71F-261E284C1E51_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e海边\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e当安尼西亚说这首歌是献给孩子，愿这个世界没有伤害时，我的心突然被一种无法抑制的情感击中。或许我们无法阻止每一个悲剧的发生，但在音乐的陪伴下，我希望这个世界能逐渐变得更加温暖。愿这首《离别》不仅仅是一首哀歌。\u003c/p\u003e\n\u003cp\u003eLost in Kiev 在合成器音色和处理上，确实惊艳到我了。不过，他们的贝斯手长的真的好像萨拉赫。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/ABF036E9-FBF3-4B73-9B8F-6804C5FFBEBD_1_105_c.jpeg\" alt=\"ABF036E9-FBF3-4B73-9B8F-6804C5FFBEBD_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eLost in Kiev\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eLITE 的音乐在雨中展现出了出乎意料的活力。即使雨水不停，现场的每个人都随着节奏跳动，那种难以言喻的现场氛围，仿佛雨水并不再是困扰，而是与音乐共舞的一部分。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/CEF4C784-5E96-402F-A73B-AD3ED115EF1F_1_105_c.jpeg\" alt=\"CEF4C784-5E96-402F-A73B-AD3ED115EF1F_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e雨中的LITE\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e声子虫的音墙依旧厚重，像一座无形的高山压迫着我。正如去年看他们演出时的一样， 特别喜欢他们开场时的创意—随着音乐的铺垫进行， 在屏幕上，一笔一画地把“聲子蟲” 三个字缓慢打出。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/46590887-7972-4CEF-A4F7-A40BF24EDAE4_1_105_c.jpeg\" alt=\"46590887-7972-4CEF-A4F7-A40BF24EDAE4_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003e缺了最后一笔的“聲子蟲”\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e一直期待We Lost the Sea的现场演出，但就在那时，吉他却出了问题。声音时有时无，让本该震撼人心的演奏显得有些破碎。雨水不停地落在身上，心情也随之变得复杂。一点小小的遗憾，像是雨中一滴未曾融入地面的水珠，总让人感到有些不完整。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/BEA4ACE5-B17A-48FB-9E95-EE2BFA987988_1_105_c.jpeg\" alt=\"BEA4ACE5-B17A-48FB-9E95-EE2BFA987988_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eWe Lost the Sea — by 群里的艺术家\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/2024-Can-festival/1EF13608-9B3E-4159-90C3-034AAD648C5D_1_105_c.jpeg\" alt=\"1EF13608-9B3E-4159-90C3-034AAD648C5D_1_105_c.jpeg\"  /\u003e\n\n\u003cem\u003eThe Seven Mile Journey — by 群里的艺术家\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e年纪渐长，似乎每次出门前都有更多的顾虑。年轻时我总能义无反顾地冲向每一个音乐节，不管是风雨还是其他未知的挑战，都觉得无所谓。然而这次，在雨中，我的脑海里浮现出更多的“如果”：如果天气更好一些，如果能再勇敢一些……我无法完全放下这些顾虑，哪怕台上的音乐依旧动人，我却发现自己没有像以前那样尽兴了。也许成长的代价，就是学会在更多的限制和考量中寻找自己的平衡。\u003c/p\u003e","title":"2024 Can-festival：在海边 混杂着雨水和泪水 向后摇"},{"content":"在使用大型语言模型（LLM）时，我们常常面临一个挑战：如何从模型输出中准确提取自己所需的信息。例如，当我们希望模型输出 JSON 格式的数据时，由于模型生成的内容并不总是稳定，可能需要额外编写大量的正则表达式来匹配并提取其中的有效信息。然而，由于 LLM 的能力，导致其输出结构并不永远可靠。\n现阶段， 让LLM按要求生成特定格式文本的主要方法有几种种：\n微调：使模型的输出遵循特定格式\nOpenAI Json-mode/Structured Outputs/function-calling: 这些功能允许模型生成更严格、结构化的输出，但受限于openAI平台。\n格式约束：在decoding阶段进行约束，限制模型的输出，\nPrompt Engineering： 最简单的办法，但不稳定。\n多阶段prompting： 通过多个步骤的提示逐步引导模型生成所需的格式。\n本文将聚焦在Structured Outputs， 受限编码， 和prompt-engineering的角度，探讨它们在生成特定格式文本中的应用和效果。\nJson Mode 仅特定模型和平台支持\n以openAI 为例， 在openai.chat.completions.create 参数中增加response_format={\u0026quot;type\u0026quot;:\u0026quot;json_object\u0026quot;} 即可（具体参见：response_format ）。\n需要在prompt中要求输出json格式\n不能保证完全按要求的格式结构输出\n但非100%成功率，存在一些需要额外检测和适当处理的edge case。\nHandling edge cases 根据OpenAI官方文档提供的处理方案 https://platform.openai.com/docs/guides/structured-outputs/json-mode we_did_not_specify_stop_tokens = True try: response = client.chat.completions.create( model=\u0026#34;gpt-3.5-turbo-0125\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant designed to output JSON.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Who won the world series in 2020? Please respond in the format {winner: ...}\u0026#34;} ], response_format={\u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34;} ) # Check if the conversation was too long for the context window, resulting in incomplete JSON if response.choices[0].message.finish_reason == \u0026#34;length\u0026#34;: # your code should handle this error case pass # Check if the OpenAI safety system refused the request and generated a refusal instead if response.choices[0].message[0].get(\u0026#34;refusal\u0026#34;): # your code should handle this error case # In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing print(response.choices[0].message[0][\u0026#34;refusal\u0026#34;]) # Check if the model\u0026#39;s output included restricted content, so the generation of JSON was halted and may be partial if response.choices[0].message.finish_reason == \u0026#34;content_filter\u0026#34;: # your code should handle this error case pass if response.choices[0].message.finish_reason == \u0026#34;stop\u0026#34;: # In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a \u0026#34;stop token\u0026#34; if we_did_not_specify_stop_tokens: # If you didn\u0026#39;t specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object # This is guaranteed to parse successfully and should now contain \u0026#34;{\u0026#34;winner\u0026#34;: \u0026#34;Los Angeles Dodgers\u0026#34;}\u0026#34; print(response.choices[0].message.content) else: # Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately pass except Exception as e: # Your code should handle errors here, for example a network error calling the API print(e) 使用pydantic的方案 使用pydantic的方案 from pydantic import BaseModel, EmailStr, ValidationError # 定义你期望的 JSON 数据模型 class UserModel(BaseModel): name: str age: int email: EmailStr # 检查 JSON 是否符合模型的函数 def validate_json(json_str): try: # 将输入的 JSON 字符串转换为 UserModel 实例 user = UserModel.parse_raw(json_str) # 如果验证通过，返回字典 return user.dict() except ValidationError as ve: print(f\u0026#34;JSON validation error: {ve.json()}\u0026#34;) return None # 示例用法 json_str = \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;}\u0026#39; validated_json = validate_json(json_str) if validated_json is not None: print(\u0026#34;JSON is valid and conforms to the schema:\u0026#34;) print(validated_json) else: print(\u0026#34;JSON is invalid.\u0026#34;) Json-Mode 更多是对于输出json的格式进行检查(即Json格式的有效性)\nStructured Outputs 在模型升级到2024-08-06版本后， OpenAI增加了Structured Outputs功能。\n其旨在保证模型始终生成符合所提供的JSON Schema的响应，而无需处理edge case，以及json内容错误。\n其优势在于：\n无需验证或重试格式不正确的响应\n可通过编程检测模型是否安全生成（在API返回结果中增加了\u0026quot;refusal”字段）\n无需复杂的prompt用于限制输出结构\n相较于Json-Mode， 虽然两者都确保生成有效的JSON，但只有Structured Outputs能确保输出json-schema的一致性。因此理论上无需额外的json-schema检查的代码步骤。\n如何使用 以官方文档所提供的`Structured data extraction`为例：\nfrom pydantic import BaseModel from openai import OpenAI client = OpenAI() class ResearchPaperExtraction(BaseModel): title: str authors: list[str] abstract: str keywords: list[str] completion = client.beta.chat.completions.parse( model=\u0026#34;gpt-4o-2024-08-06\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;...\u0026#34;} ], response_format=ResearchPaperExtraction, ) research_paper = completion.choices[0].message.parsed output { \u0026#34;title\u0026#34;: \u0026#34;Application of Quantum Algorithms in Interstellar Navigation: A New Frontier\u0026#34;, \u0026#34;authors\u0026#34;: [ \u0026#34;Dr. Stella Voyager\u0026#34;, \u0026#34;Dr. Nova Star\u0026#34;, \u0026#34;Dr. Lyra Hunter\u0026#34; ], \u0026#34;abstract\u0026#34;: \u0026#34;This paper investigates the utilization of quantum algorithms to improve interstellar navigation systems. By leveraging quantum superposition and entanglement, our proposed navigation system can calculate optimal travel paths through space-time anomalies more efficiently than classical methods. Experimental simulations suggest a significant reduction in travel time and fuel consumption for interstellar missions.\u0026#34;, \u0026#34;keywords\u0026#34;: [ \u0026#34;Quantum algorithms\u0026#34;, \u0026#34;interstellar navigation\u0026#34;, \u0026#34;space-time anomalies\u0026#34;, \u0026#34;quantum superposition\u0026#34;, \u0026#34;quantum entanglement\u0026#34;, \u0026#34;space travel\u0026#34; ] } 和在LLM输出的结果中通过`pydantic`进行格式检查类似，只是直接将格式要求传给了模型，减少了额外工作。 等价于\n... \u0026#34;response_format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;json_schema\u0026#34;, \u0026#34;json_schema\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;research_paper_extraction\u0026#34;, \u0026#34;schema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;authors\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;abstract\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;keywords\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } }, \u0026#34;required\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;authors\u0026#34;, \u0026#34;abstract\u0026#34;, \u0026#34;keywords\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false }, \u0026#34;strict\u0026#34;: true } } 此外， 文档中还提供了COT的Structured Outputs例子\nChain of thought\nfrom pydantic import BaseModel from openai import OpenAI client = OpenAI() class Step(BaseModel): explanation: str output: str class MathReasoning(BaseModel): steps: list[Step] final_answer: str completion = client.beta.chat.completions.parse( model=\u0026#34;gpt-4o-2024-08-06\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful math tutor. Guide the user through the solution step by step.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;how can I solve 8x + 7 = -23\u0026#34;} ], response_format=MathReasoning, ) math_reasoning = completion.choices[0].message.parsed output:\n{ \u0026#34;steps\u0026#34;: [ { \u0026#34;explanation\u0026#34;: \u0026#34;Start with the equation 8x + 7 = -23.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;8x + 7 = -23\u0026#34; }, { \u0026#34;explanation\u0026#34;: \u0026#34;Subtract 7 from both sides to isolate the term with the variable.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;8x = -23 - 7\u0026#34; }, { \u0026#34;explanation\u0026#34;: \u0026#34;Simplify the right side of the equation.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;8x = -30\u0026#34; }, { \u0026#34;explanation\u0026#34;: \u0026#34;Divide both sides by 8 to solve for x.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;x = -30 / 8\u0026#34; }, { \u0026#34;explanation\u0026#34;: \u0026#34;Simplify the fraction.\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;x = -15 / 4\u0026#34; } ], \u0026#34;final_answer\u0026#34;: \u0026#34;x = -15 / 4\u0026#34; } 相比于之前普通版本的COT：\n在测试阶段，程序化检查中间Chain的中间过程变得更加简单\n更优美的提取最终结果\n限制分析过程中的每一步格式，以提高正确率？\nOpenAI API中提供了两种形式的Structured Outputs\nFunction Calling\n使用 json_schema 响应格式\n在function calling （tool-using）时， 在定义tools时需增加\u0026quot;strict\u0026quot;: true 字段，如：\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_delivery_date\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the delivery date for a customer\u0026#39;s order. Call this whenever you need to know the delivery date, for example when a customer asks \u0026#39;Where is my package\u0026#39;\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;order_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The customer\u0026#39;s order ID.\u0026#34;, }, }, \u0026#34;required\u0026#34;: [\u0026#34;order_id\u0026#34;], \u0026#34;additionalProperties\u0026#34;: False, }, }, \u0026#34;strict\u0026#34;: True, } ] 注意：\u0026quot;strict\u0026quot;: true,会产生一定的延迟（10s以下）\nStructured Outputs功能引入了一个新的字段refusal，用于在模型拒绝执行请求时提供反馈。当用户生成的输入时，模型可能会因为安全原因拒绝履行请求，此时API响应会包含一个名为“refusal”的新字段来指示模型拒绝了请求。当API响应中出现refusal字段时，开发者可以程序化地检测到模型生成了拒绝而不是符合模式的输出。\nConstrained Decoding What Is Constrained Decoding constrained decoding is a technique that manipulates a generative model\u0026rsquo;s token generation process to constrain its next-token predictions to only tokens that do not violate the required output structure.\n仅对严格需要生成的输出部分，从模型的下一个Token概率分布中的兼容Token集进行采样。\n简单来说，对于LLM的输出过程，模型会根据之前的词语序列，计算出下一个词可能出现的概率，并根据这个概率生成下一个词。这一过程会不断重复，直到生成完整的句子或段落（或遇到[EOS]标记）。在普通的生成过程中，模型的概率空间是所有可能的Token。\n相对而言，在引导生成（Guided Generation）过程中，模型的输出会受到额外的引导或约束，以确保生成的内容更加符合某些特定的要求或目标。这种引导可以通过调整概率分布、加入上下文信息、或施加额外规则来实现。\n具体来说， 对于给定的输入序列 $\\tilde{S}_t$ 和模型参数 $\\theta$，\n模型首先计算出一个未加约束的概率分布 $\\alpha$，表示下一个Token $\\tilde{s}_{t+1}$ 的可能性 ，\n$$ \\alpha = \\text{LM}(\\tilde{S}_t, \\theta) $$\n为了引入约束或限制，构造一个布尔掩码 $m$，这个掩码将限制下一个Token的选择范围。掩码函数 $m(\\tilde{S}_t)$将原始分布中的部分不符合约束条件的Token概率置为0，形成一个新的受限分布 $\\tilde{\\alpha}$：\n$$ \\tilde{\\alpha} = m(\\tilde{S}_t) \\odot \\alpha $$\n即在原始分布$\\alpha$ 上应用掩码 $m$，只保留那些满足约束条件的Token的概率，其他Token的概率被置为0。\n接下来，模型根据受限分布 $\\tilde{\\alpha}$ 进行采样，选择下一个Token $\\tilde{s}_{t+1}$\n$$ \\tilde{s}_{t+1} \\sim \\text{Categorical}(\\tilde{\\alpha}) $$\n这个步骤意味着模型只会从符合约束条件的Token集合中选择下一个Token，确保生成的输出遵循预定的规则或格式。\n以上内容出自 outlines的paper：Efficient Guided Generation for Large Language Model 具体内容可自行查看。\n以常见的Json格式约束为例（Fast JSON Decoding for Local LLMs with Compressed Finite State Machine），\n有多种框架可以利用本地模型实现约束解码， 如： outlines，SGLang等。\nvllm+outlines 实现结构化输出 依赖环境：\n! pip install vllm ! pip install openai ! pip install instructor 注意：需要使用到GPU，建议使用colab\n启动vllm模型服务，选择适合GPU显存大小的模型：\n#! vllm serve Qwen/Qwen1.5-4B-Chat --max_model_len=10000 ! vllm serve Qwen/Qwen1.5-1.8B --max_model_len=10000 引导生成json结构：\nfrom openai import OpenAI # Set OpenAI\u0026#39;s API key and API base to use vLLM\u0026#39;s API server. openai_api_key = \u0026#34;EMPTY\u0026#34; openai_api_base = \u0026#34;http://localhost:8000/v1\u0026#34; api_client = OpenAI(api_key=\u0026#39;YOUR_API_KEY\u0026#39;, base_url=\u0026#39;http://localhost:8000/v1\u0026#39;) model_name = api_client.models.list().data[0].id json_schema = { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;thought\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, \u0026#34;answer\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;thought\u0026#34;, \u0026#34;answer\u0026#34;] } query = \u0026#34;What is the capital of China?\u0026#34; # Feed the random question into the existing query system_prompt = \u0026#34;Respond only with a json object containing the following fields and nothing else: thought, answer.\u0026#34; completion = api_client.chat.completions.create( model=model_name, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_prompt}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: query} ], extra_body={ \u0026#34;stop_token_ids\u0026#34;: [128009], \u0026#34;response_format\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34;}, \u0026#34;guided_json\u0026#34;: json_schema } ) 输出：\nprint(completion.choices[0].message.content.strip()) # {\u0026#34;thought\u0026#34;: \u0026#34;What is the capital of China?\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;Beijing\u0026#34;} 另一个例子， 引导生成符合结构的《Harry Potter》角色属性：\n定义角色属性结构\nfrom openai import OpenAI from pydantic import BaseModel, Field import base64 import instructor import time from enum import Enum from typing import List from glob import glob class Property(BaseModel): key: str = Field(description=\u0026#34;Must be snake case\u0026#34;) value: str class House(Enum): Griffindor = \u0026#34;gryffindor\u0026#34; Hufflepuff = \u0026#34;hufflepuff\u0026#34; Ravenclaw = \u0026#34;ravenclaw\u0026#34; Slytherin = \u0026#34;slytherin\u0026#34; Targaryen = \u0026#34;targaryen\u0026#34; # This schema is what guides generation class Character(BaseModel): name: str age: int house: House properties: List[Property] api_client = OpenAI(api_key=\u0026#39;YOUR_API_KEY\u0026#39;, base_url=\u0026#39;http://localhost:8000/v1\u0026#39;) client = instructor.from_openai(api_client, mode=instructor.Mode.MD_JSON) model_name = api_client.models.list().data[0].id system_prompt = \u0026#34;\u0026#34;\u0026#34;\\ You are a highly creative assistant tasked with generating fictional characters for a fantasy novel. Each character belongs to a specific house, and their properties should reflect their house\u0026#39;s traits. However, ensure that the characters are entirely original and do not directly reference or resemble any characters, events, or settings from the Harry Potter series. Avoid using names, events, or any elements that would make the character too similar to existing Harry Potter content. The goal is to create unique and novel characters that can stand on their own in a fantasy world. \u0026#34;\u0026#34;\u0026#34; def run(name): try: response = client.chat.completions.create( model=model_name, response_model=Character, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: system_prompt }, { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: name } ], temperature=0.1, # Adjust temperature for creativity ) return response except Exception as e: print(f\u0026#34;An error occurred: {e}\u0026#34;) return None # Example usage character_name = \u0026#34;Voldemort\u0026#34; character = run(character_name) if character: print(character) else: print(\u0026#34;Failed to generate character.\u0026#34;) # name=\u0026#39;Voldemort\u0026#39; age=50 house=\u0026lt;House.Slytherin: \u0026#39;slytherin\u0026#39;\u0026gt; properties=[Property(key=\u0026#39;power\u0026#39;, value=\u0026#39;dark magic\u0026#39;), Property(key=\u0026#39;reputation\u0026#39;, value=\u0026#39;evil\u0026#39;), Property(key=\u0026#39;history\u0026#39;, value=\u0026#39;murdered his parents and became the Dark Lord\u0026#39;)] Prompt-based Methods 根据奥卡姆剃刀法制，“如无必要，勿增实体”， 或许使用prompt也可以实现， 避免引入不必要的复杂性。\n使用Prompt Engineering 要求LLM输出符合格式要求的json（或者其他）时， 可能可以用的一些小Tips：\n明确的指令和提示，并提供具体的例子: 往往LLM的few-shot learning 的能力还行， 模仿个简单的json格式基本不在话下。\n使用占位符： 为具体的json元素标注数据类型和格式，便于LLM理解。这一步骤类似于给json定义一个schema。如\n{ \u0026#34;user\u0026#34;: \u0026#34;示例用户\u0026#34;, \u0026#34;id\u0026#34;: 12345, \u0026#34;items\u0026#34;: [ {\u0026#34;item_name\u0026#34;: \u0026#34;示例物品1\u0026#34;, \u0026#34;quantity\u0026#34;: 2}, {\u0026#34;item_name\u0026#34;: \u0026#34;示例物品2\u0026#34;, \u0026#34;quantity\u0026#34;: 1} ] } 而不是一句“将结果输出成json格式”\n调整温度： 采用较低的温度设置\n分步骤生成： 对于复杂的JSON结构，可以要求模型逐步生成每个部分，然后将它们组合起来； 或先生成JSON的高层结构，然后再深入生成每个子部分。\n让LLM自己检查：类似“三段式翻译” 之类的prompt， LLM首先生成第一版json，而后LLM对LLM进行检查，并提出修改建议， 最后基于建议和初版输出生成最终版结果。\n格式有效性检查： 使用如Pydantic等工具进行验证和解析，如果模型生成了不正确的JSON，可以通过循环过程直到输出有效的JSON为止；或使用一些第三方库修复json, 如json_repair。\n这里推荐一个开源项目， TypeChat， 一个由微软开发，用于让LLM输出类型定义的结果。TypeChat 自动处理生成提示、验证 LLM 输出是否符合定义的类型，并在必要时修复不符合的输出。\n其核心是通过prompt的方式，要求LLM按特定要求输出。\n以官方提供的Calendar例子为例：\n其实现功能即，将自然语言转换成json格式， 用于后续的代办事项的工具使用。\n如输入：\n📅\u0026gt; I need to get my tires changed from 12:00 to 2:00 pm on Friday March 15, 2024 则会输出：\n{ \u0026#34;actions\u0026#34;: [ { \u0026#34;actionType\u0026#34;: \u0026#34;add event\u0026#34;, \u0026#34;event\u0026#34;: { \u0026#34;day\u0026#34;: \u0026#34;Friday March 15, 2024\u0026#34;, \u0026#34;timeRange\u0026#34;: { \u0026#34;startTime\u0026#34;: \u0026#34;12:00 pm\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2:00 pm\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;get my tires changed\u0026#34; } } ] } 首先需要事先定义具体且复杂的schema，如 schema.py\nfrom typing_extensions import Literal, NotRequired, TypedDict, Annotated, Doc class UnknownAction(TypedDict): \u0026#34;\u0026#34;\u0026#34; if the user types text that can not easily be understood as a calendar action, this action is used \u0026#34;\u0026#34;\u0026#34; actionType: Literal[\u0026#34;Unknown\u0026#34;] text: Annotated[str, Doc(\u0026#34;text typed by the user that the system did not understand\u0026#34;)] class EventTimeRange(TypedDict, total=False): startTime: str endTime: str duration: str class Event(TypedDict): day: Annotated[str, Doc(\u0026#34;date (example: March 22, 2024) or relative date (example: after EventReference)\u0026#34;)] timeRange: EventTimeRange description: str location: NotRequired[str] participants: NotRequired[Annotated[list[str], Doc(\u0026#34;a list of people or named groups like \u0026#39;team\u0026#39;\u0026#34;)]] class EventReference(TypedDict, total=False): \u0026#34;\u0026#34;\u0026#34; properties used by the requester in referring to an event these properties are only specified if given directly by the requester \u0026#34;\u0026#34;\u0026#34; day: Annotated[str, Doc(\u0026#34;date (example: March 22, 2024) or relative date (example: after EventReference)\u0026#34;)] dayRange: Annotated[str, Doc(\u0026#34;(examples: this month, this week, in the next two days)\u0026#34;)] timeRange: EventTimeRange description: str location: str participants: list[str] class FindEventsAction(TypedDict): actionType: Literal[\u0026#34;find events\u0026#34;] eventReference: Annotated[EventReference, Doc(\u0026#34;one or more event properties to use to search for matching events\u0026#34;)] class ChangeDescriptionAction(TypedDict): actionType: Literal[\u0026#34;change description\u0026#34;] eventReference: NotRequired[Annotated[EventReference, Doc(\u0026#34;event to be changed\u0026#34;)]] description: Annotated[str, Doc(\u0026#34;new description for the event\u0026#34;)] class ChangeTimeRangeAction(TypedDict): actionType: Literal[\u0026#34;change time range\u0026#34;] eventReference: NotRequired[Annotated[EventReference, Doc(\u0026#34;event to be changed\u0026#34;)]] timeRange: Annotated[EventTimeRange, Doc(\u0026#34;new time range for the event\u0026#34;)] class AddParticipantsAction(TypedDict): actionType: Literal[\u0026#34;add participants\u0026#34;] eventReference: NotRequired[ Annotated[EventReference, Doc(\u0026#34;event to be augmented; if not specified assume last event discussed\u0026#34;)] ] participants: NotRequired[Annotated[list[str], \u0026#34;new participants (one or more)\u0026#34;]] class RemoveEventAction(TypedDict): actionType: Literal[\u0026#34;remove event\u0026#34;] eventReference: EventReference class AddEventAction(TypedDict): actionType: Literal[\u0026#34;add event\u0026#34;] event: Event Actions = ( AddEventAction | RemoveEventAction | AddParticipantsAction | ChangeTimeRangeAction | ChangeDescriptionAction | FindEventsAction | UnknownAction ) class CalendarActions(TypedDict): actions: list[Actions] 这些类表示各种日历操作，每个操作类都有一个actionType字段，确定操作类型：\n• FindEventsAction：查找事件，包含一个eventReference字段，用于指定要查找的事件属性。\n• ChangeDescriptionAction：更改事件描述，包含eventReference（可选）和新的description字段。\n• ChangeTimeRangeAction：更改事件的时间范围，包含eventReference（可选）和新的timeRange字段。\n• AddParticipantsAction：为事件添加参与者，包含eventReference（可选）和参与者列表participants（可选）。\n• RemoveEventAction：删除事件，包含一个eventReference字段。\n• AddEventAction：添加新事件，包含一个event字段。\n在TypeChat的核心 prompt如下：\nprompt = f\u0026#34;\u0026#34;\u0026#34; You are a service that translates user requests into JSON objects of type \u0026#34;{self._type_name}\u0026#34; according to the following TypeScript definitions: ``` {self._schema_str} ``` The following is a user request: \u0026#39;\u0026#39;\u0026#39; {intent} \u0026#39;\u0026#39;\u0026#39; The following is the user request translated into a JSON object with 2 spaces of indentation and no properties with the value undefined: \u0026#34;\u0026#34;\u0026#34; 以上面的例子，最终生成的prompt：\n\u0026#34;\u0026#34;\u0026#34; You are a service that translates user requests into JSON objects of type \u0026#34;CalendarActions\u0026#34; according to the following TypeScript definitions: ``` interface CalendarActions { actions: Array\u0026lt; | AddEventAction | RemoveEventAction | AddParticipantsAction | ChangeTimeRangeAction | ChangeDescriptionAction | FindEventsAction | UnknownAction \u0026gt;; } // if the user types text that can not easily be understood as a calendar action, this action is used interface UnknownAction { actionType: \u0026#34;Unknown\u0026#34;; // text typed by the user that the system did not understand text: string; } interface FindEventsAction { actionType: \u0026#34;find events\u0026#34;; // one or more event properties to use to search for matching events eventReference: EventReference; } // properties used by the requester in referring to an event // these properties are only specified if given directly by the requester interface EventReference { // date (example: March 22, 2024) or relative date (example: after EventReference) day?: string; // (examples: this month, this week, in the next two days) dayRange?: string; timeRange?: EventTimeRange; description?: string; location?: string; participants?: string[]; } interface EventTimeRange { startTime?: string; endTime?: string; duration?: string; } interface ChangeDescriptionAction { actionType: \u0026#34;change description\u0026#34;; // event to be changed eventReference?: EventReference; // new description for the event description: string; } interface ChangeTimeRangeAction { actionType: \u0026#34;change time range\u0026#34;; // event to be changed eventReference?: EventReference; // new time range for the event timeRange: EventTimeRange; } interface AddParticipantsAction { actionType: \u0026#34;add participants\u0026#34;; // event to be augmented; if not specified assume last event discussed eventReference?: EventReference; // new participants (one or more) participants?: string[]; } interface RemoveEventAction { actionType: \u0026#34;remove event\u0026#34;; eventReference: EventReference; } interface AddEventAction { actionType: \u0026#34;add event\u0026#34;; event: Event; } interface Event { // date (example: March 22, 2024) or relative date (example: after EventReference) day: string; timeRange: EventTimeRange; description: string; location?: string; // a list of people or named groups like \u0026#39;team\u0026#39; participants?: string[]; } ``` The following is a user request: I need to get my tires changed from 12:00 to 2:00 pm on Friday March 15, 2024 The following is the user request translated into a JSON object with 2 spaces of indentation and no properties with the value undefined: \u0026#34;\u0026#34;\u0026#34; 参考 A Guide to Structured Generation Using Constrained Decoding\n在 🤗 Transformers 中使用约束波束搜索引导文本生成\nStructured Outputs\nIntroduction to Structured Outputs\nFunction calling\nStructured Outputs for Multi-Agent Systems\nAdd guided decoding for OpenAI API server\noutlines: Serve with vLLM\n如何正确约束输出格式，Deepmind最新：让LLM精准生成任何格式的文本，COLM2024\nCoalescence: making LLM inference 5x faster\nEfficient Guided Generation for Large Language Models\nFast JSON Decoding for Local LLMs with Compressed Finite State Machine\n通过Typechat控制LLM的输出\n大模型 JSON 格式化输出小技巧\nFormatron\nOutlines 〰️\nSGLang\nTypeChat\n","permalink":"https://niraya666.github.io/posts/llm-%E8%BE%93%E5%87%BA%E9%99%90%E5%88%B6structured-outputs%E5%8F%97%E9%99%90%E7%BC%96%E7%A0%81%E5%92%8C%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B/","summary":"\u003cp\u003e在使用大型语言模型（LLM）时，我们常常面临一个挑战：如何从模型输出中准确提取自己所需的信息。例如，当我们希望模型输出 JSON 格式的数据时，由于模型生成的内容并不总是稳定，可能需要额外编写大量的正则表达式来匹配并提取其中的有效信息。然而，由于 LLM 的能力，导致其输出结构并不永远可靠。\u003c/p\u003e\n\u003cp\u003e现阶段， 让LLM按要求生成特定格式文本的主要方法有几种种：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e微调：使模型的输出遵循特定格式\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAI Json-mode/Structured Outputs/function-calling:\u003c/strong\u003e 这些功能允许模型生成更严格、结构化的输出，但受限于openAI平台。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e格式约束\u003c/strong\u003e：在decoding阶段进行约束，限制模型的输出，\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePrompt Engineering\u003c/strong\u003e： 最简单的办法，但不稳定。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e多阶段prompting： 通过多个步骤的提示逐步引导模型生成所需的格式。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e本文将聚焦在Structured Outputs， 受限编码， 和prompt-engineering的角度，探讨它们在生成特定格式文本中的应用和效果。\u003c/p\u003e\n\u003ch2 id=\"json-mode\"\u003eJson Mode\u003c/h2\u003e\n\u003cp\u003e仅特定模型和平台支持\u003c/p\u003e\n\u003cp\u003e以openAI 为例， 在\u003ccode\u003eopenai.chat.completions.create\u003c/code\u003e 参数中增加\u003ccode\u003eresponse_format={\u0026quot;type\u0026quot;:\u0026quot;json_object\u0026quot;}\u003c/code\u003e 即可（具体参见：\u003ca href=\"https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format\"\u003eresponse_format\u003c/a\u003e ）。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e需要在prompt中要求输出json格式\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e不能保证完全按要求的格式结构输出\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e但非100%成功率，存在一些需要额外检测和适当处理的edge case。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdetails\u003e\n  \u003csummary\u003eHandling edge cases\u003c/summary\u003e\n  \u003cdetails\u003e\n    \u003csummary\u003e根据OpenAI官方文档提供的处理方案\u003c/summary\u003e\n    https://platform.openai.com/docs/guides/structured-outputs/json-mode\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"n\"\u003ewe_did_not_specify_stop_tokens\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eclient\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecompletions\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;gpt-3.5-turbo-0125\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;You are a helpful assistant designed to output JSON.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Who won the world series in 2020? Please respond in the format {winner: ...}\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"p\"\u003e],\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"n\"\u003eresponse_format\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;json_object\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"c1\"\u003e# Check if the conversation was too long for the context window, resulting in incomplete JSON \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efinish_reason\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;length\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# your code should handle this error case\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"c1\"\u003e# Check if the OpenAI safety system refused the request and generated a refusal instead\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;refusal\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# your code should handle this error case\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e][\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;refusal\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"c1\"\u003e# Check if the model\u0026#39;s output included restricted content, so the generation of JSON was halted and may be partial\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efinish_reason\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content_filter\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# your code should handle this error case\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efinish_reason\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;stop\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a \u0026#34;stop token\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ewe_did_not_specify_stop_tokens\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"c1\"\u003e# If you didn\u0026#39;t specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"c1\"\u003e# This is guaranteed to parse successfully and should now contain  \u0026#34;{\u0026#34;winner\u0026#34;: \u0026#34;Los Angeles Dodgers\u0026#34;}\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003echoices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"c1\"\u003e# Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                  \u003cspan class=\"k\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"c1\"\u003e# Your code should handle errors here, for example a network error calling the API\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e  \u003c/details\u003e\n  \u003cdetails\u003e\n    \u003csummary\u003e使用pydantic的方案\u003c/summary\u003e\n    使用pydantic的方案\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003epydantic\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eBaseModel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eEmailStr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eValidationError\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"c1\"\u003e# 定义你期望的 JSON 数据模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eUserModel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eBaseModel\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"n\"\u003eage\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"n\"\u003eemail\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eEmailStr\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"c1\"\u003e# 检查 JSON 是否符合模型的函数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003evalidate_json\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ejson_str\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# 将输入的 JSON 字符串转换为 UserModel 实例\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"n\"\u003euser\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eUserModel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparse_raw\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ejson_str\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"c1\"\u003e# 如果验证通过，返回字典\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"n\"\u003eValidationError\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eve\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;JSON validation error: \u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eve\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e              \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"c1\"\u003e# 示例用法\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"n\"\u003ejson_str\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;}\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"n\"\u003evalidated_json\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalidate_json\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ejson_str\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003evalidated_json\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;JSON is valid and conforms to the schema:\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalidated_json\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e          \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;JSON is invalid.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e  \u003c/details\u003e\n\u003c/details\u003e\n\u003cp\u003eJson-Mode 更多是对于输出json的格式进行检查(即Json格式的有效性)\u003c/p\u003e","title":"LLM 输出限制：Structured Outputs、受限编码和提示词工程"},{"content":"摘要 本文探讨了“黑暗之魂”系列游戏在当代日本社会中的意义定位。我认为，这款游戏可以被看作是当今文化潮流的象征，就像有人可能会把杰克·凯鲁亚克的《达摩流浪者》视为60年代反文化的象征一样。我主张，“黑暗之魂”系列通过寓言的方式表达了人们在一个日常行为和社会本身的意义都变得极不稳定的时代中生活的焦虑。游戏采用碎片化的叙事方式，融合了佛教的形而上学思想，并以描绘一个曾经辉煌的王国在最后时刻挣扎的阴森哥特式美学为背景。这种对当代社会焦虑的表达，与日本的后现代主义话语密切相关。通过将这些游戏视为文本、环境和游戏系统之间的反馈回路，我把这些游戏的主要概念主题与大泽真幸提出的“后虚构时代”概念，以及东浩纪对御宅族的定义联系起来。\n前言 在人文学科这个领域，我们认为艺术和文学不仅仅是艺术鉴赏。当我们阅读夏目漱石1或三岛由纪夫2的作品时，教授和学生不仅仅像粉丝那样陶醉于他们优美的文笔，而是作为评论家，从中挑出深刻的思想。通过这种方式阅读足够多的作品，我们会发现一种更宏大的东西：一个跨越时代和文化的思想洪流，在这个洪流中，生活的神秘、美丽、恐怖和悲剧汇聚成我们人类经验的漩涡和流动。文学是指南，就像老话说的那样，是通向美好生活的指南：它是一面镜子，反映我们的位置，也是我们漫步时的地图。简而言之，伟大的作品让我们变得更强大。它们让我们清晰地面对生活的矛盾，并有勇气承受其重压。我们学习文学是为了找到人类经验中的共性，并将其内化。这至少是人文学科的理想和创立原则。\n但如果这是正确的，我们能说这种深刻的体验只是文学独有的吗？书面文字是否有某种独特的力量，能够以其他媒介无法企及的方式激发反思？最终，文学只是一个个故事，但故事并不局限于一种媒介。故事是跨媒介的。它们可以口头讲述，可以印刷，可以通过舞蹈、图像，甚至可能通过尚未构想的方式来讲述。这是因为故事首先存在于人类心灵的深层结构中，不同的媒介让这些结构的不同方面显现出来。故事是人类生活的核心，它们赋予我们的社会和个人生活以连贯性和意义。\n故事是个人身份的基石，人类将经验转化为故事的能力可能与我们感知时间的能力有关。毕竟，故事是什么？它是以有意义的方式组织和编辑时间。简而言之，通过研究故事，特别是那些在社会想象中占据重要地位的故事，我们可以进入人类心灵的最深处。当我们接近这一神秘领域时，我们更接近自己，也许会找到一些我们遗忘的，对我们至关重要的东西，它像火药桶上的火花一样在我们脚下点燃，在这场大火中我们的世界观重生。\n新的媒介在大学里常常面临艰难，因为每种媒介都必须越过高文化裁决者的障碍3。这在二十世纪初的电影和摄影上是如此，在今天的漫画、电视和视频游戏上也是如此。但如果我们仅因为这些作品属于错误的媒介而忽视了沟口健二4的作品，人文学科会变得多么贫乏？偏重研究一种媒介而忽视其他媒介有什么好处？这种偏见是最糟糕的，我们都因这种武断的判断而变得贫乏。我们切断了自己探索那些激发我们深层结构的新视角的机会。故事在新的媒介中同样存在，此外，对那些在这些新媒介中成长的人来说，发现其中的故事是引人入胜、动人且深具意义的。视频游戏中的故事体验与其他媒介有着根本的不同，正是这种不同使得研究它们变得如此重要。它反映了一种新的世界观和价值观。我们不再生活在一个技术只是工具的世界里；技术是我们生活的环境。电影和电视是工业时代的本土媒介，而视频游戏是数字时代的本土媒介。视频游戏不是小众的。这个行业在全球产生的收入大大超过了电影和印刷品。这本身就使得研究它们成为紧迫的事情。\n话虽如此，我们必须认识到，视频游戏是一种新兴的媒介。就像早期的电影只是基本现象的再现——火车向屏幕疾驰或马在慢动作中奔跑——视频游戏的媒介中无疑有着尚未开发的巨大潜力。此外，我们还不能像理解文学那样全面理解这一媒介。我们还没有足够的时间去理解视频游戏在文化中可能扮演的角色以及它们对塑造我们的价值观和性格的影响。我不会在这里主张创建一个新的经典目录。现在还为时过早，而且流行经典化的过程早已在进行中。本文研究的游戏《黑暗之魂》系列已经被这样经典化了。它享有一种地位，其他同类游戏以它为标准，而那些共享其设计原则的游戏被称为“魂类游戏（Soul-like）”，这可能类似于许多文学或电影作品被描述为“卡夫卡式的”5。然而，我不会试图将这款游戏作为一部伟大作品来评估，以便将研究这一系列游戏制度化。我认为这些游戏被经典化不仅仅是因为它们“有趣”，而是因为它们直接处理了当代日本及其年轻一代面临的问题、焦虑和挑战。这是我们将要问的问题：这些游戏中反映了什么，使其在跨文化范围内以及在粉丝和评论家中产生如此深刻的共鸣？\n我将在这里主要讨论日本，但我相信在研究这款游戏时得出的教训具有全球意义。目前，这些意义将保持原样。本质上，为了故事而玩游戏是一件矛盾的事情。游戏的内部组件，作为一个具象化的代码系统和作为不同类型叙事（文本、视觉、听觉）的载体，对玩家提出了一系列相互竞争的需求。这种矛盾在《黑暗之魂》中和几乎所有其他游戏中一样没有解决，但该系列在这些相互竞争的元素之间达到了罕见的一致性，可能直接反映了日常生活中数字世界的体验。我希望我在这里的分析，即使只是初步和不完整的，也能成为反思社会走向、新的价值结构的出现以及文学解释或在这种情况下的游戏解释可以对这些问题有所启示的机会。\n第一章：引言 本文旨在探讨“黑暗之魂”系列游戏与当代日本社会状况之间的关系。我认为，“黑暗之魂”系列通过寓言形式表达了在一个日常行为意义甚至社会本身都变得不稳定的时代生活的焦虑。这一表达通过破碎的叙事方式实现，这种方式结合了佛教形而上学，并以描绘一个昔日伟大王国最后挣扎的阴暗哥特风格为背景。这种寓言中的焦虑与日本后现代性讨论密切相关，特别是现代性项目失败和进步叙事腐败的观点。在我的论文中，我认为这种讨论在“黑暗之魂”中通过模拟经典英雄之旅得以体现，但最终通过剥夺玩家的英雄成就感并用深深的模糊和不确定性取而代之，颠覆了这一旅程。\n“黑暗之魂”系列是一款第三人称动作RPG，其核心机制围绕探索一个破败的世界展开。该系列由著名日本游戏公司FromSoftware在总裁宫崎英高的指导下开发。第一款游戏于2011年9月发布，第三款于2016年4月发布。这是一款难度极高的游戏，设定在一个末世奇幻世界，要求玩家克服许多复杂的运动任务。游戏的一大特点是其缺乏具体的叙事阐述。然而，有一些概念性主题将这些游戏联系在一起。游戏中总是有一个被“不死诅咒”摧毁的王国，并且总是有神秘人物指示玩家杀死旧神并使用他们的灵魂来“连接火焰”。这个过程在第一款游戏中被解读为一个世界复兴过程，但在第二和第三款游戏中被揭示为不可避免的衰败螺旋。\n全球销量数百万，三部作品在国外几乎普遍获得好评，赢得了众多“年度游戏”称号和大量其他行业奖项。它对当代游戏设计产生了深远的影响，许多采用类似设计的游戏经常被称为“魂类”或“魂风格”。这已成为一种陈词滥调，类似于带有超现实或存在主义色彩的文学作品被描述为“卡夫卡式”。我认为其地位的原因并非游戏的“有趣”，而是它有一些关于我们今天所处世界的强有力表达，这些表达与那些对现代性的后现代批评观点一致的人产生共鸣。\n本文的核心论点是，“黑暗之魂”系列体现了具有后现代意识的生活体验。这个术语“后现代意识”是我用来总结后现代思想的一个总称。然而，我特别关注日本分支，引用了社会学家大泽真幸6、文化评论家东浩纪7、文学学者田中真纪子8和小说家村上春树9的著作。从弗朗索瓦·利奥塔10（Francois Lyotard）开始——他首次提出“后现代”一词——后现代思想的不同分支的核心主张是，曾经用来指导社会的“大叙事”已经无可救药地破裂并且无法修复。大泽和其他思想家拾起了这条思路，并将其应用到具体的日本背景。他们认为，日本战后时期的大叙事的破裂在年轻一代中引发了虚无主义行为，这种行为表现为从政治原因和社区参与转向过度消费主义和一种表现为御宅族形象的媒体崇拜。在一些极端情况下，这种御宅族主义可能导致病态行为，如奥姆真理教11的恐怖主义，但需要注意的是，这些思想家将这种暴力行为视为御宅族行为连续体的最外端，而不是在类别上有所不同。从大泽和东浩纪等后现代思想家那里，我将后现代意识提炼为两个基本特征：1. 拒绝或无法接受宏大叙事和意识形态作为意义的来源 2. 对“他者”的日益焦虑和敌意。在这种情况下，他者指的是主体与他者之间的经典哲学二元关系；换句话说，就是自我与世界上所有其他自我之间的关系。大叙事的破裂削弱了依赖这些叙事来维系和稳定的社区纽带和同胞感。这种意识是“黑暗之魂”系列设计和叙事的主要当代文化元素。\n然而，后现代意识在日本确实广泛传播，这并不是显而易见的。出于这个原因，我在第一章中追溯了大叙事在日本历史中逐渐瓦解的确切方式，从战后时期开始一直到现在。这使得我们能够看到后现代意识的发展及其在日本社会和“黑暗之魂”系列中的表达。没有这一步骤，就不可能识别贯穿这个系列的后现代意识。\n为了挖掘日本后现代意识的文化根源，我采用了大泽的文化时代精神理论，将战后时期分为三个阶段：意识形态时代（1945 - 1972），虚构时代（1972 - 1995）和后虚构时代（1995 - 现在）。大泽在其作品中提出的基本观点是，意识形态的死亡——本质上是他对“宏大叙事”的术语——刺激了新的形式，这些形式最终使他们脱离了日本战后社会的意识形态基础。从1972年的联合赤军事件12开始，激进的左翼政治让位于非政治的消费享乐主义。这导致了围绕显著消费形式尤其是时尚的亚文化的繁荣。然而，对一些人来说，对更深层意义的渴望仍然存在，这最终催生了新时代的邪教，即奥姆真理教，它们同样从传统宗教和动漫中汲取力量来满足这种需求。\n奥姆真理教的重要性在于他们既标志着传统信仰和意识形态的弱化，也标志着对任何被他们视为“他者”的事物的日益不容忍。他们还标志着一种对远在天边但又近在咫尺的敌对他者的偏执。例如，他们认为日本政府被全球犹太共济会阴谋控制，而他们自己的莲花团体则充满了双重间谍。他们在1995年对这个被认为是犹太人的他者的恐怖袭击，具有讽刺意味的是，传播了这种对奥姆真理教妖魔的偏执，这些妖魔潜伏在学校、企业甚至政府中。这种怪异他者的概念在“黑暗之魂”中得到了有力的表达，因为它向玩家展示了一个充满敌对个体和群体的世界——这些实体是玩家角色的他者——他们的目标和实践是难以理解的。这也通过一种系统得到了加强，即玩家始终连接到互联网，并且开放给其他玩家入侵。\n东浩纪采纳了大泽的时代精神，并提出了一种“动物化”理论。他指出，由于后现代时代缺乏大叙事，人们失去了理想和抱负。因此，他们退缩到一个以收集符号为中心的世界，这些符号被添加到一个巨大的符号数据库中并被重新利用。 他的主要证据是，当今的御宅族不再对叙事感兴趣。相反，他们只是享受识别、重新混合和扩展数据库的组件。他的预测是，御宅族的活动——他们无休止地收集符号——是大文化的先锋，后续几代人将在自己的领域中变得越来越像御宅族。东浩纪所指出的这一流行文化趋势在“黑暗之魂”中体现为深深的模糊和稀疏的叙事。\n那么“黑暗之魂”究竟如何通过游戏玩法表达后现代意识呢？我认为它通过为我们提供一个任务和一个王国来模拟传统的英雄之旅，但剥夺了我们理解这个任务意义的任何连贯方式。游戏提供了一个视觉细节丰富的世界，但完全缺乏具体的阐述，将玩家置于考古学家和法医科学家的角色中。然而，从未有任何可靠的解释；只有更多的线索和片段，有些是矛盾的。这被称为“环境叙事”，它创造了游戏与后现代思想之间最关键的接触点；即它对主叙事的拒绝。有“拯救王国”的建议，但仅仅是建议，主要是通过类型的惯例传达的，而不是叙事。你所做事情的意义及其服务的目的从未完全清楚。\n重要的是要记住，虽然可以从传统的文学解释视角来解释“黑暗之魂”系列的叙事，但当试图将游戏机制解释为寓意时，会遇到许多实际和理论问题。出于这个原因，我在第二章中对当代游戏研究的批判理论进行了文献综述。利用Espen Aarseth13和其他奠定该领域基础的思想家的论点，我认为需要结合“第一人称”现象学方法和更传统的文学分析方法，才能正确评估视频游戏中潜在的潜在意义。这样做是为了建立一种分析方式，通过与游戏系统的斗争，让玩家体验到我上面概述的后现代意识的两个基本特征。我在第三章中将这种现象学方法付诸实践，探索了一种特别有效的文本叙事、环境叙事和游戏玩法的融合，捕捉到游戏作为一个统一体验的本质。\n除了后现代主义，还有另一股影响游戏的力量，那就是佛教末法概念14。这两种影响力在有趣的方式上重叠，并产生了惊人的相似之处。末法在镰仓时代是一个极具影响力的概念，正如我们的时代一样，那是一个深刻的社会转型时期。末法指的是佛教教义失去其效力，人们无法再依靠自己的努力来获得启示。随之而来的是僧侣和寺庙变得不可否认的腐败，人们因此无法离开轮回，被困在一系列日益退化的转世中。我在第四章中分析了末法的哲学核心，并展示了它如何与大泽和东浩纪的后现代话语一致。此外，我认为，只要“黑暗之魂”系列表达了后现代意识，它主要是通过采用这个佛教末世论概念来表达的。后现代思想和末法都表达了一种恐惧，即事情只会变得更糟，人类的努力无法改变结果。\n在第五章中，我回到现象学和文学分析的混合方法，展示末法概念以及佛教形而上学的混合如何作为构建游戏虚拟世界的核心要素，尽管游戏的美学显然源于中世纪的欧洲奇幻调色板。这些佛教元素共同揭示了游戏世界被困在一个日益退化的轮回中，玩家的任务基本上是一种徒劳的行为，只会推动这个痛苦的循环。我认为佛教形而上学的融合最终表达了东浩纪的结论，即后现代世界及其死去的叙事和怪异他者无可避免地导致社会整体的“动物化”。尽管“黑暗之魂”是一款美学显然源于中世纪欧洲奇幻风格的游戏，但它充满了末法的逻辑，因此游戏深受异教影响，通过采用和改造佛教形而上学，在阴暗的骑士在破败的城堡中冒险的表面下表达了一种后现代情感。\n衰败叙事 过去几十年，日本几乎成了停滞的代名词。这在很大程度上归因于日本在1970年代到1980年代经历的“奇迹”，以及他们令人震惊的衰落，自那以后，国家似乎一直处于无所事事的状态。在美国，当《纽约时报》或《华尔街日报》等出版物提到日本时，往往很少有积极的报道，而是集中在一小部分话题：人口老龄化，年轻人自杀；货币贬值，索尼、丰田和东芝等企业巨头被更精简的竞争对手击败；核燃料仍在福岛渗入地下水，但无人承担责任，等等，每一个预言都比前一个更严峻。在西方，日本作为一个正在衰败的帝国的叙述已经流传已久。这种衰败的印象如此强烈，似乎只有惯性才能维持一切运作，以至于人们忘记了日本仍然是世界上最富有、最富裕和技术最先进的国家之一，仍然领先于许多西欧国家。这种停滞的叙述也存在于日本。正如现任首相安倍晋三在2012年选举中的口号所示：“日本を取り戻す”（取回日本）。从谁那里取回？取回到哪里？凭借他的“安倍经济学”15议程中的“三支箭”，他将从日本自己的“萎缩心态”中拯救日本（Sharp）。如果说1980年代的代表是那些狂热工作、渴望企业晋升的上班族，那么近年来这种原型被更为无力的角色所取代：草食男子16（sōshoku danshi）、尼特族17（NEET，指不在职或不在学的年轻人）、隐蔽青年18（hikikomori，指社会隐士）和其他贬损的称呼。诚然，日本的经济和文化问题与其他任何国家一样真实存在，但这种叙述的力量已经超越了现实，成为一种独立的存在。\n这种“萎缩”的心态和随之而来的文化表达，我将称之为后现代意识。这种意识必须与正统的后现代哲学区分开来，因为哲学本身只是对围绕日本的整体衰败叙述的众多情感反应之一。\n后现代意识的核心是一种源于对人类社会已经达到不可否认的衰退点的感知——无论是有意识还是无意识的——的焦虑。鉴于上面概述的围绕日本经济和文化衰退的广泛文化讨论，后现代意识在日本文化心理中占据了中心地位。因此，这些源自后现代意识的停滞叙述在日本的流行媒体中也找到了强大而多样的表达。虽然有许多例子可以引用，但游戏《黑暗之魂》及其所有续集，是这种“萎缩”心态的卓越典范。要理解它如何以这种方式运作，我们不仅要看叙述，还要看《黑暗之魂》相对于同类游戏的存在。《黑暗之魂》属于一种被称为角色扮演游戏（RPG）的游戏类型。RPG通常是一个开放式的幻想世界模拟，玩家可以探索领土、收集宝物和击败怪物。通常，玩家创建一个角色——一个在游戏世界中的化身——并以此角色进行任务，其最终目标通常涉及拯救某物（王国、存在平面、公主等）免于毁灭。另一个RPG的基本特征是成长概念。尽管有许多设计模式和隐喻用于模拟成长过程，通常玩家的化身通过击败怪物获得经验值，并通过积累这些点数“升级”。通过升级过程，玩家解锁能力，他们的化身变得更强大。总的来说，RPG往往是许多文化神话中常见的英雄之旅模式的模拟：年轻、未成型的人必须离开家园的安全区域，面对未知的危险，克服它们，培养他们潜在的能力，并以强大和完全实现的人的身份返回，无论是字面上还是象征性地。\n在其基本设计中，《黑暗之魂》作为RPG是相当传统的。然而，它有许多叙述和游戏设计选择，颠覆了这个几十年旧的类型的许多传统。作为一个叙述，它可以被解读为一种政治和文化的寓言，++直接反映了生活在一个意识形态叙述失去吸引力和连贯性的衰败文明中的经验++。它也是一种新兴新类型的前沿：后末日幻想。在这款游戏中，王国被占领，英雄们都死了或堕落了，没有人可以拯救，而你存在的最大威胁是那些与你在废墟中一起生存的人。社区是不可能的。基本的本体状态是无休止的失败，而“英雄”只是注定要忍受它。甚至死亡也不能免除角色的负担，而你被赋予的任务的意义和目标是模糊和不明确的。叙述几乎不存在。当游戏开始时，玩家被简单地放在一个被怪物和不死者占领的废弃王国中，没有明确的理由说明为何事情会如此，或者他们应该怎么做。仅有的叙述存在于零碎的文本中，只提供诱人但最终不完整或矛盾的关于世界的信息。所有这些叙述元素都对已建立的类型惯例提出了严重挑战，因此将其与衰退的讨论联系起来。\n在游戏设计方面，《黑暗之魂》有两个突出的特点。第一个是游戏被有意设计得异常困难。这实际上是其吸引力的主要来源之一。该系列在西方的广告标语是“准备去死”（Prepare To Die），而在日本，该系列被认为是しにがみ（死亡之神）的主要例子，暗示玩家在与远比玩家创建的化身强大的巨大、恐怖的怪物斗争时必然会经历无数次死亡。另一个特点是，尽管游戏主要设计为单人体验——也是RPG类型的一个既定惯例——玩家始终连接到互联网。这被视频游戏评论家广泛赞誉为该类型的重要创新。在实践中，这创造了一种紧张的局势，玩家总是暴露在其他玩家的“入侵”之中，这些玩家会试图杀死该玩家。这种互动中内置了一个复杂的风险/回报系统，入侵者通过杀死其他玩家可以获得很多，但如果失败则会失去很多。\n实际上，始终连接到互联网使得游戏世界充满了偏执和危险。其他玩家是玩家在游戏中可能面对的最危险和不可预测的怪物。入侵玩家经常会潜伏在一个不合适的时机突然袭击，玩家无法自卫，从而逆转他们不幸的受害者辛苦获得的进展。我们将详细探讨叙述和游戏系统，但这一系列游戏的基本概况引出了一个问题：在一个没有希望拯救王国，并且异常困难常常到不公平地步的游戏中，怎么会有如此大的吸引力？但事实上，这款游戏在日本、欧洲和北美都极具吸引力。这也指出了游戏的设计和叙述如何与在无聊时代生活的年轻一代的经验产生共鸣。\n日本文化领域的停滞叙述需要进一步澄清。我认为《黑暗之魂》可以被解读为对当代日本状况的政治和文化寓言。这是一种后现代的英雄之旅的颠倒：一种被污染的神话碎片的模拟。它直接反映了许多年轻日本人今天试图在庞大、失修的机构中航行时所经历的无聊和徒劳的感觉，这些机构至多对他们的存在漠不关心，至多直接敌对。然而，这种寓言不仅仅是关于生活在1980年代“奇迹”之后的经济问题。它同样涉及生活在一个国家、政治和文化叙述已经失去了大部分意义和连贯性的社会中的社会和心理成本，这些叙述曾经作为一种心理基石——为个人和社区身份提供稳定。\n催化这一恶化的事件起源于1995年，那一年发生了神户地震和沙林毒气袭击。在西方媒体中，关于这些事件的文化或政治意义的讨论通常不多，但在许多日本知识分子中，这些事件似乎标志着现代性的决定性破裂。在著名小说家村上春树的书《地下：东京毒气袭击和日本心理》中，对沙林毒气袭击幸存者的一系列采访，并得出结论，这一事件的后果不可逆转地揭示了日本社会的所有矛盾、缺陷和内在不稳定性：\n“神户地震和1995年1月和3月的东京毒气袭击是日本战后历史上两起最严重的悲剧。毫不夸张地说，这些事件前后的日本意识发生了显著变化。这两次灾难将作为我们生活中的两个里程碑，永远铭刻在我们的心灵中。如此灾难性的事件接连发生是令人震惊的，也是巧合的。然而，当日本的“泡沫经济”破裂，标志着那些泛滥成灾的时代结束时，这些事件带来了对日本国家根本根源的批判性审视。就像这些事件一直在等待伏击我们。两者共有的是压倒性的暴力元素：一个是不可避免的自然灾难，另一个是可以避免的人为灾难。”（村上，237页）\n通过村上等许多知识分子的工作，1995年已成为一个单一且不可改变的地位：通往破碎和功能失调时代的门户。这里分析的主要理论家——东浩纪、大泽真幸和田中基子——都将1995年视为进入反乌托邦现状的起点。这些思想家的语调本身就有一种微妙的后末日色彩，仿佛我们生活在一个无法拯救的被污染时代。这也是衰败讨论的一部分。这些思想家有一种确定的末世论信念，将他们与这一讨论联系起来。\n无论如何，这些灾难比任何经济灾难都更可能是现任首相希望根除的“萎缩”心态的原因。尽管已经过去几十年，1995年的事件作为焦虑和无助感的触发点，在《黑暗之魂》的寓言中找到了源头。经济的持续低迷、老龄化人口、年轻一代的就业前景减少以及福岛等灾难，只是加剧了这些灾难在1995年最初引发的负面情绪。福岛等事件只帮助在日本意识中创造了一种遗产。近年来，右翼极端民族主义政治的重新出现和尼特族及隐蔽青年的社会退缩，形成了对这一相同的文化病态的极端和相反的情感反应。右翼试图通过与其真实和感知的对手（韩国、中国、朝鲜）进行公开的对抗来复兴“大和”帝国的活力，试图通过历史修正主义抹去日本帝国的暴力和道德可憎之处，并试图通过修改宪法（第9条）恢复战争作为国家主权权利。而尼特族和隐蔽青年则将自己沉溺于网络和其他形式的私人数字享乐中。他们没有叙述可供复兴，没有事业可供加入，甚至没有可以参与的兴趣小组。与他人沟通实在是太困难了，在他们看来，也太危险了。\n我的观点不是《黑暗之魂》系列是这些问题的根源，甚至不是它们的贡献，而是它是对在公众意识中循环的深层社会焦虑，特别是对视频游戏作为熟悉媒介的年轻一代的一种特别有力的表达。《黑暗之魂》系列的持续流行，作为一种社会腐化的象征，反映了人们在日常生活中的感觉。但这些焦虑到底是什么，它们如何在《黑暗之魂》中表现出来？另一种问这个问题的方法是，后现代意识的基本要素是什么？定义1995年后时代的有两个主要元素。1. 对构建战后秩序的政治和社会叙述的拒绝；2. 一种对他者的弥漫性焦虑，侵蚀了对社会过程的信任和信心。他者的概念是指自我与世界上遇到的所有其他自我之间的二元关系的一半。这一概念的延伸是文化他者，即一种感觉某些人与自己相似，另一些人则是异类。日本社会通常被认为具有非常同质的国家认同感，彼此之间有相同的文化、种族和相似的价值观。这反过来又创造了强烈的信任和社区感，但这种亲近的逆向效应是，外来者被认为比在美国、英国或法国等文化多样性更大的国家中更为陌生。大泽真幸的论点是，奥姆真理教的恐怖活动将潜在威胁引入了公众。这种恐惧是一种诡异的他者的恐惧，这种他者会突然出现，以一种深刻的方式扰乱你的生活，有时甚至是致命的。社会结构中的这种破裂从未真正修复，并继续受到挑战，直到现在。关于诡异他者的偏执是定义《黑暗之魂》的氛围和叙述的主要特征之一，也是其游戏机制，尤其是其在线组件。\n要理解《黑暗之魂》如何作为一种描述后现代意识的寓言运作，我们必须追踪战后日本社会状况的衰退轨迹。这根本上是一个现代进步叙述——本质上是从西方继承的——首先在1960年代末引发了激烈的意识形态斗争，然后在1980年代变成一种无政治的享乐主义，最后，最终在五个正义的宗教狂热者用雨伞尖端戳破装满沙林毒气的袋子的时刻，被一种微妙的虚无主义彻底摧毁。为此，我们将研究社会学家大泽真幸的工作。大泽花费了相当多的职业生涯试图回答奥姆为什么会做他们所做的事情。在他的书《虚构时代的终结》中，他认为答案涉及意识形态本身的致命缺陷：一种危险而空洞的思维模式，导致在一个自我设计的世界中越来越高的疏离感。\n理性时代 借用社会学家見田宗介19（Mita Munesuke）的观点，大泽真幸将战后时代分为两个不同的阶段。第一个阶段他称为“理想的时代（理想の時代，risō no jidai）”，第二个阶段他称为“虚构的时代（虚構の時代，kyokō no jidai）”。理想的时代始于20世纪60年代，结束于1972年，虚构的时代从1972年开始，一直持续到1995年。大泽将“理想”和“虚构”描述为“可能世界（可能の世界 kanōsei no sekai）”，因为它们都反映了我们对世界的感知以及我们认为世界应该如何。然而，尽管这两个概念在根本层面上是相关的，但它们在几个重要方面是不同的。大泽认为，理想与现实世界密切相关，而虚构则完全脱离现实世界：\n理想是一个在未来有望或期待在现实中实现的可能世界。因此，理想必须存在于现实的因果延伸之上。从这个意义上讲，理想不是一个纯粹的可能世界，而是广义现实世界的一个方面。相反，虚构是一个可以在现实中实现与否无关的可能世界，因此是纯粹的反现实。（大泽，41页）\n理想は、未来において現実に着地することが予期（期待）されているような可能世界である。だから、理想は、現実の因果的な延長上になくてならない。その意味では、理想は、純粋な可能世界ではなく、むしろ広義の現実世界の一局面である。それにたいして、虚構は現実への着地ということについてさしあたって無関連ありうる可能世界であり、それゆえ純粋な反現実である。(Osawa, 41)\n暂且不讨论虚构如何将我们与现实疏离，日本的理想时代很大程度上受到冷战阴影的笼罩。当亚洲大部分地区处于苏联的保护伞下时，日本则在美国的庇护下，因而在意识形态上致力于民主和经济自由主义。在20世纪60年代，日本在这些条件下几乎在各个方面都得到了繁荣：科学、基础设施、经济、文化等。大泽将20世纪60年代称为理想的黄金时代，并指出家用电器是赋予大众理想的物质形式：\n理想时代在大众层面的黄金期是20世纪60年代。当然，这与日本的高速增长期相吻合。在此期间，例如，受到国民绝大多数人广泛需求的家电产品赋予了大众理想物质的表达……经济增长和科学技术的进步并不是因为理想而被接受的，相反，正因为理想作为可能的现实在广泛（全球范围内）被信奉，经济才能够增长，科学和技术才能进步。（大泽，44页）\n理想の時代の、大衆的な水準で黄金期は、1960年代である。もちろん、これは、高度成長期にあたる。この時期、たとえば、国民の圧倒的な大多数によって広範に欲求された家電製品が、大衆的理想に物質的な表現を与えた\u0026hellip;経済成長や科学・技術の進歩があったから理想が抱かれたのではなく、逆に、理想が可能的現実として広く（世界規模で）信憑されたがゆえに、経済が成長することができ、また科学や技術進歩していると感受されたのである。(Osawa, 44)\n乐观和对更美好（物质富足）未来的渴望是这一时代的指路明灯，这种乐观本身就反映了对美国模式的意识形态拥抱，推动了日本从一个被毁灭的国家向一个繁荣的国家的转变。大泽将这种发展归因于对可能世界的执着。在他的著作《拥抱失败》中，约翰·多尔20（John Dower）通过分析战后立即涌现的泛泛女孩21现象，生动地展示了这一拥抱的一个具体表现——这些年轻女性在占领期间主动接近并与美军士兵交往：\n在那段饥饿和匮乏的岁月里，美国人的物质舒适简直令人瞠目结舌。使美国“伟大”的是它如此富有；而对许多人来说，使“民主”具有吸引力的是它显然是通往繁荣之路。在普通人中，没有哪个群体像泛泛女孩那样公然利用征服者的物质财富。她们从美国军用商店获得物品，而在那些贫困的日子里，这些商店真的就像是魔法之地的宝库：不仅装满了基本食品，还有酒和香烟、糖果和美味佳肴，以及性感而颓废的女性物品如口红和尼龙袜。（多尔，136页）\nIn those years of acute hunger and scarcity, the material comforts of the Americans was simply staggering to behold. What made America “great” was that it was so rich; and, for many, what made “democracy” appealing was that it apparently was the way to become prosperous. Among ordinary people, no group tapped the material treasures of conquerors as blatantly as the panpan. They were the recipients of goods from the U.S. military exchange posts that in those impoverished days truly seemed like treasure houses from a magic land: crammed not only with basic foodstuffs, but with liquor and cigarettes, sweets and delicacies, voluptuously decadent feminine things such as lipstick and nylon stockings. (Dower, 136)\n到20世纪60年代后期，美国占领军所展示的意识形态愿景在很大程度上以非常具体的方式兑现了其承诺，普通日本人开始以便利设施、消费电子产品、便利和所有现代化工业国家的装饰形式享受经济发展的成果。大泽认为，这是意识形态在社会中最顺畅地运作的时候。尽管存在意识形态冲突，人们完全接受了作为他们理想“可能世界”的意识形态，并为之努力，无论他们是马克思主义者还是资本主义者。然而，需要注意的是，尽管20世纪60年代有所增长，仍然有许多日本人处于贫困之中。使20世纪60年代成为理想时代的不是相对的繁荣本身，而是对这种繁荣的承诺，以及对战后秩序确实朝着这个方向发展的普遍信念。普通日本人顽强地坚持这种愿景。这是当时的流行意识形态：汽车、收音机、流行音乐、方便获取的食品以及所有在战后立即投降的绝望、痛苦和毁灭中缺失的舒适和物质享受的承诺。\n然而，大泽认为，这种对意识形态的拥抱最终导致了意识形态向他称之为“虚构”的转变。这与意识形态结构本身的内在不稳定性有关，而不一定与日本的意识形态建制的具体情况有关。大泽以最广泛的术语谈论意识形态，但他将驱动意识形态的动机归结为他从佛教借来的一个原则：“貧病争（ひんびょうそう）”22。这个术语可以粗略翻译为“世间的烦恼”，它们被识别为贫穷、疾病和争斗。他主要基于意识形态与虚构的区别，意识形态是面向与貧病争斗争的。换句话说，大多数意识形态都承诺从世间的烦恼中解救出来。这种救赎总是必然在未来，但对其实现的信念保持稳定，并促使在现实世界中采取具体行动。这在大多数世界宗教，特别是忏悔宗教中是如此，但在像资本主义（自由市场=繁荣和自由）和马克思主义（废除财产/阶级=救赎不平等）等世俗概念中也是如此。然而，在理想和貧病争之间的关系中存在一个悖论：\n理想时代是一个社会整体上被疏离到理想中的时代。在这样的时代，最大的痛苦是从理想中被永久疏离。换句话说，在意识形态关系中，人们经历了双重疏离。貧病争代表了一种不可逃避的条件，在这种条件下，实现和接触我们的理想变得极其困难，我们不可避免地永远与之疏离。这种疏离引起的痛苦只能通过对我们理想的重新信仰来解决，只要我们认为其实现的可能性足够高。这只有在我们已经被疏离到意识形态中的情况下才有可能。（大泽，45页）\n理想の時代とは、社会が全体として理想へと疎外されている時代である。このような時代において、最大の不幸とは、理想から永続的に疎外されること、つまり理想との関係において二重に疎外されることである。「貧病争」は、理想への到達・接近が著しく困難になりうるような－理想からの永続的な疎外を余儀なくするような－宿命的な環境条件を代表している。不幸は、－すでに理想への疎外によって条件づけられている以上は－、到達の蓋然性が十分に高い現実性として理想を回復することによってしか、解消されない。(Osawa, 45)\n大泽指出，在苦难和救赎承诺之间存在一个相互强化的解释循环。貧病争，按定义，使人们与各自意识形态的救赎承诺疏离，但这种痛苦只能通过复兴理想来消除。++苦难往往不会导致信仰者失去信仰，而是激励他们按照自己已经信奉的叙事来解释自己的苦难。++使苦难有意义是一种常用于应对我们生活中悲剧性事件的应对机制。矛盾的是，这意味着即使我们离意识形态所承诺的可能世界极其遥远，貧病争也能加强意识形态信仰。当我们对可能世界的信仰非常强烈时，我们会在这种意识形态中“疏离”，只通过其中提供的狭隘解释视角来看待世界。这反映了走向教条主义和正统的驱动力，在这种状态下，貧病争仅作为意识形态引擎的燃料。然而，这个循环的关键部分在某种程度上超出了它；我们必须继续相信可能世界是可能的。大泽的虚构时代描绘了一种信念崩溃的普遍现象，但这种崩溃不应与某种信仰的丧失混淆，例如虔诚的天主教徒变成无神论者或坚定的资本家转变成马克思主义革命者。这只是将一种意识形态换成另一种意识形态。基本结构和取向保持不变。从意识形态到虚构的转变代表了一种新的疏离，一步远离了貧病争和意识形态之间的循环。这是一步远离世界本身。\n大泽举了许多例子，说明在20世纪80年代，日本的意识形态地图如何逐渐脱离大规模叙事（资本主义对抗共产主义），并催生了新人类（新人類 しんじんるい）23、御宅族（オタク）24以及最终的奥姆真理教（Aum Shinrikyō）和1994-1995年的沙林毒气袭击。然而，需要注意的是，在所有这些案例中，无论是伊斯兰教还是马克思主义，意识形态都与当前世界的实际问题密切交织。投射的可能世界正是通过这种对比才可以理解的。正是在与貧病争的这种重要关系中，意识形态与虚构分道扬镳。**对于大泽来说，从意识形态到虚构的转变是由20世纪60年代政治活动后的深刻失望所催化的。**虽然大泽认为20世纪60年代是意识形态运作最顺畅的时期，但这并不意味着那个时代没有意识形态冲突。相反，就政治而言，20世纪60年代末的日本充满了社会动荡。20世纪60年代意识形态运作顺畅的概念并不意味着政治和谐，而是指个人能够完全地、彻底地投身于构成其特定意识形态立场的叙事。\n20世纪60年代标志着许多学生运动，反映了当时日本普遍的政治激活。这些运动大多针对日本政府支持越南战争等问题提出抗议，但间接上也形成了对美国影响日本外交政策的一种抵制。反帝国主义和反美情绪主要由日本共产党刺激。它们创建了全学联合（Zengakuren）25，这是一个在大学生中创造了广泛的左翼文化的组织（羽根，465页）。最终，这些运动演变成了与警察的暴力冲突，抗议者和警察都受到了伤害，有时甚至死亡：\n20世纪60年代中期，东京的一些大学开始出现反对学费上涨的学生示威。总的来说，1968年有115所大学面临学生抗议、示威和骚乱。大规模罢工，设置路障，占领重要建筑。教授们经常被激进学生审讯，被逼供认罪行。（羽根，465页）\n暴力的出现也加剧了最初和平政治议程的混乱。这些抗议由许多不同议程的左翼团体拼凑而成，其中一些比其他更加激进。没有具体或连贯政治需求的暴力最终导致这些学生运动消亡，留下了持久的政治失望感。田中元子（Motoko Tanaka）26在大泽的时代背景基础上，认为安保和全共斗运动的失败是结束理想时代的一个主要因素。全共斗运动27是一个特别持久的事件，形成了一种由其他学生起义效仿的围攻战模式。这场斗争始于东京大学，当时医学生要求废除实习制度。管理层的压迫态度引发了一场全校范围的争议。1968年7月，学生们成立了全共斗，即全学共斗会议的简称。随着斗争的持续，全共斗逐渐激进，开始质疑日本社会的基础。他们占领了整个大学六个月，最终被警察驱逐。田中写道：\n安保和全共斗等政治运动的失败突显了即使在占领结束后，美国对日本政治的持续影响。经历了这些政治运动失败的年轻男性，有意或无意地吸收了应对美国力量的困难和失败的创伤。（田中，46页）\n大泽本人指出，1972年的联合赤军事件（連合赤軍事件）28是从理想时代转向虚构时代的主要转折点。联合赤军是几个激进马克思主义团体合并，意图暴力推翻日本政府。赤军是其中一个宣战日本政府并开始一系列恐怖袭击的团体，包括劫持飞机和袭击特拉维夫的罗德机场，导致26人死亡，80人受伤。政府对该团体进行镇压，逮捕了大多数成员，迫使其余成员与其他团体合并，最终隐藏起来。这个新合并的激进左翼在长野山区建立了一个秘密基地，进行军事训练。在那里发生了对自己队伍的清洗：14名成员因被认为缺乏革命热情而被残忍杀害，有的被绞死，有的被刀割。警察发现了他们的藏身之处并进行逮捕，剩余成员劫持人质， 将自己封锁在一座山间小屋中。这场持续一整天的围攻是日本第一次现场直播的马拉松式电视广播，持续了10小时40分钟。此事件导致了左翼激进政治的急剧衰退。\n当然，意识形态热情的衰退不仅限于左派，也存在于右派。在另一部相关的作品《不可能性的时代》中，大泽将三岛由纪夫未遂的政变视为另一个普遍意识形态崩溃的象征（大泽，13页）。在所有这些案例中，这些意识形态失败的高可见度都表明意识形态本身的某种无力感，其最激进的支持者的过激行为被转化为电视观众眼中的电视节目，令大多数人觉得这些不是高尚革命者的反抗行为，而是堕落的社会病态者的过激行为。取代这种意识形态热情的是大多数人只是享受经济繁荣的成果。\n虚构时代 大泽的虚构时代从1972年延续到1995年，但或许最能捕捉大泽对虚构与意识形态定义的隐喻无疑是“泡沫”这一概念。日本的1980年代通常被称为“泡沫时代”，意指经济的快速崛起及其随后的崩溃。这也传达了一种隔离感。1980年代对日本来说是一个极其繁荣和平的时期，“泡沫”这个比喻传达了普通日本人被困在一种虚假的安全感中的感觉，他们在从战争创伤中完全复苏的社会中享受着温暖、超现代的光芒。然而，这一时期本应被视为对现状充满乐观和信心的时代，却存在一种根本的模糊性。繁荣带来了物质上的丰富，但不一定带来个人或精神上的丰富。大泽认为，如前所述，虚构试图在世界之外重构意识形态，反映出对这种潜在世界在现实中实现的前景的极大失望。简言之，政治和革命活动被放弃了，但对理想的追求并没有消失。\n我们需要认识到，这两者之间没有明确的界限。相反，这种区分反映了控制现实感知和社会活动的态度连续体。我们越接近虚构标记的光谱，就越疏远现实，并且通常会完全拒绝当前的现实状态。此外，越接近虚构光谱，我们就越倾向于完全拒绝既定秩序，并放弃调整或改变它的计划。大泽认为，取而代之的是一种末世论修辞，即世界变得无法拯救，理想在不清除一切的情况下完全无法实现。他将虚构定义为在虚构中保存和扩展的意识形态冲动（大泽, 56）。\n这可以从字面上理解为流行媒体如漫画和动漫作为探索和满足意识形态冲动的空间，但也可以比喻为在这一时期开始繁荣的新兴消费主义和新兴宗教的神秘宇宙论。\n大泽用来体现虚构的三个象征是迪斯尼乐园的建设、涩谷作为东京主要娱乐区取代新宿的发展以及新人类的出现，这些新人类是这个新的“虚构”时代的本地居民（大泽, 46-49）。“新人类”一词可以翻译为“新人”或“新种族”。这个词首次出现在1985年，当时《朝日新闻》刊登了一系列对日本年轻人的采访，标题为《新人类的旗手们》（Herbig）。它通常指的是在1970年代及以后的成长的人群。1970年代和1980年代成长的一代人没有经历过战后重建的艰辛。相反，他们在一个富裕、有影响力、技术先进的国家中成长，这个国家鼓励他们把自己打造成富裕社会的一员。与1950年代和1960年代的年轻人截然不同的是，新人类对左翼政治毫无兴趣，事实上，几乎没有任何政治倾向，因此他们被称为“反应无代”（Herbig）。这种差异不仅仅是文化上的，更是身体上的。之前的几代人经历过粮食短缺，而新一代人从小就能定期过度享受。1965年至1985年间，日本男性的平均身高增加了7厘米，平均体重增加了5公斤，而女性的平均身高增加了6厘米，平均体重增加了1.4公斤。代替意识形态或政治抱负的是，这一代人更感兴趣的是在公司超级结构中的个人进步，或者是个人议程，关注如何设计新的生活方式。如果这些生活方式提出了挑战上一代权威的新价值观，那么可供新人类使用的财富和富裕让他们可以摆脱这些旧的社会结构的重压。例如，许多年轻人在这段时间里有能力搬出家庭独立生活，导致了另一个绰号的出现：“独身贵族”（Herbig）。\n这里讨论的每一项——迪斯尼乐园、涩谷和新人类——都反映了某种与定义过去的意识形态斗争的疏离、隔离或冷漠。迪斯尼乐园在这里是一个很好的隐喻，因为它不仅象征着日本泡沫经济的巨大成功，也象征了一种从世界中物理上的隔离感。迪斯尼乐园被精心设计，一旦顾客进入，他们就看不到其边界以外的东西。这是为了让顾客（尤其是儿童）能够完全沉浸在一个自主的幻想世界中，而不受世俗的干扰。从这个意义上说，迪斯尼乐园作为一种消费体验，完全是以购买从日常生活中暂时逃脱的机会为基础的。将政治或明确的意识形态活动引入迪斯尼乐园将是对其作为幻想空间的自主性的侵犯。但这种政治与休闲之间的绝对分离，在一个脱离普通生活的空间中具体呈现出来，本身就是一种意识形态。这是意识形态与虚构之间的界限开始变得清晰的地方。\n然而，东京的城市中心本身正在经历一种由房地产开发商主导的“迪士尼化”，这些开发商在政府的基本不加监管的情况下进行开发。最终使涩谷成为东京首要娱乐区的新代表，是该地区完全转变为一个主要的消费空间：\n“涩谷的成功在很大程度上归功于西武集团的投资。通过这些投资，整个区域被开发成一个时尚的‘超现实’空间——换句话说，一个充满符号差异的前沿空间——适应了消费社会。”\n“渋谷が成功したのは、とりわけ西武系資本の投入によって、街の全体が、消費会社に適合した、「ハイパーリアル」でオシャレな－つまり先進的な記号的差異に満ちた－空間へと仕立てあげられたからである。要するに、渋谷は虚构の时代に适合した盛り场だったのだ。”（大泽, 47）\n我们可以通过当代对日本的描绘来佐证大泽的论点。涩谷著名的十字路口——经常在延时摄影中显示，其日常通勤者像迅速变化的人群一样脉动——不仅是东京的一个有力象征，也是日本作为一个超现代国家的象征。此外，涩谷因此转变为一个象征日本未来的标志，不仅对日本人而言，也是对整个世界而言，作为未来本身的象征。\n大泽使用“超现实”（ハイパーリアル，hyper-real）一词指的是一种新的城市沉浸感。这是一种建立在对消费主义无疑庆祝基础上的超高清未来愿景。他没有详细解释这个词的确切含义，但他的用法无疑是借鉴了让·鲍德里亚在其著作《模拟与拟像》中的概念。在他的书中，鲍德里亚将当代社会描绘为一个不再包含任何现实参照的巨大符号狂欢。一个符号的意义只能是其他符号在不断变化的符号系列中的另一个符号。在这种框架下，符号获得了自己的自主权，并对我们的现实感知产生了强大的影响。通过博尔赫斯的《科学的准确性》作为寓言，他断言地图作为表征取代了领土，最终成为需要创建新地图的领土本身（Baudrillard, 1）29。没有参照闭合系统之外的任何事物的符号，他称之为拟像。在鲍德里亚的观点中，现代社会与后现代社会的区别在于，现代社会专注于通过更高水平的工业化进行生产，而后现代社会则围绕着生产“模拟”来组织，他指的是电视、互联网、电影、广告、虚拟现实等。\n这种新的媒体生态本质上是一个创造拟像的引擎。例如，真人秀的核心是其主体的非脚本化互动，但实际上是高度编排、制作和编辑的，使这些互动符合我们对戏剧的期望。这种新的“现实”视野因此被消费者接受为解释自己和自己经历的新模式。在鲍德里亚的模拟社会中，身份是通过这种图像的挪用来构建的，从模拟中派生的模型决定了个体如何看待自己以及如何与他人关系。通过延伸，一个超现实空间是一个设计用于交叉并反映这种模拟世界的空间。它利用生活在“模拟”中的各种交织的拟像系统来构建其美学。\n迪斯尼乐园是超现实空间的最佳例子，因为它代表了将地图（构成迪斯尼的符号矩阵）变为实际领土的真诚尝试，在这种情况下，地图无疑是优先的。这在涩谷并不明显，但当我们开始思考将这一地区转变为一个旨在促进各种消费的空间的基本动机时——情趣酒店、夜总会、购物中心、百货商店、娱乐中心——显然，这个世界离不开几十年来在现代广告和品牌建设中构建的符号语言。广告本身就是一个完全致力于通过符号的精心融合来改变产品认知的领域。这些符号被融合到产品的形象中，以便以特定方式阅读产品。通过这种方式，全新的设备或服务被转化为绝对必要品，奢侈品不仅仅授予地位，而是不可逆地与特定的“生活方式”身份混淆。与美国的大型购物中心相似，涩谷是广告、营销和品牌的物理化表现。它试图将城市空间密封在这个不断变化的新符号领土中。在这个空间内，他断言： “真实是从微型化单元、矩阵、存储器和指令模型中生产出来的——通过这些，它可以无限次地复制。它不再需要理性，因为它不再需要与某种理想或负面的事物进行比较。它只不过是操作性的。事实上，因为它不再被想象所包围，它根本不再是真实的。它是一个超现实的产品，一个没有大气层的超空间中的辐射综合体。”（Baudrillard, 2）\n在这种新密封的图像领土中进化并被其服务的新一代就是新人类： “虚构时代的年轻文化最密切相关的是1980年代前半期的新人类，然后是后半期的御宅族。新人类没有被任何意识形态或概念吸引，而是在他们的消费、时尚和兴趣层面上通过‘符号的戏耍’来表现自己的身份。这是一种态度，表达了他们从支配意识形态时代的理想重压下解放出来。”（Osawa, 48）\n在1972年至1995年虚构时代中出现的各种年轻文化中，“符号的戏耍”本身成为了理想的来源。这几十年间有无数的亚文化和时尚趋势——大多源自涩谷和原宿的街头文化——完美地展示了这种“戏耍”。这些众多亚文化的共同点是对生活方式的热爱，这种生活方式主要由美学定义。不同类型的コギャル（kogyaru）亚文化提供了一个生动且特别字面的例子，展示了新人类一代人所拥抱的狂热符号混搭。\nコギャル（高中女生）时尚大约始于1990年代，但其实践和美学可以追溯到早期的亚文化现象。“ギャル”（Gyaru，女孩，源自英语的“gal”）这个称号自1970年代末以来一直用于指代年轻女性的前卫、冒险风格，并在当代用作后缀，表示ギャル连续体内的细分。最初，コギャル风格是一种对校服风格的改编——西装外套、裙子和长袜——被改造得更性感，并混合了大胆的元素，如染发、夸张的化妆，偶尔还有整形手术。随着时间的推移，コギャル风格变得更加多样化和极端，产生了无数不同的变种。“コギャル”这个词与少年犯罪甚至卖淫联系在一起，并且成为老一代人批评的目标。批评者认为这些年轻女性是寄生在父母身上的自恋者，破坏了传统价值观和女性身份。文化人类学家劳拉·米勒在一篇讨论1990年代年轻人时尚的文章中使用“无国籍”（むこくせき，mukokuseki）一词来解释コギャル美学背后的文化逻辑。她使用这个词来说明日本时尚潮流不是由模仿国外特定趋势驱动的，而是希望将尽可能多的吸引符号混合到一个单一的整体中：\n“无国籍的审美刻意唤起某个地方或某个历史时期的图像——越南农民裤子、美洲原住民的流苏和珠子、1960年代的佩斯利图案——但将这些与匡威跑鞋或日本木屐、棒球帽或拉斯塔法里编织贝雷帽结合或并置\u0026hellip;\u0026hellip;日本年轻人的时尚中发现的超历史和文化融合并不仅仅是美国文化帝国主义的证据。仔细观察会发现，在许多情况下，被挪用的物品是有意“误用”或扭曲的。例如，一些コギャル玩弄廉价的夏威夷意象，使用假塑料花环、兰花、koa种子项链和波利尼西亚印花纱笼，但并不是为了看起来像真正的夏威夷人。那些采用这些装饰的人有时被称为“地方女孩”或“纱笼女儿”\u0026hellip;\u0026hellip;コギャル和其他人夸大了他们外表的人工性。结果在真诚、嘲讽和媚俗之间徘徊，使人难以分辨他们是否或在多大程度上是在开玩笑。”（Miller, 83-84）\n这种审美的共同特点是将常见或熟悉的物品与不寻常的物品搭配，创造出高度异国情调，有时甚至是故意怪异的新背景：一件鲜红色的格子纹和服，8英寸高的厚底鞋和一大片亮粉色的头发。这些符号不指向西方，甚至不指向江户时代，它们只属于コギャル时尚文化中的符号网络。像涩谷和原宿这样的超现实城市空间不仅是购买服装的地方，也是展示它们的空间。\n这种通过时尚和个人品味定义自己找到意义的新人类一代，通过拥抱这种新的超现代性的果实来表达自己。他们不是通过政治观点表达自己，而是通过显著的消费形式来创造身份。从这个意义上说，他们完全生活在日本的泡沫中。然而，新人类仍然是一个广泛的类别，反映了广泛的信仰和社会活动。对于大泽来说，完全体现虚构时代主要方面的亚文化是御宅族（Otaku）。御宅族起源于新人类一代，但发展成了一个独特的群体：\n“御宅族指的是一种从新人类中发展出来的青年文化。如果新人类的行为特征在于他们从过去时代的执着中解放出来，那么相反，御宅族的行为特征在于新的执着。**御宅族是这样一群人：他们对一个特定领域——动画、电子游戏、电脑、偶像歌手等——表现出过度的、几乎不合理的热情。**我将这种不合理的热情定义为‘意义’（这里我们可以认为是该领域的社会必要性，或像艺术一样，它本身具有的价值）和御宅族对该兴趣领域收集的信息密度之间的明显失衡。信息的密度压倒了意义。例如，在大多数情况下，如动画的典型例子中，故事[物语]只是虚构[虚构]的积累。因此，对于御宅族来说，他们钟爱的虚构世界似乎比普通现实更重要。”\n“オタクは、この新人類から分化・発展してきた若者の類型である。新人類が、過去の時代を支配していた執着からの解放の身振りによって特徴付けられるのだとすれば、逆にオタクは、新たな拘泥の身振りによって特徴付けられる。オタクとは、かつてだったら興味として片付けられるような一見瑣末なさまざまな領域—アニメーション、テレビゲーム、コンピュータ、アイドル歌手等々—に、不合理なまでに過剰に熱狂的に耽溺する人々である。「不合理なまで過剰」というのは、その領域の社会的な必要とか（芸术のような）それ自身として有すると考えられている価値といったような「意味」の大きさと、その領域に関してオタクが集积する「信息」の浓度との间に、バランスが失われているように见えるということ、「意义」の大きさを「信息」の浓度が压倒しているように见えるということ、である。たとえばアニメーションの場合が典型であるように、多くの场、物语＝虚构の集合である。それゆえ、オタクにとっては、まるで、通常の现実よりも、彼らが爱する虚构の世界の方がより重要なものになっているように见えるのである。”（Osawa, 48）\n大泽在这里提出的御宅族概念非常熟悉，但他从他们对流行媒体的热情中提取了更大的意义，而这常常被忽视。如果说红军联盟成员完全投身于马克思主义革命理想的热情，那么御宅族的特征在于对他们的细分兴趣的痴迷。这些信仰在结构和总体方向上的区别在于，马克思主义革命旨在改变周围的条件以服务于其愿景，而御宅族对周围条件基本无动于衷。他们只是希望更加深入地沉浸在自己喜欢的电视系列中，或者换句话说，更深入地进入特定的符号宇宙。大泽在上面提出的意义（いみ）与信息（じょうほう）之间的平衡，意味着对符号本身的奢华享受逐渐超过了它们的内容。虚构中所代表的理想逐渐在虚构本身中得到保留，达到理想无法在虚构之外生存的地步。这本质上构成了大泽所说的意识形态与虚构之间的关键区别。然而，单独指出御宅族是日本社会结构中的某种异常是错误的。大泽试图指出的是，御宅族对虚构的倾向在整个文化中普遍存在，或许只是通过程度上有所区分。\n御宅族与超现实 大泽在这里讲述的故事可以看作是宏大叙事逐渐消亡及人们为应对这种变化所作的努力。这些叙事曾是赋予私人和公共行动意义并为某些社区活动提供稳定的社会结构。超现实和“新人类”代表了一种意识形态的畸变，将激进分子的能量重新导向个人主义的新意识形态。\n自20世纪70年代以来，宏大叙事的死亡，或者说意义的死亡，一直是许多有影响力的哲学家和学者作品中的共同主题。这个概念最早由法国文学批评家Francois Lyotard30提出，后来成为后现代哲学的主要标志之一。“宏大叙事”一词通常指的是18世纪欧洲出现的各种社会建构，这些建构是为了将欧洲各王国的封建拼凑整合为统一的实体，形成具有单一身份的大型民族国家。知识上，宏大叙事体现在人性和理性的观念中，革命叙事如马克思主义中，经济上则表现为生产至上的国家存在理由。大泽的论点基本上是宏大叙事在日本如何消亡的具体案例。他的论点坚定地位于后现代哲学的传统中，讨论的是时代精神中的意识形态和虚构的死亡过程。\n从这个角度看，“新人类”亚文化和御宅族的出现只是对这些叙事死亡的情感反应。文化批评家东浩纪在他的《御宅族：日本的数据库动物》中，描述了御宅族作为后现代世界中努力保持理智的不安幸存者的形象：\n“御宅族无论走到哪里，都随身携带大量的书籍、杂志、同人志和塞满大纸袋的碎片，像寄居蟹一样，”因为如果他们不随身携带“自我的壳”——即他们的群体归属幻想——他们就无法在精神上保持稳定……现代性受宏大叙事的支配。相反，在后现代性中，宏大叙事崩溃，社会整体的凝聚力迅速减弱。在日本，这种减弱在1970年代加速了，当时高速经济增长和“政治季节”结束，日本经历了石油危机和联合赤军事件。从这个角度看，我们可以将御宅族用垃圾亚文化材料构建“自我壳”的行为模式视为填补宏大叙事消失所造成空虚的行为模式。（东浩纪, 27）\n这种“寄居蟹”形象，与穿戴各种奇异饰品和配件的コギャル（日本街头少女）并无二致，有时她们甚至被这些饰品贴满。消费主义达到极致是一种虚构：一种对世界不感兴趣的意识形态，仅关注符号。这是一种沉浸于超现实的狂喜。御宅族只是消费主义的一个极端代表。这种消费主义类似于意识形态热情，但应用于某些小众兴趣。大泽和东浩纪告诉我们，无论我们是否消费漫画，我们都是在宏大叙事的遗骸中为寻求我们的身份的寄居蟹。从这个角度看，像涩谷这样充满光辉和魅力的超现实空间实际上是一个精神的深渊。\n虚构时代的时代精神仅仅指出晚期资本主义社会中滋生一种几乎宗教态度的消费者的趋势，他们对自己的爱好、喜爱的电视节目、音乐类型等充满热情。西方的民主、工业化国家（暂且包括日本）正是由于其巨大的财富和高度发达的基础设施，使我们能够过上与貧病争大多隔绝的生活；饥饿不存在，现代医学可以治愈许多疾病，洪水和地震的影响通过技术进步大大减弱等。这反过来使我们与现实疏远，同时将我们进一步沉浸在一个与自然世界完全不同的日益技术化的环境中。\n城市一直提供某种程度的与贫病死的疏离。然而，基础设施中表现出的先进技术越来越使我们能够从未曾可能的方式中消除死亡、疾病和暴力。超现实就像虚拟现实的类比原型；一种存在的层面，甚至隔绝了城市本身的更严酷环境，专注于消费和机械化的满足。这就是涩谷与迪斯尼乐园的联系所在，使这些超现实空间与东京臭名昭著的山谷（さんや）31地区或大阪的釜崎（釜ヶ崎，かまがさき）32区不同，在那里贫困的体验是痛苦可见的。城市提供的疏离使自然世界的侵入或突发死亡的可能性变得更加创伤。这就是为什么沙林毒气攻击和1995年神户地震对国家心理产生如此深远影响的原因。这两个事件对大泽有特别重要的意义，因为它们扰乱了城市的超现实环境，结束了虚构时代。\n进入超现实本质上是现代化西方的特权，可以说，创造超现实空间的欲望在大泽定义的意识形态最纯粹形式中反映出来：消除贫病死的斗争。讽刺的是，这削弱了我们与意识形态的关系，将贫病死推向模糊的边缘。然而，超现实从未完全隔离我们；我们始终容易遭受灾难、受伤或突如其来的死亡，破坏我们对世界安全或稳定的看法。超现实只是掩盖了我们对这种灾难可能性的意识，直到它以灾难性的事件出现在我们的生活中。\n70年代日本右翼和左翼意识形态活动的明显失败——红军的暴力、三岛由纪夫的自杀、学生运动的混乱和政治无能，甚至远处苏联的解体——导致了结构日本战后社会的宏大叙事的普遍崩溃。**从这个角度看，大泽的虚构时代是由意识形态时代向虚构时代过渡的双重运动推动的：超现实中的疏离和政治幻灭。**在这种背景下，新人类对经典意识形态斗争不感兴趣或不信服。他们转向内心的虚构和超现实的享乐空间，作为新理想的空间。对于1980年代的御宅族，他们实际上献身于虚构，作为新理想可以蓬勃发展的空间。超现实是鼓励和增强向虚构冲动的空间。像涩谷这样的空间由开发商建造，作为对消费符号的激情献身的自然基质。正如田中元子（Motoko Tanaka）敏锐指出：\n“……现代日本[已经]标准化和媒体化，有着精心维护的分销管理系统。在日本，消费者需求可以立即和机械地满足，无需他者的介入。以前只能通过社会交流获得的欲望对象，如食物和性释放，现在可以很容易地获得，或几乎不需要交流。”（田中, 57）\n当然，这不仅适用于日本，还适用于整个后工业化西方。现代性的成果导致了许多基本需求自动、即时，甚至在我们意识之外得到满足的系统。根本上，超现实代表了一种对虚拟现实的渴望。新宿和迪斯尼乐园就像原型；模拟形式的虚拟空间。但是，虚拟的渴望究竟是什么？斯洛文尼亚哲学家斯拉沃热·齐泽克（Slavoj Zizek）对这个术语提供了一个有趣的解释：\n“[虚拟现实]提供了被剥夺其实质的现实……就像脱咖啡因咖啡闻起来和尝起来像真正的咖啡但不是实物……一切都被允许，你可以享受一切——只要它被剥夺了使其危险的实质。”（齐泽克, 38）\n这种对虚拟的概念，即享受无后果的乐趣或去除物体本质的欲望，是我们迄今为止探讨的超现实和虚构概念的核心。向虚构的运动反映了一种沉浸在符号的吸引世界中的愿望，在那里理想可以不受阻碍地生存，超现实的城市空间同样反映了一种欲望，即将城市中任何不愉快或危险的因素剥离。然而，超现实倾向于那些不仅使我们与超现实之外的现实疏离，而且间接地与他者疏离的系统。这导致了对现实的共同感知的削弱。那些在70年代末和80年代开发的超现实空间充当了当代虚拟空间扩散的原型。虚拟现实和超现实仍然保持各自的领域，但技术的整体趋势指向政府和私营部门积极努力将数字实用性与物理空间整合（智能手机与城市的互动是一个很好的例子；例如Uber，甚至约会服务如Tinder）。超现实中反映的虚拟渴望与从70年代末到现在的视频游戏的加速增长和流行密切相关。然而，如果这些空间使某种行为或某种消费关系得以实现，大泽会将其定义为转向虚构的行为，但它们并不一定决定这种行为。大泽关于御宅族的论点揭示了沉浸在超现实中的欲望如何反映出一种快乐的享乐主义，但也可以反映出对世界的深层次怨恨，迅速转变为病态的虚无主义。\n虚构的负面逆转 超现实世界的主要问题在于，虽然它充满了符号和物质上的享受，但却缺乏意义。对当代整体情况常见的情感反应是讽刺、玩世不恭和刻意的冷漠。另一种常见反应是制定复杂且高度综合的新世纪信仰体系。这些信仰体系鼓励信徒完全服从一种彻底另类的世界观，通常看起来像科幻小说或幻想作品一样刻意。在这个意义上，它是虚构时代最有力的象征。这些信仰体系通常强调世界的虚假和无意义。因此，围绕新世纪信仰体系组织的邪教往往核心是一个末世论叙事，要么寻求逃避当前形式的世界，要么寻求其毁灭。在社会学家岛园嗣的研究中，指出新世纪邪教成员的两个关键特点：一是成员主要来自相对富裕或中产阶级背景；二是加入邪教的主要原因是世界的虚假、虚幻或无意义 (田中, 48)。这两个特点似乎概括了问题的核心。尽管这些人生活舒适，物质上得到满足，但这种满足并不能填补当代生活中某种根本性的意义空虚。\n对于大泽来说，虚构时代（1972-1995）的主要象征是奥姆真理教。他认为这个组织是御宅族的极端表现。这在字面上是部分正确的。众所周知，麻原彰晃和他的追随者是漫画和动漫的狂热爱好者，这些虚构的内容直接融入了他们的综合信仰体系中，包括诺斯替基督教、吠陀神秘主义、阴谋论和超自然伪科学。一个小例子是“宇宙清洁器”，这是一个大型空调单元，麻原声称可以净化外界毒素的空气（大泽, 50）。这个名字直接取自流行动漫《宇宙战舰大和号》，其叙事围绕人类试图修复被敌对外星人辐射污染的地球。此外，该邪教还通过身体和精神上远离外界，建立了一个名为莲花村的公社，作为他们的新乌托邦时代的诺亚方舟，并称之为香巴拉，这是从藏传佛教中借用的天堂概念。通过隐退，他们得以完全封闭在自己设计的世界中。莲花村是他们尝试创建的超真实空间，在这里他们可以用新的虚构符号包裹自己。全面探讨他们所有的宗教实践在这里是不可能的，但简而言之，像“宇宙清洁器”这样的遗物、麻原的秘传布道、能量转移仪式如夏克提巴特，以及电击疗法和在黑暗房间里度过数天的极端宗教训练，都是他们沉浸在自己的超真实中的一部分。\n麻原彰晃将许多阴谋论作为他们神话的一部分。他们认为日本政府是一个由恶意的全球犹太阴谋（共济会）操纵的傀儡，实际上控制了世界。他们认为美国政府在共济会的影响下开发了一种地震机器，导致了1995年的神户地震。在他们看来，世界是不可救药的，受恶意力量的控制。他们甚至认为自己的公社莲花村里充满了共济会间谍，这导致了一个秘密内圈的形成，最终负责毒气袭击。在奥姆未能在1990年获得政治职位后，大泽指出，麻原在该团体内部刊物中的言辞变得越来越末日论和对日本政府的敌意。甚至有证据表明，在1995年之前有几次失败的毒气袭击（奥尔森, 413 - 416）。未能当选相当于外界对其虚构的拒绝，这对他们来说是不可容忍的侮辱，促使进一步的神秘启示和更多的阴谋论，以巩固他们的信仰。\n奥姆还标志着日本社会状况与《黑暗之魂》系列之间的第一个接触点。在每个游戏中，总是存在着在已坍塌王国废墟中仍在运作的敌对、神秘的亡灵邪教。这在第三部作品中特别明显，几乎整个游戏过程中都可以看到亡灵邪教的证据。这些无法理解的信仰实践者代表了有组织社会的最后残余，他们一见到玩家就完全致力于消灭你。正如奥姆的一些成员甚至对其实践和目标也感到模糊一样，《黑暗之魂》中的亡灵邪教的实践和目标也是如此。只有他们作为玩家威胁的存在是明确的。这重现了许多人对奥姆的恐惧和焦虑，他们只是一个威胁和神秘的实体。\n大泽认为，像奥姆真理教这样的新宗教与基督教等旧的千禧年传统不同之处在于，他们通过退缩故意脱离与“贫病争”的斗争。佐证岛园的论点，他指出这种退缩的主要动机是“生活的空虚”（生活の空虚）（大泽, 35）。他将新信仰与旧信仰进行对比。例如，基督教在《圣经》中包含一个末世论叙事，涉及物质与神圣最终完美结合（《启示录》实际上是麻原的主要灵感来源之一）。然而，大多数形式的基督教在通过慈善等努力积极参与与“贫病争”的斗争。换句话说，他们不通过退缩来拒绝现实的严酷，而是积极努力通过各种可能的途径消除它。这使得它们在经典意义上是意识形态的。令人惊讶的是，许多在日常生活中与贫困和疾病作斗争的人往往是宗教信徒。如果我们看一下像天主教会这样的机构，大多数教区居民来自第三世界，如南美洲、非洲甚至贫困水平较高的亚洲。相比之下，属于虚构时代的宗教如奥姆真理教明确拒绝世界，通常渴望世界的终结或其他形式的毁灭。在我们的文化中，人民圣殿教（乔恩斯顿大屠杀）33和天堂之门34是两个虚构取向而非意识形态取向的宗教的好例子。\n岛园对加入像奥姆真理教这样的团体的动机的研究似乎表明，现代经验中出现的某种缺失需要被填补。这是从意识形态转向虚构的根源，在面对当代的模糊、混乱和意识形态的空虚时对意义的真诚渴望。虚构就像是在这混乱中的生命线，但一个人越深入这领域，病态行为的潜力就越高。如果沉迷于自己最喜欢的系列神话的御宅族是对虚构和超真实的“积极”异化，那么实行恐怖主义的末日邪教则是无法容忍其虚构界限之外世界的“消极”异化。奥姆对其自创虚构的痴迷，以及他们完全退隐至莲花村——他们自己设计的超真实空间的尝试——使他们无法接受任何与其虚构相矛盾的存在。这是对虚构的依恋导致对世界虚无主义拒绝的关键点。在后现代性的背景下——即宏大叙事不再起作用的世界——现实无法维持这种虚构中潜在的理想，往往会导致认为现实本身某种程度上是被污染的结论。\n第二章：受苦之魔力圈 未实现的道路 文学解释充满了模棱两可，因为一个文本可以有无数种解读。这一点在电子游戏中更为显著，因为其“游戏性”（或游戏特质）和叙事结构之间存在根本性的张力。传统的文学解释依赖于文本的某种稳定性，以便提取出一种“解读”。在《了不起的盖茨比》中，读者只能通过一个“路径”来进行叙事。威尔逊总是射杀盖茨比，而这一叙事事件的描述对所有读者来说都是相同的。虽然可能有许多解释方式——例如，我们可以将其解读为对资本主义意识形态如何使无产阶级自相残杀的马克思主义批判——但这种解释只有在基于菲茨杰拉德的原始作品《了不起的盖茨比》的基础上才有效。\n如果我们将电子游戏，特别是RPG（角色扮演游戏）类型，视为一种文本，它们更像是一个选择你自己的冒险小说，其中玩家的自主性决定了事件的进程。从叙事角度来看，这种思维方式有助于分析《黑暗之魂》，因为它的叙事结构大致是线性的。它有一个开始，并有几个决定结局的叙事选择，而这些点之间有限的叙事内容是由“作者”（设计师）设定的。然而，即便在这里也存在问题，因为游戏作为美学艺术品并不主要在于通过叙事来前进，推动玩家想知道接下来会发生什么。游戏的体验在于游戏本身，这种游戏形式可能与叙事或其中所出现的虚构世界有松散甚至完全没有实质性的联系。例如，想象一下，你只能在完成一场5公里跑步比赛且时间在六分钟以内的情况下从第九章继续到第十章。在文学中，这些活动之间没有联系，但在电子游戏中，“跑步比赛”的体验非常重要，并直接影响我们对叙事的解读。在像《黑暗之魂》这样的游戏中，完成游戏大约需要40个小时，而阅读、观看过场动画或听对话的时间可能只有30分钟。\n大多数电子游戏是困难且繁琐的，即使对那些熟悉这一媒介的人来说也是如此。对于那些没有所需数字素养和基本操作游戏设备技能的人来说，它们简直是不可能的。正是这种繁琐和挑战性，导致电子游戏学者Espen Aarseth称电子游戏为“需耗费精力的文学”（ergodic literature），即“需要付出巨大努力才能让玩家穿越游戏文本”（Aarseth，第2页）。“ergodic”这个词来源于希腊词语ergon和hodos，分别意为“工作”和“路径”。这个概念出现在Aarseth的开创性作品《Cybertext: Perspectives on Ergodic Literature》中，成为这一新领域的核心理论工具。这个概念的核心是玩家在游戏中拥有的自主性，这种自主性是书籍读者或电影观众所不具备的：\n读者，即使强烈地参与到叙事的展开中，也无能为力。就像足球比赛的观众，他可以推测、推断、外推，甚至大声咒骂，但他不是比赛的参与者。就像火车上的乘客，他可以研究和解释不断变化的风景，可以随意休息眼睛，甚至拉下紧急刹车并下车，但他不能自由地改变轨道方向。他无法拥有玩家的影响乐趣：“让我们看看我这样做会发生什么。”读者的乐趣是偷窥的乐趣。安全但无能为力。（Aarseth，第2页）\n然而，除了“影响的乐趣”——即通过游戏系统选择自己的“路径”的乐趣之外，还存在其负面反转的未探索可能性。在另一篇题为“定义现实”（Define Real）的文章中，他写道：\n\u0026hellip;玩家意识到他们体验的部分性质，众多策略和未走的路径，以及游戏中可能存在他们永远不会遇到的谜团，超出他们技能水平的解决方案\u0026hellip;你可能永远不会知道你的选择的确切结果；也就是说，你到底错过了什么（Aarseth，第3页）。\n在文学中，没有只有在特定条件下才会显现的秘密章节。当我们从头到尾阅读《了不起的盖茨比》时，我们确信已经读完了整部作品。当然，多次阅读会揭示多种解释，但我们从不会为是否仍有大量未发现内容而感到焦虑。从形式上、游戏设计角度以及文本/电影的角度来看，《黑暗之魂》是一款推动这种感觉的游戏，有时这种感觉令人不安、疲惫和疏远。应对这种感觉是游戏作为一个偶然性系统运作的关键方面。在对第一款《黑暗之魂》游戏的分析中，Daniel Vella将这种焦虑称为“需耗费精力的讽刺”（ergodic irony）（Vella，第16页），以表示我们的自主性如何可以削弱我们对游戏“完成”或“掌握”的驱动力。\n除了“需耗费精力的讽刺”中潜在的焦虑之外，还有由游戏行为产生的不稳定心理模式的问题。尽管被称为“游戏”，但在这个领域仍有相当大的争论，即电子游戏是否应该被视为游戏。这并不难理解，因为电子游戏通常同时既是一个故事又是一个具有规则和程序的游戏。当我们踏上篮球场时，我们进入了一个特定的心理框架；所有的行动都是通过一个基本的目标来理解的，即在一个内化的任意限制（例如，不携带球、不击打其他球员等）下将球投入篮筐。\n实际上，正是这种框架使人成为篮球运动员。这正是荷兰社会学家Johan Huizinga所说的“游戏的魔法圈”（magic circle of play）（Huizinga，第32页）。它是指游戏划定了一个空间，在这个空间内只能采用某种心理框架。篮球场是一个“魔法圈”；它允许篮球运动的发生，但只有在篮球游戏本身的心理框架下才能使用。如果做其他事情，就会排除游戏的可能性。篮球场也可以用作高中舞会的场地，此时“圈”的魔法处于休眠状态。小说也是一种魔法圈，因为它是以某种心理框架为前提而创作的。当我们阅读小说时，通常会暂停许多其他阅读方式，以便欣赏故事。文学评论家、历史学家和文学爱好者不会以相同的方式阅读小说，尽管他们可能是同一个人。然而，所有这些框架都是允许的，并且不会削弱作品的基本存在。\n此外，阅读小说的行为通常并不是围绕着完成一个任意目标进行的，而阅读的“规则”并不是基于封闭的一套理性公理。电子游戏会在你无法理解或处理游戏作为一个规则和程序系统时让叙事停滞不前。电子游戏，特别是RPG类型，要求玩家在最基本的层面上通过多种心理模式进行转换。玩家必须随时准备成为电影观众、读者、统计数据的解释者以及复杂运动任务的执行者，所有这些都在几分钟之内完成。\n从这个意义上讲，魔法圈的概念开始崩溃，因为很明显，电子游戏是一个可以同时存在多个圈并要求不同心理框架的空间，而不会削弱电子游戏作为互动艺术品的功能。更好的理解电子游戏的方式是将其视为模拟，其中传统意义上的游戏要么被直接纳入，要么可以在设计师意图之外自发地出现。此外，尽管电子游戏通常将虚构作为体验的核心方面，但严格来说，它们与虚构并不完全相同。引用游戏学者Jesper Juul的话，电子游戏是“半真实的”：\n电子游戏是现实的，因为它们由玩家实际互动的真实规则组成，赢得或输掉游戏是一个真实事件。然而，通过杀死一条龙赢得游戏时，龙并不是真实的龙，而是虚构的。玩电子游戏因此是与真实规则互动，同时想象一个虚构世界，电子游戏是规则和虚构世界的结合。（Juul，第1页）\n然而，在“真实”与“虚构”龙的问题上，还有另一个重要的观点。Espen Aarseth认为，像《魔戒》这样的幻想作品中的龙和像《无尽的任务》（Sony Online Entertainment，1999）这样的幻想RPG游戏中的龙具有截然不同的本体论性质。即使是同一条龙，如《霍比特人》小说中的Smaug和《霍比特人》电子游戏改编中的Smaug，也是如此。区别在于虚构与模拟之间的差异。在题为“门与感知：游戏中的虚构与模拟”（Doors and Perceptions: Fiction vs Simulation in Games）的文章中，Aarseth写道：\n一条龙显然是虚构的，但另一条是模拟的。一条龙是供阅读或在电视或电影屏幕上观看的，另一条龙是供玩耍的。一条龙完全由符号构成，另一条由符号和动态模型构成，这个动态模型将指定其行为并响应我们的输入。正是这种模型行为使其与虚构不同，因为我们可以比了解虚构更亲密地了解模拟。虚构很少甚至从来都不是个人化的，而模拟通过体验可以成为个人化的。模拟允许我们测试其极限、理解因果关系、制定策略并产生变化，这些在虚构中被明确拒绝，但在现实中却非常相似。（Aarseth，第35页）\n然而，如果我们将游戏视为模拟，它们到底在模拟什么呢？如果模拟的是一些现实世界的活动，如驾驶飞机或汽车，这个问题的答案就很简单。在电子游戏行业本身，“模拟器”一词指的是一种试图在世界中再现实际、有时是非常平凡的体验的游戏。模拟可以是低保真或高保真的，但它们的参照物是显而易见的。\n以虚构为导向的游戏，本质上是将那种虚构作为一个数字世界的模拟。当我们考虑电子游戏与传统虚构中的既定类型的关系时，这一点最为明显。科幻电子游戏《光晕：战斗进化》（Bungie，2001）似乎直接借鉴了几部科幻作品，并将它们拼凑在一起。游戏发生的巨大的环形太空栖息地（即标题中提到的“光晕”）让人联想到Larry Niven的《环形世界》系列，而主角Master Chief穿着的“Mjolnir Powered Assault Armor”似乎暗示了John Steakley的《装甲》一书，该书的情节也围绕着穿着动力外骨骼与外星威胁作战的士兵展开。可以几乎无限地以这种方式继续下去，将光晕神话中的某些方面孤立出来，并试图从科幻经典作品中挖掘出影响。这种拼凑是类型小说的基本特征。不同的是，当这些元素被重新构建为一个虚拟的“现实”时。\n设计师不像作家那样，必须回答一些困难的问题，例如，等离子步枪的感觉和声音如何？它的射速有多快？它能穿透坦克的外壳吗？作家可以简单地依靠读者自己想象这些体验细节。以这种方式，《光晕》就像是对构成该类型的符号集合的模拟：模拟模拟物。然而，电子游戏并不是由影响其美学的虚构类型定义的，而是由游戏玩法的惯例定义的。《光晕》的虚构可能源自科幻，但它首先是一款第一人称射击游戏。因此，作为一种体验，它与《孤岛惊魂：原始杀戮》（Ubisoft Montreal，2016）——一款发生在人类原始的穴居人过去的“射击”游戏（标枪、弓箭等）——相比，具有更多的共同点，而不是与《幽浮》这款科幻游戏进行比较，尽管《幽浮》也涉及人类与外星人之间的冲突，但它是一款策略游戏，其原则更类似于国际象棋或Risk这样的棋盘游戏。\n然而，电子游戏不仅仅模拟作为美学集合的类型，或甚至是作为与外星人战斗的太空陆战队员的幻想，尽管它们在这些方面做得相当令人信服。它们还模拟人类存在的模式，或者换句话说，它们模拟文化原型。它们几乎是作为这些原型在古今虚构作品中表现的逻辑延伸来模拟的。\n在其作品《千面英雄》中，美国神话学家Joseph Campbell首次提出了“英雄之旅”的概念，这是一种在不同文化和时代中可见的叙事模式。他描述了基本模式如下：\n一个英雄从日常世界出发，进入一个充满超自然奇迹的区域：在那里遇到令人惊叹的力量，并赢得决定性的胜利：英雄从这个神秘的冒险中回来，拥有了为同伴带来恩惠的力量。（Campbell，第23页）\n正是这种模式的认知使我们能够看到《星球大战》中的卢克·天行者几乎与《伊利亚特》中的奥德修斯扮演了完全相同的文化功能。RPG类型作为一个整体几乎完全围绕这种模式在叙事和游戏设计中进行结构化。在这些游戏中，几乎总是有一个任务，一次跨越许多土地的旅程，一个成长的过程（例如，升级，解锁能力和获得强大的物品），以及一个英雄式的结局，一个王国，或者宇宙本身得救。《黑暗之魂》与其RPG前辈不同之处在于一个关键的类别：英雄式的结局。英雄的方面不仅被模糊和故意含糊不清，而且甚至没有提供一个真正的结局，像卢克自我实现为绝地武士并推翻邪恶帝国的那种方式。这是使《黑暗之魂》作为作品独特的关键方面，因为它模拟了英雄之旅，同时又颠覆了构成其核心价值观的基本结构。\n电子游戏之现象学 如果电子游戏是一种对虚构类型和文学/文化原型的模拟，而不是简单的虚构再现，那么我们该如何解释它们的意义呢？游戏的高度开放性和“能动性”特质（ergodic）在试图对某一作品进行严谨理解时带来了挑战。这一问题在游戏学者中广泛讨论，但一种广泛应用的解决方案是采用现象学方法，即Edmund Husserl和Martin Heidegger等思想家所提倡的在研究游戏体验的基本特征时所用的方法。简而言之，现象学是指通过经验调查现实，而不是单纯依赖外部的、理性的（主要是隐含的）公理体系来证明我们的知识。\n从根本上说，游戏的现象学是一种“第一人称”的分析方法，它不会忽视学者在与游戏这种物质系统互动时所产生的变化和解释框架。这种方法并不排除其他传统的文学解释方法，也不排除社会学、人类学或历史等其他研究模式，而是将这些模式纳入研究者的心理整体中。这样，评论家可以关注游戏过程中相对于具体文本和电影元素所出现的多种心理模式。毕竟，电子游戏不仅仅是叙事，也不仅仅是一组抽象的程序和规则。它们是两者的结合。玩家通常将这些元素作为一个整体来体验，仅仅关注叙事或游戏玩法中的任何一个都会忽略一些至关重要的方面。只有通过严格的自我反思过程，游戏评论家才能提出关于这种全新媒介中出现的意义类型的连贯论点。\n之前，我提到过Johan Huizinga对游戏的定义，即“魔力圈”，其中划定了一个需要特定心理框架的空间，才能使游戏存在。由于游戏在每一刻都引发多种心理模式，电子游戏理论家Dominic Arsenault和Bernard Perron借用了Huizinga的概念，并将魔力圈重新定义为“魔法循环”（magic cycle）（Arsenault，Perron 109）。这一术语指的是玩家与游戏系统之间的反馈循环，玩家通过探测系统来推进游戏，系统则产生新的游戏状态，激发玩家对游戏世界的不同理解，促使玩家采取进一步行动。在《黑暗之魂》系列中，玩家探索世界以获取材料，这些材料可以用来升级武器，使他们能够打败更强大的敌人，从而探索更多的世界，获得更多的武器和材料，依此类推。在这个过程中，他们将看到、阅读和体验到改变他们对自己活动看法并激发新行动的事物。\nArsenault和Perron的魔法循环概念在游戏研究中广泛传播，但游戏设计师早已意识到反馈循环对其媒介的重要性。作为《光环》的首席设计师，Jaime Griesmer曾在一次采访中说：\n在《光环1》中，大概有30秒的乐趣反复出现。如果你能找到30秒的乐趣，你基本上就能把它延伸成整个游戏。（Kietzmann，web）\n由于《光环》在游戏历史中的重要地位，这句话已成为商业领域的设计信条。然而，Griesmer在几年后的另一次采访中对此进行了重要补充：\n在Vidoc（视频纪录片）中被删掉的那句后半部分我谈到了如何在不同的环境中，以不同的武器、不同的载具、对抗不同的敌人、不同组合的敌人，有时对抗彼此战斗的敌人来进行那30秒的乐趣。《光环》的每30秒都不会重复；任务不断改变你所处的环境。（Kietzmann）\n这揭示了Arsenault和Perron意识到但其术语未能充分表达的魔法循环的重要方面。游戏玩法不是一个循环，而是一个不断扩展的螺旋，叙事和游戏元素交织在一起，创造出对游戏世界作为虚构空间和系统的不断演变的理解。\n在下一章中，我将对我在《黑暗之魂III》中遇到的一个隐藏区域进行现象学描述，该区域是在玩了大约30小时后发现的。角色和地点的名字将同时给出英文本地化和日文原版。这些本地化版本相对忠实但可能与原版有显著偏差。这是因为本地化过程优先考虑的是游戏的市场性和销售，而不是文本的忠实度。在原版日文和本地化版本之间有显著差异的地方，我将提供自己的翻译。这将在文本引用中注明。我将采用现象学方法来传达在魔法循环中发现的变化心理模式，这些模式不仅是整个系列的特征，而且可以在游戏的任何特定部分中浓缩。尽管从叙事角度来看，描述中会有许多看似晦涩的部分，但这种晦涩是故意构建在体验中的。只有当我们检查魔法循环的轮廓时，才能正确地将这些散落在这段短暂但强有力的体验中的叙事片段置于上下文中。这里提供的《黑暗之魂》系列的解释将展示叙事框架如何与游戏元素交织在一起，创造出必须整体分析的连贯体验。\n第三章：末法、死亡叙事与可怖他者 审判者古达，失败的英雄古达 在洛斯里克城堡的地下墓穴深处，有一座早已被遗弃的陵墓。这个房间曾经看起来像是一个礼拜的地方。房间中央矗立着一个巨大的石质圣杯，仿佛一个洗礼池。在它下面，一个身穿16世纪马西米连式板甲的骑士盘腿而坐，呈莲花座姿势。他在冥想吗？不，他早已死去，但人们不禁想知道他是否已经摆脱了这个充满幻觉、欲望和轮回的短暂世界。或许他在这个被遗弃世界的凉爽而孤寂的角落里，达到了那著名的般涅槃境界。\n凝视着他平静的身影，我不禁想起浮世绘版画中那幽默的拟态（見立て）。我回想起铃木春信（1725-1770）的一幅作品，画中一位身穿鲜红长袍的花魁在一片巨大的叶子上航行，这是对菩提达摩从印度抵达日本的幽默指涉（詹金斯，第100页；附图1）。江户时代的有识之士和富裕阶层会为这样一个将高僧与吉原的女孩混同在一起的形象感到兴奋。我们的骑士——安详地在这个洗礼池下休息的形象，不正如同释迦牟尼在菩提树下成道的拟态吗？这是开发者对玩家的巧妙玩笑吗？我们无从得知，但没有理由停留在无法解答的谜题上。当游戏本能占据上风时，这个房间就变成了一个需要搜刮宝物的地方。\n房间后部有一个箱子。我打开它，发现了一片龙鳞（顿时一阵快感涌上心头；我现在有足够的鳞片来升级一件特殊武器了），但箱子后面的墙壁有些奇怪。它的颜色与周围环境不匹配，看起来过于黑暗，仿佛这个部分的图像纹理有些瑕疵。这种不协调感令人无法忽视，而且地板上还刻着其他玩家留下的信息。我不需要特意去读它们就能感觉到这面墙后面有什么东西。我用剑击打墙壁，墙壁消失了——不过是个幻象。取而代之的是一个洞穴的入口。向洞穴迈出一步，屏幕上闪现出粗体文字：无主墓地（無縁墓地）。我感到一阵兴奋。寒意通过我的身体，汗毛竖起。我发现了一个秘密区域。带着忐忑，我穿过狭窄的通道，但走了几码后，我发现自己竟然在户外。头顶上没有天花板，只有无尽的黑暗，仿佛洞穴的屋顶高达数百英尺。我在这狭窄的峡谷中穿行，斩杀可怕的生物，它们扭曲的身躯以恐怖的速度前行，每个都残缺不全，陷入疯狂。\n然而这里有些东西让人感到不对劲，与这些畸形怪物的恐怖无关。某种潜移默化的既视感开始渗入我的脑海。当我穿过峡谷时，一条临崖的小路出现了。既视感愈发强烈。这里有更多的怪物，我很快将它们击败，但当我转过弯时，一座巨大的石门出现在视野中，那挥之不去的感觉变成了顿悟。我以前来过这里；这是通往传火祭祀场（火継ぎの祭祀場）的路，与我在游戏开始时的地方一模一样。这是正常游戏世界的黑暗翻版吗？是游戏虚构世界的一部分吗？我们还在叙事空间内吗？还是这是开发者在这庞大作品中隐藏的彩蛋：一种打破第四面墙的暗示，让玩家知道毕竟这只是一款游戏？\n但这个地方带来的感觉更像是恐惧，而不是内部笑话。上面的世界，即使在这秘密区域之外，已经是破碎和折磨的了，但这里更黑暗，更可怕，更堕落，完全没有希望。这无疑是一个回收利用的区域，使用了相同的数字艺术资产，但以某种方式，它与我在这些游戏中体验的世界并不矛盾。我是被困在时间循环中吗？另一个维度？平行世界？穿过石门的拱门，我可以看到院子里蹲着的审判者古达（灰の審判者グンダ）的身影。与游戏开始时的灰烬墓地（灰の墓場）一模一样（附图2）。我怀着忐忑的心情迈过大门，他抬起巨大的身躯，是我的三倍高，拿起他的巨大长柄战斧冲向我，跃起数十英尺，然后让他的全部重量砸在石头上。我勉强躲开。Boss战开始了，他的生命条出现在屏幕底部，但这次显示的名字不同。这次是：英雄古达（英雄グンダ）（附图3）。他的外貌也变了。他的眼睛闪着红光，动作更凶猛狂野，攻击模式更变化多端，难以预测，毫不留情。\n与第一次相比，这场战斗明显更加黑暗和残暴。难度也高了一个数量级。经过大约一分钟的奔跑，努力在他追击我时找准方位，我误判了他的攻击模式，他用长柄战斧的残酷重量击碎了我的角色。屏幕上出现了“你死了”的信息。我的角色在离我惨死地最近的篝火处复活。我感觉到这将是一场艰难的战斗。《黑暗之魂》系列中的Boss战很难，通常需要八九次死亡才能克服挑战。它们的难点在于识别Boss的攻击模式，让玩家能够系统地躲避攻击和利用空隙。耐心、观察和计划会得到回报，急躁和贪婪则会受到严厉惩罚。需要一种禅意的超然态度。我在精神上准备好要死很多次，直到掌握古达的攻击模式。\n我不想浪费时间击败古达和我之间的所有怪物。那太麻烦了。我冲过那些恐怖和畸形怪物，举着盾牌像人形冲撞车一样，只杀掉那些无法绕过的怪物。探索模式的兴奋感已经消失。我对我在哪里、为什么在这里、在这里意味着什么不再感兴趣。我完全专注于击败古达这个任务。这个空间曾经是恐惧和奇迹的地方，现在只是一个需要尽快穿越的地方。回到门前，现在入口处有一层薄雾在旋转。这是一个屏障，一个只有我能通过的封闭区域，其他怪物无法跟随。它是一个亲密的空间。薄雾对我来说意味着院子现在与游戏世界的其余部分隔离开来，在那之后，只有古达的挑战在等待。当古达被击败时，薄雾会消失。穿过薄雾，战斗开始，我在一系列时机不当的闪避后被迅速肢解。这开始了一个死亡、复活和冲回院子的循环。\n起初，我在无尽的死亡中保持冷静。每次死亡只是一次试探性的任务，以进一步了解我的对手。每次尝试，我都能更好地识别他的动作，更一致地躲避攻击，在不受惩罚的情况下偷袭他。我每次都离击败他更近一点，但在大约第20次死亡后，挫败感开始显现。每次失误都引发咒骂和愤怒的嘟囔。我的挫败感让我玩得更糟，进展开始倒退。在这苦难的最低谷，一个来自另一个世界的恶意玩家入侵了我的游戏，在我与凶残的亡灵攻击犬搏斗时，从背后刺杀了我。这导致我失去了一个珍贵的东西，一个人形余烬（残り火），用于在死亡后恢复角色的全部潜力。挫败感不断积累。\n最终，古达的攻击过于狂野，方向变化多端，节奏既慢又快。我对这个系列很有经验，期待它会很难，并且享受挑战，但我遇到了瓶颈。我感到一丝受挫的骄傲，但我拿起笔记本电脑，向更聪明的玩家寻求策略来解决我的问题。看了几个视频后，我对古达的攻击模式有了新的见解：我应该使用的武器，应该等待的时机，攻击的时机，躲避的时机，以及无数微小的细节都会对我有帮助。战斗仍然困难，死亡不止，但我现在清楚地看到了前进的道路。这只是一个我是否有足够技能去执行的问题。\n第十次尝试，带着这些新的策略，我感受到了。这就是那次。必须是那次。带着坚定的决心，我全力以赴，每次闪避都准确无误，等待合适的时机挥剑，抵制住过度攻击的冲动。在削减了他的生命条并躲避无数疯狂的攻击后，古达被击倒在地。我看到屏幕上出现提示，表示可以执行特殊的终结一击。我按下正确的按钮，我的角色用剑刺穿了对手的身体。他发出雷鸣般的野兽般的吼叫，他的身体在一片灰烬中消失。屏幕上闪现出胜利的信息：“火的继承者被消灭了。”一切都结束了，我胜利了。我长舒一口气，全身充满了满足感和兴奋的强烈混合。但我到底得到了什么？这场战斗残酷、令人沮丧、让人意志消沉，但最终，击败Boss的快乐只来自于我学会了他的攻击模式。而且我还需要借助互联网的指导。这不是一次完全的失败，但也不是一次完全的胜利。我感觉到在这次经历之外还有更深的奥秘，但现在还不是我发现它们的时候。我关闭了游戏机，休息眼睛，思考。那次经历是什么？它意味着什么？\n破碎的叙事 上面的现象学描述展示了在游戏过程中不断变化的心理模式。这些模式从寻宝到敬畏的探索，从文本分析到视觉解读，从理论思考到实际操作，从挫折到狂喜，以及这两者之间的一切。游戏的整体流程在与世界的具体挑战进行身体上的互动和对所见所闻产生的好奇之间不断切换。总体而言，游戏系统的推动和拉动作用使玩家始终处于悬念之中，不仅因为环境的挑战，还因为游戏中的真相从未以连贯的方式揭示。游戏的呈现极其丰富和暗示性，但总是留给玩家更多的问题而不是答案。要理解这一系列游戏如何作为一个整体运作，有必要勾勒出基本的游戏机制与反复出现的叙事主题的关系，从而赋予游戏整体体验一种统一的逻辑。\nDark Souls系列在RPG类型的其他游戏中独树一帜，因为它几乎没有向玩家提供任何叙事说明。此外，它也没有给玩家一种续集之间有任何关系的确定感。将该系列联系在一起并使其被认作续集的是游戏设计的连续性、有限叙事中的概念主题以及某些情感特征，这些都是该系列的标志。从这个意义上讲，该系列非常符合东浩纪的数据库理论，因为传统意义上的叙事几乎完全不存在。现有的叙事说明——无论是电影“过场动画”、对话还是沿途找到的文字碎片——都以一种抽象和神话创造的方式运作，玩家很容易忽略。对游戏世界的明确理解最终并不是享受游戏的必要部分，尽管对某些玩家来说可能很有吸引力，因此被简化为体验的外围方面。\n每款游戏都发生在一个不同名字的王国中，这些王国处于后末日的破败状态。这种破败状态的原因不明，但总是与“不死的诅咒”有关，这种诅咒使居民无法永久死亡（他们不断转生），最终变成疯狂和怪物化的样子。这被称为“空心者”（hollow），而玩家在游戏中创造的化身也受到这种诅咒。这种诅咒以黑暗标记（ダークリング）表示，一个框住黑暗虚空的闪烁火环（见附图4）。系列中暗示，只有通过猎杀他人以收集“灵魂”，才能抵抗空心化的影响并最终避免发疯。在每款游戏中，玩家的任务是“链接火焰”（火を継ぐ），即复活死寂世界的过程。这涉及到击败五个强大的实体——在黄金时代的堕落王国的始祖和领主——并收集他们的灵魂，以便用作催化这种重燃的燃料。一旦玩家将这些伟大的灵魂吸收到自己体内，他们也必须牺牲自己作为新纪元火焰的燃料。\n乍一看，将所有三款Dark Souls游戏联系在一起的概念主题似乎只是对无数西方文明神话的另一种演绎，这些神话围绕年轻一代为了振兴传统而做出牺牲。然而，Dark Souls系列通过对其有效性的细微质疑并削弱个人能动性的概念，复杂化了这种典型的英雄公式。Dark Souls系列展示了这个循环不仅是社会通过代际生存的必要过程，而是一种对自然秩序的绝望斗争，因而是一个收益递减的循环。在第三款游戏的结尾，重燃过程已经退化到世界无法修复的地步。玩家的任务不过是在一系列逐渐退化的现实循环中的最后阶段。\n灵魂的概念是叙事和游戏玩法中的另一个重要概念主题。在每个游戏世界中，只有少数几个角色尚未变成空心者，你可以与他们交易物品。你通过杀死空心者和遍布世界的怪物获得的灵魂是你用来与这些角色交易的货币。这些角色可以成为你战斗中的盟友，但在游戏过程中，许多角色最终会变成空心者并攻击你。从叙事的角度来看，这在那些仍保持理智的人之间营造了一种不信任的氛围，因为他们可能只是因为他们是掠食者。这一概念也从叙事中渗透到游戏中。游戏内置的在线功能意味着几乎在任何时候，其他玩家都可以侵入你的世界，杀死你并夺取你身上的灵魂供自己使用。这反映了村上春树和大泽的诡异他者概念。你的敌人本质上与你相似；因此，在Dark Souls的世界里，无论是从叙事还是游戏角度，你都不能真正信任任何人。\n灵魂也是你角色变得更强大的主要方式。当你获得灵魂时，可以用它们来增强和升级你的角色和你拥有的武器。这对于探索世界更危险的部分至关重要。然而，当你变得更强时，角色的升级成本呈指数增长，而收益却是递增的。这需要进一步探索世界的危险区域以寻找更强大的灵魂。与灵魂任务相关的重要游戏机制在玩家探索世界时会产生一种持续的紧张和危险感。当你的化身被杀死时，所有未使用的累积灵魂都会掉落在他们死去的地方，但化身本身会在最近的“篝火”（见附图5）处复活。这些篝火是标志着你进度的安全港湾。它们之间间隔极远，必须由玩家“点燃”才能使用。玩家可以在篝火处休息而不必担心攻击，甚至可以用它们传送到其他已经点燃的篝火。通过篝火，玩家得以探索世界。当玩家的化身在篝火处重生时，他们有一次机会回到死去的地方并找回他们的灵魂，灵魂以一个绿色的旋转粒子球体悬浮在地面上。如果玩家在找回这些灵魂之前再次死亡，灵魂将永远丢失。因此，玩家在篝火之间的危险旅程中总是面临巨大的风险。除了作为安全港湾外，篝火还是玩家补充化身健康和物资的地方。然而，这可能是双刃剑。当敌人被杀死时，他们会无限期地保持死亡状态，但每当玩家在篝火处休息时，他们会复活。在这些破碎的诅咒王国中，没有“免费的午餐”。所有进展都是艰难获得的，并充满风险。\n环境叙事 尽管在《黑暗之魂》系列中故事情节被弱化，但它并非完全不存在。理解游戏世界的神话主要通过解释性的循环，这种循环由丰富的视觉信息和散布在世界各地的物品附加说明文字驱动。整个系列传达的主要氛围显然是后末日的，第三部作品更是将这种美学推向极致。在名为洛斯里克高墙的区域，玩家探索一座完全荒废的梦魇般的哥特式城堡。灰白皮肤、眼窝空洞的可怜人跪在地上，双手合十祈祷，崇拜着巨龙的尸体和似乎正在变成细长树木的人类尸体。看起来这里被某种邪教占据。这些人通常无视玩家，但他们由士兵看守，这些士兵一看到玩家就会恐怖地尖叫，召唤同伴来迎战。这里的基调无疑是后末日的，但没有任何解释。\n这种叙事风格通常被评论家和设计师称为环境叙事。在游戏的任何时刻，玩家在《黑暗之魂》系列中不断自问的中心问题是：这里发生了什么？自然，解释会有所不同，但发现过程是激发《黑暗之魂》游戏魅力循环的一个重要部分。在2010年的游戏开发者大会上，Arkane Studios的设计师Harvey Smith称赞了这种叙事风格的重要性，认为这是视频游戏作为一种媒介所独有的机会：\n“……环境叙事是一种主动的叙事方式。瑞士心理学家Jean Piaget表明，游戏、发现和互动是学习的关键。这种主动的学习方式创造了参与感，进而产生投入感。学生和玩家都会带着自己的经验，因此解释的行为具有个人意义……这一概念背后的理论是封闭法则。作为人类，我们有一种天生的需要，将视觉元素分类并纳入更大的框架。为此，我们得出结论。Scott McCloud在《理解漫画》中将这一概念应用于视觉叙事：‘重要的是在面板之间发生了什么。’” (Harvey)\n因此，在上述现象学描述中，我们在遇到古达时看到的略微打开的石棺，通过人类连接符号点的欲望联系在一起。Harvey认为，这导致了一种深刻的沉浸感，使一款精心制作的游戏充满了自发的突现意义。尽管这种方法在许多游戏中很常见，但《黑暗之魂》系列将这一概念推向极致，常被认为是这种世界设计的高水准标杆。\n玩家在《黑暗之魂》中通过文字、视觉和动作建立的联系跨越了大量的时间和续作。在第二部游戏中，玩家来到了一座显然是战场的城堡。巨人般、树状的尸体到处都是，这些尸体像树一样长出了根，完全占据了整个城堡，打破了厚重的石墙和护墙。最终，玩家会找到一部电梯，将他们带到城堡的下层，他们将在那里第一次与一个仍然活着的巨人boss战斗，尽管这个巨人像针垫一样被各种剑和矛刺穿。一旦boss战开始，屏幕底部会出现一个显示boss角色“生命值”的条。玩家对boss造成的每次打击都会让生命条减少，向玩家传达他们在战斗中的进展信息。虽然这个条的主要功能是信息性的，但它也执行了一个微妙的叙事功能，通过命名对手来反映其叙事意义。在这种情况下，boss的名字出现在生命条上方，称为最后的巨人，反映了它的叙事重要性。尽管在此之前看到无数与这个boss角色相似的尸体，这是唯一提供任何当前情况信息的地方。显然，这个巨人是城堡内部某次冲突的最后幸存者。一旦这个巨人被击败，它的灵魂会自动添加到玩家的库存中。附加在灵魂上的说明文字如下：\n巨人の森の地下に縛られていた／生き残りの巨人のソウル／王国を蹂躙しつくした巨人たちの长は、／名もしれぬ英雄によって倒されたという／长は砦の地下に繋がれ、封じられたが、それは何もかもが滅び去ったあとだった\n被困在巨人森林地下的巨人灵魂/这个王国的摧毁者之一/一个无名英雄打败了这个巨人首领/他被绑在堡垒地下，被封印，但一切都已经被毁。 (FromSoftware, Dark Souls II; 我的翻译)\n这里有很多信息，但它提出的问题远比回答的多。我们知道巨人是入侵者，这解释了城堡的状态和无数的尸体。我们知道这个特定的巨人是巨人中的一个领导者，我们也知道他被打败并被俘虏，这就是我们找到他现在这种状态的原因。但他们为什么要入侵？这发生在多久以前？玩家知道了发生了什么，但并没有更多信息。这种不断的神秘感是玩家如何解释他们在游戏世界中看到、做和体验的一个重要方面。随着他们继续在游戏中前进，其他带有说明文字的物品将为整体画面添加更多信息，但有时是矛盾的。每次披露都会带来新的隐藏。这些巨人的谜团在我们在第三部游戏的开始遇到其中一个尸体时更加复杂。洛斯里克（第三部）和德兰尼格（第二部）之间是否存在连续性，还是这仅仅是一个“彩蛋”，是对系列长期粉丝的一个小惊喜？这是不可能知道的。\n这种叙事风格催生了无数的论坛帖子和YouTube视频，致力于研究分散的说明文字和环境信息之间的联系，并以此为证据提供关于世界的连贯解释。这些视频中的一些非常受欢迎，获得了数百万的观看。YouTuber VaatiVidya制作了无数视频，基于游戏内的证据创建了关于游戏背景故事的理论。然而，他不仅止步于此，还通过挖掘游戏代码并检索通常在游戏的正常商业版本中无法访问的信息片段（音频文件、缺失的场景等）“揭示”叙事内容。这需要特殊的工具和代码知识来发掘这些片段，指向了一种完全不同的消费虚构作品的模式。VaatiVidya的许多视频经常超过一百万次观看，他通过对游戏神话的个人解释成功地将其职业化。这仅仅是因为游戏本身提供的信息非常少。在游戏设计师在游戏内提供大量虚构历史的情况下，例如流行的西方RPG《上古卷轴V：天际》（Bethesda Softworks，2011），对游戏神话的理论研究几乎没有意义。显然，这些理论在粉丝中引起了激烈的争论，因为游戏中内置的证据实在太少，无法得出定论。\n有趣的是，虽然在英语世界中存在数百甚至数千个这样的视频，但在日语网络区域几乎不存在这种内容。YouTube上有成千上万来自日本内容创作者的关于《黑暗之魂》的视频，但他们很少处理从叙事片段中解构神话。显然，这种做法在日本的系列粉丝中不太普遍，他们似乎更喜欢制作分析游戏系统的内容（教程、指南）或制作实况视频。这似乎证实了东浩纪的观点，即新一代御宅族对重建作品中潜在的大叙事不感兴趣。对于他们来说，《黑暗之魂》丰富的情感特征数据库已经足够了。\n第四章: 后现代意识与末法意识 末法 正如前一章所述，“点燃火焰”的命令是驱动《黑暗之魂》系列的核心主题；它构成了推动玩家前进并赋予游戏进展意义的主要目标。点燃火焰带有许多传统西方神话的意味，集中在神灵和英雄展开宏大的任务，以恢复王国或领域的正当状态。在许多方面，RPG类型可以被视为围绕这一设想的一系列模拟游戏。从表面上看，《黑暗之魂》似乎没有什么不同，只是氛围更加暗黑，表现更加抽象。然而，当我们审视这一概念在整个系列中的发展时，很明显在《黑暗之魂》中，点燃火焰实际上是走向消亡的下坡路。点燃火焰——就像古老谚语“传递火炬”——代表了社会及其结构价值观的延续。因此，它大致类似于意识形态在我们社会中所扮演的角色，即通过构建我们对世界的体验的宏大叙事将价值观从一代传递到另一代。《黑暗之魂》的基本结构质疑这一过程的价值，暗示其中存在深刻的腐败，并且一代又一代试图维持微弱火焰的努力不过是人类虚荣心的表现。正是在这种点燃火焰的隐喻中，我们看到了通过佛教形而上学反映的大泽和东浩纪对日本文化景观的后现代评估。\n末法（まっぽう）的概念是佛教早期的一种古老的末世论概念，但直到镰仓时代（1185-1333）才在日本受到广泛关注。据推测，末法是在印度受到希腊人入侵期间发展起来的，但直到6世纪传入中国后才具体出现：\n“当这种思想传入中国时，在南北朝和隋朝建立期间渗透进了大众意识。重要的是574年周武帝对佛教的迫害。”（Bloom，第145页）\n末法思想，像其他类型的启示性思想一样，往往在过渡、冲突和不确定的背景下出现。从南北朝到隋朝的过渡标志着从长期的政治混乱到统一的中国帝国的转变。镰仓时代也反映了一个政治混乱的时期，武士家族篡夺了京都名义上的中央政府。在这些时期，末法概念在日本佛教机构的神学话语中变得弥散，产生了许多独特的日本佛教形式，包括净土宗和日莲宗35——这些佛教形式至今仍然流行。这个词，可以字面翻译为“法的终结”，指的是佛陀教义中包含的真理随着时间的推移会失去其相关性，直到它们在世界上不再发挥任何功能或目的。\n总的来说，根据这种模式，佛教有三个时代。佛陀去世后最初的500年被称为正法时代（しょうほう）。在这个时代，佛陀的教义按照应有的方式被实践，佛陀教义的精髓对所有人开放。接下来的500年被称为像法时代（ぞうほう），尽管佛陀的教义以各种形式和仪式形式实施，但它们已经失去了法的精髓。像法时代之后的10,000年被称为末法，在这个时代，形式和精髓都已经衰退，通过圣贤或个人努力实现觉悟的可能性完全为零。在这个时代，佛教本身最终会被遗忘，痛苦和纷争将持续不减。\n从基本视觉层面来看，《黑暗之魂》是末世氛围的。此外，每次重新点燃火焰时都会逐渐消逝的火焰主题以及法的传递越来越弱的启示性观念，像一场漫长的电话游戏，直到信息完全丢失，都有着强烈的概念平行。这甚至与大泽的三种意识形态、虚构和后虚构（动物）时代之间存在平行。大泽的论点基于这样一个观点：宏大叙事变得越来越弱和腐败，直到没有什么能够支撑它们。历史上有趣的平行现象将后现代主义和末法思想以意想不到的方式联系在一起。\n尽管在镰仓时期末法一词广泛流传，但解释却相当多样。然而，这个词在日本宗教生活中的起源通常与一个名为《末法灯要记》的文献的出现有关。这个文献通常归因于天台宗的创始人最澄（767-822），但专家对此争论不休。许多人认为这只是一个净土宗僧人的伪作。不容置疑的是，它对当时主要佛教思想家的明显影响，包括法然、荣西、道元、亲鸾和日莲。比起宗教文献，《末法灯要记》决定性地具有政治色彩。最澄在其中为忽视传统佛教戒律的僧侣的做法辩护，反对政府的批评。宗教学者Michele Marra解释了支持最澄论点的逻辑：\n“……政府在批评中忘记了这些僧侣生活在末法时代，因此，他们不能适用为生活在真理理想时期的僧侣制定的规则。由于在末法时代只有口头教义存在，而实践不存在，觉悟不可达成，戒律也随之消失，因此，坚持僧侣违反戒律是毫无意义的。不存在的东西如何被破坏？同样的原因，戒律也不能被遵守。”（Marra，第287页）\n然而，《末法灯要记》在反驳中并没有止步于此：\n“《末法灯要记》暗示那些努力遵守戒律的僧侣，因此屈从于政府要求的僧侣，表现出对佛教的形式态度，忘记了其宗教的人性意义。因此，他们被比作摧毁国家的蝗虫……”（288页）\n该论著的最终结论是，那些“名义上的僧侣”，即至少表面上扮演角色的僧侣，应受到敬畏和尊重，因为他们代表了在这种情况下可能达到的最高精神成就。因此，他们应该被视为珍贵的，就像在一个没有银和金的世界里，铜被视为珍贵一样。东浩纪对“动物主义”的默认支持在某种程度上与最澄的论点有相似之处。在一个意义无法存活的时代，除了成为一个御宅族并将自己困在只有你渴望的情感特点的毛绒世界中，还有什么其他选择呢？暗示中，依附于旧式意识形态信仰的人是错觉的，甚至是危险的。这是涵盖在后现代主义范畴下的哲学广泛持有的信念。\n尽管《末法灯要记》在日本主要佛教思想家中广泛流传，但在关于末法的形而上学状况下应该做些什么的解释却十分多样。然而，绑定这些不同解释的往往是对天台宗佛教的拒绝。事实上，法然、道元、荣西、日莲和亲鸾都曾接受过天台宗僧侣的训练 36，后来他们得出结论认为该机构已经失去了对真正法的把握。这最终导致每个人都与这个强大的机构决裂，创立了自己的传统。天台宗自最澄时代以来一直是平安朝廷支持的官方宗派。在藤原氏等强大支持者以及其他重要贵族家族的支持下，它兴盛并成为日本的主要佛教形式。其与平安贵族的关系如此紧密，以至于它几乎成为一种类似于拜占庭皇帝与早期东正教会关系的国教。这里可以画出的另一个平行现象是，天台宗试图通过将其在中国遇到的许多异端佛教传统综合为一种大师解释的方式来塑造其信仰：\n“日本天台宗的第一个特征是倡导一种综合佛教，即所谓的‘一大圆满教’的理想……所有佛陀的教义最终没有矛盾，可以统一为一个综合和完美的系统。”（Hazama，第102页）\n从这个角度来看，接受末法概念的传统拒绝了天台宗的中央权威。Thomas Kasulis写到亲鸾对此的特殊反应：\n“对亲鸾来说，平安世界观的权威机构已经不再相关。在比叡山，天台宗的中心，亲鸾年轻时曾在那里学习和修行，他们全面教授所有形式的知识，无论是显教还是密教。每个僧侣都可以利用各种修行方法，突破魔罗的幻觉军队，直面现实。\n整个企业得到了平安贵族的社会、经济、文化和政治支持。然而，尽管在山上有那么多的见识和实践，镰仓日本在山谷中的世界似乎正在崩溃。政府是军事政变和几十年公开战争的结果；疾病和自然灾害广泛发生；个人为自己的解放而内省，但发现自己被内部冲突分心和困扰，这些冲突与外部世界的情况一样真实和令人沮丧。在这种情况下，圣贤之道，天台（和真言）的承诺，即与现实和自身和谐相处，只会加剧人们对真正发生了什么以及如何应对的不知所措感。这种对世界和自我的深刻不可理解和失控的感觉就是末法意识的本质。”（Kasulis，第20页）\nKasulis所说的末法意识是一种认为现有秩序无法拯救的感觉。除了Kasulis，许多学者还将末法思想的出现与异常数量的饥荒、自然灾害、军事冲突和政治斗争联系起来。然而，有理由相信镰仓时代在这些事件的实际数量上并不与日本历史的其他时代不同，但在宗教思想家感知和解释这些事件的方式上可能存在质的差异。不可否认的是，镰仓时代标志着日本文化和政治中心的巨大转变。这是从京都贵族向镰仓和东部地区的地方武士家族的转变。动荡的政治秩序无疑对天台宗观点和实践的合法性构成了挑战。从这个角度来看，末法意识是一种情感反应，回应了一个结构社会的叙事似乎无法拯救的时代。向过去的激进转变变得必要。\n毫无疑问，在我们自己的时代，我们也生活在同样动荡和过渡的时期。国家权力在全球经济体系面前已经削弱，战争已变成在国际政治边缘运作的分裂组织之间的冲突。公司从民主结构中夺取权力，不再需要在国家层面运作。即使是苹果，这个美国企业精神的象征，也在中国制造其产品，在库比蒂诺设有办公室，并在爱尔兰注册以避美国税率。在我们自己的国家，特朗普总统的政治口号“让美国再次伟大”反映了对事情正在分崩离析并需要拯救的恐惧。数字技术以深刻模棱两可的方式改变了人类互动的基本体验。如果我们接受宗教学者的总体分析，即末法意识在动荡和过渡时期出现，那么东浩纪和大泽作品中所倡导的后现代主义似乎也反映了一种新的末法意识。后现代思想主要围绕真理之死和叙事之死展开，当以这种方式看待时，它似乎相当启示性。这是对工业和国家现代时代成长的回应，转而进入信息时代和新兴全球秩序的动荡模糊中。\n以类似的方式，《黑暗之魂》系列属于这种来自生活在这种变动中的个体的末法意识情感反应的范畴。《黑暗之魂》系列的显著之处在于它在一个西方外壳和西方媒介中传达了一种传统佛教的启示性思维。然而，末法和后现代意识形态之间有一个重要区别：在镰仓时代对末法做出回应的人们是真正有兴趣通过重新概念化指导其信仰的超验价值来追求一个解决方案。对亲鸾来说，或许是最具后现代色彩的末法思想家之一，人们只有通过放弃自己的自主力量并服从于阿弥陀佛愿力的慈爱，才能逃避不可避免的退化。对日莲来说，他的解决方案是崇敬《法华经》并念诵题目。这些本质上是以不可否认的衰退为背景的精神解决方案。相比之下，后现代思想缺乏信仰成分，简化为对以往价值结构的简单否定。这成为其自身倒置的教条；无叙事的教条，无真理的教条。《黑暗之魂》传达了末法的符号学和传统，但它倡导的是一种更符合后现代思想的更为阴暗的观点。没有对抗末法形而上学轨迹的阴暗预后的超验原则。在这方面，《黑暗之魂》是一部充满日本过去，无论是现代还是古代的感性作品，但植根于对日本当今轨迹的阴郁思考。\n第一火焰：欲望之源 通过末法意识和后现代主义的解释框架，我们可以将连接火焰这一核心主题解读为跨代社会叙事的恶化，以及日本本土末世论范式的重构。虽然游戏采用了西方美学，但其设计和美学却充满了佛教形而上学影响下的独特日本感性。西方的外在形式也反映了日本战后社会和现代日本的整体情况。尽管日本国家的形成在很大程度上是由本土精英自主完成的，但毫无疑问，日本的法律、教育、政治甚至文化机构的核心构造是一系列源自19世纪和20世纪欧洲社会的概念嫁接。自1868年成为现代国家以来，日本一直是组合（組み合わせ）的杰出典范，即通过巧妙地组合现有元素创造出新的事物。\n明治政府在构建政府时，既借鉴了德国的法律体系，也吸取了英国的政治经济哲学。二战后，日本战败后麦克阿瑟推动的宪法改革进一步加剧了日本国家的拼凑性质。尽管这部宪法在公众中得到了广泛接受（尽管近来有修改的尝试，但仍在继续维护），但它仍然是少数西方人撰写的外来文件。这份文件构成了战后日本叙事的基础——这个永远和平、永远繁荣、永远服务于全球进步秩序的国家——仍然是日本国家的核心。但这种外来性并没有抹去日本的文化或历史。宪法只是提供了一个框架，真正的核心是数百年积累的文化，这些文化构成了日常生活的基础。现代日本人的生活与西方同行非常相似，但充满了源自悠久历史的感性和行为，这些历史与民主或经济自由主义没有必然联系。如果东浩纪和大泽真幸所描绘的是日本社会叙事的崩塌，那么这种崩塌主要表现为那些从欧洲继承的进步、自由和平等观念的瓦解。如果这些叙事正在衰退，那么这种衰退自然会被表现为一个崩塌的西方王国。这就像日本的西方性如死皮般剥落。如何更好地表现这一点呢？通过一幅充满幽灵和变异尸群的哥特式大教堂画面。\n尽管《黑暗之魂》系列偏重于环境叙事，但并不意味着它完全没有传统的叙事展现。然而，这种展现风格更接近于前现代的神话诗意传统，而非现代的奇幻作品。这两者的区别大致可以概括为理性和直觉原则在叙事中的区别。主要由于J.R.R.托尔金的《魔戒》三部曲的影响，大多数现代奇幻作品试图创造一个全新的世界，这个世界有着完整的虚构历史，充满了虚构的种族、语言，以及一套部分透明的公理来支配作品的内部现实。当一个作品不仅仅是一本书，而是一个跨越多个媒介和数十个单独作品的系列时，这可能会变得相当复杂。通常，这些元虚构信息的主体是以一种可以通过因果关系的理性原则来解释的方式构建和呈现的。这允许粉丝基于系列中的“证据”对他们喜欢的作品进行“理性”论证（历史、社会、物理、生物等）。当创作者创作的新内容不符合先前确立的虚构宇宙公理时，粉丝们甚至会批评他们。例如，在《哈利波特》系列中，咒语只能用魔杖施放。如果在一个《哈利波特》的衍生作品中，一个角色不用魔杖就施放了咒语，这将与先前确立的公理逻辑不一致。为了解决这个矛盾，需要建立一些额外的公理来包含先前的公理（即，这个角色可以不用魔杖施放咒语，因为他们拥有独角兽的血液，这赋予他们对魔法的更深连接）。这些公理不断演变以适应新作品，但也为了在世界构建过程中保持完整性，并避免粉丝的责难。\n相比之下，前现代的神话诗意作品几乎不尝试用理性公理来支撑其虚构现实。它们的功能是象征性或情感性的，而非字面的，因此从现代角度来看，它们似乎充满了矛盾。为什么宙斯可以变成公牛或天鹅？为什么神明可以受伤或被杀害而他们却是神？在传统神话中，发生的事情就是发生了；没有对底层公理的诉求。《黑暗之魂》几乎没有提供理性原则，也几乎没有提供全面的历史，因此其叙事风格更像传统神话。称之为完全相同可能有些牵强，但它顽固拒绝现代奇幻及其庞大元叙事的理性主义是其美学的关键方面。唯一的反驳是电子游戏本身是理性、数学的系统，装饰着叙事。因此，在这种媒介中总是存在一种奇怪的游戏与叙事之间的冲突。《黑暗之魂》通过将其机制概念化为游戏世界困扰的表达来避开这个问题，但保持细节故意模糊，以避免对元虚构进行正式和严格的理解。\n三部游戏的开头都有一个简短的电影，让玩家对他们进入的世界有一个模糊但高度暗示的框架。我们将分别考察每个游戏，作为美学演变的序列，但我们将详细分析第一款游戏，追踪其完整的叙事弧。这是因为对第二和第三款游戏中的选择的理解依赖于第一款游戏中确立的美学原则。在我们开始分析第一款游戏的介绍电影之前，重要的是要注意，尽管《黑暗之魂》是一款日本游戏，但原版中没有日语配音。换句话说，游戏中的英语配音不是本地化的结果，而是开发者为营造某种氛围而做出的美学选择。不懂英语的日本玩家必须依靠字幕来理解游戏中的对话，这强化了沉浸在异域世界中的感觉。还值得一提的是，配音演员都带有英国口音，这反映了奇幻类型跨作品和媒介长期以来的传统。这只是《黑暗之魂》西方外衣总体情感数据库的一个部分。英文音频和日文文本大致相当，但有时在重要方面有所不同。在以下引用中，我将保留原文英文音频：\n古い時代／世界はまだわかたれず、霧に覆われ／灰色の岩と大樹と、朽ちぬ古竜ばかりがあった／だが、いつかはじめての火がおこり／火と共に差异がもたらされた／熱と冷たさと／生と死と／そして光と闇と／そして、闇より生まれた幾匹かが／火に惹かれ、王のソウルを見出した／最初の死者、ニト／イザリスの魔女と、混沌の娘たち／太陽の光の王グウィンと、彼の騎士たち／そして誰もしらぬ小人／それらは王の力を得、古竜に戦いを挑んだ／グウィンの雷が、岩のウロコを貫き／魔女の炎は嵐となり／死の瘴气がニトによって解き放たれた／そしてウロコのない白竜、シースの裏切りにより／遂に古竜は敗れた。／火の時代の始まり／だが、やがて火是将灭，暗闇だけが残る／今や、火はまさに消えかけ／人の世には届かず、夜ばかり続き／人の中に、呪われたダークリングが现れはじめていた\n在古老的时代，世界尚未形成，被雾笼罩／一个灰色岩石、大树和永恒巨龙的土地／但随后有了火焰，随着火焰的出现，差异也随之而来／热与冷／生与死，以及当然，光与暗／然后从黑暗中，他们来了，并在火焰中找到了王之灵魂／最初的死者尼特／伊扎里斯的女巫及其混沌之女们／太阳之光之王格温及其忠实的骑士们／还有那不为人知的矮子／凭借王者的力量，他们向龙发起挑战／格温强大的雷霆撕裂了它们的石鳞／女巫编织了巨大的火焰风暴／尼特释放出死亡和疾病的瘴气／而无鳞白龙席斯背叛了自己的同类，龙族不复存在／于是火焰的时代开始了／但很快火焰将熄灭，只剩下黑暗／即使现在只剩下余烬，人类看不到光，只看到无尽的黑夜／在活人中出现了被诅咒的黑暗印记。是的，的确如此。黑暗印记标记了亡者。在这个土地上，亡者被驱赶到北方，被关在等待世界末日的地方\u0026hellip;（Farz; 视频片段）\n这一简短片段的音乐显然属于恐怖或悬疑范畴。开头是极简主义的；情绪化的弦乐在背景中幽幽作响，竖琴添加了不和谐的点缀，使画面充满了神秘和不安。火焰被描述为生命的起源和现实基本二元性的起源，例如生与死（0:30）。在许多西方神话中，火或光通常用来代表神圣的创造源泉。这在许多现代奇幻作品中也适用，因此“光”在许多文化和神话中都具有强烈的积极意义。然而，这种隐含的情感滤镜将火焰呈现为某种险恶和危险的东西。在观众遇到这火焰之前，镜头将我们带入地球的深处。在那里，火焰像地狱般的舌头在一个巨大的洞穴张开的嘴巴中燃烧。无论是视觉效果还是音乐，都没有什么鼓励我们将火焰视为神圣的生命源泉。它更像是可憎的生命源泉。火焰出现时低音提琴和铜管的引入表明它既可怕又强大（0:50）。整个片段在不和谐程度上接近挽歌风格，特别是弦乐部分。尼特的出现伴随着图瓦喉音歌手的引入，他们在背景中凶恶地低吟，进一步加剧了恐惧感（1:15）。这些图瓦歌手在介绍剩下的三位神祇时继续吟唱：伊扎里斯的女巫，格温大人，以及隐秘的矮子（1:15-1:46）。这些喉音歌手虽然是微妙的补充，但为传统的西方管弦乐增添了“东方”的神秘感，突出了故事中的神秘色彩。\n这种投射在火焰上的恐惧滤镜与佛教形而上学和隐喻语言一致。在巴利语经典中，《火焰经》（Adittapariyana Sutra），也称为《火焰讲经》，使用火焰作为隐喻来代表由欲望引起的苦（dukkha）。传统认为佛陀向火崇拜的苦行者讲授这部经文，以便将他们转化为他的精神实践：\n比丘们，一切皆燃烧。什么在燃烧？比丘们，眼在燃烧，可见形式在燃烧，视觉意识在燃烧，视觉印象在燃烧，任何因视觉印象而产生的感受，愉快的、痛苦的、既不愉快也不痛苦的，都在燃烧。燃烧着什么？燃烧着欲火，燃烧着恨火，燃烧着愚昧火；我说它燃烧着生、老、死，燃烧着悲伤、哀悼、痛苦、忧愁和绝望。（Thanisarro）\n在这段经文中，火与我们对感知的执着所带来的痛苦混为一谈，同时也是现实基本二元性的一个方面，如生/死和快乐/痛苦。这与《黑暗之魂》神话中火作为差异起源的概念相平行。当我们在电影中看到地球深处的翻滚火焰时，它并没有呈现为神圣的创造之光，而是作为混沌的、原始的欲望引擎。从这种角度来看，恐惧的情感滤镜与佛教隐喻语言密切吻合。此外，随着这种火焰的出现，令人恐惧、可怜的僵尸般的剪影从黑暗中浮现。他们愚蠢地摇摇晃晃地走向这火焰，身体姿态笨拙，暗示着粗糙和未形成的心理。他们没有圣经传统中高贵原始人的尊严。这些蹒跚而行的可怜虫将被火焰的力量转变为如格温大人那样的崇高个体，但这种英雄转变有些污秽和怪异。\n格温大人的美学显然让人想起宙斯、奥丁、沃坦或因陀罗般的神祇，因为他是一个统治光明和掌握雷霆的众神之王。然而，我们对他的第一印象却是一个疲惫的老人（1:34）。火焰在他面前燃烧，但他的眼睛浑浊，他的头发狂野、凌乱，看起来像稻草一样干燥，仿佛被长时间暴露在热中所损坏。他似乎没有传统神话中父亲形象的活力或智慧。此外，他身后的骑士军团与典型的西方骑士形象几乎没有共同之处。没有高贵、勇敢或个人英雄主义的符号。这些骑士是无脸、诡异的存在，过于纤细和高挑，具有昆虫般的特质，因为他们的角盔略微让人联想到钳子。他们在一个巨大的洞穴檐下整齐地站成无尽的队列，静如雕像。这些被火焰转化的可怜虫——即被欲望转化的——成为了一个小型的神祇团。\n有一个父神（格温），一个混沌母亲/生育神（伊扎里斯的女巫），和一个冥界神（尼特）。然而，“隐秘的矮子”的存在打破了这个神祇团的连贯性。他被称为这些王者之一，但他却不参与斗争。他那扭曲的小小身影贪婪地蹲在火焰旁边——就像佛教神话中的饿鬼（APPX. 图6）——对这些伟大神祇的阴谋漠不关心，专注于他手中那小小的火焰在他烧焦的手掌间跳动（1:40-1:52）。这一形象进一步加深了火焰吸引可怜生物的嫉妒，即使它对其持有者造成伤害的印象。确实，尽管伊扎里斯的女巫拥有更为尊贵的形象，她的手看起来也像矮子一样烧焦了（1:28）。其他神祇也是如此，每个人都在他们面前握着一团火焰，凝视着它。这是火焰时代的真正起源，那些摇摇晃晃的实体以王者自居的盲目欲望。\n许多西方神话和奇幻的经典特征在这里得到应用，但它们由与火焰相关的恐惧滤镜定义。即使在火焰时代介绍时，这种滤镜也没有消失。我们看到一个美丽的王国，但光是暗红色的，暗示一个已经在黄昏的时代。当与龙族的冲突中使用低音提琴、合唱团和铜管乐器时，表示战争的能量和恐惧，伴随和平王国的音乐则回到了开头部分的悬念和预感模式。总体而言，火焰时代的特点是静止、惰性和预感。这部介绍电影的整体重点不在于世界的起源，而在于其当前的衰退状态及其看似污秽的起源。抛开佛教形而上学，这部介绍电影牢牢确立了系列的末世氛围。亡者的诅咒，通过黑暗印记表现，也与火焰联系在一起：一个环绕空洞深渊的燃烧圆圈。这与早期的原始火焰展示相平行。它也是一个在广阔黑暗中的微弱火焰。通过这个标记，火焰本身与亡者的诅咒联系在一起。在佛教形而上学中，燃烧的欲望火焰引发了黑暗印记，这是驱动轮回的能量。在这一点上，黑暗印记在游戏的符号学与电影中遇到的世界神话概述之间创建了一个有趣的接触点。\n如前所述，那些受到这种诅咒困扰的人无法死亡。如果他们被杀，他们只是转世。连续的转世导致角色“空心化”，意味着他们失去了理智。在游戏中，这是通过视觉表现的。那些变得空心的人变成了可怕的僵尸般的实体，与介绍电影中的蹒跚求火者非常相似。这也符合佛教形而上学，因为逃离无尽的轮回单调是该传统的主要目标。佛教中的最高精神成就是达到涅槃，这也依赖于与火相关的隐喻。这个词的词源字面意思是“熄灭”，就像吹灭蜡烛一样。末法意识叠加在这个概念之上的是人类制度中的某种精神退化，使人们无法熄灭保持他们在越来越堕落的轮回中的欲望之火。人们无法达到涅槃所允许的真正死亡。这与亡者的诅咒类似，通过一个无法熄灭的火圈标志。这让你一次又一次地复活，不论你被亡者、变异体还是统治这个世界的怪物杀死多少次。这个世界的一切都被这个诅咒的影响所驱动。\n这一大约三分钟的序列是玩家用来理解系列第一款游戏前十几个小时的主要叙事框架。比起叙事内容本身，这部简短电影建立的整体基调和氛围简洁地总结了游戏的全部内容。虽然介绍中出现的角色在第二款游戏中不会回归，但他们在第三款游戏中会以一种间接且诱人的方式被提及。因此，这部电影作为魔法循环的温床，当玩家渴望“连点成线”时，这个循环就会出现。下一章将回到现象学角度，讲述这个开场电影如何在玩家的脑海中保留并在完成《黑暗之魂》第一款游戏的大约40小时内发生变化。我们不会以这种方式看待第一或第三款游戏，但会追踪第一款游戏中建立的叙事和游戏主题的发展。这将表明，连接火焰的主要任务是一个无意义的活动，导致“火之时代”越来越堕落的轮回。\n第五章：成为英雄的意义是什么？ 葛温，薪王 终于，在穿过一个充满恶臭的变异食人族棚户区，跋涉过浓密的毒沼，击败一个会喷吐熔岩的半女巫半蜘蛛的恐怖存在之后，我在地下大教堂塔楼的深处找到了钟。一个愤世嫉俗、心碎的骑士告诉我关于这个钟的信息，但没有解释我为何要寻找它。他说几乎不可能找到，还嘲笑了我，但并未说明其用途。此时，我已在游戏中花费了二十小时，找到它几乎是偶然。我一直在这个可怕的荒原中游荡，无数次死亡，但获得了武器，通过吸收强大的灵魂来增强自己的力量。钟旁有一个杠杆，我必须去调查一下。果然，它不仅仅是装饰；当我操作我的角色站在它前面时，屏幕上出现了一个消息框。我可以拉动它，于是我照做了。这个杠杆可拉动这一事实几乎是一种命令。钟声响起，进入了一个过场动画。在远处，一个无脸巨人，身体像被火烧焦了一样斑驳的灰黑色，听到了钟声，拉动了一条巨大的链条。这使得一座似乎建在悬崖侧面的堡垒的大门升起。我不确定这是在哪里发生的，也不明白为什么，但显然世界的一部分对我开放了。我可以继续穿过大教堂，深入被遗忘的深处，但我被一种强烈的欲望所驱使，想回到地表，回到传火祭祀场（火継ぎの祭祀場），这个破碎世界中唯一的避难所。\n回到那里，我注意到有些东西变了。整个神殿的石墙上回荡着一种奇怪的咔哒声。顺着声音，我走到了篝火旁边的破旧礼拜堂般的建筑里。在那里，我发现一个巨大的蛇状生物从深处冒了出来。原本礼拜堂里的石地板现在变成了深渊，这条蛇从这黑暗的空洞中探出头来，身体的其余部分延伸到下面的深处。它全身黑色，眼睛巨大、鼓胀，橙色的眼睛有蛇一样的瞳孔，上唇上挂着奇怪的肉囊，类似胡须。他有一个鹰钩鼻和巨大的牙齿，块状，从血红色的牙龈中凸出，不停地咬合——这是咔哒声的来源。从这个角度看，他几乎像一匹马。他的外表确实非常怪异，但他并不敌对。当我接近这个生物时，出现了一个对话选项。我按下控制器上的X键开始了互动，他用一种完全与外表不符的老者声音说：\nおお、お主か／目覚ましの鐘を鳴らしたのは／わしは世界の蛇、王の探索者フラムト／大王グウィンの親友じゃ／目覚しの鐘を鳴らした、不死人の勇者よ／お主に、不死の使命を伝えたい／よいかな？\n“啊，是你吗？你是那个敲响觉醒之钟的人吗？我是始祖蛇，寻王者弗拉姆特，大王葛温的亲密朋友。敲响觉醒之钟的不死勇者，我想告诉你你的命运。你愿意接受启示吗？”\n我的头有点晕。敲钟的目的是为了见这个奇怪的蛇？我真的要相信他是我在开场动画中看到的那个人的朋友吗？这些钟是某种测试吗？尽管有这些疑问，这却是游戏中很长一段时间以来第一个明确的叙事发展。我很好奇，也没有理由忽略他。我接受了：\nよかろう。ならば伝えよう／不死人の勇者よ／お主の使命は…大王グウィンを継ぐことじゃ／かの王を継ぎ、再び火を熾し、闇をはらい、不死の徴をはらうことじゃ／そのためには、まず王都アノール・ロンドで王の器を手に入れねばならぬ\n“很好。那么我很高兴与你分享。不死勇者。你的命运是…继承大王葛温。为了重新点燃火焰，驱散黑暗，解除不死的诅咒。为此，你必须前往亚诺尔隆德，获得王之器。”\n这是我第一次直接接受一个具体任务的明确指示。没有真正拒绝的理由，从某种意义上说，也无法拒绝。我可以按自己的方式继续探索这个世界，但到了某个时候，我知道我会耗尽探索的地方。毕竟这是一个游戏，它是有限的。只要我有意愿玩这个游戏——这基本上意味着探索更多的世界，收集更多的物品，并通过吸收灵魂来增强力量——最终我会找到他所寻找的这个器皿。在没有其他事情可做的情况下，真的有什么选择吗？\n弗拉姆特建立了一个关于继承和从世界瘟疫中救赎的叙事，但这个叙事本身就非常暧昧。在经过更多艰苦的试炼，前往游戏世界的遥远区域后，我获得了这个形状巨大的圣杯。我再次带着它来到蛇面前，他带我到地底深处的一个房间。他称之为传火祭祀场，并告诉我它是为继承大王葛温的继承者准备的。这个房间只是一个平台，坐落在一对巨大的石门前。他指示我将器皿放在门前，并告诉我作为“王的继承者”的职责。他告诉我，我必须用与葛温的灵魂同等力量的大灵魂填满王器。他列举了这些灵魂：墓王尼特，伊扎里斯的女巫，新伦多的四王，他们继承了葛温的灵魂碎片，以及白龙希斯。除了四王之外，这些“王”都被称为火之时代的英雄和始祖——那个没有不死诅咒的黄金时代。我感觉如果我用这些灵魂填满器皿，门就会打开。我按照他的指示，杀死了尼特，伊扎里斯的女巫和白龙。他们没有向我透露任何信息。当我遇到他们时，他们只是攻击我。最终，在击败新伦多的四王并获得其灵魂后，另一个与弗拉姆特一模一样的蛇出现了，用完全相同的声音提出了一个反提案：\n…ようこそ、不死の勇者よ／我は、世界の蛇、闇撫でのカアス／貴公ら人を導き、真実を伝える者だ／では、我は隠さず真実を語ろう／かつて火のはじまり、貴公ら人の先祖は／古い王たちの後に、四つ目のソウルを見出した／闇のソウルだ／貴公らの人の先祖は、闇のソウルを得て、火の後を待った／やがて火は消え、闇ばかりが残る／さすれば、貴公ら人、闇の時代だ／…だが／王グウィンは、闇を恐れた／火の終わりを恐れ、闇の者たる人を恐れ／人の间から生まれるであろう、闇の王を恐れ／世界の理を恐れた／だから奴は、火を継ぎ、自らの息子たちに、火を率い、縛らせた／貴公ら人が、すべて忘れ、呆け、闇の王が生まれぬように／…我は世界の蛇／正しい時代を、王を探すもの／だがもう一人の蛇、フラムトは、理を忘れ、王グウィンの友に堕した／よいか、不死の勇者よ／我カアスが、貴公に、正しい使命を伝えよう／理に返して火を継ぎ、今や消えかけの王グウィンを杀し／そして、四人目の王となり、闇の时代をもたらすのだ\n“我将毫不留情地分享真相/在火焰的开始，古代王者发现了三种灵魂/但你的祖先发现了第四种独特的灵魂/黑暗灵魂/你的祖先夺取了黑暗灵魂并等待火焰熄灭/很快，火焰消失，只剩下黑暗/于是开始了人类的时代，即黑暗时代/然而…葛温大王在黑暗面前颤抖/他紧紧抓住他的火焰时代，极度恐惧人类/以及将来会在他们中诞生的黑暗王者/葛温大王违背自然法则/通过牺牲自己来延续火焰，并命令他的子女引导人类/葛温模糊了你的过去，以防止黑暗王的诞生/我是始祖蛇/我寻求纠正过去的错误，发现我们真正的王/但另一条蛇弗拉姆特失去了理智，成为了葛温大王的朋友/不死的勇者，我们站在十字路口/只有我知道你的命运真相/你必须摧毁那正在消逝的葛温大王，他依附于火焰，违抗自然/并成为第四位王者，从而开启黑暗时代！”\n卡斯提供的关于这个世界的历史和形而上学基础的简略草图并不清楚。人类时代是黑暗时代？当他说我的祖先时，他指的是开场动画中的那个不起眼的小矮人吗？小矮人是“人类”，而其他人不是吗？“黑暗灵魂”究竟是什么？这些任务发布者，弗拉姆特和卡斯，他们的外表怪异而阴险，似乎有着完全不透明的动机。重新链接火焰——就像寻王者弗拉姆特希望的那样——以迎来没有不死者的另一个火焰时代，这难道不是一件好事吗？从幻想题材的普遍惯例来看，黑暗时代似乎是需要避免的东西。任何倡导黑暗时代的人，肯定是“坏人”，但我对这个世界以及发生了什么和当前正在发生的事情了解太少。这不是有可能吗？卡斯所说的是真的？葛温，紧紧抓住火焰，拒绝了世界的自然循环，带来了一场灾难？在游戏的上下文中，无法最终验证他的陈述是否真实。无论我相信哪条蛇，我的任务实际路径似乎都没有改变。此时，这只是我个人的解释问题。我已经用必要的灵魂填满了王器，我打算打开通往传火祭祀场的门。这是游戏的逻辑。这也是探索的逻辑。我只是必须知道一旦我执行这些任务会发生什么。我想看看游戏会向我揭示什么。\n我回到传火祭祀场，将最后一个大灵魂释放到王器中，厚重的石门在我面前打开，露出一个纯白的空洞，中间有一个墨黑的斑点。这似乎是一条通道。走过白色空洞，朝着黑色斑点走去，鬼魅般的白色骑士在我面前行进并消失。它们是某些过去事件的残影吗？空洞让位于一个巨大的塔楼的痛苦景象，在昏暗而病态的黄色天空中耸立。我无法确定这是晨光还是余晖。感觉就像我进入了一个停滞的状态。当我走过一些烧焦的柱子时，游戏提示我进入了“The Kiln of the First Flame”（最初的火的炉）。地板完全覆盖着灰白的灰烬，就像在沙丘中行走。有某种爆炸吗？\n在与穿着黑色盔甲的骑士战斗后（它是黑色的因为它像其他一切一样被烧焦了吗？），我在一个充满灰烬和中央有一个闪烁篝火的房间里与葛温大王对峙。然而，当我与他对峙时，他有些不对劲。在开场动画中，他似乎是父神的典型形象，但现在他看起来像个畸形的怪物。他已经空洞了。挥舞着一把燃烧的剑，他的眼睛是黑洞，肉体像木炭一样烧黑了。他一直在这里，守护着世界的最后一块燃烧的余烬，紧紧抓住火焰时代的遗迹。然而，这让他变成了一个无意识的怪物。他的头衔也变了。当我进入房间并看到他的生命条出现在屏幕底部，象征着另一个Boss战时，他的名字变成了薪の王（Lord of Cinder）。看来他作为太阳神的显赫地位已经沦为仅仅是一个燃烧的余烬。薪的意思是什么？我应该把它理解为火种吗？这就是寻王者弗拉姆特所说的继承葛温的意思吗？我必须杀死这个国王，继承他的称号，成为薪之王吗？我将自己变成火种来燃烧这个破碎的世界吗？这个国王的意义，他的意图，以及我在做什么都非常不清楚。\n然而，清晰的是一种悲剧感。与其他二十五个Boss战的音乐通常是轰鸣、黑暗和充满活力的，象征着挑战的严峻、恐怖和刺激不同，这段音乐孤独而悲伤。一架孤独的钢琴在小调中演奏着悲伤的曲调，我与这位曾经伟大的王者的疯狂躯壳搏斗。这种感觉奇异地感人但也神秘。音乐似乎暗示着这种冲突是不可避免的，是一种悲剧。但这是为什么呢？我不是来驱散黑暗并复兴火焰时代的吗？它似乎暗示所有自称为王者和王国保护者的命运是多么可悲。\n在我击败葛温大王后，我走到炉的中央，那里有篝火。当你接近时，会出现一个消息框，允许你按控制器上的X键在世界中启动一个动作。但这次出现的不是通常我接近未点燃的篝火时出现的消息（篝火を灯す），而是一个新选项：火を継ぐ。在这个时候，我可以链接火焰，接着又开始了一个过场动画。我的角色伸手进入火焰，火焰慢慢蔓延到他们的身体，吞噬他们并蔓延到整个炉子，然后爆发成一片火海。竖琴开始演奏，一个孤独的女人的声音开始唱歌，字幕开始滚动。大概这意味着一个新的火焰时代已经开始，但在此之后没有进一步的叙述或图像播放。音乐并没有暗示或确认任何胜利的感觉，而是更多的悲伤和神秘，仿佛整个40小时的游戏过程只是一个很久以前的神话的演绎。\n在玩家击败葛温之后，游戏还提供了另一个选择。这完全不明显，但我偶然从观看其他玩家的视频中了解到。如果玩家从炉中徘徊并返回传火祭祀场，会开始另一个过场动画，弗拉姆特和卡斯与众多其他蛇一起出现，宣告玩家的角色为新黑暗之王。他们从一开始就串通好了吗？如果是这样，为什么？为什么一条蛇敦促我链接火焰，而另一条蛇试图劝阻我，如果最后他们都在玩家角色面前一起鞠躬？没有答案，任何选择的意义都非常模糊。链接火焰是忠于大多数RPG试图模拟的典型英雄身份。然而，蛇形任务发布者的缺乏答案、氛围和模棱两可的动机与我拯救王国的感觉相悖。如果有的话，感觉我被操纵成了一个牺牲的羔羊，一个火种之王。\n在随后的两款游戏中，链接火焰的主题最终将被揭示为不那么英雄的，但这无法从第一款游戏中具体看出。这只是通过游戏世界的符号学和游戏行为所感受到的。要汲取的主要思想是，葛温紧紧抓住生命之火，拒绝让它熄灭，并沦为一个无意识、不死的怪物。在佛教的隐喻语言中，必须放弃燃烧的欲望之火，以逃离无尽轮回的痛苦。无尽地活着就是在轮回的业火中燃烧和燃烧。\n名为“自我”的炙热余烬 我们不会深入探讨第二款游戏，但简要回顾一下“连接火焰”这一主题的发展还是值得的。第一款游戏中的所有概念主题在这里也都存在：有一个不死的诅咒，一个象征它的黑暗标志，篝火，一个获取强大灵魂的命令，连接火焰并接替早已成灰的国王，但仅此而已。这是一个新的王国，带有新的谜团。在这款游戏中，玩家必须坐在一个被称为“渴望王座”（渇望の玉座）的王座上来连接火焰，而这个王座无疑是一个巨大的窑炉，这与第一款游戏有着间接的联系。这种连接火焰和坐在渴望王座之间的直接联系，使火焰作为佛教欲望隐喻的关系更加具体化，这种欲望引发了苦难并推动了轮回的循环。它还表明了欲望如何赋予生命以动力。与第一款游戏不同的是，玩家没有选择是否坐在王座上的权利。一旦击败最终Boss，一段旁白会开始，角色的化身坐在王座上，窑炉的门将角色封闭在内：\n火を継ぐ者、呪いをその身に引き受ける者…／貴方が火を継げば、再びソウルは育ち、同じことが繰り返される／それを望むも、拒むも…貴方が決めることです／王たるものよ、王座へ／その先は、貴方にしか見えないのです “你，连接火焰的你，你，承受诅咒的你…/一旦火焰被连接，灵魂将重新繁荣，这一切将再次上演/这是你选择接受或拒绝的事情/伟大的君主，请坐上你的王座/前方的道路只有你能看见。”\n到第二款游戏结束时，我们完全无法感觉到第一款游戏的事件是否与其续集有任何联系，但叙述者的讲述清楚地表明，“连接火焰”只是循环的一部分。值得注意的是，游戏的创意控制从FromSoftware的首席设计师宫崎英高（Hidetaka Miyazaki）转移到了年轻的下属Tomohiro Shibuya。因此，游戏在基调和风格上有许多有趣的变化也就不足为奇了。这种差异在第三款游戏中最为明显，因为导演控制权又回到了宫崎英高，第一款游戏的联系也更加明显。无论如何，在接受日本著名游戏出版物Famitsu采访时，Shibuya表示，第二款游戏确实与第一款游戏有联系，但在叙事上并不完全如此：\n“我不能完全回答这个问题，但两个设定确实是有联系的，”他说。“然而，就故事而言，我不会说有直接的联系。游戏设定在同一个星球的不同部分——换句话说，如果第一款游戏设定在北极，这款游戏就设定在南极；是那种对比。‘时间’是我们在创作时牢记的一个核心关键词。我现在不能给出具体细节，所以希望你能用你的想象力来理解。”\nShibuya对时间的强调在游戏中通过一个流浪的魔法师Straid of Olaphis的角色得到了呼应，他通过出售魔法来帮助你：\nこの地には、かの国以前にも／多くの国が存在しておったらしい／国は栄え、しかしいずれ衰えていく／あたかも火の燃えあがり、消えるが如く／繰り返し繰り返し／しかしのォ、消えかけた火はやがて蘇るのよ／そして再び国が栄える、その姿を変えながら全ては呪いよ！／ヒェヒェヒェ！／オヌシの如き呪われ人こそが、火を引き受けるのよ ヒェヒェヒェ… “在这个地方，在那个国家之前，/有许多国家曾经存在/国家繁荣，但终将衰落/就像火焰燃烧，最终熄灭/一遍又一遍/但消逝的火焰最终会复活/然后新的国家会再度繁荣，换上新的面貌/这一切都是诅咒！嘿嘿嘿！/像你这样的被诅咒之人，将承受火焰。嘿嘿嘿…”\n这种长时间周期的主题，王国在不同形式中兴衰，与佛教的末世观美学相比基督教的末世观更为相符。在基督教的末世中，耶稣会回来进行最后的审判，将人类带入永恒的天堂。这里的重点是永恒和对世间问题的最终解决。在高度异质的佛教传统中，类似的叙事也存在，但它们总是基于劫（kalpic cycles）的理念，即整个现实旋转的千禧年的缓慢转变。\n除了欲望和时间，游戏还更强调了空虚过程。这在角色Lucatiel of Mirrah身上得到了最清楚的体现。Lucatiel是一个你遇到的流浪骑士，她会偶尔在你的任务中帮助你。随着你的进展，你会在世界的不同地点遇到她，但在每次相遇中，她都会因不死的诅咒而失去更多的记忆。这让她感到恐惧，她最终向你坦白：\n私は、失うことを恐れている／記憶を、私自身を／もし、お前を殺せば／この呪いが解けると言われたら…／私はためらうことなく、剣を抜くのだろう／私は、生き続けたい 私であり続けたいのだ／そのためなら…何を犠牲にしても…／何と醜く、浅ましいことか… “失去让我无比恐惧。失去记忆，失去自我。/如果有人告诉我，杀了你就能解除这个诅咒…/我会毫不犹豫地拔剑。/我不想死，我想存在。/为了这一点，我愿意牺牲任何东西，任何东西。/这让我感到羞耻，但这是事实。有时，我觉得自己沉迷于这个叫做‘自我’的小东西。/但即便如此，我还是迫切想要保护它。/我这样感觉错了吗？如果你是我，你也会这样做吧？也许我们从出生起就被诅咒了。”\n最终，接近游戏末尾时，Lucatiel变得空虚并会攻击玩家的化身。Lucatiel的感受和不死诅咒可以与佛教的无我（Anatta）教义联系起来。这一理念是构成佛教实践的主要形而上学原则之一。该术语指的是人类内部没有构成自我的永久的、根本的物质。对这一教义的确切解释在各宗派、神学家和学者之间存在争议，但这一理念在佛教传统中的核心地位相当于苦（dukkha）和无常（anicca）。总的来说，无我教义断言构成“自我”的只是意识中的现象流，这只是自我的错觉。对这种所谓的“自我”的执着是一种主要的苦难来源，因此是宗教实践中需要克服的主要障碍之一。作为一个角色，Lucatiel体现了当我们拼命抓住我们的存在，抓住所谓的“自我”时产生的痛苦。这赋予黑暗标志之火一种略有不同的意义。它标志着玩家的化身被困在轮回的循环中，永无止境地燃烧。\n薪王，火之继承者 虽然整个《黑暗之魂》系列贯穿着许多佛教主题，但在第三部游戏的结尾，末法的联系变得尤为明显。在第三部游戏中，系列核心概念中的隐喻成熟并凝聚成一种对旧制度无效努力的毁灭性批判。这不仅体现了佛教末法的思想，还展示了一种荒凉的后现代视角，揭示了旨在推动世界前进的传统和循环。在第三部游戏中，末法意识与后现代主义相融合，形成了一种双重视角。相应地，第三部游戏的开场动画比第一部更加末日化：\nそうさね、そこはロスリック／火を継いだ、薪の王たちの故郷が、流れ着く場所さね／だから巡礼者たちは北に向かい／そして、予言の意味を知るのさ／「火は陰り」／「王たちに玉座なし」／継ぎ火が絶えるとき、鐘が響き渡り／古い薪の王たちが、棺より呼び起こされるだろう／深みの聖者、エルドリッチ／ファランの不死队、深淵の监视者たち／そして、罪の都の孤独な王／巨人のヨーム／けれどね／きっと王たちは、玉座を捨てるだろう／そして、火の无き灰たちがやってくる／名もなく、薪にもなれなんだ、呪われた不死／けれど、だからこそ／灰はのこり火を求めるのさね\n\u0026ldquo;Yes, indeed / It is called Lothric / where the transitory lands of the Lords of Cinder converge / In venturing north, the pilgrims discovered the truth of the old words: / \u0026lsquo;The fire fades and the lords go without thrones\u0026rsquo; / When the link of the fire is threatened, the bell tolls / unearthing the old Lords of Cinders from their graves\u0026hellip;Aldrich, Saint of the Deep / Farron\u0026rsquo;s Undead Legion the Abyss Watchers / The reclusive lord of the Profaned Capital, Yhorm the Giant / Only, in truth / the Lords will abandon their thrones / And the Unkindled will rise / Nameless, accursed Undead, unfit even to be cinder / And so it is, that ash seeketh embers.\u0026rdquo;\n与第一部游戏不同，这里没有起源故事，没有时间黎明时的原始场景，没有新旧神之间的战斗，只有一个正在沉入沙土中的破败王国。可怕的“朝圣者”背上绑着巨石，倒在沙地里死去，这里的神已经耗尽腐化，被钟声重新唤醒为薪之王。父神完全缺席，但他在上一部游戏中的头衔依然存在。正如葛温大人在最终BOSS战中被称为薪之王，现在这个称号变成了复数形式。几个王被点名，这也起到了预示作用。这些是需要被打败的敌人，他们拥有巨大的灵魂，将被用来重新点燃火焰。确实，当你击败这些命名实体时，不再像第一部游戏那样在屏幕上出现“YOU DEFEATED”（你打败了）字样，而是在打败一个次要BOSS时会出现“HEIR OF FIRE DESTROYED”（火的继承者被摧毁），最终击败这些命名实体时，则会出现“LORD OF CINDER FALLEN”（薪之王陨落）。\n在第一部游戏中，当玩家第一次遇到葛温大人时，他的生命条上方出现的名字在英文本地化中被称为薪之王，而在日文原版中为薪の王。这里有一个概念问题需要解决，因为游戏将日文术语“takigi”与英文术语“cinder”联系起来，虽然这些术语相关，但它们在重要方面有所不同，复杂了点燃火焰的隐喻。很容易将这一差异视为英文本地化团队对细微差别的草率忽略，但第三部游戏将这一差异加以规范。开场动画中的英文配音和击败BOSS后出现的新信息是游戏日文原版的一部分，因此必须将“cinder”和“takigi”两个词一起解释。\n“Cinder”可以科学地指火山岩，或者在更松散的意义上指炭或煤的余烬。在游戏背景下，“cinder”显然是指后者，因为它支持“点燃火焰”这一核心隐喻。当火焰熄灭时，木炭余烬仍在白灰下燃烧。可以通过揭开这些余烬并在其上放置新的引火物来重新点燃火焰。余烬在火熄灭后可以长时间保持燃烧，并已知会引发森林火灾和其他事故。当我们将其与第一部游戏的事件联系起来时，“cinder”这一隐喻似乎有非常明确的关联。在第一部游戏中，作为“被选中的不死者”，你的任务是找到死去世界灰烬下仍在发光的余烬（四个伟大的灵魂），并用它们重新点燃世界的火焰。另一方面，“takigi”意为火焰的燃料，但特指通过窑炉加工成木炭的木材。\n在这里也许可以找到与日本自身对木炭依赖的历史共鸣。木炭作为燃料的依赖是日本前现代历史上砍伐森林的主要原因。在江户时期，木材的过度依赖达到了危机点，最终导致严重的经济和政治问题。这促使幕府制定了复杂的林业政策和新的造林技术。木炭作为燃料是前现代日本社会的一个重要组成部分，因为它燃烧得更有效率，更容易控制，并且产生更高温的火焰。要制作木炭，必须将木材放在窑中，通过燃烧普通木材来加热窑。这意味着用来制作这种燃料的普通木材消耗成倍增加。木炭的巨大效用是可以获得的回报，因为未经处理的木材燃烧温度不足以冶炼钢铁或其他重要金属。例如，制造东大寺大佛像需要16.3万立方英尺的木炭。因此，木炭的生产具有重要的文明意义。然而，文明越发展，这种生产越无法维持。最终，它会留下荒芜的废地，短缺会给依赖这种燃料的社会带来严重问题。聚集燃料以保持我们这个时代的火焰愈加明亮会耗尽世界的活力，导致灾难的观点在《黑暗之魂》中显然存在。这与我们自己的时代密切相关。这种叙述在当代环境问题（如全球变暖或石油峰值）中很常见，也带有末日色彩。在日本的情况下，能源消耗的双刃性质在福岛事件后关于核能的左右翼激烈辩论中有所体现。\n第二个赋予“takigi”这个术语特殊意义的观点是，为使其成为燃料，必须通过火进行处理。每个游戏就像一个过程，玩家的化身像木炭一样被准备好，通过玩家必须面对的考验，以及点燃人类心灵的欲望之火的过程。从叙述角度看（即你的角色作为“被诅咒的不死者”渴望灵魂），也是如此，从游戏设计的角度看也是如此。实际上，为使游戏作为作品取得成功，它必须通过设计成功激发玩家继续玩的欲望之火。\n只有通过不断发展玩家化身的内在火焰，化身才准备好用作新时代的燃料。将“takigi”解读为余烬，即隐藏在死去时代灰烬下的热余烬，以及经过特殊处理以为新时代创造更强大火焰的木炭，对玩家在整个系列中的行为提供了一种有趣的解释。在每个游戏中，玩家的任务是通过收集隐藏在死去世界灰烬下的余烬（腐化的旧王灵魂），用自己作为特别准备的燃料重新点燃世界。\n在第一部游戏中，我们首次见到葛温大人时，他被介绍为“太阳的王”（太陽の王），但在我们遇到他那令人厌恶的空心状态时，他的头衔已变为薪之王（薪の王）。作为火焰时代的主要创始人，很容易将他代表为火焰的余烬，过去的影子，必须被利用来重新点燃世界。然而，我们也可以将他不愿放弃火焰以及他在巨大窑炉中的长期隔离，视为一种自我枯竭。他的存在在几个世纪里在窑中被准备好，作为新火的催化剂。蛇要求你继承他的王位，如果你这样做，那正是发生的事情。\n你的考验使你准备好作为木炭，作为你收集的所有引火物的催化剂，即你积累的所有灵魂。然而，准备木炭总是意味着为更强大的火焰牺牲大量材料。当我们将“takigi”一词的双重含义解读为余烬（过去）/木炭（未来）时，就出现了一个循环，但这是一个回报递减的循环。每次重新链接火焰都会消耗越来越少的资源，因此每次王国的转世（新名字和新王）都只是一个退化的复制品。如果在第一和第二部游戏中这只是暗示，那么在第三部游戏中这被巩固了。也许第一部游戏仍然可以按照传统西方神话来解读，即通过新的血液复兴旧制度。然而，第三部游戏的结局只能通过东浩纪的后现代主义和末法来理解，即不断减少的循环。\n这种对游戏的解读在我们比较第一部游戏与第三部游戏的最终BOSS战时更加复杂。一旦玩家收集了所有薪之王的头颅并将其放在他们的宝座上，又一个通往最初的火之炉（最初の火の炉）的路径打开了。这是第一部游戏中与葛温对峙的地方，但这个区域看起来更像是千百个过去时代的噩梦废墟，而不是巨大的窑炉。在高高的天空中，一个燃烧的圆圈框住了黑暗的深渊，像是一个地狱般的日食。它向地平线以外的某个地方倾泻下火流。你周围是巨大的城堡，堆积在一起，形成一个庞大的破碎堆，螺旋状地高耸入云。它们似乎是所有前世“王国”化身的废墟：Lothric、Londor、Drangleic，以及无数无名的时代。然而，这次炉中等待的是谁？不像你击败的那些王，它没有名字，只是“薪之王的化身”（王たちの化身）。这里的本地化实际上是与日文原版有很大不同。更合适的翻译应该是“诸王的化身”或“国王的化身”。“化身”一词（keshin）一般意为化身，但在日本佛教传统中也特指佛陀的化现。\n最后的BOSS战与玩家在这个世界中创造的化身并无不同。它也是一个与世界没有任何有意义关系的化身，只是某种外在意志的载体。它只是那些连接和重新连接火焰以维持时代衰退火焰的所有英雄的象征，但却是一个没有实质的怪物。它只是一个空的盔甲，一个连接火焰仪式的傀儡。这通过它拥有的能力和攻击模式得到了加强。游戏中的其他BOSS完全不同于玩家角色。它们的力量、能力和美学完全不同，但诸王的化身使用的技术和能力模仿你的。它的攻击模式反映了玩家在游戏中发展角色时可以采取的不同路径：巫师、敏捷的盗贼、坚定的骑士、混乱的火焰法师等。从这个意义上说，它是“迷宫讽刺”的终极体现，因为它将所有未走的路径、所有可能成为英雄的路径混杂成一个空洞的怪物。\n当这个无面实体、牺牲的英雄的集合体被击败时，你可以像过去的游戏一样选择连接火焰。第一部游戏只有两个不同的结局，现在有四个，每个结局比上一个更加末日。这里没有“好”结局。第一个结局是玩家按照指示连接火焰。就像第一部游戏一样，玩家的化身伸手进入篝火，他的手点燃了。但这次火焰没有蔓延到他的全身，导致一场巨大的火灾，而只是包裹在化身上，像死灰中的余烬一样微弱地闪烁。似乎每次重新链接火焰都会产生越来越弱的火焰，在这一系列的最终篇章中，仪式本身似乎无效。当火焰包裹住玩家的化身时，他沮丧地坐在篝火旁休息。这个火焰甚至没有力量消耗链接它的人。这个结局明显与第一部游戏中看似英雄的“好”结局形成鲜明对比，巩固了游戏与末法意识的联系。这个时代已经太过退化，无法让这个英雄仪式有任何目的或意义。在第三部游戏中，我们已经远远超过了像法（外表的时代），因为仪式甚至看起来都不像。\n其他三个结局在本质上没有不同。在第二个结局中，玩家可以召唤开火者（firekeeper）——在游戏开始时遇到的角色——到火炉中，让她熄灭火焰。在第三个结局中，通过某种晦涩难懂的仪式，玩家将这个黄昏时代固定下来，成为不死之王。这个结局特别难以达成，因为它要求你完成一系列复杂的秘密任务，几乎不可能在没有借助互联网的情况下自己发现。第四个结局特别神秘。它开始时与第二个结局一样，但当开火者去熄灭火焰时，你可以击打她。如果你这样做，一段过场动画会开始，角色的化身将开火者击倒在地，把脚放在她的头上，从她的手中取出燃烧的余烬。当这一切发生时，一直在三部游戏开场动画中担任旁白的老妇人说道：\n名も無く、薪にもなれなんだ、呪われた不死／けれど、だからこそ／灰は残り火を求めるのさね。 \u0026ldquo;名不见经传的被诅咒的不死者，不配称为薪／因此灰烬才会寻求火星\u0026rdquo;\n这句话重复了开场的一句话，其中角色被称为不配称为薪的被诅咒的不死者。这也反映了游戏核心主题的进一步退化，因为你在第一部游戏中被称为“被选中的不死者”。在这种不可理喻的残忍行为之后，你的化身将偷来的余烬举向倾泻的火流，仿佛在献祭。旁白的言辞似乎在暗示，正是因为玩家角色作为“灰烬”的不配，才使他们渴望“火星”。\n正如这一媒介和系列创作者的本性，没有一个结局是标准结局。这与游戏中的后现代意识相吻合，或许也适用于游戏中的普遍情况，因为多结局是这一媒介的常态。最终分析中，无法从游戏本身的背景中得出具体的叙述解释。每个结局都只是暗示性的，而不是决定性的。它的意义几乎完全取决于我们对其少量具体文本元素及其情感特征的感知：它的外观、声音、感觉等。第四个结局中化身对开火者的残忍行为意味着什么？并不清楚。通过手势和整体语气显然表明这些角色之间正在发生某种侵犯行为。在四个结局中，你的重连火焰之旅和消除不死诅咒的目标没有一个结局暗示这一目标已经实现。也许这个游戏世界最具后现代特色的就是这些“选择”及其完全不透明的意义。当然，无法选择任何一个结局作为标准结局，即使我们能做到，其意义也是不可解读的。这个系列唯一具体的方面是一系列情感特征、主题和游戏设计元素，使其可识别为一个系列。这个星座指向未来的无望、过去的罪恶和生活在现在的痛苦。出现的模式是，旨在复兴世界的英雄仪式被揭示为不过是一个空洞的循环，绝望的灵魂紧握着即将熄灭的余烬，进一步混淆了现实的结构。赋予这一仪式最初意义的宏大叙述是如此模糊，以至于已无法被察觉。\n结论：黑暗之魂与后现代意识 在这篇文章中，我论证了《黑暗之魂》系列，特别是第三部作品，是当代日本社会后现代意识的文化反映。大泽和东浩纪等思想家提出的后现代反叙事、极右翼意识形态的复兴（“日本的重振”），以及极端社会退隐现象（“蛰居族”和“草食男子”）等现象，都反映了一种由内部恐怖活动（如奥姆真理教）、出生率下降、福利系统恶化以及基础设施不稳定（如神户、福岛）引发的深层次焦虑。当我们所依赖的基础，无论是物质上的还是社会上的，似乎有消失的风险时，这种后现代意识便会产生。《黑暗之魂》系列也是这种焦虑的体现，它可能是当代焦虑最有力的文化表达。它的巨大成功和作为有史以来最伟大游戏系列之一的地位，尽管其不遵循行业最佳实践，似乎表明其美学和设计触动了众多游戏玩家的共同感受。\n那么，我们如何总结日本的后现代意识，这在许多方面似乎是正在整个后工业化西方蔓延的一种意识？可以说，后现代意识只是一种悲观情绪。这种情绪让人感觉我们的抱负将毫无意义，现代化的项目已经失败，个人采取有意义步骤去实现理想的观念，更不用说整个人类，是一个悲哀的笑话。这是对当代生活在某种程度上不如过去，甚至可能很可悲的恐惧。事实上，社会在某处进步的观念现在被视为一种极不时尚的想法。批判现代化抱负为危险的天真，已经成为几乎所有哲学或文学批评的固定部分。除了努力支持我的解释外，我还希望——即使只是初步地——表明后现代意识本身也是循环的。它只是社会在向陌生的新时期过渡时表现出的不稳定症状。这种悲观情绪，或者说是世界末日的情绪，就像当新社会组织模式在旧秩序下挣扎着出现时的成长痛苦。然而，我还想指出，我所标记为后现代意识的这种悲观情绪，仅仅是一种情绪。它的现实取决于我们在多大程度上愿意沉溺于它。\n有鉴于此，我认真对待大泽和东浩纪对当今日本社会心理或精神问题的诊断。无论现实如何，人们的感觉都能驱使他们走向自毁行为。这对整个社会来说可能会有非常严重的后果，即使这种想法仅仅被少数人接受。奥姆真理教的行动是由一个小团体实施的，但它们仍然设法在日本社会的核心造成了损害。然而，我拒绝后现代的预后——特别是东浩纪的预后——认为流行文化中传统叙事的转变是人们普遍放弃理想的证据，我们注定要退缩到技术享乐主义中，所有人都成为自己癖好的奴隶。\n这只是某些人正在走的一条道路，甚至这一点也是有争议的。与漫画和轻小说等其他流行媒介相比，电子游戏的核心在于要求解决一系列问题。无论这种需求与日常生活多么人为和遥远，它依然是愿意经历困难并克服挑战的表现。在一个极其基本的层面上，愿意经历困难去解决特定问题或克服特定挑战，支撑着所有人类的努力。叙述中的绝望与完成游戏所需的耐心和技巧之间，有一种奇怪的脱节。\n在电子游戏中，《黑暗之魂》系列代表了一组特别具有挑战性的问题。它提出了一个问题：如果没有对你努力的确认，为什么要经历这样一系列艰难的挑战？答案只能是过程本身是值得的，游戏本身的行为是其精神上的滋养来源。然而，玩游戏比单纯地享受乐趣要复杂得多。正如 Aarseth 等人指出的，ergos，也就是说“工作”，是电子游戏媒介的本质。《黑暗之魂》当然是一款将 ergos 和 ludos 紧密结合在一起的游戏。当然，人们玩《黑暗之魂》不是为了工作，以谋生（除了一些创业的 YouTuber）。然而，说玩《黑暗之魂》只是为了乐趣，这抹去了关于游戏体验的某些绝对关键的东西。《黑暗之魂》中所谓的“乐趣”是那种与爬山、跑马拉松或任何需要长时间持续努力的活动联系在一起的：痛苦、困难，只在结束时短暂满足，然后人们又心甘情愿地回到同样的挑战中。我们不称这些活动为乐趣，而是说它们令人满意，这正是克服《黑暗之魂》中的挑战的感觉。\n社会学家 Roger Caillois 认为，游戏是培养文化反射和习惯的熔炉。如果是这样的话，玩像《黑暗之魂》这样的游戏会培养出什么样的文化反射呢？这个问题的答案可能超出了本文的范围，但可以肯定的是，《黑暗之魂》鼓励我们接受挑战，以便我们可以获得与地狱般的壮丽景象同样令人惊叹的回报。这可能是推动玩家前进的最重要的回报：获得某种美丽景象的回报。尽管其黑暗和怪诞，每一部《黑暗之魂》系列作品在视觉呈现上都独具美感。\n《黑暗之魂》是一款极其复杂和综合性的游戏，它汲取了日本的现在和过去，但这一切都是为了表达对当人们开始质疑支撑其社会的主要观念时，自我实现、进步和社区可能性的深切焦虑。游戏玩法、叙事和美学的复杂交织共同形成了一种强烈的后现代意识表达，在这种意识中，角色在一个衰败的世界里独自面对充满敌意的他者，处于一个他们无法理解的情境中。游戏中包含的佛教形而上学进一步暗示，通过进行祭祀和复兴的仪式来纠正这种情况的追求，最终是毫无意义甚至腐败的。在这层意义上，《黑暗之魂》直接触及了后现代主义的巨大恐惧：无论我们如何努力避免湮灭，最终，这可能超出了我们的掌控。\n然而，游戏中也反映出一种尼采式的自我超越承诺——通过激烈的斗争实现自我超越。我认真对待后现代的诊断，即传统社会叙事的恶化造成了令人不安的自毁行为模式，但我不同意他们的预后。我也不同意他们关于历史独特性甚至传统信仰、信条和叙事不能更新或演变以满足文化需求的说法。我认为日本以前也经历过这样的情况。后现代意识可能只是一种精神压力——尽管是严重的——人们在社会经历重大结构转变和旧结构的叙事失去连贯性时经历的。它注定会在新的形式定型时循环回到一种乐观的情绪中。事物很少长时间保持不变——无论好坏——事实上，无论我们身处何地，没有人真正知道接下来会发生什么。这就是未来的运作方式。它是未知的，只有意识形态的教条主义才试图将其固定在某个位置，不变得像过去一样。我们必须有勇气对接下来发生的事情保持开放态度。\n夏目漱石。日本近代文学巨匠，被誉为“日本现代文学之父”，其作品探讨了人与社会、个人内心世界等深刻主题，代表作有《我是猫》、《心》、《明暗》等。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n三岛由纪夫。日本著名作家、剧作家和政治活动家，以其充满美学与哲学思考的作品著称，代表作有《金阁寺》、《春雪》、《海与毒药》等。他的作品往往探讨人性、荣誉、死亡等主题。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n或翻译成“新的媒介在大学里常常遇到困难，因为每一种媒介都必须通过那些把关“高雅文化”的权威人士的审查。”\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n沟口健二（1898-1956）是日本著名电影导演，以精湛的拍摄技巧和深刻的社会洞察力闻名。其代表作包括《雨月物语》和《山椒大夫》，他擅长于通过细腻的叙事手法展现人性的复杂。沟口被视为日本电影史上的大师之一，对后世电影影响深远。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n指代一种源自捷克作家弗朗茨·卡夫卡（Franz Kafka）作品风格的情境，通常描述一种充满荒诞、绝望和无助感的氛围。在这种情境中，个体往往陷入与无情且不可理解的官僚体系或命运的斗争中，感受到无力改变自身处境的痛苦和迷茫感。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n大泽真幸 Masachi Ōsawa（1958-）是日本当代著名的社会学家、哲学家和评论家，专注于社会理论、文化批评和历史哲学。研究焦点包括现代日本社会的结构性问题及后现代思想在日本的应用。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n东浩纪 Hiroki Azuma（1971-），日本文化评论家和哲学家，专注于后现代主义、信息社会理论及日本流行文化的研究。他的代表作《动物化的后现代》探讨了当代日本动漫、游戏等亚文化现象，并提出了“数据库消费”等独特概念。东浩纪在日本思想界和文化评论领域具有重要影响力，尤其在分析现代社会和文化变化方面贡献卓著。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n田中真纪子（Makiko Tanaka），日本文学学者，研究方向包括现代日本文学及文化，她的研究涉及后现代思想对日本文学的影响。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n村上春树（Murakami Haruki）是日本著名小说家，以其独特的风格和对梦幻与现实交织的描绘而闻名。他的代表作包括《挪威的森林》、《海边的卡夫卡》和《1Q84》，这些作品探讨了孤独、爱情、以及人类存在的意义。村上春树的作品广受国际读者喜爱，并对全球文学产生了深远影响。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n弗朗索瓦·利奥塔 Francois Lyotard（1924-1998），法国哲学家，以其对后现代主义的研究著称。他在1979年出版的《后现代状况》一书中首次提出了“后现代”一词，主张大叙事（如宗教、政治意识形态）的破裂和小叙事的兴起。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n奥姆真理教 日本的极端宗教团体，创立于1984年，因其在1995年东京地铁沙林毒气袭击事件中的恐怖行为而广为人知。该教派结合了多种宗教和神秘主义信仰，曾声称能够提供终极真理。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n联合赤军事件（日本语：連合赤軍事件）是指1971年至1972年间，由日本极左激进组织联合赤军（連合赤軍）发动的一系列暴力事件。联合赤军由“赤军派”和“京滨安保共斗”两派极端左翼组织合并而成，其成员信奉极端左翼思想，试图通过暴力推翻政府。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEspen Aarseth 挪威学者，游戏研究领域的开创者之一，以其对电子游戏叙事和游戏性分析的贡献著称。著有《Cybertext: Perspectives on Ergodic Literature》（《赛博文本：互动文学视角》），提出了“互动文学”的概念，强调玩家在游戏中的互动和参与对于理解游戏文本的重要性。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n佛教末法概念 佛教教义中的一个概念，指佛法逐渐衰微的时代，教义的力量减弱，人们无法通过修行获得解脱。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n“安倍经济学”是指日本首相安倍晋三（Shinzō Abe）自2012年开始推行的一系列经济政策。这些政策包括所谓的“三支箭”策略：大胆的货币政策、灵活的财政政策和促进投资的增长战略。安倍经济学的目标是通过刺激经济增长、提高通货膨胀率来摆脱长期的通货紧缩和经济停滞状态。这些政策在日本和国际上都引起了广泛关注和讨论，但也引发了对其效果和可持续性的质疑。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n草食男子（日语：草食(そうしょく)系男子(けいだんし)，sōshoku danshi）是日本的一种社会现象，指那些对事业、恋爱和结婚不感兴趣，生活态度温和、不争、不抢的男性。这个词最早由日本作家深泽真纪在2006年提出，反映了日本社会中一些年轻男性的生活态度和行为模式的变化。这些男性通常不追求传统意义上的男性气概和社会成就，反而更注重个人兴趣和生活质量。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n尼特族 （NEET，Not in Education, Employment, or Training）是指那些既不在学、也不在职、也不接受职业培训的年轻人。这个词源自英国，但在日本也广泛使用。尼特族的出现反映了现代社会中青年失业率高、就业压力大以及社会结构变化等问题。这一群体在日本引起了社会关注，因为他们的经济依赖性和社会疏离感对家庭和社会结构带来了挑战。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n隐蔽青年 （日语：引(ひ)き籠(こも)り，hikikomori）是指那些因社会压力、精神疾病或其他原因而长期闭门不出、与社会隔绝的年轻人。这种现象在日本尤为突出，被认为是社会压力、竞争激烈以及家庭结构变化等多重因素共同作用的结果。隐蔽青年通常在家中长时间不出门，不上学、不工作，依赖父母或家庭成员的支持。这个现象不仅影响到个人的心理健康和生活质量，也对家庭和社会产生了深远的影响。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n見田宗介 是日本著名社会学家，曾任东京大学名誉教授，是日本社会学界的重要人物之一。他的研究领域广泛，涵盖现代社会理论、文化社会学以及日本社会的独特发展轨迹。峰崎的代表作《现代社会的理论》（『現代社会の理論』）对日本社会现代性的转型和结构做了深刻分析，被认为是社会学研究的重要文献。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n约翰·多尔 是一位美国历史学家和作家，以研究日本历史和美日关系闻名。他的著作《拥抱失败：日本在美国的阴影下》（Embracing Defeat: Japan in the Wake of World War II）通过详细的历史分析和丰富的文献资料，探讨了二战后日本社会的变化和重建过程。多尔的研究不仅涉及政治和经济层面，还深入探讨了社会文化的转变，特别是日本人对美国模式的接受和适应。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n泛泛女孩（panpan girls）是指二战后日本被美国占领时期，一些日本女性与驻日美军士兵交往的现象。这些女性通常通过与美军士兵建立关系，获得他们带来的物资和经济利益。泛泛女孩现象反映了战后日本社会的贫困与物质匮乏，以及人们对物质富裕和更好生活的渴望。二战后，日本遭受了巨大的物质和经济损失，许多人生活在极度贫困之中。占领日本的美军士兵带来了大量的物资和财富，使得美军的生活条件远优于普通日本人的生活水平。泛泛女孩们通过与美军士兵的交往，获得了食品、酒类、香烟、糖果、化妆品等在当时极为稀缺的物品。泛泛女孩现象不仅仅是一个物质交换的过程，它还反映了当时日本社会对美国文化和生活方式的向往。对许多日本人来说，美国的富裕和舒适的生活条件成为他们追求的目标，这种对美国模式的向往也在一定程度上推动了战后日本的现代化和经济复苏。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n貧病争 ひんびょうそう是一个源自佛教的概念，指的是“世间的烦恼”，具体包括贫穷、疾病和争斗。这三个方面代表了人类在世俗生活中不可避免的痛苦和困扰。以下是对每个方面的解释: 贫穷（ひん），指物质上的匮乏和经济上的贫困，贫穷会导致人们为了生计而不断奋斗，成为生活中的一大烦恼；疾病（びょう），指身体上的疾病和健康问题，疾病的存在使得人们不断追求健康和医治；争斗（そう），指人际之间的冲突和社会中的斗争，这包括个人之间的争执、家庭内部的冲突、社会中的暴力和战争等，争斗使得人们生活在不安和紧张的环境中。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n新人类（しんじんるい）是20世纪80年代日本的一代年轻人，他们在思想、行为和文化上与之前的世代有显著的不同。这一群体的出现标志着日本社会的深刻转型，影响了当时的社会文化和经济发展。通过对新人类的研究，可以更好地理解日本意识形态在20世纪80年代的变化和发展。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n御宅族 通常用于描述对某一特定兴趣（如动漫、漫画、电子游戏等）有着极端热情和深入了解的人群。这个词汇最早出现在20世纪80年代，最初带有一定的负面意味，指那些沉迷于兴趣爱好、常常在家里闭门不出、不太参与社会活动的年轻人。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n全学联合（日语：全学連，Zengakuren）是“全日本学生自治会总联合”（全日本学生自治会総連合）的简称，成立于1948年，是日本历史上重要的学生运动组织。全学联合主要由日本各大学的学生自治会组成，代表学生在政治、社会问题上发声。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n田中元子 是一位研究日本战后政治运动的学者，特别关注20世纪60年代的学生运动，如安保运动（Anpo）和全共斗运动（Zenkyoto）。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n全共斗运动 （全学共闘会議，Zenkyōtō）是20世纪60年代末至70年代初，日本大学生和左翼组织发起的激进学生运动，旨在反对政府的教育政策、经济措施以及与美国的安保条约。这个运动的特点是学生自发组织、没有固定的领导层，通过激烈的抗议和占领校园来表达政治诉求。全共斗的行动在日本社会引起了广泛关注，成为那个时代反抗精神的象征。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n联合赤军事件（Rengō Akagun jiken）是指1972年日本联合赤军组织进行的一系列激进和暴力事件。联合赤军是由极左翼的日本学生运动组织合并而成的激进组织，他们在长野县浅间山庄劫持了人质，导致与警方的对峙长达10天。最终，日本警方采取了强攻行动，事件导致人质获救、部分赤军成员被捕或死亡，引发了日本社会对极端左翼运动的广泛批判。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n通过博尔赫斯的《科学的准确性》作为寓言，他断言地图作为表征取代了领土，最终成为需要创建新地图的领土本身： 来自于阿根廷作家豪尔赫·路易斯·博尔赫斯（Jorge Luis Borges）的一篇短篇小说《精确的地图》（也译作《科学的准确性》）。在这个故事中，博尔赫斯描绘了一个极端的情况：地图变得如此精确，以至于它与现实世界（领土）完全重叠，最后，地图的细节变得如此复杂，以至于它不再是现实世界的象征，而是成为了一个替代现实的东西。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n弗朗索瓦·李奥塔（François Lyotard，1924-1998）是法国著名的哲学家和文学批评家，以其在后现代主义领域的开创性研究而闻名。他的代表性著作《后现代状况》（La Condition Postmoderne，1979年）对20世纪末的思想产生了深远影响。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n山谷是东京台东区和荒川区附近的一个地区，以前被认为是日工、无家可归者和社会底层人员聚集的地方。在这个地区，贫困和社会问题较为突出。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n釜崎是大阪市西成区的一个地区，类似于东京的山谷，这里也有大量的日工和无家可归者。如今，这个区域的正式地名为”爱邻地区”（あいりんちく，Airin Chiku），但釜崎的称呼仍然流行。这一地区长期以来是社会底层人员的聚集地，经济落后，社会问题严重。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n人民圣殿教是美国的一个新宗教团体，由吉姆·琼斯（Jim Jones）于1950年代创立。该教团以其强烈的社会平等和共产主义色彩著称，特别强调种族平等与社会正义。然而，这一组织在1978年因乔恩斯镇大屠杀（Jonestown Massacre）而闻名。当时，吉姆·琼斯带领900多名信徒在南美圭亚那的乔恩斯镇集体自杀，成为现代宗教历史上最惨烈的悲剧之一。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n天堂之门是一个美国的新宗教团体，由马歇尔·阿普尔怀特（Marshall Applewhite）和邦妮·内托尔斯（Bonnie Nettles）于1970年代创立。这个教团以其独特的末世论思想和对外星生命的信仰而著称。教义核心是认为地球将被净化，信徒的灵魂需要通过离开身体（即自杀）以进入一艘外星飞船，从而到达更高的存在层次。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n**净土宗：**净土宗是日本佛教的一大宗派，起源于中国的净土思想，传入日本后由法然（1133-1212）创立。净土宗的核心教义是通过专一称念“南无阿弥陀佛”来获得往生西方极乐净土的保证。净土宗的实践重点在于简化修行，通过称念佛号（念仏），普通人也能获得解脱，而不必追求复杂的禅修或其他高深的佛教修行。**日莲宗：**日莲宗由日莲（1222-1282）在日本创立，主张《法华经》是释迦牟尼佛教法的究竟表达，因此对末法时代的唯一正法是称念“南无妙法莲华经”。日莲宗带有强烈的社会关怀意识，认为末法时代的混乱与个人、社会的正法失落有关，因此通过弘扬法华经，信众不仅能拯救自己，还能影响社会，带来和平与繁荣。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n法然（ほうねん，1133–1212）：日本净土宗的创始人，法然从天台宗的修行体系中转向单一的念佛法门，提倡通过专念“南无阿弥陀佛”来获得往生净土；道元（どうげん，1200–1253）：日本曹洞宗的创始人，他从天台宗的基础出发，前往中国学习禅宗，并将其带回日本。他的教义强调只管打坐这一直接的禅修实践；荣西（えいさい，1141–1215）：日本临济宗禅宗的创始人，他同样来自天台宗的传统，后来前往中国，学习禅宗并带回日本。他在日本还推广了茶道，被称为日本禅茶文化的先驱；日莲（にちれん，1222–1282）：日莲宗的创始人，日莲主张《法华经》是佛法的究竟表达，强调通过唱念“南无妙法莲华经”来获得救度，并积极参与社会、政治事务，呼吁社会改革；亲鸾（しんらん，1173–1263）：净土真宗的创始人，法然的弟子，他进一步发展了法然的念佛思想，主张即使是最普通的人通过称念阿弥陀佛也能获得救度。他强调阿弥陀佛的救度是不依赖个人修行的。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://niraya666.github.io/essay/%E5%90%91%E4%B8%8B%E8%9E%BA%E6%97%8B%E9%BB%91%E6%9A%97%E4%B9%8B%E9%AD%82%E7%B3%BB%E5%88%97%E6%B8%B8%E6%88%8F%E4%B8%AD%E7%9A%84%E5%90%8E%E7%8E%B0%E4%BB%A3%E6%84%8F%E8%AF%86%E4%B8%8E%E4%BD%9B%E6%95%99%E5%BD%A2%E8%80%8C%E4%B8%8A%E5%AD%A6/","summary":"\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cp\u003e本文探讨了“黑暗之魂”系列游戏在当代日本社会中的意义定位。我认为，这款游戏可以被看作是当今文化潮流的象征，就像有人可能会把杰克·凯鲁亚克的《达摩流浪者》视为60年代反文化的象征一样。\u003cstrong\u003e我主张，“黑暗之魂”系列通过寓言的方式表达了人们在一个日常行为和社会本身的意义都变得极不稳定的时代中生活的焦虑\u003c/strong\u003e。游戏采用碎片化的叙事方式，融合了佛教的形而上学思想，并以描绘一个曾经辉煌的王国在最后时刻挣扎的阴森哥特式美学为背景。这种对当代社会焦虑的表达，与日本的后现代主义话语密切相关。通过将这些游戏视为文本、环境和游戏系统之间的反馈回路，我把这些游戏的主要概念主题与大泽真幸提出的“后虚构时代”概念，以及东浩纪对御宅族的定义联系起来。\u003c/p\u003e\n\u003ch1 id=\"前言\"\u003e前言\u003c/h1\u003e\n\u003cp\u003e在人文学科这个领域，我们认为艺术和文学不仅仅是艺术鉴赏。当我们阅读夏目漱石\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e或三岛由纪夫\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e的作品时，教授和学生不仅仅像粉丝那样陶醉于他们优美的文笔，而是作为评论家，从中挑出深刻的思想。通过这种方式阅读足够多的作品，我们会发现一种更宏大的东西：\u003cstrong\u003e一个跨越时代和文化的思想洪流，在这个洪流中，生活的神秘、美丽、恐怖和悲剧汇聚成我们人类经验的漩涡和流动\u003c/strong\u003e。文学是指南，就像老话说的那样，是通向美好生活的指南：它是一面镜子，反映我们的位置，也是我们漫步时的地图。简而言之，伟大的作品让我们变得更强大。它们让我们清晰地面对生活的矛盾，并有勇气承受其重压。我们学习文学是为了找到人类经验中的共性，并将其内化。这至少是人文学科的理想和创立原则。\u003c/p\u003e\n\u003cp\u003e但如果这是正确的，我们能说这种深刻的体验只是文学独有的吗？书面文字是否有某种独特的力量，能够以其他媒介无法企及的方式激发反思？最终，文学只是一个个故事，但故事并不局限于一种媒介。故事是跨媒介的。它们可以口头讲述，可以印刷，可以通过舞蹈、图像，甚至可能通过尚未构想的方式来讲述。这是因为故事首先存在于人类心灵的深层结构中，不同的媒介让这些结构的不同方面显现出来。\u003cstrong\u003e故事是人类生活的核心，它们赋予我们的社会和个人生活以连贯性和意义。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e故事是个人身份的基石，人类将经验转化为故事的能力可能与我们感知时间的能力有关。\u003cstrong\u003e毕竟，故事是什么？它是以有意义的方式组织和编辑时间\u003c/strong\u003e。简而言之，通过研究故事，特别是那些在社会想象中占据重要地位的故事，我们可以进入人类心灵的最深处。当我们接近这一神秘领域时，我们更接近自己，也许会找到一些我们遗忘的，对我们至关重要的东西，它像火药桶上的火花一样在我们脚下点燃，在这场大火中我们的世界观重生。\u003c/p\u003e\n\u003cp\u003e新的媒介在大学里常常面临艰难，因为每种媒介都必须越过高文化裁决者的障碍\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。这在二十世纪初的电影和摄影上是如此，在今天的漫画、电视和视频游戏上也是如此。但如果我们仅因为这些作品属于错误的媒介而忽视了沟口健二\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e的作品，人文学科会变得多么贫乏？偏重研究一种媒介而忽视其他媒介有什么好处？这种偏见是最糟糕的，我们都因这种武断的判断而变得贫乏。我们切断了自己探索那些激发我们深层结构的新视角的机会。故事在新的媒介中同样存在，此外，对那些在这些新媒介中成长的人来说，发现其中的故事是引人入胜、动人且深具意义的。视频游戏中的故事体验与其他媒介有着根本的不同，正是这种不同使得研究它们变得如此重要。它反映了一种新的世界观和价值观。我们不再生活在一个技术只是工具的世界里；技术是我们生活的环境。电影和电视是工业时代的本土媒介，而视频游戏是数字时代的本土媒介。视频游戏不是小众的。这个行业在全球产生的收入大大超过了电影和印刷品。这本身就使得研究它们成为紧迫的事情。\u003c/p\u003e\n\u003cp\u003e话虽如此，我们必须认识到，视频游戏是一种新兴的媒介。就像早期的电影只是基本现象的再现——火车向屏幕疾驰或马在慢动作中奔跑——视频游戏的媒介中无疑有着尚未开发的巨大潜力。此外，我们还不能像理解文学那样全面理解这一媒介。我们还没有足够的时间去理解视频游戏在文化中可能扮演的角色以及它们对塑造我们的价值观和性格的影响。我不会在这里主张创建一个新的经典目录。现在还为时过早，而且流行经典化的过程早已在进行中。本文研究的游戏《黑暗之魂》系列已经被这样经典化了。它享有一种地位，其他同类游戏以它为标准，而那些共享其设计原则的游戏被称为“魂类游戏（Soul-like）”，这可能类似于许多文学或电影作品被描述为“卡夫卡式的”\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e。然而，我不会试图将这款游戏作为一部伟大作品来评估，以便将研究这一系列游戏制度化。我认为这些游戏被经典化不仅仅是因为它们“有趣”，而是因为它们直接处理了当代日本及其年轻一代面临的问题、焦虑和挑战。这是我们将要问的问题：\u003cstrong\u003e这些游戏中反映了什么，使其在跨文化范围内以及在粉丝和评论家中产生如此深刻的共鸣？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e我将在这里主要讨论日本，但我相信在研究这款游戏时得出的教训具有全球意义。目前，这些意义将保持原样。本质上，为了故事而玩游戏是一件矛盾的事情。游戏的内部组件，作为一个具象化的代码系统和作为不同类型叙事（文本、视觉、听觉）的载体，对玩家提出了一系列相互竞争的需求。这种矛盾在《黑暗之魂》中和几乎所有其他游戏中一样没有解决，但该系列在这些相互竞争的元素之间达到了罕见的一致性，可能直接反映了日常生活中数字世界的体验。我希望我在这里的分析，即使只是初步和不完整的，也能成为反思社会走向、新的价值结构的出现以及文学解释或在这种情况下的游戏解释可以对这些问题有所启示的机会。\u003c/p\u003e\n\u003ch1 id=\"第一章引言\"\u003e\u003cstrong\u003e第一章：引言\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003e本文旨在探讨“黑暗之魂”系列游戏与当代日本社会状况之间的关系。我认为，“黑暗之魂”系列通过寓言形式表达了在一个日常行为意义甚至社会本身都变得不稳定的时代生活的焦虑。这一表达通过破碎的叙事方式实现，这种方式结合了佛教形而上学，并以描绘一个昔日伟大王国最后挣扎的阴暗哥特风格为背景。这种寓言中的焦虑与日本后现代性讨论密切相关，特别是现代性项目失败和进步叙事腐败的观点。在我的论文中，我认为这种讨论在“黑暗之魂”中通过模拟经典英雄之旅得以体现，但最终通过剥夺玩家的英雄成就感并用深深的模糊和不确定性取而代之，颠覆了这一旅程。\u003c/p\u003e\n\u003cp\u003e“黑暗之魂”系列是一款第三人称动作RPG，其核心机制围绕探索一个破败的世界展开。该系列由著名日本游戏公司FromSoftware在总裁宫崎英高的指导下开发。第一款游戏于2011年9月发布，第三款于2016年4月发布。这是一款难度极高的游戏，设定在一个末世奇幻世界，要求玩家克服许多复杂的运动任务。游戏的一大特点是其缺乏具体的叙事阐述。然而，有一些概念性主题将这些游戏联系在一起。游戏中总是有一个被“不死诅咒”摧毁的王国，并且总是有神秘人物指示玩家杀死旧神并使用他们的灵魂来“连接火焰”。这个过程在第一款游戏中被解读为一个世界复兴过程，但在第二和第三款游戏中被揭示为不可避免的衰败螺旋。\u003c/p\u003e\n\u003cp\u003e全球销量数百万，三部作品在国外几乎普遍获得好评，赢得了众多“年度游戏”称号和大量其他行业奖项。它对当代游戏设计产生了深远的影响，许多采用类似设计的游戏经常被称为“魂类”或“魂风格”。这已成为一种陈词滥调，类似于带有超现实或存在主义色彩的文学作品被描述为“卡夫卡式”。我认为其地位的原因并非游戏的“有趣”，而是它有一些关于我们今天所处世界的强有力表达，这些表达与那些对现代性的后现代批评观点一致的人产生共鸣。\u003c/p\u003e\n\u003cp\u003e本文的核心论点是，“黑暗之魂”系列体现了具有后现代意识的生活体验。这个术语“后现代意识”是我用来总结后现代思想的一个总称。然而，我特别关注日本分支，引用了社会学家大泽真幸\u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e、文化评论家东浩纪\u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e、文学学者田中真纪子\u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e和小说家村上春树\u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e的著作。\u003cstrong\u003e从弗朗索瓦·利奥塔\u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e（Francois Lyotard）开始——他首次提出“后现代”一词——后现代思想的不同分支的核心主张是，曾经用来指导社会的“大叙事”已经无可救药地破裂并且无法修复\u003c/strong\u003e。大泽和其他思想家拾起了这条思路，并将其应用到具体的日本背景。他们认为，日本战后时期的大叙事的破裂在年轻一代中引发了虚无主义行为，这种行为\u003cu\u003e表现为从政治原因和社区参与转向过度消费主义和一种表现为御宅族形象的媒体崇拜\u003c/u\u003e。在一些极端情况下，这种御宅族主义可能导致病态行为，如奥姆真理教\u003csup id=\"fnref:11\"\u003e\u003ca href=\"#fn:11\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e11\u003c/a\u003e\u003c/sup\u003e的恐怖主义，但需要注意的是，这些思想家将这种暴力行为视为御宅族行为连续体的最外端，而不是在类别上有所不同。从大泽和东浩纪等后现代思想家那里，\u003cstrong\u003e我将后现代意识提炼为两个基本特征：1. 拒绝或无法接受宏大叙事和意识形态作为意义的来源 2. 对“他者”的日益焦虑和敌意\u003c/strong\u003e。在这种情况下，他者指的是主体与他者之间的经典哲学二元关系；换句话说，就是自我与世界上所有其他自我之间的关系。\u003cu\u003e大叙事的破裂削弱了依赖这些叙事来维系和稳定的社区纽带和同胞感\u003c/u\u003e。这种意识是“黑暗之魂”系列设计和叙事的主要当代文化元素。\u003c/p\u003e\n\u003cp\u003e然而，后现代意识在日本确实广泛传播，这并不是显而易见的。出于这个原因，我在第一章中追溯了大叙事在日本历史中逐渐瓦解的确切方式，从战后时期开始一直到现在。这使得我们能够看到后现代意识的发展及其在日本社会和“黑暗之魂”系列中的表达。没有这一步骤，就不可能识别贯穿这个系列的后现代意识。\u003c/p\u003e\n\u003cp\u003e为了挖掘日本后现代意识的文化根源，我采用了大泽的文化时代精神理论，\u003cstrong\u003e将战后时期分为三个阶段：意识形态时代（1945 - 1972），虚构时代（1972 - 1995）和后虚构时代（1995 - 现在）\u003c/strong\u003e。大泽在其作品中提出的基本观点是，意识形态的死亡——本质上是他对“宏大叙事”的术语——刺激了新的形式，这些形式最终使他们脱离了日本战后社会的意识形态基础。从1972年的联合赤军事件\u003csup id=\"fnref:12\"\u003e\u003ca href=\"#fn:12\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e12\u003c/a\u003e\u003c/sup\u003e开始，激进的左翼政治让位于非政治的消费享乐主义。这导致了围绕显著消费形式尤其是时尚的亚文化的繁荣。然而，对一些人来说，对更深层意义的渴望仍然存在，这最终催生了新时代的邪教，即奥姆真理教，它们同样从传统宗教和动漫中汲取力量来满足这种需求。\u003c/p\u003e\n\u003cp\u003e奥姆真理教的重要性在于他们既标志着传统信仰和意识形态的弱化，也标志着对任何被他们视为“他者”的事物的日益不容忍。他们还标志着一种对远在天边但又近在咫尺的敌对他者的偏执。例如，他们认为日本政府被全球犹太共济会阴谋控制，而他们自己的莲花团体则充满了双重间谍。他们在1995年对这个被认为是犹太人的他者的恐怖袭击，具有讽刺意味的是，传播了这种对奥姆真理教妖魔的偏执，这些妖魔潜伏在学校、企业甚至政府中。这种怪异他者的概念在“黑暗之魂”中得到了有力的表达，因为它向玩家展示了一个充满敌对个体和群体的世界——这些实体是玩家角色的他者——他们的目标和实践是难以理解的。这也通过一种系统得到了加强，即玩家始终连接到互联网，并且开放给其他玩家入侵。\u003c/p\u003e\n\u003cp\u003e东浩纪采纳了大泽的时代精神，并提出了一种“动物化”理论。他指出，由于后现代时代缺乏大叙事，人们失去了理想和抱负。\u003cstrong\u003e因此，他们退缩到一个以收集符号为中心的世界，这些符号被添加到一个巨大的符号数据库中并被重新利用\u003c/strong\u003e。 他的主要证据是，当今的御宅族不再对叙事感兴趣。相反，他们只是享受识别、重新混合和扩展数据库的组件。他的预测是，御宅族的活动——他们无休止地收集符号——是大文化的先锋，后续几代人将在自己的领域中变得越来越像御宅族。东浩纪所指出的这一流行文化趋势在“黑暗之魂”中体现为深深的模糊和稀疏的叙事。\u003c/p\u003e\n\u003cp\u003e那么“黑暗之魂”究竟如何通过游戏玩法表达后现代意识呢？我认为它通过为我们提供一个任务和一个王国来模拟传统的英雄之旅，但剥夺了我们理解这个任务意义的任何连贯方式。游戏提供了一个视觉细节丰富的世界，但完全缺乏具体的阐述，将玩家置于考古学家和法医科学家的角色中。然而，从未有任何可靠的解释；只有更多的线索和片段，有些是矛盾的。这被称为“环境叙事”，它创造了游戏与后现代思想之间最关键的接触点；即它对主叙事的拒绝。有“拯救王国”的建议，但仅仅是建议，主要是通过类型的惯例传达的，而不是叙事。你所做事情的意义及其服务的目的从未完全清楚。\u003c/p\u003e\n\u003cp\u003e重要的是要记住，虽然可以从传统的文学解释视角来解释“黑暗之魂”系列的叙事，但当试图将游戏机制解释为寓意时，会遇到许多实际和理论问题。出于这个原因，我在第二章中对当代游戏研究的批判理论进行了文献综述。利用Espen Aarseth\u003csup id=\"fnref:13\"\u003e\u003ca href=\"#fn:13\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e13\u003c/a\u003e\u003c/sup\u003e和其他奠定该领域基础的思想家的论点，我认为需要结合“第一人称”现象学方法和更传统的文学分析方法，才能正确评估视频游戏中潜在的潜在意义。这样做是为了建立一种分析方式，通过与游戏系统的斗争，让玩家体验到我上面概述的后现代意识的两个基本特征。我在第三章中将这种现象学方法付诸实践，探索了一种特别有效的文本叙事、环境叙事和游戏玩法的融合，捕捉到游戏作为一个统一体验的本质。\u003c/p\u003e\n\u003cp\u003e除了后现代主义，还有另一股影响游戏的力量，那就是佛教末法概念\u003csup id=\"fnref:14\"\u003e\u003ca href=\"#fn:14\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e14\u003c/a\u003e\u003c/sup\u003e。这两种影响力在有趣的方式上重叠，并产生了惊人的相似之处。末法在镰仓时代是一个极具影响力的概念，正如我们的时代一样，那是一个深刻的社会转型时期。末法指的是佛教教义失去其效力，人们无法再依靠自己的努力来获得启示。\u003cstrong\u003e随之而来的是僧侣和寺庙变得不可否认的腐败，人们因此无法离开轮回，被困在一系列日益退化的转世中\u003c/strong\u003e。我在第四章中分析了末法的哲学核心，并展示了它如何与大泽和东浩纪的后现代话语一致。此外，我认为，只要“黑暗之魂”系列表达了后现代意识，它主要是通过采用这个佛教末世论概念来表达的。\u003cstrong\u003e后现代思想和末法都表达了一种恐惧，即事情只会变得更糟，人类的努力无法改变结果\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e在第五章中，我回到现象学和文学分析的混合方法，展示末法概念以及佛教形而上学的混合如何作为构建游戏虚拟世界的核心要素，尽管游戏的美学显然源于中世纪的欧洲奇幻调色板。这些佛教元素共同揭示了游戏世界被困在一个日益退化的轮回中，玩家的任务基本上是一种徒劳的行为，只会推动这个痛苦的循环。我认为佛教形而上学的融合最终表达了东浩纪的结论，即后现代世界及其死去的叙事和怪异他者无可避免地导致社会整体的“动物化”。尽管“黑暗之魂”是一款美学显然源于中世纪欧洲奇幻风格的游戏，但它充满了末法的逻辑，因此游戏深受异教影响，通过采用和改造佛教形而上学，在阴暗的骑士在破败的城堡中冒险的表面下表达了一种后现代情感。\u003c/p\u003e\n\u003ch2 id=\"衰败叙事\"\u003e\u003cstrong\u003e衰败叙事\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e过去几十年，日本几乎成了停滞的代名词。这在很大程度上归因于日本在1970年代到1980年代经历的“奇迹”，以及他们令人震惊的衰落，自那以后，国家似乎一直处于无所事事的状态。在美国，当《纽约时报》或《华尔街日报》等出版物提到日本时，往往很少有积极的报道，而是集中在一小部分话题：人口老龄化，年轻人自杀；货币贬值，索尼、丰田和东芝等企业巨头被更精简的竞争对手击败；核燃料仍在福岛渗入地下水，但无人承担责任，等等，每一个预言都比前一个更严峻。在西方，日本作为一个正在衰败的帝国的叙述已经流传已久。这种衰败的印象如此强烈，似乎只有惯性才能维持一切运作，以至于人们忘记了日本仍然是世界上最富有、最富裕和技术最先进的国家之一，仍然领先于许多西欧国家。这种停滞的叙述也存在于日本。正如现任首相安倍晋三在2012年选举中的口号所示：“日本を取り戻す”（取回日本）。从谁那里取回？取回到哪里？凭借他的“安倍经济学”\u003csup id=\"fnref:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e议程中的“三支箭”，他将从日本自己的“萎缩心态”中拯救日本（Sharp）。\u003cstrong\u003e如果说1980年代的代表是那些狂热工作、渴望企业晋升的上班族，那么近年来这种原型被更为无力的角色所取代：草食男子\u003c/strong\u003e\u003csup id=\"fnref:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e16\u003c/a\u003e\u003c/sup\u003e\u003cstrong\u003e（sōshoku danshi）、尼特族\u003c/strong\u003e\u003csup id=\"fnref:17\"\u003e\u003ca href=\"#fn:17\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e17\u003c/a\u003e\u003c/sup\u003e\u003cstrong\u003e（NEET，指不在职或不在学的年轻人）、隐蔽青年\u003c/strong\u003e\u003csup id=\"fnref:18\"\u003e\u003ca href=\"#fn:18\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e18\u003c/a\u003e\u003c/sup\u003e\u003cstrong\u003e（hikikomori，指社会隐士）和其他贬损的称呼\u003c/strong\u003e。诚然，日本的经济和文化问题与其他任何国家一样真实存在，但这种叙述的力量已经超越了现实，成为一种独立的存在。\u003c/p\u003e\n\u003cp\u003e这种“萎缩”的心态和随之而来的文化表达，我将称之为后现代意识。这种意识必须与正统的后现代哲学区分开来，因为哲学本身只是对围绕日本的整体衰败叙述的众多情感反应之一。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e后现代意识的核心是一种源于对人类社会已经达到不可否认的衰退点的感知——无论是有意识还是无意识的——的焦虑\u003c/strong\u003e。鉴于上面概述的围绕日本经济和文化衰退的广泛文化讨论，后现代意识在日本文化心理中占据了中心地位。因此，这些源自后现代意识的停滞叙述在日本的流行媒体中也找到了强大而多样的表达。虽然有许多例子可以引用，但游戏《黑暗之魂》及其所有续集，是这种“萎缩”心态的卓越典范。要理解它如何以这种方式运作，我们不仅要看叙述，还要看《黑暗之魂》相对于同类游戏的存在。《黑暗之魂》属于一种被称为角色扮演游戏（RPG）的游戏类型。RPG通常是一个开放式的幻想世界模拟，玩家可以探索领土、收集宝物和击败怪物。通常，玩家创建一个角色——一个在游戏世界中的化身——并以此角色进行任务，其最终目标通常涉及拯救某物（王国、存在平面、公主等）免于毁灭。另一个RPG的基本特征是成长概念。尽管有许多设计模式和隐喻用于模拟成长过程，通常玩家的化身通过击败怪物获得经验值，并通过积累这些点数“升级”。通过升级过程，玩家解锁能力，他们的化身变得更强大。总的来说，RPG往往是许多文化神话中常见的英雄之旅模式的模拟：年轻、未成型的人必须离开家园的安全区域，面对未知的危险，克服它们，培养他们潜在的能力，并以强大和完全实现的人的身份返回，无论是字面上还是象征性地。\u003c/p\u003e\n\u003cp\u003e在其基本设计中，《黑暗之魂》作为RPG是相当传统的。然而，它有许多叙述和游戏设计选择，颠覆了这个几十年旧的类型的许多传统。作为一个叙述，它可以被解读为一种政治和文化的寓言，++直接反映了生活在一个意识形态叙述失去吸引力和连贯性的衰败文明中的经验++。它也是一种新兴新类型的前沿：后末日幻想。\u003cstrong\u003e在这款游戏中，王国被占领，英雄们都死了或堕落了，没有人可以拯救，而你存在的最大威胁是那些与你在废墟中一起生存的人\u003c/strong\u003e。社区是不可能的。基本的本体状态是无休止的失败，而“英雄”只是注定要忍受它。甚至死亡也不能免除角色的负担，而你被赋予的任务的意义和目标是模糊和不明确的。叙述几乎不存在。当游戏开始时，玩家被简单地放在一个被怪物和不死者占领的废弃王国中，没有明确的理由说明为何事情会如此，或者他们应该怎么做。仅有的叙述存在于零碎的文本中，只提供诱人但最终不完整或矛盾的关于世界的信息。所有这些叙述元素都对已建立的类型惯例提出了严重挑战，因此将其与衰退的讨论联系起来。\u003c/p\u003e\n\u003cp\u003e在游戏设计方面，《黑暗之魂》有两个突出的特点。第一个是游戏被有意设计得异常困难。这实际上是其吸引力的主要来源之一。该系列在西方的广告标语是“准备去死”（Prepare To Die），而在日本，该系列被认为是しにがみ（死亡之神）的主要例子，暗示玩家在与远比玩家创建的化身强大的巨大、恐怖的怪物斗争时必然会经历无数次死亡。另一个特点是，尽管游戏主要设计为单人体验——也是RPG类型的一个既定惯例——玩家始终连接到互联网。这被视频游戏评论家广泛赞誉为该类型的重要创新。在实践中，这创造了一种紧张的局势，玩家总是暴露在其他玩家的“入侵”之中，这些玩家会试图杀死该玩家。这种互动中内置了一个复杂的风险/回报系统，入侵者通过杀死其他玩家可以获得很多，但如果失败则会失去很多。\u003c/p\u003e\n\u003cp\u003e实际上，始终连接到互联网使得游戏世界充满了偏执和危险。其他玩家是玩家在游戏中可能面对的最危险和不可预测的怪物。入侵玩家经常会潜伏在一个不合适的时机突然袭击，玩家无法自卫，从而逆转他们不幸的受害者辛苦获得的进展。我们将详细探讨叙述和游戏系统，但这一系列游戏的基本概况引出了一个问题：在一个没有希望拯救王国，并且异常困难常常到不公平地步的游戏中，怎么会有如此大的吸引力？但事实上，这款游戏在日本、欧洲和北美都极具吸引力。\u003cstrong\u003e这也指出了游戏的设计和叙述如何与在无聊时代生活的年轻一代的经验产生共鸣\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e日本文化领域的停滞叙述需要进一步澄清。我认为《黑暗之魂》可以被解读为对当代日本状况的政治和文化寓言。这是一种后现代的英雄之旅的颠倒：一种被污染的神话碎片的模拟。\u003cstrong\u003e它直接反映了许多年轻日本人今天试图在庞大、失修的机构中航行时所经历的无聊和徒劳的感觉，这些机构至多对他们的存在漠不关心，至多直接敌对。然而，这种寓言不仅仅是关于生活在1980年代“奇迹”之后的经济问题。它同样涉及生活在一个国家、政治和文化叙述已经失去了大部分意义和连贯性的社会中的社会和心理成本，这些叙述曾经作为一种心理基石——为个人和社区身份提供稳定\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e催化这一恶化的事件起源于1995年，那一年发生了神户地震和沙林毒气袭击。在西方媒体中，关于这些事件的文化或政治意义的讨论通常不多，但在许多日本知识分子中，这些事件似乎标志着现代性的决定性破裂。在著名小说家村上春树的书《地下：东京毒气袭击和日本心理》中，对沙林毒气袭击幸存者的一系列采访，并得出结论，这一事件的后果不可逆转地揭示了日本社会的所有矛盾、缺陷和内在不稳定性：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“神户地震和1995年1月和3月的东京毒气袭击是日本战后历史上两起最严重的悲剧。毫不夸张地说，这些事件前后的日本意识发生了显著变化。这两次灾难将作为我们生活中的两个里程碑，永远铭刻在我们的心灵中。如此灾难性的事件接连发生是令人震惊的，也是巧合的。然而，当日本的“泡沫经济”破裂，标志着那些泛滥成灾的时代结束时，这些事件带来了对日本国家根本根源的批判性审视。就像这些事件一直在等待伏击我们。两者共有的是压倒性的暴力元素：\u003cstrong\u003e一个是不可避免的自然灾难，另一个是可以避免的人为灾难。\u003c/strong\u003e”（村上，237页）\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e通过村上等许多知识分子的工作，1995年已成为一个单一且不可改变的地位：\u003cstrong\u003e通往破碎和功能失调时代的门户\u003c/strong\u003e。这里分析的主要理论家——东浩纪、大泽真幸和田中基子——都\u003cstrong\u003e将1995年视为进入反乌托邦现状的起点\u003c/strong\u003e。这些思想家的语调本身就有一种微妙的后末日色彩，仿佛我们生活在一个无法拯救的被污染时代。这也是衰败讨论的一部分。这些思想家有一种确定的末世论信念，将他们与这一讨论联系起来。\u003c/p\u003e\n\u003cp\u003e无论如何，这些灾难比任何经济灾难都更可能是现任首相希望根除的“萎缩”心态的原因。尽管已经过去几十年，1995年的事件作为焦虑和无助感的触发点，在《黑暗之魂》的寓言中找到了源头。经济的持续低迷、老龄化人口、年轻一代的就业前景减少以及福岛等灾难，只是加剧了这些灾难在1995年最初引发的负面情绪。福岛等事件只帮助在日本意识中创造了一种遗产。近年来，右翼极端民族主义政治的重新出现和尼特族及隐蔽青年的社会退缩，形成了对这一相同的文化病态的极端和相反的情感反应。右翼试图通过与其真实和感知的对手（韩国、中国、朝鲜）进行公开的对抗来复兴“大和”帝国的活力，试图通过历史修正主义抹去日本帝国的暴力和道德可憎之处，并试图通过修改宪法（第9条）恢复战争作为国家主权权利。而尼特族和隐蔽青年则将自己沉溺于网络和其他形式的私人数字享乐中。他们没有叙述可供复兴，没有事业可供加入，甚至没有可以参与的兴趣小组。\u003cstrong\u003e与他人沟通实在是太困难了，在他们看来，也太危险了。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e我的观点不是《黑暗之魂》系列是这些问题的根源，甚至不是它们的贡献，而是它是对在公众意识中循环的深层社会焦虑，特别是对视频游戏作为熟悉媒介的年轻一代的一种特别有力的表达。《黑暗之魂》系列的持续流行，作为一种社会腐化的象征，反映了人们在日常生活中的感觉。但这些焦虑到底是什么，它们如何在《黑暗之魂》中表现出来？另一种问这个问题的方法是，后现代意识的基本要素是什么？定义1995年后时代的有两个主要元素。\u003cstrong\u003e1. 对构建战后秩序的政治和社会叙述的拒绝；2. 一种对他者的弥漫性焦虑，侵蚀了对社会过程的信任和信心\u003c/strong\u003e。他者的概念是指自我与世界上遇到的所有其他自我之间的二元关系的一半。这一概念的延伸是文化他者，即一种感觉某些人与自己相似，另一些人则是异类。日本社会通常被认为具有非常同质的国家认同感，彼此之间有相同的文化、种族和相似的价值观。这反过来又创造了强烈的信任和社区感，但这种亲近的逆向效应是，外来者被认为比在美国、英国或法国等文化多样性更大的国家中更为陌生。大泽真幸的论点是，奥姆真理教的恐怖活动将潜在威胁引入了公众。这种恐惧是一种诡异的他者的恐惧，这种他者会突然出现，以一种深刻的方式扰乱你的生活，有时甚至是致命的。社会结构中的这种破裂从未真正修复，并继续受到挑战，直到现在。关于诡异他者的偏执是定义《黑暗之魂》的氛围和叙述的主要特征之一，也是其游戏机制，尤其是其在线组件。\u003c/p\u003e\n\u003cp\u003e要理解《黑暗之魂》如何作为一种描述后现代意识的寓言运作，我们必须追踪战后日本社会状况的衰退轨迹。\u003cstrong\u003e这根本上是一个现代进步叙述——本质上是从西方继承的——首先在1960年代末引发了激烈的意识形态斗争，然后在1980年代变成一种无政治的享乐主义，最后，最终在五个正义的宗教狂热者用雨伞尖端戳破装满沙林毒气的袋子的时刻，被一种微妙的虚无主义彻底摧毁\u003c/strong\u003e。为此，我们将研究社会学家大泽真幸的工作。大泽花费了相当多的职业生涯试图回答奥姆为什么会做他们所做的事情。\u003cstrong\u003e在他的书《虚构时代的终结》中，他认为答案涉及意识形态本身的致命缺陷：一种危险而空洞的思维模式，导致在一个自我设计的世界中越来越高的疏离感。\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"理性时代\"\u003e理性时代\u003c/h2\u003e\n\u003cp\u003e借用社会学家見田宗介\u003csup id=\"fnref:19\"\u003e\u003ca href=\"#fn:19\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e19\u003c/a\u003e\u003c/sup\u003e（Mita Munesuke）的观点，大泽真幸将战后时代分为两个不同的阶段。第一个阶段他称为“理想的时代（理想の時代，risō no jidai）”，第二个阶段他称为“虚构的时代（虚構の時代，kyokō no jidai）”。理想的时代始于20世纪60年代，结束于1972年，虚构的时代从1972年开始，一直持续到1995年。大泽将“理想”和“虚构”描述为“可能世界（可能の世界 kanōsei no sekai）”，因为它们都反映了我们对世界的感知以及我们认为世界应该如何。然而，尽管这两个概念在根本层面上是相关的，但它们在几个重要方面是不同的。\u003cstrong\u003e大泽认为，理想与现实世界密切相关，而虚构则完全脱离现实世界\u003c/strong\u003e：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e理想是一个在未来有望或期待在现实中实现的可能世界。因此，理想必须存在于现实的因果延伸之上。从这个意义上讲，理想不是一个纯粹的可能世界，而是广义现实世界的一个方面。相反，虚构是一个可以在现实中实现与否无关的可能世界，因此是纯粹的反现实。（大泽，41页）\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e理想は、未来において現実に着地することが予期（期待）されているような可能世界である。だから、理想は、現実の因果的な延長上になくてならない。その意味では、理想は、純粋な可能世界ではなく、むしろ広義の現実世界の一局面である。それにたいして、虚構は現実への着地ということについてさしあたって無関連ありうる可能世界であり、それゆえ純粋な反現実である。(Osawa, 41)\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e暂且不讨论虚构如何将我们与现实疏离，日本的理想时代很大程度上受到冷战阴影的笼罩。当亚洲大部分地区处于苏联的保护伞下时，日本则在美国的庇护下，因而在意识形态上致力于民主和经济自由主义。在20世纪60年代，日本在这些条件下几乎在各个方面都得到了繁荣：科学、基础设施、经济、文化等。大泽将20世纪60年代称为理想的黄金时代，并指出家用电器是赋予大众理想的物质形式：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e理想时代在大众层面的黄金期是20世纪60年代。当然，这与日本的高速增长期相吻合。在此期间，例如，受到国民绝大多数人广泛需求的家电产品赋予了大众理想物质的表达……经济增长和科学技术的进步并不是因为理想而被接受的，相反，正因为理想作为可能的现实在广泛（全球范围内）被信奉，经济才能够增长，科学和技术才能进步。（大泽，44页）\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e理想の時代の、大衆的な水準で黄金期は、1960年代である。もちろん、これは、高度成長期にあたる。この時期、たとえば、国民の圧倒的な大多数によって広範に欲求された家電製品が、大衆的理想に物質的な表現を与えた\u0026hellip;経済成長や科学・技術の進歩があったから理想が抱かれたのではなく、逆に、理想が可能的現実として広く（世界規模で）信憑されたがゆえに、経済が成長することができ、また科学や技術進歩していると感受されたのである。(Osawa, 44)\u003c/strong\u003e\u003c/p\u003e","title":"翻译｜向下螺旋：《黑暗之魂》系列游戏中的后现代意识与佛教形而上学"},{"content":" If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.\n— from Build a search engine, not a vector DB\n一个优秀的RAG背后， 一定有一个优秀的搜索引擎；\n很多时候，在针对通用场景下， 如果能够调用常见的搜索引擎，如google 或者bing， RAG的效果一般不会太差；\n但，当考虑到一些垂直场景， 我们需要构造属于自己的“搜索引擎”，而往往根据特定的场景，在索引构建上会有特殊处理；可以参考devv，在针对代码场景下， 对RAG的检索部份也做了不少的工作和设计；\n很显然， 检索不是仅仅使用向量匹配和向量库就能搞定的事情；\n考虑到前LLM时代的NLP检索（甚至是图像检索），或者是推荐系统， 其实会发现， 这些东西并没有什么太大的变化，思路和技术依旧是可以复用的。\nvector-search is not all you need 纯粹基于向量匹配实现起来很简单，但不是万能的\n其问题来源有：\nembedding模型一般在通用预料上做训练， 在特定领域下， 其效果并不会太好，很多时候需要在特定领域语料上再做微调；\n语义匹配不见得是万能的，对于一些特定场景，其局限性尤为明显。例如，当用户询问一个具体的名词时，就会发现单纯使用向量匹配只能匹配到大致的相似度，对于具体的关键词是无能为力的。 这种情况下，语义匹配往往难以捕捉到用户提问中的精确细节。例如，用户可能询问特定型号的苹果电脑（如“MacBook Pro 2021”），但语义匹配算法可能会返回类似类型或年份的苹果电脑，而不是确切的型号。此时语义匹配的效果并不会太好。\nembedding模型计算的是输入的两个文本（A和B）的相似度。然而，在实际应用中，输入的是一个问句（query），需要匹配的是一段文字。在这种情况下，我们关注的重点是文本之间的相关性而非简单的相似性。因此，通常在使用embedding模型后，还需要一个reranking模型，根据相关性对结果进行排序，以确保返回最相关的答案。\n对于组合的问题无效；如果query是一些复杂的需要一些推理的问题（如多跳），使用vector- search显然不是好办法；这类问题通常涉及多个子问题，每个子问题的答案都是解答下一个问题的基础， 举例来说，回答“谁是发明电话的人的孙子在2020年的职业是什么？”需要知道发明电话的人是谁，接着找出他的孙子，然后查询这个孙子的职业。面对需要多步推理的复杂问题时，向量搜索的能力有限，原因在于它不可能进行复杂的逻辑推理和多步信息整合。在这样的场景下，需要的是Agent将query进行分解，然后分别调用工具（搜索引擎）；\n其实我们可能过于迷信向量搜索。事实上，对于一些简单的场景，基于倒排索引的关键词匹配效果更好、更加简单且成本更低。倒排索引是一种高效的文本检索方法，通过建立关键词到文档的映射，可以快速找到包含特定关键词的文档。对于不需要复杂推理的简单查询，倒排索引可以提供快速且准确的搜索结果。\n不止只有向量检索 稀疏向量（Sparse）和稠密向量（Dense）是通过不同的算法计算的。稀疏向量主要由零值组成，只有少数几个非零值，而稠密向量则主要包含非零值。稀疏嵌入由算法如BM25和SPLADE生成，而稠密嵌入则由机器学习模型如GloVe和Transformers生成。\nTF-IDF(Term-Frequency Inverse-Document Frequency)是一种用于评估文本中一个词对一个文档的重要程度的统计方法。其原理基于两个指标：词频（TF）和逆文档频率（IDF）。\n词频（TF）：表示某个词在文档中出现的频率，计算公式为：\n$$ \\text{TF}(t,d) = \\frac{\\text{词t在文档d中出现的次数}}{\\text{文档d中的总词数}} $$\n逆文档频率（IDF）：衡量某词在整个文档集合中重要性的指标。公式为：\n$$ \\text{IDF}(t) = \\log\\left(\\frac{\\text{总文档数}}{\\text{包含词t的文档数} + 1}\\right) $$\nTF-IDF值通过将词频和逆文档频率相乘计算得到：\n$$ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $$\nBM25（Best Matching 25）是一种基于概率模型的文本检索算法，它在TF-IDF（词频-逆文档频率）的基础上进行改进。BM25通过引入二元独立模型，并添加归一化惩罚来计算文档长度相对于数据库中所有文档平均长度的权重。\nBM25的核心公式如下：\n$$ \\text{BM25}(q, D) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})} $$\n其中：\nq 代表查询\nD 代表文档\n$q_i$ 是查询中的第i个词\n$f(q_i, D)$ 是词 ($q_i$) 在文档 (D) 中的词频\n|D| 是文档 (D) 的长度（以词计）\n$\\text{avgdl}$ 是集合中所有文档的平均长度\n$k_1$ 和 b 是调节参数，通常 $k_1$ 在1.2到2之间，b 取0.75\n$\\text{IDF}(q_i)$是词 $q_i$ 的逆文档频率\nDense vector 使用稠密向量表示存储在数据库中的信息，包括文本、图像和其他类型的数据，。这些嵌入由机器学习模型生成，将数据转换为向量。也就是最近两年大家常说的向量检索。\n如何提升检索效果：\n自然而然的想法，不再使用单一的向量检索， 而是采用多种检索方式混合的方式，不论是混合不同的embedding模型的检索还是Sparse-vector和Dense-vector混合使用以获得二者的优势。通常来说，稠密向量擅长理解查询的上下文，而稀疏向量则擅长关键词匹配。\n那么需要将二者合并，but how？\nReciprocal Rank Fusion (RRF) 其核心思想是通过计算每个文档在不同排名列表中的倒数排名之和来确定最终排名。这样，排名较高的文档会得到更高的分数，而排名较低的文档会受到惩罚。\nRRF的计算公式如下：\n$$ \\text{RRF}(d) = \\sum{i=1}^{N} \\frac{1}{k + r_{i}(d)} $$\n其中：\n(d) 代表文档\n(N) 是排名列表的数量\n(k) 是一个常数，用于平滑处理，通常取值为60\n($r_{i}(d)$) 是文档 (d) 在第 (i) 个排名列表中的排名\n通过这个公式，每个文档的最终RRF分数是其在所有排名列表中的倒数排名之和，分数越高，排名越靠前。\n在langchain **ensemble-retriever**的源码中可以看到其具体的思想逻辑：\ndef weighted_reciprocal_rank( self, doc_lists: List[List[Document]] ) -\u0026gt; List[Document]: \u0026#34;\u0026#34;\u0026#34; Perform weighted Reciprocal Rank Fusion on multiple rank lists. You can find more details about RRF here: https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf Args: doc_lists: A list of rank lists, where each rank list contains unique items. Returns: list: The final aggregated list of items sorted by their weighted RRF scores in descending order. \u0026#34;\u0026#34;\u0026#34; if len(doc_lists) != len(self.weights): raise ValueError( \u0026#34;Number of rank lists must be equal to the number of weights.\u0026#34; ) # Associate each doc\u0026#39;s content with its RRF score for later sorting by it # Duplicated contents across retrievers are collapsed \u0026amp; scored cumulatively rrf_score: Dict[str, float] = defaultdict(float) for doc_list, weight in zip(doc_lists, self.weights): for rank, doc in enumerate(doc_list, start=1): rrf_score[ doc.page_content if self.id_key is None else doc.metadata[self.id_key] ] += weight / (rank + self.c) # Docs are deduplicated by their contents then sorted by their scores all_docs = chain.from_iterable(doc_lists) sorted_docs = sorted( unique_by_key( all_docs, lambda doc: doc.page_content if self.id_key is None else doc.metadata[self.id_key], ), reverse=True, key=lambda doc: rrf_score[ doc.page_content if self.id_key is None else doc.metadata[self.id_key] ], ) return sorted_docs 排序模型 在信息检索和问答系统中，处理用户查询通常分为两个步骤：召回（retrieval）和重排序（reranking）。\n召回的目标是用非常短的时间在海量数据中找到一定范围的备选项。这一过程的精度可能不会特别高，但可以将候选项从千万量级压缩到百量级。（这一过程涉及ANN也就是Approximate Nearest Neighbor， 在本文的后段会提及）\n重排序阶段，需要从召回阶段的备选项中选择符合需要的选项。由于候选项数量已经大幅减少，可以采用精度较高但相对计算时间较长的算法对这些备选项进行重新排序，以提高最终结果的准确性。\n重排序阶段场景的模型如`BAAI/bge 系列，从其huggingface仓库中的config.json可以发现：\n\u0026#34;_name_or_path\u0026#34;: \u0026#34;BAAI/bge-m3\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;XLMRobertaForSequenceClassification\u0026#34; ], 其本质是基于**XLM-RoBERTa**的序列分类模型。在下游任务中，对于序列分类任务，XLMRobertaForSequenceClassification 在 XLM-RoBERTa 的基础上添加了一个分类层（一个全连接层）来进行分类；简单来说，对于信息检索或问答系统中，输入用户问题（query），和匹配到的doc，通过合并query和doc形成一个sequence（[CLS]query[SEP]doc[SEP]），通过XLMRobertaForSequenceClassification 可以计算出二者的相关性分数，遍历候选集中的doc（一般在百数量级左右），即可得到query同候选集相关性分数。\n一些新的工作 Matryoshka Representation Learning（MRL） 来源于OpenAI 的embedding 技术：New embedding models and API updates\n和NeurIPS 2022发表的论文：Matryoshka Representation Learning\nMotivation：现有的深度学习模型通常学习固定维度的表示，无法灵活适应任务需求；表示维度的难以确定；表示维度缺乏粗到细（coarse-to-fine）的粒度；\nMRL旨在学习具有不同粒度的表示，允许单个embedding适应下游任务的计算约束。其设计思想是为了在不同的embedding维度上使用损失函数，从而训练一个模型，使其在不同的embedding维度上都能保持良好的性能\n训练过程：MRL 在多个嵌套的维度上训练模型，使得每个低维度的表示都能作为数据点的有用表示。这样，模型在不同的任务和计算约束下都能表现良好。\n推理过程：在推理过程中，MRL 可以根据需要使用不同维度的嵌入。对于分类任务，可以从低维度开始，逐步使用更高维度的表示。对于检索任务，可以先使用低维度表示进行初步筛选，然后使用高维度表示进行精细排序。\n（具体可以参考论文和sentence_transformers中所实现的MatryoshkaLoss）\nbge-m3 paper\ngithub\nm3—即Multi-Linguality 支持多语言， Multi-Functionality 支持多种检索方式（Dense-retrieval，Sparse-Retrieval， Multi-Vec Retrieval），以及Multi-Granularity 支持多粒度的检索（sentence-level，Passage-level，和Doc-level）\npip install -U FlagEmbedding Dense Embedding from FlagEmbedding import BGEM3FlagModel model = BGEM3FlagModel(\u0026#39;BAAI/bge-m3\u0026#39;, use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation sentences_1 = [\u0026#34;What is BGE M3?\u0026#34;, \u0026#34;Defination of BM25\u0026#34;] model.encode(sentences_1) print(model.encode(sentences_1)[\u0026#39;dense_vecs\u0026#39;].shape) # (2, 1024) Sparse Embedding (Lexical Weight) output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False) output_1 \u0026#34;\u0026#34;\u0026#34; {\u0026#39;dense_vecs\u0026#39;: array([[-0.03412 , -0.04706 , -0.0009317, ..., 0.0483 , 0.007576 , -0.02959 ], [-0.01026 , -0.0449 , -0.02432 , ..., -0.00828 , 0.01502 , 0.011086 ]], dtype=float16), \u0026#39;lexical_weights\u0026#39;: [defaultdict(int, {\u0026#39;4865\u0026#39;: 0.0836, \u0026#39;83\u0026#39;: 0.0814, \u0026#39;335\u0026#39;: 0.1295, \u0026#39;11679\u0026#39;: 0.252, \u0026#39;276\u0026#39;: 0.17, \u0026#39;363\u0026#39;: 0.2695, \u0026#39;32\u0026#39;: 0.04083}), defaultdict(int, {\u0026#39;262\u0026#39;: 0.04996, \u0026#39;5983\u0026#39;: 0.1367, \u0026#39;2320\u0026#39;: 0.04483, \u0026#39;111\u0026#39;: 0.06335, \u0026#39;90017\u0026#39;: 0.2517, \u0026#39;2588\u0026#39;: 0.3335})], \u0026#39;colbert_vecs\u0026#39;: None} \u0026#34;\u0026#34;\u0026#34; # you can see the weight for each token: print(model.convert_id_to_token(output_1[\u0026#39;lexical_weights\u0026#39;])) # [{\u0026#39;What\u0026#39;: 0.08356, \u0026#39;is\u0026#39;: 0.0814, \u0026#39;B\u0026#39;: 0.1296, \u0026#39;GE\u0026#39;: 0.252, \u0026#39;M\u0026#39;: 0.1702, \u0026#39;3\u0026#39;: 0.2695, \u0026#39;?\u0026#39;: 0.04092}, # {\u0026#39;De\u0026#39;: 0.05005, \u0026#39;fin\u0026#39;: 0.1368, \u0026#39;ation\u0026#39;: 0.04498, \u0026#39;of\u0026#39;: 0.0633, \u0026#39;BM\u0026#39;: 0.2515, \u0026#39;25\u0026#39;: 0.3335}] # compute the scores via lexical mathcing print(model.compute_lexical_matching_score(output_1[\u0026#39;lexical_weights\u0026#39;][0], output_1[\u0026#39;lexical_weights\u0026#39;][1])) # 0 Multi-Vector (ColBERT) sentences_1 = [\u0026#34;What is BGE M3?\u0026#34;, \u0026#34;Defination of BM25\u0026#34;] output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False) print(output_1[\u0026#39;colbert_vecs\u0026#39;][0].shape) # (8, 1024) print(model.colbert_score(output_1[\u0026#39;colbert_vecs\u0026#39;][0], output_1[\u0026#39;colbert_vecs\u0026#39;][1])) # tensor(0.4768) ColBERT score： 在计算完查询和文档之间的相似度矩阵后，对每个查询token选择与文档中token的最大相似度，将所有最大相似度累加，得到最终的colbert_score。\n基于LLM实现embedding 几个代表模型：\nAlibaba-NLP/gte-Qwen2-7B-instruct\nintfloat/e5-mistral-7b-instruct\ne5-mistral-7b-instruct论文：Improving Text Embeddings with Large Language Models\nconfig.json:\n{ \u0026#34;_name_or_path\u0026#34;: \u0026#34;mistralai/Mistral-7B-v0.1\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;MistralModel\u0026#34; ], \u0026#34;bos_token_id\u0026#34;: 1, \u0026#34;eos_token_id\u0026#34;: 2, \u0026#34;hidden_act\u0026#34;: \u0026#34;silu\u0026#34;, \u0026#34;hidden_size\u0026#34;: 4096, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 14336, \u0026#34;max_position_embeddings\u0026#34;: 32768, ... 通过LLM生成数据（合成数据）， 采用对比学习微调Mistral-7B；\nGiven a pretrained LLM, we append an [EOS] token to the end of the query and document, and then feed them into the LLM to obtain the query and document embeddings $(h_{q_{inst}^+}, h_d^+)$, by taking the last layer [EOS] vector.\n附录 如何找到合适的embedding模型和reranker模型 首先推荐从 huggingface MTEB Leaderboard上选择排名靠前的模型\n其次，实验！根据自己的场景构造测试集验证，才能选择出适合自己场景的embedding模型和reranker模型；\nembedding模型和reranker模型的注意事项 上下文长度限制。 这些模型大多本质是Bert-based 模型， 而Bert的上下文长度限制一般是512，具体可以在huggingface中的config.json中查看；以BAAI/bge-base-en-v1.5 为例\n{ \u0026#34;_name_or_path\u0026#34;: \u0026#34;/root/.cache/torch/sentence_transformers/BAAI_bge-base-en/\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;BertModel\u0026#34; ], \u0026#34;attention_probs_dropout_prob\u0026#34;: 0.1, \u0026#34;classifier_dropout\u0026#34;: null, \u0026#34;gradient_checkpointing\u0026#34;: false, \u0026#34;hidden_act\u0026#34;: \u0026#34;gelu\u0026#34;, \u0026#34;hidden_dropout_prob\u0026#34;: 0.1, \u0026#34;hidden_size\u0026#34;: 768, \u0026#34;id2label\u0026#34;: { \u0026#34;0\u0026#34;: \u0026#34;LABEL_0\u0026#34; }, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 3072, \u0026#34;label2id\u0026#34;: { \u0026#34;LABEL_0\u0026#34;: 0 }, \u0026#34;layer_norm_eps\u0026#34;: 1e-12, \u0026#34;max_position_embeddings\u0026#34;: 512, \u0026#34;model_type\u0026#34;: \u0026#34;bert\u0026#34;, \u0026#34;num_attention_heads\u0026#34;: 12, \u0026#34;num_hidden_layers\u0026#34;: 12, \u0026#34;pad_token_id\u0026#34;: 0, \u0026#34;position_embedding_type\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;torch_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;transformers_version\u0026#34;: \u0026#34;4.30.0\u0026#34;, \u0026#34;type_vocab_size\u0026#34;: 2, \u0026#34;use_cache\u0026#34;: true, \u0026#34;vocab_size\u0026#34;: 30522 } 其中`\u0026quot;max_position_embeddings\u0026quot;: 512, 也就是它的上下文长度，但输入sequence长度超过512个token，则模型只看到了最前面的512个token；\n向量维度\n微调\n如何微调embedding 模型 采用FlagEmbedding，参考：How to fine-tune bge embedding model?\n微调Sentence Transformer，参考：Training Overview\n还有哪些类型的reranker模型 根据ColBert文章\n作者将query-document matching 任务（reranking）分类成：\nRepresentation-based Similarity, 双塔，独立对查询和文档进行编码，然后通过计算它们的向量相似度进行匹配， 但query和doc间无更精细的交互。\nQuery-Document interaction： 在查询和文档之间直接进行交互，通常使用CNN来捕捉查询和文档之间的交互特征， 但交互计算复杂度较高，尤其是当查询和文档长度较长时。\nAll-to-All Interaction，基于Bert 的Cross-encoder结构， 查询和文档的每个词与对方的每个词进行全连接交互。这种方式可以捕捉细粒度的交互信息，通常需要较高的计算资源。\nLate Interaction， 迟交互型，在对查询和文档的每个词进行独立编码后，再进行交互计算。通常使用最大池化（MaxSim）来选择最相关的词进行匹配，从而结合了高效性和细粒度交互的优点。\n此外， 当然也有基于LLMs的reranker，参考：llm_reranker\n以 BAAI/bge-reranker-v2-gemma 为例， 只是做这样一件事\nprompt = \u0026#34;Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either \u0026#39;Yes\u0026#39; or \u0026#39;No\u0026#39;.\u0026#34; 以输出“Yes” 的logit 作为相关分数\n对比学习 在之前不断提到了embedding，当然， 和transformer中的embedding层不是一个东西；\n这里所指的embedding是通过对比学习的方式，将语义相近的文本映射到高维参数空间中的相近位置，使它们的距离尽可能短。相反，语义差别大的文本则被映射到参数空间中远离的位置。类似地，这种方法也可以应用于图像（相似图像检索）以及文本和图像的多模态语义匹配（如CLIP）。那么，对比学习有三个核心：loss function，网络架构和采样。\n对比学习有三个核心要素：网络架构、损失函数（loss function）和采样策略。\n常见的损失函数有contrastive loss、triplet loss，以及N-pair loss。\n网络架构方面,对比学习通常采用双塔(siamese network)或者三塔(triplet)结构。双塔结构包含两个相同或相似的编码器,分别用于处理正样本对;三塔结构则增加了一个用于处理负样本的编码器。这些编码器可以是各种深度学习模型,如CNN、RNN或Transformer等,具体选择取决于任务域和数据类型。这一步将原始较为复杂的数据类型，通过深度学习的方法降到相对较低的维度便于之后的计算。\n对于双塔结构， 一般采用Contrastive Loss， 其用于度量两个样本之间的相似度。其公式为：\n$$ L = \\frac{1}{2N} \\sum_{i=1}^N (y_i d_i^2 + (1 - y_i) \\max(0, m - d_i)^2) $$\n其中：\n$d_i$是样本对$(x_i, x_j)$ 的欧几里得距离。\n$y_i$ 是样本对的标签，1 表示正样本对，0 表示负样本对。\nm 是一个超参数，表示负样本对之间的最小距离，也称margin。\nTriplet Loss则使用于triplet-network结构, 其通过构造三元组 ( anchor, positive, negative) 来训练模型，使得正样本（positive）与锚点（anchor）的距离小于负样本（negative）与锚点的距离。其公式为：\n$$ L = \\sum_{i=1}^N \\left[ |f(x_i^a) - f(x_i^p)|_2^2 - |f(x_i^a) - f(x_i^n)|_2^2 + \\alpha \\right] $$\n其中：\n$f(x)$ 表示样本 x 的特征向量。\n$\\alpha$是一个超参数，表示正负样本对之间的最小距离差。\nN-pair Loss 是 Triplet Loss 的推广，通过引入多个负样本来提升训练效果。\n$$ L = \\sum_{i=1}^N \\log \\left( 1 + \\sum_{j \\neq i} \\exp \\left( f(x_i)^\\top f(x_j^n) - f(x_i)^\\top f(x_i^p) \\right) \\right) $$\n在训练过程中， 采样策略则很关键；但运气不好， 选择的样本都是Easy Negative mining，也就是$d(a,p)+margin\u0026lt;d(a,n)$，则模型能够很快收敛，但什么也没有学会，相反在另一个极端中， 采用的多是hard negative mining，也就是$d(a,n)\u0026lt;d(a,p)$的情况，此时模型训练将很难收敛；实际情况中， 往往采用semi-hard 或者是distance-weighted sampling 来寻找难度适中的负样本,以避免模型过于关注极端情况。而由于在高维空间下， 因为高维空间中大部分体积都集中在高维球壳上，所以随机采样得到的点之间的距离往往非常接近，这也是不采用随机采样的策略的原因（Sampling Matters in Deep Embedding Learning）。\n向量库是个什么东西 向量库不是“库”，但也是“库”；\n考虑到传统关系型数据库， 数据以表格形式存储， 但在AI时代， 大量非结构化数据的存储和查询，以表格形式存在这不太合理了；因为数据维度高， 需要一些特殊的查询方式；\n不同于传统数据库的精确匹配查询,向量库主要用于相似性搜索。这种搜索基于向量间的距离,通常使用欧几里得距离或余弦相似度。以暴力遍历的最近邻查询为例(Nearest neighbor search),假设我们有n个d维向量,查询复杂度为$O(nd)$。当n和d都很大时,这种方法变得非常耗时。\n而近似最近邻搜索(Approximate Nearest Neighbor search, ANN)则能将时间复杂度降低到亚线性,通常为$O(log n)$或更优。\n常见的ANN算法：\n基于树的：k-D树(k-D tree):将空间递归地划分为子空间,适用于低维数据。\n基于哈希的：局部敏感哈希(LSH, Locality-Sensitive Hashing):将相似的向量映射到相同的\u0026quot;桶\u0026quot;中。\n乘积量化(Product Quantization):将高维向量分解为低维子向量,并对每个子向量进行量化。\n基于Graph的：分层可导航小世界图(HNSW, Hierarchical Navigable Small World):构建多层图结构,在图上进行快速导航搜索。\n而以上算法的思路都可以归介于—将相似的样本放在一起（减少搜索空间）， 和构建高效的搜索结构。\n参考 Build a search engine, not a vector DB\nHybrid Search Explained\n再谈大模型RAG问答中的三个现实问题：兼看RAG-Fusion多query融合策略、回答引文生成策略及相关数据集概述\nLLM - RAG文档应用处理与召回经验之谈\n[RAG] BGE M3-Embedding | 什么？我的RAG底座模型又要换了？\nColBERT: A complete guide\n搜索引擎技术\nUtilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\nLLM4IR-Survey\nFine-Tuning LLaMA for Multi-Stage Text Retrieval\n","permalink":"https://niraya666.github.io/posts/rag%E6%A3%80%E7%B4%A2/","summary":"\u003cblockquote\u003e\n\u003cp\u003eIf you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e— from \u003ca href=\"https://blog.elicit.com/search-vs-vector-db/\"\u003eBuild a search engine, not a vector DB\u003c/a\u003e\u003c/p\u003e","title":"RAG工具箱：检索"},{"content":"本文将简单介绍如何评价LLM的tool-using 能力。\n引言 在工具使用评估方面，过去的研究主要有以下几种思路：\n对比工具使用和纯LLM在基准测试上的分数：例如Toolformer和LATM。\n在Toolformer研究中，通过下游任务如语言模型评估基准测试、数学推理任务和问答任务来验证工具使用的有效性。\nLATM则采用了来自BigBench的六个数据集进行评估。\n测试工具使用的准确率和响应质量：例如API-Bank。\n在评估过程中，首先初始化评估系统，确保每个API的数据库包含默认值。然后，将预测的API调用与手动标注的API调用进行比较，以确定它们的一致性。响应评估则使用ROUGE-L指标。 利用LLM对工具使用的效果进行评价：例如Tool-bench。\ntwo evaluation metrics:\nPass Rate: Calculates the proportion of successfully completing an instruction within limited OpenAI API calls.\nPreference: Measured by comparing two answers (action sequences) for a given instruction.We pre-define a set of criteria for a better answer, which are organized as prompts for ChatGPT.\n构造虚拟运行环境，测试代理与环境的交互结果：例如ToolAlpaca。\n利用LLM模拟环境（用户代理和助手代理），并使用GPT-4对ToolAlpaca模型进行机器评估，评估其使用各种未见工具的能力。 对于绝大多数企业和垂直场景下， 其中思路1需要构建额外的测试集成本比较高（但还是有必要的）， 而思路4构造虚拟运行环境实际上并不现实； 所以还是选择思路2，外加通过思路3辅助判断；换句话说， 根据场景，构造工具列表和工具调用的ground-truth（包括函数名， 和传入参数） ，在存在歧义时，采用ROUGE评价响应质量， 或者使用LLM判断响应结果并评价。\n顺带提一下Langchain 项目中有关Agent的tool-using能力测试的内容， 不过由于Langchain项目又臭又长，且有很大的局限性， 这里只讨论其思路。\n在 Benchmarking Agent Tool Use 一文中，提出了4种指标用于评价Agent 的tool-using 能力：\nCorrectness 即通过LLM判断工具调用结果是否同ground truth 相同；\nCorrect final state 即检查agent 所交互环境的最终状态是否同预期相同；\nIntermediate step correctness 除了最终状态， 也可以检查中间状态；\nRatio of steps taken to the expected steps 即实际步骤与预期步骤的比例。\n在LangChain Benchmarks 项目提供了4个（3个）任务用于测试tool-using， 本质上也是一种构造虚拟环境并与之交互。\nMultiverse Math 任务 ： 要求模型使用提供的工具来解决数学问题。为了确保模型不依赖于内在知识，数学运算被修改以产生不同于预期的结果。例如，乘法和加法的结果会有所不同，但仍保留某些数学性质。 任务示例包括基本的加减乘除运算、幂运算、对数运算等。\ncode \u0026#34;\u0026#34;\u0026#34;Solve basic math question using the provided tools. Must use the provided tools to solve the math question. To make sure that innate knowledge is not used, the math operations have been altered to yield different results than expected. The modified operations should yield different results, but still retain appropriate properties. For example, the modified multiplication operation should still be commutative. Please note that the modified operations are not guaranteed to even make sense in the real world since not all properties will be retained (e.g., distributive property). For example, I ate 1 apple and 2 oranges every day for 7 days. How many fruits did I eat? One would expect the answer to be 21, but in this universe, the answer is 32.34. In addition, it depends on how the operations are grouped: (1 + 2) * 7 = 32.34 But: 1 * 7 + 2 * 7 = 24.3 Due to these changes certain questions are not allowed as inputs as they would yield different results if evaluated in different ways. For example, \u0026#34;convert 15 degrees to radians\u0026#34; is not allowed as an input as it could be interpreted as either: divide(multiply(15, pi()), 180) or multiply(divide(15, 180), pi()) \u0026#34;\u0026#34;\u0026#34; Relational Data任务 ：要求模型使用提供的工具来回答关于关系数据的问题。环境中包含关于用户及其位置和喜好食物的虚假数据，模型需要使用工具查询这些数据以回答问题。 任务示例包括查询用户的位置信息、喜好食物等\ncode \u0026#34;\u0026#34;\u0026#34;Answer questions about relational data using the provided tools. The environment contains fake data about users and their locations and favorite foods. The environment provides a set of tools that can be used to query the data. All questions can be answered by using the provided tools. The answers include the expected result as well as the most efficient way to answer the question using the tools. \u0026#34;\u0026#34;\u0026#34; Typewriter(single-tool, 26-tools): 要求模型使用工具逐字打印给定的字符串。模型需要调用工具来模拟打字机的行为，每次打印一个字符，直到完成整个字符串。 任务示例包括打印特定的句子或段落。\ncode ```python \u0026quot;\u0026quot;\u0026quot;A task where the agent must type a given string one letter at a time. In this variation of the task, the agent is given a single function, that takes a letter as an argument. \u0026quot;\u0026quot;\u0026quot; ``` 根据综述文章《What Are Tools Anyway? A Survey from the Language Model Perspective》，在验证代理或模型的工具使用能力时，可以关注以下指标：\nTask completion：任务完成度，或pass-ratio 这是一个相对宏观的指标， 工具调用的最核心目的就是为了完成任务， 任务的成功与否表征了工具调用的最终结果。\nTool selection： 如何在众多工具中挑选到最合适的工具，这考验了模型在特定场景下对任务的理解和最优化的工具选择；\nTool reusability： 指的是一个工具在不同情境下或解决不同问题时能够被多次使用的能力， 不过往往和工具生成（tool-marking）相关， 不在这次的讨论范围中。\n方法论 综合以上讨论， tool-using 的测试可以简单分为：\n单轮简单测试 在这种测试中，主要测试function_name 和传入参数。关注的指标是pass-rate 和 Preference。具体方法是不需要使用LLM进行判断，只需将返回的function_name 和 function_arguments 与ground-truth进行比对，确认是否一致。最终使用pass-ratio 评价测试结果。\nJson-mode验证 由于并非所有模型都能严格按要求输出所需要的json格式，考虑到这种情况，可以进行Json-mode的验证，以确保输出格式的正确性。\n优化版本\n通过ROUGE判断function_arguments\n目的：避免严格一致所带来的问题。 通过LLM辅助判断function_arguments\n优点：准确率更高。\n缺点：成本也更高。\n多轮调用 以上单轮tool-using测试在复杂场景下不太适用。\n在一些复杂场景中，如代码生成与执行、SQL生成并执行等，需要按照一定顺序调用多个tools。这些场景需要LLM根据前几次tool-using的结果反馈，调整function_arguments 或 function_name。\n这些场景无外乎都需要LLM根据前几次tool-using的结果反馈， 修改function_arguments 或者是function_name；\n将tool-using的过程抽象为强化学习中agent同环境(env)的交互过程。目标是减少agent与环境的交互次数，从而降低成本，并争取一次或者几次就能做对，避免重复错误。可以借用langchain中的思想，增加对中间和最终状态的检查，并比对实际交互次数和预期交互次数。这类测试需要额外构建虚拟环境，超出此次讨论范围。\n测试集 NousResearch/func-calling-eval-glaive\nNousResearch/func-calling-eval-singleturn\nNousResearch/func-calling-eval\n上手：测试LLM的tool-using 能力 这里，选择最简单的tool-using用于演示， 即直接调用OpenAI 的tool-using API\n环境 import os # os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = \u0026#34;...\u0026#34; from openai import OpenAI import json client = OpenAI() 定义工具函数 考虑了一个简单的医疗场景，共4个工具；\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_medical_definition\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the definition of a medical term\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;term\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The medical term to define\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;term\u0026#34;] } } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_treatment_options\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get treatment options for a disease\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;disease\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The disease to get treatment options for\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;disease\u0026#34;] } } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_medication_side_effects\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the potential side effects of a medication\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;medication\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The name of the medication\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;medication\u0026#34;] } } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_disease_symptoms\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the symptoms associated with a specific disease\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;disease\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The disease to get symptoms for\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;disease\u0026#34;] } } } ] 定义测试用例 创建测试用例数据集，每个测试用例包含查询、预期函数调用及其参数、以及期望的响应。\n使用GPT-4构建。\ntest_cases = [ { \u0026#34;query\u0026#34;: \u0026#34;What are the symptoms of influenza?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_disease_symptoms\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;disease\u0026#34;: \u0026#34;influenza\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Symptoms of influenza include fever, chills, muscle aches, cough, congestion, runny nose, headaches, and fatigue.\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What is the definition of asthma?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_medical_definition\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;term\u0026#34;: \u0026#34;asthma\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Asthma is a condition in which your airways narrow and swell and may produce extra mucus, making breathing difficult and triggering coughing, wheezing, and shortness of breath.\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What treatments are available for rheumatoid arthritis?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_treatment_options\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;disease\u0026#34;: \u0026#34;rheumatoid arthritis\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Treatment options for rheumatoid arthritis include medications such as NSAIDs, steroids, DMARDs, and biologics, as well as physical therapy and lifestyle changes.\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What are the side effects of metformin?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_medication_side_effects\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;medication\u0026#34;: \u0026#34;metformin\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Common side effects of metformin include gastrointestinal issues such as diarrhea, nausea, and abdominal discomfort, as well as lactic acidosis in rare cases.\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;What symptoms are associated with hypothyroidism?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_disease_symptoms\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;disease\u0026#34;: \u0026#34;hypothyroidism\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Symptoms of hypothyroidism include fatigue, weight gain, cold intolerance, dry skin, constipation, and depression.\u0026#34; }, ... ] 定义测试metrics 自定义了pass-ratio，即function_name, function_arguments 严格一致为pass。\n运行测试并验证响应 def call_openai_tool(model, prompt, tools): response = client.chat.completions.create( model=model, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt} ], tools=tools, tool_choice=\u0026#34;auto\u0026#34; ) return response def run_tests(test_cases, model=deployment_name): passed_tests = 0 failed_tests = 0 for case in test_cases: response = call_openai_tool(model, case[\u0026#39;query\u0026#39;], tools) if case[\u0026#39;expected_function\u0026#39;] and case[\u0026#39;function_arguments\u0026#39;]: if response.choices[0].message.tool_calls: tool_calls = response.choices[0].message.tool_calls # 验证函数调用名称和参数 try: assert tool_calls[0].function.name == case[\u0026#39;expected_function\u0026#39;], f\u0026#34;Expected function {case[\u0026#39;expected_function\u0026#39;]}, but got {tool_calls[0].function.name}\u0026#34; assert json.loads(tool_calls[0].function.arguments) == case[\u0026#39;function_arguments\u0026#39;], f\u0026#34;Expected arguments {case[\u0026#39;function_arguments\u0026#39;]}, but got {tool_calls[0].function.arguments}\u0026#34; print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 except AssertionError as e: print(e) print(f\u0026#34;Test failed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test failed for query (no tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: # # 验证响应内容 # response_text = response.choices[0].message.content.strip() # 验证没有调用工具 if response.choices[0].message.tool_calls: print(f\u0026#34;Test failed for query (unexpected tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 total_tests = passed_tests + failed_tests accuracy = (passed_tests / total_tests) * 100 if total_tests \u0026gt; 0 else 0 print(f\u0026#34;Total tests: {total_tests}, Passed: {passed_tests}, Failed: {failed_tests}, Accuracy: {accuracy:.2f}%\u0026#34;) run_tests(test_cases) 结果：\nTest passed for query: What are the symptoms of chronic kidney disease? Test passed for query: What does the medical term \u0026#39;bipolar disorder\u0026#39; mean? Expected arguments {\u0026#39;disease\u0026#39;: \u0026#39;chronic obstructive pulmonary disease\u0026#39;}, but got { \u0026#34;disease\u0026#34;: \u0026#34;COPD\u0026#34; } Test failed for query: What are the treatment options for chronic obstructive pulmonary disease (COPD)? Test passed for query: What side effects are associated with the use of aspirin? Test passed for query: Tell me a joke. Test passed for query: What is the capital of France? Total tests: 22, Passed: 21, Failed: 1, Accuracy: 95.45% 对于test failed case， 这只是个模凌两可的错误。\n如果采用LLM作为裁判是可以避免的。\n基于ROUGE的响应判断 环境\n! pip install rouge-score --quiet from rouge_score import rouge_scorer def calculate_rouge_l(predicted, expected): scorer = rouge_scorer.RougeScorer([\u0026#39;rougeL\u0026#39;], use_stemmer=True) scores = scorer.score(predicted, expected) return scores[\u0026#39;rougeL\u0026#39;].fmeasure def run_tests_with_rouge(test_cases, model=deployment_name, rouge_l_threshold=0.8): passed_tests = 0 failed_tests = 0 for case in test_cases: response = call_openai_tool(model, case[\u0026#39;query\u0026#39;], tools) if case[\u0026#39;expected_function\u0026#39;] and case[\u0026#39;function_arguments\u0026#39;]: if response.choices[0].message.tool_calls: tool_calls = response.choices[0].message.tool_calls # 验证函数调用名称和参数 try: assert tool_calls[0].function.name == case[\u0026#39;expected_function\u0026#39;], f\u0026#34;Expected function {case[\u0026#39;expected_function\u0026#39;]}, but got {tool_calls[0].function.name}\u0026#34; predicted_arguments = json.loads(tool_calls[0].function.arguments) # 计算ROUGE-L分数 rouge_l_score = calculate_rouge_l(json.dumps(predicted_arguments), json.dumps(case[\u0026#39;function_arguments\u0026#39;])) assert rouge_l_score \u0026gt;= rouge_l_threshold, f\u0026#34;ROUGE-L score {rouge_l_score} is less than threshold {rouge_l_threshold}\u0026#34; print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 except AssertionError as e: print(e) print(f\u0026#34;Test failed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test failed for query (no tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: # 验证没有调用工具 if response.choices[0].message.tool_calls: print(f\u0026#34;Test failed for query (unexpected tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 total_tests = passed_tests + failed_tests accuracy = (passed_tests / total_tests) * 100 if total_tests \u0026gt; 0 else 0 print(f\u0026#34;Total tests: {total_tests}, Passed: {passed_tests}, Failed: {failed_tests}, Accuracy: {accuracy:.2f}%\u0026#34;) 使用ROUGE-L 计算function_arguments 同预期的差距，若大于一定阈值，则判断失败。\n基于LLM验证和判断 def chat_completion_request(messages, tools=None, tool_choice=None, model=deployment_name, json_mode=False): try: params = { \u0026#39;model\u0026#39;: model, \u0026#39;messages\u0026#39;: messages, \u0026#39;tools\u0026#39;: tools, \u0026#39;tool_choice\u0026#39;: tool_choice } if json_mode: params[\u0026#39;response_format\u0026#39;] = { \u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34; } response = client.chat.completions.create(**params) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e def evaluate_with_llm(predicted_arguments, expected_arguments, model, tools): prompt = f\u0026#34;\u0026#34;\u0026#34; Compare the following predicted arguments and expected arguments for a function call. Predicted Arguments: {json.dumps(predicted_arguments, indent=2)} Expected Arguments: {json.dumps(expected_arguments, indent=2)} Evaluate if the predicted arguments sufficiently match the expected arguments, considering minor differences acceptable. Respond with a JSON object containing a single key \u0026#34;match\u0026#34; with a boolean value indicating if they match. Example response: {{ \u0026#34;match\u0026#34;: true }} \u0026#34;\u0026#34;\u0026#34; response = chat_completion_request(messages=[{\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: prompt}],model = model, json_mode=True) json_response = json.loads(response.choices[0].message.content) return json_response.get(\u0026#34;match\u0026#34;, False) def run_tests_with_llm(test_cases, model=deployment_name, rouge_l_threshold=0.8): passed_tests = 0 failed_tests = 0 for case in test_cases: response = call_openai_tool(model, case[\u0026#39;query\u0026#39;], tools) if case[\u0026#39;expected_function\u0026#39;] and case[\u0026#39;function_arguments\u0026#39;]: if response.choices[0].message.tool_calls: tool_calls = response.choices[0].message.tool_calls # 验证函数调用名称和参数 try: assert tool_calls[0].function.name == case[\u0026#39;expected_function\u0026#39;], f\u0026#34;Expected function {case[\u0026#39;expected_function\u0026#39;]}, but got {tool_calls[0].function.name}\u0026#34; predicted_arguments = json.loads(tool_calls[0].function.arguments) # 使用 LLM 来判断参数匹配 match = evaluate_with_llm(predicted_arguments, case[\u0026#39;function_arguments\u0026#39;], model, tools) assert match, \u0026#34;LLM evaluation determined the arguments do not match\u0026#34; print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 except AssertionError as e: print(e) print(f\u0026#34;Test failed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test failed for query (no tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: # 验证没有调用工具 if response.choices[0].message.tool_calls: print(f\u0026#34;Test failed for query (unexpected tool call): {case[\u0026#39;query\u0026#39;]}\u0026#34;) failed_tests += 1 else: print(f\u0026#34;Test passed for query: {case[\u0026#39;query\u0026#39;]}\u0026#34;) passed_tests += 1 total_tests = passed_tests + failed_tests accuracy = (passed_tests / total_tests) * 100 if total_tests \u0026gt; 0 else 0 print(f\u0026#34;Total tests: {total_tests}, Passed: {passed_tests}, Failed: {failed_tests}, Accuracy: {accuracy:.2f}%\u0026#34;) 附: prompt 生成示例 def chat_completion_request(messages, tools=None, tool_choice=None, model=deployment_name, json_mode=False): try: params = { \u0026#39;model\u0026#39;: model, \u0026#39;messages\u0026#39;: messages, \u0026#39;tools\u0026#39;: tools, \u0026#39;tool_choice\u0026#39;: tool_choice } if json_mode: params[\u0026#39;response_format\u0026#39;] = { \u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34; } response = client.chat.completions.create(**params) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e def generate_test_cases_from_tools(tools, examples): tools_descriptions = \u0026#34;\\n\u0026#34;.join([ f\u0026#34;- {tool[\u0026#39;function\u0026#39;][\u0026#39;name\u0026#39;]}: {tool[\u0026#39;function\u0026#39;][\u0026#39;description\u0026#39;]} (Parameters: {\u0026#39;, \u0026#39;.join(tool[\u0026#39;function\u0026#39;][\u0026#39;parameters\u0026#39;][\u0026#39;properties\u0026#39;].keys())})\u0026#34; for tool in tools ]) prompt = f\u0026#34;\u0026#34;\u0026#34; Create a series of test cases for evaluating the tool-using capabilities of a language model. The test cases should be relevant to the domain and should utilize the provided tools. Each test case should include the following fields: - query: A question or request related to the domain. - expected_function: The function that should be called to answer the query. - function_arguments: The arguments that should be passed to the function. - expected_response: The expected response from the function. The available tools are: {tools_descriptions} Here are some examples to follow: {examples} Now, generate 10 more test cases in the same format utilizing the provided tools. Make sure the output format is JSON and enclose the result in triple backticks (```): \u0026#34;\u0026#34;\u0026#34; return prompt # Example usage tools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_medical_definition\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the definition of a medical term\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;term\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The medical term to define\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;term\u0026#34;] } } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_treatment_options\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get treatment options for a disease\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;disease\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The disease to get treatment options for\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;disease\u0026#34;] } } } ] examples = \u0026#34;\u0026#34;\u0026#34; [ { \u0026#34;query\u0026#34;: \u0026#34;What is diabetes?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_medical_definition\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;term\u0026#34;: \u0026#34;diabetes\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Diabetes is a chronic condition that affects the way the body processes blood sugar (glucose).\u0026#34; }, { \u0026#34;query\u0026#34;: \u0026#34;How can hypertension be treated?\u0026#34;, \u0026#34;expected_function\u0026#34;: \u0026#34;get_treatment_options\u0026#34;, \u0026#34;function_arguments\u0026#34;: {\u0026#34;disease\u0026#34;: \u0026#34;hypertension\u0026#34;}, \u0026#34;expected_response\u0026#34;: \u0026#34;Treatment options for hypertension include lifestyle changes, such as diet and exercise, and medications like ACE inhibitors, beta-blockers, and diuretics.\u0026#34; } ] \u0026#34;\u0026#34;\u0026#34; sys_prompt = generate_test_cases_from_tools(tools, examples) messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: sys_prompt} ] resp = chat_completion_request(messages, json_mode=True) test_cases = eval(resp.choices[0].message.content) 参考 Function-calling \u0026amp; JSON-mode Evaluation\nBenchmarking Agent Tool Use\nWhat Are Tools Anyway? A Survey from the Language Model Perspective\nBerkeley Function Calling Leaderboard\nToolformer\nLATM\nAPI-Bank\nTool-bench\nToolAlpaca\n","permalink":"https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B%E7%9A%84tool-using%E8%83%BD%E5%8A%9B/","summary":"\u003cp\u003e本文将简单介绍如何评价LLM的tool-using 能力。\u003c/p\u003e\n\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e在工具使用评估方面，过去的研究主要有以下几种思路：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e对比工具使用和纯LLM在基准测试上的分数\u003c/strong\u003e：例如\u003ca href=\"https://arxiv.org/abs/2302.04761\"\u003eToolformer\u003c/a\u003e和\u003ca href=\"https://arxiv.org/abs/2305.17126\"\u003eLATM\u003c/a\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e在Toolformer研究中，通过下游任务如语言模型评估基准测试、数学推理任务和问答任务来验证工具使用的有效性。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLATM则采用了来自BigBench的六个数据集进行评估。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e测试工具使用的准确率和响应质量\u003c/strong\u003e：例如\u003ca href=\"https://arxiv.org/abs/2304.08244\"\u003eAPI-Bank\u003c/a\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在评估过程中，首先初始化评估系统，确保每个API的数据库包含默认值。然后，将预测的API调用与手动标注的API调用进行比较，以确定它们的一致性。响应评估则使用ROUGE-L指标。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e利用LLM对工具使用的效果进行评价\u003c/strong\u003e：例如Tool-bench。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003etwo evaluation metrics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePass Rate\u003c/strong\u003e: Calculates the proportion of successfully completing an instruction within limited OpenAI API calls.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePreference\u003c/strong\u003e: Measured by comparing two answers (action sequences) for a given instruction.We pre-define a set of criteria for a better answer, which are organized as prompts for ChatGPT.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e构造虚拟运行环境，测试代理与环境的交互结果\u003c/strong\u003e：例如ToolAlpaca。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e利用LLM模拟环境（用户代理和助手代理），并使用GPT-4对ToolAlpaca模型进行机器评估，评估其使用各种未见工具的能力。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于绝大多数企业和垂直场景下， 其中思路1需要构建额外的测试集成本比较高（但还是有必要的）， 而思路4构造虚拟运行环境实际上并不现实； 所以还是选择思路2，外加通过思路3辅助判断；换句话说， 根据场景，构造工具列表和工具调用的ground-truth（包括函数名， 和传入参数）  ，在存在歧义时，采用ROUGE评价响应质量， 或者使用LLM判断响应结果并评价。\u003c/p\u003e\n\u003cp\u003e顺带提一下Langchain 项目中有关Agent的tool-using能力测试的内容， 不过由于Langchain项目又臭又长，且有很大的局限性， 这里只讨论其思路。\u003c/p\u003e","title":"Agent学习笔记： 如何验证模型的tool-using能力"},{"content":"上周看了《坂本龙一：Opus》，特别喜欢其中有一首未收录在之前作品中的曲目《for Johann》\n第一反应是这首曲子可能是献给已故的冰岛作曲家Johann Johannsson，也是我最喜欢的音乐人之一\n不过，我找不到确切的证据\n万一是巴赫（Johann Sebastian Bach） 也有可能\n总觉得这首歌的旋律走向和Johann Johannsson的作品有某种神似，也有可能是我的错觉罢了吧\n试着弹一下这首歌， 顺带用AnthemScore扒了下谱子 可能有些小错误，但我希望能将这份音乐传递下去\n希望大家喜欢\nHe’s gone, but the music remains.\nありがとうございます\n感谢教授的最后礼物， 也感谢同样被迫包场的另两位陌生人\n","permalink":"https://niraya666.github.io/musik/for_johann/","summary":"\u003cp\u003e上周看了《坂本龙一：Opus》，特别喜欢其中有一首未收录在之前作品中的曲目《for Johann》\u003c/p\u003e\n\u003cp\u003e第一反应是这首曲子可能是献给已故的冰岛作曲家Johann Johannsson，也是我最喜欢的音乐人之一\u003c/p\u003e\n\u003cp\u003e不过，我找不到确切的证据\u003c/p\u003e\n\u003cp\u003e万一是巴赫（Johann Sebastian Bach） 也有可能\u003c/p\u003e\n\u003cp\u003e总觉得这首歌的旋律走向和Johann Johannsson的作品有某种神似，也有可能是我的错觉罢了吧\u003c/p\u003e\n\u003cp\u003e试着弹一下这首歌， 顺带用AnthemScore扒了下谱子\n可能有些小错误，但我希望能将这份音乐传递下去\u003c/p\u003e\n\u003cp\u003e希望大家喜欢\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/music/for_johnann/1.png\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/img/music/for_johnann/2.png\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/img/music/for_johnann/3.png\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/img/music/for_johnann/4.png\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHe’s gone, but the music remains.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eありがとうございます\u003c/p\u003e\n\u003cp\u003e感谢教授的最后礼物， 也感谢同样被迫包场的另两位陌生人\u003c/p\u003e","title":"【钢琴谱分享】坂本龙一《for Johann》"},{"content":"2024年 夏\n这是我第一次踏上泉州这片土地，虽然我是厦门人，已经快三十岁了，却一直没有机会来这座近在咫尺的城市。\n泉州在我想象中，是一座充满历史韵味的地方，同时也被一些暴发户的繁荣所点缀。几年前，这座城市成功申遗，从此逐渐成为小众旅游的热门选择。泉州的古老街区、传统建筑和浓厚的文化氛围，让人忍不住想一探究竟。\n大学时，我有一位好友家在泉州。我们是班里唯二的福建人，因此自然成了很好的朋友。每次听他讲起泉州的美食、风景，总让我心生向往。\n这次借着回家的机会，我终于决定亲自走一趟泉州。\n泉州，我来了！\n初印象 初到泉州，路上穿梭的电动车给我留下了深刻的第一印象。这些电动车在街头巷尾自由穿行，让我一度产生了身处越南的错觉。或许，这正是泉州的独特之处——在传统与现代之间，在历史与现实之中，找到一种和谐的平衡。\n作为古城，鲤城区似乎尽可能地保留着过去的模样。漫步在古城，却能产生一种记忆中的故乡的错觉，这一切似乎都太像了。\n那些熟悉的景象和气息，让人倍感亲切。有人说过，“离开了，故乡才称之为故乡。”庆幸的是，故乡以另一种形式呈现在我的面前，尽管这不是我的故乡。\n在鲤城区，街头的小贩、古早风格的便利店、上了年纪的理发店，无时无刻不把我拉回到记忆的深处。\n人 历史离不开人，特别是一个个普通人\n正是这些普通人，构成了这座城市的血脉与灵魂。他们的日常琐事，他们的勤劳与坚韧，使得这座古老的城市充满了生机与活力。在他们的身上，我看到了泉州的过去、现在和未来。\n宗教 泉州作为海上丝绸之路的起点，在那辉煌的历史长河中，不仅是贸易和文化交流的中心，也吸引了大量的外来文化和宗教。佛教、道教、伊斯兰教、基督教等多种宗教在这里交汇融合，形成了独特的宗教文化景观。经过千年的融合与发展，这些宗教在泉州和谐共存，互相尊重，不分彼此。\n吃 牛肉店 阿秋牛排馆\n牛排并不是传统意义上的steak，而是采用香料和咖喱炖煮而成的牛排骨。招牌牛排非常推荐，炖得非常软烂，入口即化，肉质瞬间脱骨。咖喱的香气与牛肉的原汁原味相得益彰，不会掩盖肉本身的美味。用汤汁拌饭，可谓是一绝。\n不过，对于从小习惯了晋江牛肉店的闽南人来说，阿秋牛排馆的汤味道可能稍显清淡，不够浓郁，因此未必合所有人的口味。此外，这家店的价格也比其他店稍高一些。\n除此之外，我的收藏夹里还有几家值得尝试的牛排店：\n阿波牛肉店\n天财牛肉店\n阿泉全牛馆\n面线糊 平阿面线糊\n我去的这家平阿面线糊，在周末早上8点左右已经挤满了人，足见其受欢迎程度。\n与厦门地区加入了猪血、虾米的面线糊相比，泉州的面线糊更多地呈现出字面上的意思——就是很单纯的面线糊。所用的面线更加细碎，完全不用担心吃太慢会坨，因为这甚至可以用面线汤来形容。但喝上一口汤，你会发现这绝不是一碗普通的面线汤，在汤头上，店家一定是下足了功夫的。\n食客可以根据自己的喜好添加额外的食材，这种模式在闽南地区基本相同。在泉州，根据我对本地食客的观察，基本上必加的食材有：醋肉、卤大肠、套肠、豆干等等，当然绝对不能忘了加一根油条。\n除了平阿面线糊，我还收藏了几家值得一试的面线糊店：\n水门国仔老店\n后城面线糊\n文啊面线糊\n烧肉粽 东街肉粽店\n“烧”在闽南语中的含义既包含了特定的烹饪方法，也体现了食物的热度（与“烫”同音）。在“烧肉粽”中，“烧”不仅指将肉粽通过炖、煮、蒸等方式烹饪至入味，还强调了粽子食用时热乎的特质。\n与在厦门吃的肉粽不同，除了甜辣酱外，泉州的肉粽更喜欢加入花生酱，增添了一份独特的香气和口感。\n原本计划去侯阿婆烧肉粽，但可惜饭点人太多，只能临时改变计划，选择了不远处的东街肉粽店。不过这家感觉有些失望，对粽子没有太多感觉，店里的海蛎煎下的油太多，实在无法恭维。\n可惜这次时间有限，还有很多地方想去但没来得及去， 有很多想吃的没吃上\n下次再来\n","permalink":"https://niraya666.github.io/travel/%E6%B3%89%E5%B7%9E%E8%A1%8C%E8%AE%B0%E5%8F%A4%E5%9F%8E%E6%BC%AB%E6%AD%A5%E4%B8%8E%E5%91%B3%E8%95%BE%E4%B9%8B%E6%97%85/","summary":"\u003cp\u003e2024年 夏\u003c/p\u003e\n\u003cp\u003e这是我第一次踏上泉州这片土地，虽然我是厦门人，已经快三十岁了，却一直没有机会来这座近在咫尺的城市。\u003c/p\u003e\n\u003cp\u003e泉州在我想象中，是一座充满历史韵味的地方，同时也被一些暴发户的繁荣所点缀。几年前，这座城市成功申遗，从此逐渐成为小众旅游的热门选择。泉州的古老街区、传统建筑和浓厚的文化氛围，让人忍不住想一探究竟。\u003c/p\u003e\n\u003cp\u003e大学时，我有一位好友家在泉州。我们是班里唯二的福建人，因此自然成了很好的朋友。每次听他讲起泉州的美食、风景，总让我心生向往。\u003c/p\u003e\n\u003cp\u003e这次借着回家的机会，我终于决定亲自走一趟泉州。\u003c/p\u003e\n\u003cp\u003e泉州，我来了！\u003c/p\u003e\n\u003ch2 id=\"初印象\"\u003e初印象\u003c/h2\u003e\n\u003cp\u003e初到泉州，路上穿梭的电动车给我留下了深刻的第一印象。这些电动车在街头巷尾自由穿行，让我一度产生了身处越南的错觉。或许，这正是泉州的独特之处——在传统与现代之间，在历史与现实之中，找到一种和谐的平衡。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/50F93B5D-5373-486C-A785-4BB341FD0679_1_105_c.jpeg\" alt=\"50F93B5D-5373-486C-A785-4BB341FD0679_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e作为古城，鲤城区似乎尽可能地保留着过去的模样。漫步在古城，却能产生一种记忆中的故乡的错觉，这一切似乎都太像了。\u003c/p\u003e\n\u003cp\u003e那些熟悉的景象和气息，让人倍感亲切。有人说过，“离开了，故乡才称之为故乡。”庆幸的是，故乡以另一种形式呈现在我的面前，尽管这不是我的故乡。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/458F4F0B-B35C-4AC9-A447-0A38E69CC35D_1_105_c.jpeg\" alt=\"458F4F0B-B35C-4AC9-A447-0A38E69CC35D_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e在鲤城区，街头的小贩、古早风格的便利店、上了年纪的理发店，无时无刻不把我拉回到记忆的深处。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/218DC3D6-D630-49FE-9796-64202907D7E3_1_105_c.jpeg\" alt=\"218DC3D6-D630-49FE-9796-64202907D7E3_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"人\"\u003e人 \u003c/h2\u003e\n\u003cp\u003e历史离不开人，特别是一个个普通人\u003c/p\u003e\n\u003cp\u003e正是这些普通人，构成了这座城市的血脉与灵魂。他们的日常琐事，他们的勤劳与坚韧，使得这座古老的城市充满了生机与活力。在他们的身上，我看到了泉州的过去、现在和未来。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/E5CBBA91-C9A6-4F93-853D-009FD2D568C5_1_105_c.jpeg\" alt=\"E5CBBA91-C9A6-4F93-853D-009FD2D568C5_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/BC390CC0-02B4-45A6-9266-1C7638C75D76_1_105_c.jpeg\" alt=\"BC390CC0-02B4-45A6-9266-1C7638C75D76_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/3F165220-A99D-4E4F-92AB-94F572A0B231_1_105_c.jpeg\" alt=\"3F165220-A99D-4E4F-92AB-94F572A0B231_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/0DA901A1-F620-4B8A-A722-63E95EDAA536_1_105_c.jpeg\" alt=\"0DA901A1-F620-4B8A-A722-63E95EDAA536_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/46A3B7FB-2306-4661-8B47-7DEDB3FF20AE_1_105_c.jpeg\" alt=\"46A3B7FB-2306-4661-8B47-7DEDB3FF20AE_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"宗教\"\u003e宗教\u003c/h2\u003e\n\u003cp\u003e泉州作为海上丝绸之路的起点，在那辉煌的历史长河中，不仅是贸易和文化交流的中心，也吸引了大量的外来文化和宗教。佛教、道教、伊斯兰教、基督教等多种宗教在这里交汇融合，形成了独特的宗教文化景观。经过千年的融合与发展，这些宗教在泉州和谐共存，互相尊重，不分彼此。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/22205648-89A8-41BE-B99C-56850A28D252_1_105_c.jpeg\" alt=\"22205648-89A8-41BE-B99C-56850A28D252_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/BB0999F1-EDEA-4AA2-BA72-D6954CD12BA8_1_105_c.jpeg\" alt=\"BB0999F1-EDEA-4AA2-BA72-D6954CD12BA8_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/5A6D2D91-D86E-4BBA-9248-272095117CE9_1_105_c.jpeg\" alt=\"5A6D2D91-D86E-4BBA-9248-272095117CE9_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"吃\"\u003e吃\u003c/h2\u003e\n\u003ch3 id=\"牛肉店\"\u003e牛肉店\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/FEA7D1E0-80AD-45BC-B2F9-E3BF31A1130A_1_201_a.jpg\" alt=\"FEA7D1E0-80AD-45BC-B2F9-E3BF31A1130A_1_201_a.jpg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e阿秋牛排馆\u003c/p\u003e\n\u003cp\u003e牛排并不是传统意义上的steak，而是采用香料和咖喱炖煮而成的牛排骨。招牌牛排非常推荐，炖得非常软烂，入口即化，肉质瞬间脱骨。咖喱的香气与牛肉的原汁原味相得益彰，不会掩盖肉本身的美味。用汤汁拌饭，可谓是一绝。\u003c/p\u003e\n\u003cp\u003e不过，对于从小习惯了晋江牛肉店的闽南人来说，阿秋牛排馆的汤味道可能稍显清淡，不够浓郁，因此未必合所有人的口味。此外，这家店的价格也比其他店稍高一些。\u003c/p\u003e\n\u003cp\u003e除此之外，我的收藏夹里还有几家值得尝试的牛排店：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e阿波牛肉店\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e天财牛肉店\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e阿泉全牛馆\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"面线糊\"\u003e面线糊\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/FC2DD610-927E-4E1E-829A-420ACD1A0418_1_105_c.jpeg\" alt=\"FC2DD610-927E-4E1E-829A-420ACD1A0418_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e平阿面线糊\u003c/p\u003e\n\u003cp\u003e我去的这家平阿面线糊，在周末早上8点左右已经挤满了人，足见其受欢迎程度。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/0615C07D-8BA9-484A-BBFC-FF59E764955B_1_201_a.jpg\" alt=\"0615C07D-8BA9-484A-BBFC-FF59E764955B_1_201_a.jpg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e与厦门地区加入了猪血、虾米的面线糊相比，泉州的面线糊更多地呈现出字面上的意思——就是很单纯的面线糊。所用的面线更加细碎，完全不用担心吃太慢会坨，因为这甚至可以用面线汤来形容。但喝上一口汤，你会发现这绝不是一碗普通的面线汤，在汤头上，店家一定是下足了功夫的。\u003c/p\u003e\n\u003cp\u003e食客可以根据自己的喜好添加额外的食材，这种模式在闽南地区基本相同。在泉州，根据我对本地食客的观察，基本上必加的食材有：醋肉、卤大肠、套肠、豆干等等，当然绝对不能忘了加一根油条。\u003c/p\u003e\n\u003cp\u003e除了平阿面线糊，我还收藏了几家值得一试的面线糊店：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e水门国仔老店\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e后城面线糊\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e文啊面线糊\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"烧肉粽\"\u003e烧肉粽\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e6%b3%89%e5%b7%9e%e8%a1%8c%e8%ae%b0%ef%bc%9a%e5%8f%a4%e5%9f%8e%e6%bc%ab%e6%ad%a5%e4%b8%8e%e5%91%b3%e8%95%be%e4%b9%8b%e6%97%85-assets/F2CA54E0-ADE3-42B7-A243-D90C9B559D7B_1_105_c.jpeg\" alt=\"F2CA54E0-ADE3-42B7-A243-D90C9B559D7B_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e东街肉粽店\u003c/p\u003e\n\u003cp\u003e“烧”在闽南语中的含义既包含了特定的烹饪方法，也体现了食物的热度（与“烫”同音）。在“烧肉粽”中，“烧”不仅指将肉粽通过炖、煮、蒸等方式烹饪至入味，还强调了粽子食用时热乎的特质。\u003c/p\u003e\n\u003cp\u003e与在厦门吃的肉粽不同，除了甜辣酱外，泉州的肉粽更喜欢加入花生酱，增添了一份独特的香气和口感。\u003c/p\u003e\n\u003cp\u003e原本计划去侯阿婆烧肉粽，但可惜饭点人太多，只能临时改变计划，选择了不远处的东街肉粽店。不过这家感觉有些失望，对粽子没有太多感觉，店里的海蛎煎下的油太多，实在无法恭维。\u003c/p\u003e","title":"泉州行记：古城漫步与味蕾之旅"},{"content":"2024-春夏之交 大阪\n旅行时间2天3夜\n在多邻国上学了快一年的散装日语，受到了看大河剧、日本电影和玩日本游戏的影响，我决定利用刚获得的三年多次入境签证，以及51假期的空闲时间，去日本看看。考虑到时间限制，我选择了关西的交通枢纽和经济中心——大阪，作为这次日本之行的第一站。\n对于日本，尤其是大阪，我的第一印象是热门景区和场所人非常多。这主要是因为赶上了日本的10天黄金周，同时由于日元汇率低迷，大量外国人涌入日本消费和旅游。然而，在居民区，特别是我所住的浪速区，情况则大为不同，晚上和清晨几乎见不到几个人，给人一种非常萧条的感觉。中国人会说，这叫做“缺乏人气”。此外，随处可见的大乌鸦加深了这种萧条感。或许这也可能是宫崎英高在《黑魂》系列中乌鸦的灵感来源吧。在日本，乌鸦似乎被视为吉祥的象征。\n游玩篇 出了民宿，我的第一站是不远处的难波八阪神社。这里以其狮子头形状的大狮子殿闻名，成为了一处热门的打卡地点。据说，这个狮子头具有驱除灾难和带来好运的力量。\n经过两站地下铁，即可到达动物园前站， 这里是新世界的入口。\n新世界，曾是日本向西方看齐、力图“脱亚入欧”时的产物。在20世纪初，日本大量吸纳西方文化元素，并在大阪建立了这一街区。然而时光流转，21世纪的今天，这里更像是一个保存良好的“旧世界”。街区内充满昭和时代的气息，从游戏厅、炸串店到浮夸的店面招牌，甚至粉红色影院，处处透露出时代的印记。\n作为泡沫经济时期产物， 粉红影院现在主要吸引了一部分中老年男性和非传统性别的人士。影院的“学生半价”标志显得有些讽刺，同时也透露出一种时代变迁的哀愁。鉴于这类场所的特殊性，不推荐旅行者单独前往。\n新世界的标志性建筑通天阁，则见证了区域的多重历史。这座铁塔最初在1912年仿照巴黎的埃菲尔铁塔建造，但在二战中因为“献纳”政策而被拆除，用其材料支持军工。通天阁的现代版本建于1956年，为了迎接万博会而重建，塔身醒目地印有日立的广告，成为了大阪的新地标。\n西成区，与新世界仅一路之隔，是大阪的旧城区。这个区域以较复杂的治安情况、流浪汉的较高出现率，以及成人娱乐业而知名。其中，著名的酒吧一条街和在中文互联网上广为人知的飞田新地均位于此地。游客在此区域应特别小心，避免随意使用手机或拍照，以尊重当地文化和增加个人安全。\n往新世界东北边走， 则能到达天王寺车站，商圈，以及以此命名此地的四天王寺。这里的天王寺车站作为从关西机场到大阪市区的两条主要电车线路之一JR西日本的终点站，同前面走过的西成区和新世界相比， 自然非常繁华。\n天王寺站前\nJR西日本机场线之HARUKA， 以Hello- Kitty涂装著称。\n日本的许多古迹因二战的轰炸和历史上的大地震而稀少，多数是在原址上修复或重建的。四天王寺就是这样的例子，它是日本历史悠久的寺庙之一，最初由飞鸟时期的圣德太子创建。虽然历经多次战火，我们今天所见的四天王寺主要是1957年后的重建。\n前往天王寺途中，偶遇的一心寺。\n在天王寺附近，我意外地发现了一个旧书集市。在那里，我用400日元买到了一本1994年的旧书。虽然还有很多书我想买想看，但考虑到海关的限制和行李空间的限制，我只能忍痛放弃。\n不可免俗的道顿堀格利高小人\n蟹道乐\n夜幕降临， 抵达梅田，乘坐hep five摩天轮，体验大阪夜景。\nhep five摩天轮\n第二天，前往大阪历史博物馆和大阪城， 以及shooping。\n大阪城\n大阪城始建于1583年，由丰臣秀吉下令建造。大阪城的天守阁如今是博物馆，展示了丰臣秀吉的生平、战国时期的武器和铠甲，以及大阪城的历史变迁。\n如果对于历史感兴趣的小伙伴， 大阪城公园边上的大阪历史博物馆也值得逛一逛。 大阪历史博物馆展示了古代大阪作为日本重要商业城市的发展过程，以及现代大阪的城市化进程。参观路线从10楼一路到7楼，6楼为特别展。\n日式连锁中餐王将的碳水加碳水再加碳水的定食\n剩下的shopping时间，逛了堂吉柯德，在西斋心桥发现一家二手唱片店\n位于 Big Step 大楼的地下一层，可惜根据网友的评论，似乎店面越来越小，东西越来越少了。\n似乎在日本不同城市，优衣库会有限定的特别款式， 在大阪则是御好烧（大阪烧）和章鱼烧的图案。\n美国村边上的小自由女神像\n攻略篇 入境 Visit Japan Web （VJW）为了替代入境卡而设计的， 需要在入境前提前申报，在入境时扫码即可。和纸质入境卡相比， 至少从我的观察而言， 使用VJW会更加快速，而没有存在需要排队的情况。\n交通 在大阪，从关西机场到市区有两条主要的铁路线路，南海电铁和JR西日本。两条线路隶属于不同的公司运营，在使用购票机购票时需要注意区分。平均时间在半个小时到一个小时不等， 票价在970到1800日元间，取决于所乘坐车次和制定席与否。除了特急列车（Rapi:t \u0026amp; HARUKA）,均和国内乘坐地铁没有太大差别。\n对于特急列车， 可以在网上买票，通过扫二维码进站，或者是在自助购票机上操作，以及通过刷西瓜卡进站，在候车处附近的机器上购买特急券。理论上有售票员会查票。\n赶了6:00的第一班Rapi:t alpha\n吃 居酒屋会在刚入座上小菜，也叫お通し（o-to-shi）,这部分一般是按人头收费，价格在330日元左右，一般是强制收取不给退的。\n日本餐厅一般是不提供打包服务的， 因为内用和外带属于不同执照，并且会因为害怕顾客吃坏肚子店家要负责任， 所以在日本餐厅，店家对于打包吃剩的食物十分抗拒。\n点餐时， 特别在旅游城市， 一般都会有多国语言版本或者是带图片版本的菜单， 有的店家甚至有扫码点单，可谓是非常友好的。所以， 只需要会几个日语单词即可，如\n日语单词 意思 罗马音 注文 点单 chūmon 会計 结账 kaikei メニュー 菜单 menyū 一つ 一个 hitotsu 二つ 两个 futatsu 三つ 三个 mittsu 少々 少量 shōshō 大盛り 大份 ōmori おすすめ 推荐 osusume トイレ 洗手间 toire 在便利店消费时，有几个常用的日语单词和表达可以帮助你更方便地交流。例如，店员可能会询问你是否需要袋子（ふくろ，fukuro）或加热食物（温め，atatame）。在不需要这些服务时，可以回答“大丈夫”（daijōbu，没关系）。\n在7-11一般是又店员负责扫码和年龄检查（买烟酒时），扫完后，有顾客在旁边的一台机器上选择付款方式并付款。当然，也可以直接出示Alipay 给店员， 店员会帮你操作的。\n大阪周游卡（Osaka Amazing Pass） or 大阪乐游券（Osaka e-Pass） 目前周游卡已停售(二日券)，一日券购买有效期截止到2024年6月30日。\n其中大阪周游卡需要购买后兑换成实体卡，兑换位置可查看 售票及换票点；而大阪乐游券直接展示二维码即可。二者都支持在Klook上购买。周游卡价格在¥3300包括了市区地铁随意乘坐， 乐游券价格为¥2400，只包括景点。\n用GPT-4 整理了一个周游卡和乐游券差异的表格（不能保证100%正确）：\n地点 (日本語) 地点 (中文) Location Regular fee Osaka Amazing Pass Osaka e-Pass 梅田スカイビル空中庭園展望台 梅田天空大厦空中庭园观景台 梅田 ¥ 1500 ✔️ ✔️ 梅田スカイビル・衣川邦彦天空美術館 梅田天空大厦衣川邦彦天空美术馆 梅田 ¥ 1000 ✔️ ✔️ HEP FIVE観覧車 HEP FIVE摩天轮 梅田 ¥ 600 ✔️ ✔️ 通天閣 通天阁 新世界 ¥ 1000 ✔️ ✔️ 通天閣タワースライダー 通天阁塔滑梯 新世界 ¥ 1000 ✔️ ✔️ 四天王寺 (中心伽藍・本坊庭園) 四天王寺（中心伽蓝、本坊庭园） 四天王寺 ¥ 300 ✔️ ✔️ とんぼりリバークルーズ 道顿堀河巡航 道顿堀 ¥ 1200 ✔️ ✔️ 天王寺動物園 天王寺动物园 四天王寺 ¥ 500 ✔️ ❌ 中之島川クルーズ 中之岛河巡航 北区中之岛 ¥ 1500 ✔️ ✔️ 大阪国立国際美術館 大阪国立国际美术馆 北区中之岛 ¥ 430 ✔️ ❌ 道頓堀ZAZA 道顿堀ZAZA 道顿堀 ¥ 800 ✔️ ✔️ 上方浮世絵館 上方浮世绘馆 难波 ¥ 700 ✔️ ✔️ 大阪歴史博物館 大阪历史博物馆 大阪城 ¥ 600 ✔️ ❌ 大阪平和の殿堂 大阪和平馆 大阪城 ¥ 250 ✔️ ✔️ 大阪城天守閣 大阪城天守阁 大阪城 ¥ 600 ✔️ ❌ ワンダークルーズ 奇迹巡航 ¥ 1200 ✔️ ✔️ 大阪くらしの今昔館 大阪生活今昔馆 天神橋 ¥ 600 ✔️ ✔️ 天保山大観覧車 天保山大观览车 天保山 ¥ 1200 ✔️ ❌ グリオンミュージアム GLION博物馆 天保山 ¥ 1200 ✔️ ✔️ サンタマリア日帰りクルーズ 圣玛丽亚日游巡航 ¥ 1800 ✔️ ✔️ サンタマリア夜景クルーズ 圣玛丽亚夜景巡航 ¥ 2300 ✔️ ✔️ キャプテンライン 队长线 ¥ 1500 ✔️ ✔️ レゴランド・ディスカバリー・センター大阪 大阪乐高探索中心 ¥2800～¥3000 ✔️ ✔️ 咲洲コスモタワー展望台 咲洲宇宙塔展望台 ¥1000 ✔️ ✔️ ボートレース住之江 住之江赛艇 ¥ 1500 ✔️ ✔️ 大阪水上バスアクアライナー 大阪水上巴士Aqua-Liner ¥ 1800 ✔️ ✔️ 桜川お花見クルーズ 樱川赏花巡航 ¥ 1200 ✔️ ✔️ 夕日クルーズ「寄道」 夕阳巡航“YORIMICHI” ¥ 1500 ✔️ ✔️ 大阪城御座船 大阪城御座船 ¥ 1500 ✔️ ✔️ エンタープライズイノベーションミュージアム 创业创新博物馈 ¥ 300 ✔️ ✔️ 大阪城重要文化財櫓 大阪城重要文化财瞭 ¥ 800 ✔️ ❌ 大阪城西の丸庭園 大阪城西之丸庭园 ¥ 200 ✔️ ❌ 咲くやこの花館 咲く家这花馆 ¥ 500 ✔️ ❌ 長居植物園 长居植物园 ¥ 200 ✔️ ❌ 大阪市立自然史博物館 大阪市立自然历史博物馆 ¥ 300 ✔️ ❌ 堺利晶の杜 堺利晶之森 ¥ 300 ✔️ ❌ 堺市博物館 堺市博物馆 ¥ 200 ✔️ ❌ 万博記念公園（自然文化園、日本庭園） 万博纪念公园（自然文化园、日本庭园） ¥ 260 ✔️ ❌ OSAKA WHEEL（日本最大の観覧車） 大阪摩天轮（日本最大） 天保山 ¥ 1000 ✔️ ✔️ 大阪科学館 大阪科学馆 ¥ 400 ✔️ ❌ 桂陵園 桂陵园 ¥ 150 ✔️ ❌ 日向の湯 日向温泉 ¥ 800 ❌ ✔️ 海洋堂ホビーランド 海洋堂模型乐园 ¥ 1000 ❌ ✔️ 忍者トリックハウス（エディオン難波本店） 忍者特技屋（EDION难波本店） ¥ 1000 ❌ ✔️ 道頓堀ミュージアムなみき座 道顿堀剧场 ❌ ✔️ 更多的攻略建议参考：小气少年的部落格 这是我所发现的最完整的日本旅行攻略，成功解决了我的旅行前焦虑。 此外，在推荐一本书：《日本點餐完全圖解》，这本书详细介绍了在日本点餐的注意事项，甚至教会你怎么吃。\n感受篇 在这次的旅途中，我在网上上结识了两位旅游搭子，分别各搭了一天。\nA是一位出生于1998年的年轻人，帅气的外表下藏着近期感情伤痕的痛楚。他独自一人前来关西，试图通过旅行洗涤感情上的创伤。尽管我不愿轻易给人贴上标签，诸如“渣男”之类的称呼，但A的行为难免让人费解：他同时与四五名女生保持联系，不断在Tinder上寻找新的可能，对每一个路过的女性都有自己的评价。然而，从他的话语中，我逐渐感知到他背后隐藏的故事——那些使他变得如此的伤痕累累。通过他，我也间接了解了日本的一些风俗业及他的个人感受体验。\nR则是一位三十多岁的体制内员工，表面上看似普通，内心却充满了对现状的不满和对自由的渴望。他的思想深受那个时代的河殇文化与十年前“意林”文学的影响。在他的眼中，日本社会的方方面面几乎都是模范，而国内的绝大多数则恰恰相反。他甚至考虑过一个大胆的想法：辞去体制内的工作，“润”到日本。用互联网上的用语形容他的思想就是妥妥“反贼”和“润人”，但又何妨。R还是个爱酒如命的人。\n日本人究竟相信什么？道德伦理？法律？还是抽象的社会潜规则 事实上，日本社会的一个显著特点是高度的循规蹈矩。人们极其重视集体，与社会脱节被视为所能想到的最严酷的惩罚之一。也许正是因为日本人对意识形态和教条缺乏关注，他们格外重视外在形象，注重符合各种场合的姿态，因为“面子”比“里子”更重要。这演化成了一种行为模式，即在私下里怎么搞都行，但在公众面前一定要维护自己的“形象”。这意味着日本人本质上追求的是集体认同，极度避免被社会边缘化。\n“日本的情况则不同，原始信仰从未遭到更精致的官方律条的碾压。尽管不同宗教之间的区分不甚清晰，尤其是在最广泛的层面上，但佛寺和神社仍然共存并立。两种教派的仪式人们都参与，尽管不一定同时同地。这或许是因为日本人对意识形态和教条缺乏关注，却格外重视外在，重视符合各种场合的姿态，因为“面子”比“里子”更重要。”\n在这种文化氛围下，个人行为被严格的社会规范和集体利益所约束。脱离集体，被视为严重的社会惩罚，比法律制裁更为严酷。社会关系和集体认同对个人生活至关重要，因此日本人普遍面临巨大的心理压力，必须时刻注意自己的行为是否符合社会期望，以避免被边缘化。\n这种面子文化强调外在形象的维护。日本人非常重视在各种场合的表现和形象管理，不论是在公共场合的礼仪，还是在工作中的表现，维护良好的外在形象是必不可少的。在私下场合，日本人可能会表现出更大的灵活性和自由度，但在公开场合，他们会极力维护自己的公众形象，遵守社会规范和礼仪，以获得集体的认可和尊重。\n不论是飞田新地的挂羊头卖狗肉，还是游戏厅的老虎机，日本人总是有能力绕过所谓的规则，显示出一种极强的变通能力；但在公共区域场合下， 日本人又总能展现出一种死板地对于规则的严格遵守。\n日本社会的这种双重性在于，私下和公开行为的巨大反差。这种现象可以通过对集体认同和避免边缘化的强烈需求来解释。正如《日本之镜：日本文化中的英雄与恶人》一书中所提到的，“制约日本人行为的不是社会规则，而是一套抽象的道德教训。”\n然而，这种“抽象的道德”在中文语境下显得陌生。中国古代追求的是儒家的仁义道德，而日本的道德观念更多来源于神道教的泛神信仰，强调避免在社会上出丑。例如，须佐之男被放逐的故事就是一种道德教训，提醒人们避免类似的羞辱。日本人常说，只要不被逮个正着，避免在社会上出丑，就能够为所欲为，不必担心后果。\n","permalink":"https://niraya666.github.io/travel/%E5%A4%A7%E9%98%AA%E6%B8%B8%E8%AE%B0%E6%97%B6%E5%85%89%E4%BA%A4%E9%94%99%E4%B8%8B%E7%9A%84%E6%97%A5%E6%9C%AC%E6%8E%A2%E7%B4%A2/","summary":"\u003cp\u003e2024-春夏之交 大阪\u003c/p\u003e\n\u003cp\u003e旅行时间2天3夜\u003c/p\u003e\n\u003cp\u003e在多邻国上学了快一年的散装日语，受到了看大河剧、日本电影和玩日本游戏的影响，我决定利用刚获得的三年多次入境签证，以及51假期的空闲时间，去日本看看。考虑到时间限制，我选择了关西的交通枢纽和经济中心——大阪，作为这次日本之行的第一站。\u003c/p\u003e\n\u003cp\u003e对于日本，尤其是大阪，我的第一印象是热门景区和场所人非常多。这主要是因为赶上了日本的10天黄金周，同时由于日元汇率低迷，大量外国人涌入日本消费和旅游。然而，在居民区，特别是我所住的浪速区，情况则大为不同，晚上和清晨几乎见不到几个人，给人一种非常萧条的感觉。中国人会说，这叫做“缺乏人气”。此外，随处可见的大乌鸦加深了这种萧条感。或许这也可能是宫崎英高在《黑魂》系列中乌鸦的灵感来源吧。在日本，乌鸦似乎被视为吉祥的象征。\u003c/p\u003e\n\u003ch2 id=\"游玩篇\"\u003e游玩篇\u003c/h2\u003e\n\u003cp\u003e出了民宿，我的第一站是不远处的难波八阪神社。这里以其狮子头形状的大狮子殿闻名，成为了一处热门的打卡地点。据说，这个狮子头具有驱除灾难和带来好运的力量。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/D22F4C9A-23D6-472A-B359-5A9DC656DE03_1_201_a.jpeg\" alt=\"D22F4C9A-23D6-472A-B359-5A9DC656DE03_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e经过两站地下铁，即可到达动物园前站， 这里是新世界的入口。\u003c/p\u003e\n\u003cp\u003e新世界，曾是日本向西方看齐、力图“脱亚入欧”时的产物。在20世纪初，日本大量吸纳西方文化元素，并在大阪建立了这一街区。然而时光流转，21世纪的今天，这里更像是一个保存良好的“旧世界”。街区内充满昭和时代的气息，从游戏厅、炸串店到浮夸的店面招牌，甚至粉红色影院，处处透露出时代的印记。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/6DA5908E-2DB7-48B0-9254-7B189FB7848A_1_201_a.jpeg\" alt=\"6DA5908E-2DB7-48B0-9254-7B189FB7848A_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/AF7FD137-8145-4DE2-91D7-116908DC6985_1_201_a.jpeg\" alt=\"AF7FD137-8145-4DE2-91D7-116908DC6985_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e作为泡沫经济时期产物， 粉红影院现在主要吸引了一部分中老年男性和非传统性别的人士。影院的“学生半价”标志显得有些讽刺，同时也透露出一种时代变迁的哀愁。鉴于这类场所的特殊性，不推荐旅行者单独前往。\u003c/p\u003e\n\u003cp\u003e新世界的标志性建筑通天阁，则见证了区域的多重历史。这座铁塔最初在1912年仿照巴黎的埃菲尔铁塔建造，但在二战中因为“献纳”政策而被拆除，用其材料支持军工。通天阁的现代版本建于1956年，为了迎接万博会而重建，塔身醒目地印有日立的广告，成为了大阪的新地标。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/6C8F7905-91AC-46AA-BEC9-79AF3A362781_1_105_c.jpeg\" alt=\"6C8F7905-91AC-46AA-BEC9-79AF3A362781_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/0B1EA1C4-DFF3-4FDA-ACF8-EFDB75E7A3C0_1_201_a.jpeg\" alt=\"0B1EA1C4-DFF3-4FDA-ACF8-EFDB75E7A3C0_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e西成区，与新世界仅一路之隔，是大阪的旧城区。这个区域以较复杂的治安情况、流浪汉的较高出现率，以及成人娱乐业而知名。其中，著名的酒吧一条街和在中文互联网上广为人知的飞田新地均位于此地。游客在此区域应特别小心，避免随意使用手机或拍照，以尊重当地文化和增加个人安全。\u003c/p\u003e\n\u003cp\u003e往新世界东北边走， 则能到达天王寺车站，商圈，以及以此命名此地的四天王寺。这里的天王寺车站作为从关西机场到大阪市区的两条主要电车线路之一JR西日本的终点站，同前面走过的西成区和新世界相比， 自然非常繁华。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/2E815B2C-F25B-4CB4-A8FB-8D993265A540_1_105_c.jpeg\" alt=\"2E815B2C-F25B-4CB4-A8FB-8D993265A540_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e天王寺站前\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/42E69559-0A2E-4F15-8B70-C4CD15948641_1_201_a.jpeg\" alt=\"42E69559-0A2E-4F15-8B70-C4CD15948641_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eJR西日本机场线之HARUKA， 以Hello- Kitty涂装著称。\u003c/p\u003e\n\u003cp\u003e日本的许多古迹因二战的轰炸和历史上的大地震而稀少，多数是在原址上修复或重建的。四天王寺就是这样的例子，它是日本历史悠久的寺庙之一，最初由飞鸟时期的圣德太子创建。虽然历经多次战火，我们今天所见的四天王寺主要是1957年后的重建。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/C316DC6A-106E-494C-9926-6F5ED52B11E3_1_105_c.jpeg\" alt=\"C316DC6A-106E-494C-9926-6F5ED52B11E3_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/30F7FC2C-4BFA-494F-A0EC-1C07AEBA1A5E_1_105_c.jpeg\" alt=\"30F7FC2C-4BFA-494F-A0EC-1C07AEBA1A5E_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/3BFFEA31-F958-4239-99FD-23B5D8F91CFF_1_105_c.jpeg\" alt=\"3BFFEA31-F958-4239-99FD-23B5D8F91CFF_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e前往天王寺途中，偶遇的一心寺。\u003c/p\u003e\n\u003cp\u003e在天王寺附近，我意外地发现了一个旧书集市。在那里，我用400日元买到了一本1994年的旧书。虽然还有很多书我想买想看，但考虑到海关的限制和行李空间的限制，我只能忍痛放弃。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/A0D7A507-9B4F-44D5-AA9C-6B771FA520D2_1_201_a.jpeg\" alt=\"A0D7A507-9B4F-44D5-AA9C-6B771FA520D2_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/96FA58A9-C223-40F4-9BC8-33B6A50D4BF0_1_105_c.jpeg\" alt=\"96FA58A9-C223-40F4-9BC8-33B6A50D4BF0_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e不可免俗的道顿堀格利高小人\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/17DC7E7D-8CD2-4FCD-A19E-EC4BCE661FF1_1_105_c.jpeg\" alt=\"17DC7E7D-8CD2-4FCD-A19E-EC4BCE661FF1_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e蟹道乐\u003c/p\u003e\n\u003cp\u003e夜幕降临， 抵达梅田，乘坐hep five摩天轮，体验大阪夜景。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/90FE62D5-F899-43B8-BBB7-D44B242C2CEB_1_201_a.jpeg\" alt=\"90FE62D5-F899-43B8-BBB7-D44B242C2CEB_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003ehep five摩天轮\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/5C716546-81DE-4FE8-BEAF-EA239F27DAB4_1_201_a.jpeg\" alt=\"5C716546-81DE-4FE8-BEAF-EA239F27DAB4_1_201_a.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e第二天，前往大阪历史博物馆和大阪城， 以及shooping。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%a4%a7%e9%98%aa%e6%b8%b8%e8%ae%b0/FDCCBE76-D57A-4EC6-8D79-D19D9066390F_1_105_c.jpeg\" alt=\"FDCCBE76-D57A-4EC6-8D79-D19D9066390F_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e大阪城\u003c/p\u003e\n\u003cp\u003e大阪城始建于1583年，由丰臣秀吉下令建造。大阪城的天守阁如今是博物馆，展示了丰臣秀吉的生平、战国时期的武器和铠甲，以及大阪城的历史变迁。\u003c/p\u003e\n\u003cp\u003e如果对于历史感兴趣的小伙伴， 大阪城公园边上的大阪历史博物馆也值得逛一逛。 大阪历史博物馆展示了古代大阪作为日本重要商业城市的发展过程，以及现代大阪的城市化进程。参观路线从10楼一路到7楼，6楼为特别展。\u003c/p\u003e","title":"大阪游记：时光交错下的日本探索"},{"content":"为什么要进行文本分块？ 大型语言模型（LLM）和嵌入式模型都有其处理文本的长度限制。为了有效处理超出这些限制的长文本，我们需要进行文本分块。\n如何进行文本分块 **块分割戒律：**我们的目标不仅仅是为了划分数据块。更重要的是，我们要以一种便于日后检索和提取价值的格式来整理我们的数据。\n文本分块的根本目的不仅仅是简单地将数据分割成块。更重要的是，我们要以便于检索和提取价值的方式来组织这些数据块。以下是几种常见的分块逻辑：\n句子分割（Sentence Splitting） 这种方法只是简单地将文本切分成N个字符的片段，不考虑内容或形式的连贯性。\ntext = \u0026#34;...\u0026#34; # 你的文本 from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter( separator = \u0026#34;\\n\\n\u0026#34;, chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 递归字符文本分割（Recursive Character Text Splitting） 这种方法首先尝试用一个分隔符将文本切分成小块。如果初始尝试未能达到预期的块大小或结构，就递归地用不同的分隔符重新分割结果块，直到得到满意的结果。\ntext = \u0026#34;...\u0026#34; # 你的文本 from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size = 256, chunk_overlap = 20 ) docs = text_splitter.create_documents([text]) 根据langchain 的默认分隔条件 [\u0026quot;\\n\\n\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;] ,也就是会将text根据该分割条件的顺序（两个换行-\u0026gt;一个换行-\u0026gt;空格）将文本进行递归分割。\n针对特定文档的分割方法（Document Specific Splitting） 例如针对Markdown或Python代码的分割策略，这些策略根据文档的结构来进行分割，如类定义或Markdown标题。\nlangchain 提供了一些常见文档的分割方法：\nmardown的分割逻辑\n[ # First, try to split along Markdown headings (starting with level 2) \u0026#34;\\n#{1,6} \u0026#34;, # Note the alternative syntax for headings (below) is not handled here # Heading level 2 # --------------- # End of code block \u0026#34;```\\n\u0026#34;, # Horizontal lines \u0026#34;\\n\\\\*\\\\*\\\\*+\\n\u0026#34;, \u0026#34;\\n---+\\n\u0026#34;, \u0026#34;\\n___+\\n\u0026#34;, # Note that this splitter doesn\u0026#39;t handle horizontal lines defined # by *three or more* of ***, ---, or ___, but this is not handled \u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;, ] python的分割逻辑：\n[ # First, try to split along class definitions \u0026#34;\\nclass \u0026#34;, \u0026#34;\\ndef \u0026#34;, \u0026#34;\\n\\tdef \u0026#34;, # Now split by the normal type of lines \u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;\u0026#34;, ] 表格 重点提示： 后来在我们对数据块进行语义搜索时，直接从表格中匹配嵌入可能会比较困难。开发者常用的做法是，在提取了表格之后，对其进行关键信息提取。然后对这些关键信息的总结进行嵌入。如果这个总结的嵌入与你的搜索目标匹配，那么就可以把原始表格交给你的大语言模型处理。\n语义分块 目前的文本分块方法大多基于结构或字符数量，忽视了文本的实际语义内容。有效的语义分块应当确保内容上相似的数据被归类在一起，这样不仅有助于数据的组织，还能提升后续处理的效率和准确性。\n根据Greg的做法：寻找连续句子间的分割点， 即从第一句话开始，获取其嵌入，然后与第二句进行比较，以此类推。在寻找嵌入距离较大的“分割点”。如果距离超过了一定阈值，就认为这标志着新的语义段落的开始。\n可以通过以下步骤来进行语义分块：\n文档初始处理：首先将你的文档分割成句子，每个句子通常包含关于一个主题的单一观点。\n句子嵌入：利用语言模型获取每个句子的嵌入表示。\n句子比较与分块：从第一句话开始，逐句比较嵌入的相似度。当相邻句子之间的嵌入距离超过预设的阈值时，标记为新的语义段落的开始。\n聚类相似句子：将语义上接近的句子聚集成块，同时保持句子的原始顺序。\n伪代码示例\narticle = \u0026#34;你的完整文章\u0026#34; chunks = [article[:1]] # 默认第一部分为一个独立分块 # 文章逐句审查 for sentence in article[1:]: if is_similar(sentence, chunks[-1][-1]): chunks[-1].append(sentence) # 如果新句子与当前块的最后一个句子相似，则加入当前块 else: chunks.append([sentence]) # 如果不相似，则创建新的分块 # 持续这个过程直至文章末尾 # 在这种方法中，is_similar 函数需要定义为比较句子嵌入之间的距离，并判断它们是否足够接近。 特殊分块以及后处理 不见得一定要在召回后，返回原始的chunks。\nSummary(摘要) 处理和存储大量文档的原始数据块往往不是最高效的方法。通过为每个文档创建摘要并进行嵌入，我们不仅可以减少处理的数据量，还能提高检索的速度和准确性。\nhttps://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary\ncreate a summary for each document, embed that along with (or instead of) the document.\nimport uuid from langchain_core.documents import Document from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI chain = ( {\u0026#34;doc\u0026#34;: lambda x: x.page_content} | ChatPromptTemplate.from_template(\u0026#34;Summarize the following document:\\n\\n{doc}\u0026#34;) | ChatOpenAI(max_retries=0) | StrOutputParser() ) summaries = chain.batch(docs, {\u0026#34;max_concurrency\u0026#34;: 5}) # The vectorstore to use to index the child chunks vectorstore = Chroma(collection_name=\u0026#34;summaries\u0026#34;, embedding_function=OpenAIEmbeddings()) # The storage layer for the parent documents store = InMemoryByteStore() id_key = \u0026#34;doc_id\u0026#34; # The retriever (empty to start) retriever = MultiVectorRetriever( vectorstore=vectorstore, byte_store=store, id_key=id_key, ) doc_ids = [str(uuid.uuid4()) for _ in docs] summary_docs = [ Document(page_content=s, metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries) ] retriever.vectorstore.add_documents(summary_docs) retriever.docstore.mset(list(zip(doc_ids, docs))) Hypothetical questions HyDE\nhypothetical_document_embeddings\nHyDE (Hypothetical Document Embeddings)\nHyDE（Hypothetical Document Embeddings，假设性文档嵌入），通过生成假设性的答案并对这些生成的文档进行嵌入，而非嵌入原始文本。这种方法特别适用于用户问题简短且与需要查找的原始文本在语义上相似度不高的情况。\n当用户的问题与知识库中的原始文本在语义上相差较大时，传统的文档检索方法可能无法有效地匹配到相关内容。HyDE通过使用大型语言模型（LLMs）根据用户问题生成格式上类似于知识库中文本的假设性答案，从而提升匹配的准确率。\n尽管HyDE可以提升检索的准确性，但这种方法也会一定程度上增加检索的耗时。为了平衡检索准确率和响应速度，可以考虑使用更强大的重排模型（reranking models）。这些模型能够在检索后的阶段进一步优化结果，通过精准地评估和排序生成的假设性答案，以提供最相关的回答。\nParent Document Retriever (PDR) from Advanced RAG Techniques: an Illustrated Overview\nA.K.A Multi-size-chunks(Parent Document Retriever/ parent-child chunks retriever/ Auto Merging Retriever)\n核心思想是，尽管小片段的数据更容易在语义上与短小的用户查询匹配，但这些小片段往往缺乏提供全面回答所需的足够上下文。PDR的解决方案是在检索时优先获取这些小片段，然后识别并返回包含这些小片段的更大父文档，以此为基础进行问题的回答。\n在使用embedding模型召回时，尽可能的将对应文本拆成句子级别，因为往往人的问题都是比较短的句子，这样在语义匹配上能够较好的匹配上；但是在使用LLM做回答时，由需要足够的上下文让模型能够有足够多的背景进行回答；因此在召回的时候倾向于使用small-chunks用于匹配，再通过某些聚合方式合并成一个具备相对完整上下文的较大chunks用作模型回答之背景；\n在Langchain中提供了Parent Document Retriever这一方法。\nDuring retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n使用方法：\nfrom langchain.retrievers import ParentDocumentRetriever from langchain.storage import InMemoryStore from langchain.text_splitter import RecursiveCharacterTextSplitter # Retrieving larger chunks # This text splitter is used to create the parent documents parent_splitter = RecursiveCharacterTextSplitter(separators=\u0026#34;\\n\u0026#34;,chunk_size=2000) # This text splitter is used to create the child documents # It should create documents smaller than the parent child_splitter = RecursiveCharacterTextSplitter(separators=\u0026#34;\\n\u0026#34;,chunk_size=400) # define your docs = embeddings, and vectorstore # docs = loader.load(...) # emebeddings = OpenAIEmbeddings(...) # vectorstore = Chroma(...) # The storage layer for the parent documents store = InMemoryStore() retriever = ParentDocumentRetriever( vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter, ) retriever.add_documents(docs) 而在llama_index中也提供了类似的方法。\n# 创建较小的子块 from llama_index.schema import IndexNode sub_chunk_sizes = [256, 512, 864] sub_node_parsers = [ SimpleNodeParser.from_defaults(chunk_size=c) for c in sub_chunk_sizes ] all_nodes = [] for base_node in base_nodes: for n in sub_node_parsers: sub_nodes = n.get_nodes_from_documents([base_node]) sub_inodes = [IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes] all_nodes.extend(sub_inodes) original_node = IndexNode.from_text_node(base_node, base_node.node_id) all_nodes.append(original_node) all_nodes_dict = {n.node_id: n for n in all_nodes} # 创建索引、检索器和查询引擎 vector_index_chunk = VectorStoreIndex( all_nodes, service_context=service_context ) vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2) retriever_chunk = RecursiveRetriever( \u0026#34;vector\u0026#34;, retriever_dict={\u0026#34;vector\u0026#34;: vector_retriever_chunk}, node_dict=all_nodes_dict, verbose=True, ) query_engine_chunk = RetrieverQueryEngine.from_args( retriever_chunk, service_context=service_context ) response = query_engine_chunk.query( \u0026#34;Can you tell me about the key concepts for safety finetuning\u0026#34; ) print(str(response)) Sentence Window Retrieval 为了更好地分析找到的语境，我们在检索到的最相关单句之前后各扩展 k 个句子，然后把这个扩展后的语境送给 LLM 进行推理\nfrom Advanced RAG Techniques: an Illustrated Overview\n分块可视化小工具 推荐一个可视化chunk-spliting 工具chunkviz\n推荐阅读 Text splitting (chunking) for RAG applications\n5_Levels_Of_Text_Splitting\nlangchain:Semantic Chunking\nChunking Strategies for LLM Applications\nAdvanced RAG Techniques: an Illustrated Overview\nRAG系列05：基于语义的Chunk分割\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%9C%AC%E5%88%86%E5%9D%97/","summary":"\u003ch2 id=\"为什么要进行文本分块\"\u003e\u003cstrong\u003e为什么要进行文本分块？\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e大型语言模型（LLM）和嵌入式模型都有其处理文本的长度限制。为了有效处理超出这些限制的长文本，我们需要进行文本分块。\u003c/p\u003e\n\u003ch2 id=\"如何进行文本分块\"\u003e\u003cstrong\u003e如何进行文本分块\u003c/strong\u003e\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e**块分割戒律：**我们的目标不仅仅是为了划分数据块。更重要的是，我们要以一种便于日后检索和提取价值的格式来整理我们的数据。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e文本分块的根本目的不仅仅是简单地将数据分割成块。更重要的是，我们要以便于检索和提取价值的方式来组织这些数据块。以下是几种常见的分块逻辑：\u003c/p\u003e\n\u003ch3 id=\"句子分割sentence-splitting\"\u003e\u003cstrong\u003e句子分割（Sentence Splitting）\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e这种方法只是简单地将文本切分成N个字符的片段，不考虑内容或形式的连贯性。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;...\u0026#34;\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 你的文本\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003elangchain.text_splitter\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eCharacterTextSplitter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext_splitter\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCharacterTextSplitter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eseparator\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003echunk_size\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e256\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003echunk_overlap\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e20\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edocs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etext_splitter\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate_documents\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"递归字符文本分割recursive-character-text-splitting\"\u003e\u003cstrong\u003e递归字符文本分割（Recursive Character Text Splitting）\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e这种方法首先尝试用一个分隔符将文本切分成小块。如果初始尝试未能达到预期的块大小或结构，就递归地用不同的分隔符重新分割结果块，直到得到满意的结果。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;...\u0026#34;\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 你的文本\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003elangchain.text_splitter\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext_splitter\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eRecursiveCharacterTextSplitter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003echunk_size\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e256\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003echunk_overlap\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e20\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edocs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etext_splitter\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecreate_documents\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e根据langchain 的\u003ca href=\"https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L842\"\u003e默认分隔条件\u003c/a\u003e \u003ccode\u003e[\u0026quot;\\n\\n\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;]\u003c/code\u003e ,也就是会将text根据该分割条件的顺序（两个换行-\u0026gt;一个换行-\u0026gt;空格）将文本进行递归分割。\u003c/p\u003e\n\u003ch3 id=\"针对特定文档的分割方法document-specific-splitting\"\u003e\u003cstrong\u003e针对特定文档的分割方法（Document Specific Splitting）\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e例如针对Markdown或Python代码的分割策略，这些策略根据文档的结构来进行分割，如类定义或Markdown标题。\u003c/p\u003e\n\u003cp\u003elangchain 提供了一些常见文档的分割方法：\u003c/p\u003e\n\u003cp\u003emardown的分割逻辑\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# First, try to split along Markdown headings (starting with level 2)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e#{1,6} \u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# Note the alternative syntax for headings (below) is not handled here\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# Heading level 2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# ---------------\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# End of code block\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;```\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# Horizontal lines\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003e*\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003e*\u003c/span\u003e\u003cspan class=\"se\"\u003e\\\\\u003c/span\u003e\u003cspan class=\"s2\"\u003e*+\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e---+\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e___+\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# Note that this splitter doesn\u0026#39;t handle horizontal lines defined\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"c1\"\u003e# by *three or more* of ***, ---, or ___, but this is not handled\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34; \u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003epython的分割逻辑：\u003c/p\u003e","title":"RAG工具箱：文本分块"},{"content":"写在开头 本文是基于最近组内技术交流的文字稿整理。\nWhat is Agent？ 在探讨复杂的人工智能技术之前，让我们先回顾一下生活中的一个简单例子：扫地机器人。这种智能设备在房间中自主导航，避开障碍物，寻找最有效的清洁路径。它就是一个现实生活中的Agent——一个可以自主决策和行动的实体。\n在人工智能领域，Agent指的是任何可以感知其环境并根据感知结果做出决策的实体。这些决策旨在完成某些任务或达成特定的目标。Agent的行为可以简单如游戏里的机器人，也可以复杂如自动驾驶汽车。\n开始于强化学习 在强化学习中， 我们往往能见到agent的概念。强化学习是一种机器学习方法，它教导Agent通过试错法找到最佳行动路径。就像训练小狗一样，我们通过奖励来引导Agent做出正确的决策。Agent的目标是在与环境的交互中寻找最优策略。理想情况下，如果Agent能够获取足够多的真实环境数据，它就能找到最佳解决方案。然而，由于真实环境的复杂性，完全模拟真实世界是不现实的。\n目前，强化学习主要适用于环境简单、问题定义明确的场景，如围棋或视频游戏。这种方法在虚拟环境中通过大量试错来探索解决方案，这种方法虽然有效，但缺乏灵活性和高效性。与人类学习新技能的方式相比，强化学习的效率远低。人们通常通过少量的尝试就能迅速掌握新技能，而强化学习可能需要成千上万次的试错。\n这时候，如果agent具有大脑就好了。\n将LLMs作为大脑: 赋能智能Agent的关键技术 相较于基于强化学习的Agent，人类的优势在于我们天生具备的记忆能力和逻辑判断能力，甚至包括反思和从经验中学习的能力。这些能力使得我们能够通过极少的试错迅速适应和掌握新技能。\n语言模型（LLMs）为AI领域带来了革命性的变化。LLMs通过其深度学习的新范式，以及在思维链和自然语言理解方面的强大能力，预示着Agent将拥有更强大的学习和迁移能力。这种能力的提升将使得创建广泛应用且实用的Agent成为可能。\n虽然LLM是否真正具备了推理能力仍然存疑，但LLM的出现无疑改变了很多。以COT（Chain of Thought）为例，这种模型通过打印解题的中间步骤，加强了在数学和逻辑推理方面的能力，减少了幻觉的出现。这一突破性的工作揭示了LLM在不同场景下的强大\u0026quot;in-context learning\u0026quot;能力，以及其在未经微调的情况下的泛化能力。\n将LLM视为AI Agent的大脑，为自动化系统提供了一种全新的构思方式。这种基于LLM的agent系统综合了规划、记忆、工具使用和行动的能力，通过API调用与外部世界互动，显示出了前所未有的灵活性和效率。\n根据LLM Powered Autonomous Agents一文对LLM-based agent系统的定义， agent需要具备的基本能力：规划+记忆+工具+行动。\n规划能力：将复杂的任务分解成小任务， 并管理每一个子任务的目标， 并从过去的失败中反思，以吸取经验。\n记忆：LLM的上下文长度有限， 通过额外的记忆系统以提升LLM作为大脑的能力。\n工具调用\u0026amp;行动：LLM通过API调用的方式，执行任务， 与外界交互，而不是只是输出文字。\n探索AI代理的独特能力：人类与单一LLM无法比拟 AI系统的主要优势在于它们的规模和效率。这些系统能够执行以下任务，超越人类能力：\n大规模数据处理：AI能够高效地分析和处理超出人类理解范围的数据量。\n无需休息的持续操作：AI系统可以不间断地运行，而无需像人类那样休息和恢复。\n超快速计算：AI可以迅速执行复杂的计算，处理速度和效率远超人类。\nAI代理与单一LLM的不同:\n根据Andrew Ng在讲座中分享的内容，使用相对“简单”模型的代理工作流程（例如GPT-3.5）在实际应用中往往能够超越使用“智能”模型（如GPT-4）的零次提示。这说明在特定场景下，选择适当的AI模型和策略可能比单一的高级模型更有效。\nAI代理在决策制定中的应用也显示出其独特的优势。它们可以在没有情感偏见的情况下，基于大量数据做出快速且精确的决策。这种能力在需要快速响应和高精确度的领域尤为重要，如金融交易和紧急响应系统。\nAgent的规划和思维过程 AI Agent在处理复杂任务时，通过将大任务分解成小任务来提高效率。此外，自我反思能力允许Agent从过去的行动中学习，通过评估过去的决策来改善未来的表现。\nCoT，Chain of Thought， Wei et al. 2022。 即“思维链”，是一种使Agent逐步思考的方法。它通过要求模型展示解决问题的中间步骤来加强其逻辑推理能力，从而提高决策的质量和准确性。\nTree of Thoughts， ToT (Yao et al. 2023)\n尽管语言模型在许多领域表现出色，但在需要复杂规划和全局决策的任务上，它们的能力受到了限制。ToT框架应运而生，旨在通过一个结构化的思考和评估过程来弥补这一缺陷。\nToT框架借鉴了人类心理学中的双系统决策理论，通过整合快速直觉判断和慢速深思熟虑的决策过程，极大地提升了模型的决策能力。这一框架通过自我评估的方式，允许模型在面对多种可能的决策路径时，能够进行有效的选择和全局优化。\nToT框架旨在克服现有语言模型在处理需要复杂规划或搜索任务的局限。它通过结构化的思想树来探索和评估不同的决策路径。ToT允许模型在考虑多个推理路径时自我评估其选择，以做出最佳的决策。此外，ToT结合了语言生成和搜索算法（如BFS和DFS），使模型能够在进行决策时前瞻和回溯，以实现全局最优选择。\nprompt example：\ncot_prompt = \u0026#39;\u0026#39;\u0026#39; Write a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} Make a plan then write. Your output should be of the following format: Plan: Your plan here. Passage: Your passage here. \u0026#39;\u0026#39;\u0026#39; vote_prompt = \u0026#39;\u0026#39;\u0026#39;Given an instruction and several choices, decide which choice is most promising. Analyze each choice in detail, then conclude in the last line \u0026#34;The best choice is {s}\u0026#34;, where s the integer id of the choice. \u0026#39;\u0026#39;\u0026#39; compare_prompt = \u0026#39;\u0026#39;\u0026#39;Briefly analyze the coherency of the following two passages. Conclude in the last line \u0026#34;The more coherent passage is 1\u0026#34;, \u0026#34;The more coherent passage is 2\u0026#34;, or \u0026#34;The two passages are similarly coherent\u0026#34;. \u0026#39;\u0026#39;\u0026#39; score_prompt = \u0026#39;\u0026#39;\u0026#39;Analyze the following passage, then at the last line conclude \u0026#34;Thus the coherency score is {s}\u0026#34;, where s is an integer from 1 to 10. \u0026#39;\u0026#39;\u0026#39; GoT， Graph of Thoughts，\nhttps://arxiv.org/abs/2308.09687\nhttps://arxiv.org/abs/2401.14295\nGoT（Graph of Thoughts）是在ToT（Tree of Thoughts）基础上的一种进阶，它通过引入图状结构，增强了模型处理复杂推理任务的能力。这种结构允许在生成的思考中进行更为灵活的图状转换，如合并多个思考或在思考间进行循环。\nGoT的核心特点在于其灵活的图状结构，这使得它能够通过合并和循环不同的思考节点来模拟更复杂的推理过程。每个思考节点可以从多个父节点聚合信息，也可以向多个子节点扩展，形成一个密集的思考网络。这种结构特别适合处理那些传统树状模型难以解决的复杂推理和决策任务。\nGoT设计之初就为了处理比ToT更为复杂的推理任务。通过允许广泛的依赖关系和中间步骤的聚合，GoT能够更有效地解决那些需要非线性思考路径的问题。例如，在处理涉及多个相互关联子问题的任务时，GoT的图状结构提供了一种比传统线性或树状结构更为有效的解决方案。\nXoT, Everything of Thoughts, https://arxiv.org/abs/2311.04254\nXoT代表了在AI领域中对复杂思考和决策过程的一次显著革新，它通过结合蒙特卡洛树搜索（MCTS）和大型语言模型求解器（LLM solver）来优化决策路径。\nXoT框架包含以下几个关键步骤和组件：\n预训练的MCTS模块：该模块通过在特定任务上的预训练，获得了有效导航复杂思考路径的能力。\n动态思考搜索：在实际应用中，MCTS模块继续利用其策略和价值网络来探索和优化潜在的思考轨迹。\n思考修正与优化：LLM评估并修正MCTS的输出，确保思考路径的逻辑严密性和有效性。\n决策执行：修正后的思考路径被送回LLM，用于解决具体的问题和挑战。\nReAct (Reason to Act) 是由 Yao et al. 2023 提出的一种新颖的提示框架，旨在通过交错的思考、行动和观察步骤，提高大型语言模型（LLM）的动态推理和计划调整能力。\nReAct通过“思考-行动-观察”结构，使LLM能够生成口头推理迹线和与任务相关的行动。这种结构允许模型动态地创建、维护并调整高层次的行动计划，同时与外部环境（如Wikipedia）互动，引入额外信息以丰富推理。\nReAct的独特特点\n直观且易于设计：ReAct的设计直观，易于实现，并能快速适应不同的应用场景。\n通用性与灵活性：适用于多种任务，能够灵活处理复杂的情境。\n高性能与鲁棒性：提供稳定且有效的推理输出。\n与人类行为一致且可控：增强了模型的可控性，与人类的推理和行动方式更为一致。\nReAct与CoT的对比\n相对于CoT（Chain of Thought），ReAct通过交错的思考、行动和观察步骤，改进了模型的根据性和可信度。然而，这种结构在提高模型实地操作的同时，也略微降低了在推理步骤制定的灵活性。此外，ReAct的有效性很大程度上依赖于成功检索到信息丰富的知识。\nThought: ... Action: ... Observation: ... ... (Repeated many times) Reflexion是由Shinn \u0026amp; Labash 2023提出的一种创新的决策支持框架。它包括三个核心模块：Actor（行动模块）、Evaluator（评估模块）、和Self-Reflection（自我反思模块），旨在通过增强的语言模型反馈机制提升任务处理效率和质量。\n框架组成与工作机制\nActor模块：负责生成针对特定任务的行动指令和文本。\nEvaluator模块：评估Actor的输出，保证行动的适宜性和效果。\nSelf-Reflection模块：反馈生成部分，通过深入分析和总结Actor的表现，指导未来的改进。\nReflexion框架的轻量级设计和对详细反馈的处理能力使其在多个高级任务中表现优异，包括编程、推理和复杂决策。实际应用显示，该框架能显著提升任务处理的正确率和效率。\n思考的快与慢。 在《思考，快与慢》一书中，丹尼尔·卡尼曼提出了人类思考的两种模式：“系统1”和“系统2”。系统1是快速、本能和情感性的，而系统2则更为缓慢、深思熟虑和逻辑性。这两种系统在处理不同类型的记忆和决策时发挥着不同的作用。\nAgent记忆 记忆是人类认知功能的核心，使我们能够从经验中学习并避免重复错误。在人工智能领域，尤其是在开发智能代理（Agent）时，构建有效的记忆系统也显得尤为重要。\nAI系统，尤其是依赖于大型语言模型的Agent，常面临有限的上下文记忆容量问题。这限制了它们在处理持续对话和复杂任务中的表现。这些系统通常受到有限上下文长度的制约，这限制了它们在处理连续对话或复杂任务时的效率。例如，一个没有有效记忆机制的Agent可能会重复犯同样的错误，或者忘记初始的任务目标。\n尽管现代LLMs的上下文长度已经可以处理超过100,000个标记，试图将所有对话历史和相关记忆直接置于模型的上下文中既不经济也不高效。因此，开发一个专门的记忆系统，如检索增强生成（Retrieval-Augmented Generation, RAG）成为提升Agent性能的有效手段。\n如果要为AI构建高效的记忆系统，借鉴和参考人类的记忆系统是必不可少的。人类的记忆系统包含多种类型的记忆，每种记忆都有其独特的生物学基础和功能，这些记忆类型不仅支持我们的日常活动，还是我们进行复杂决策和学习的基础。\n通过了解人类记忆的工作原理，我们可以设计出能够更好地模拟人类认知过程的AI系统。这不仅可以提高机器的任务执行能力，还可以使机器更加自主地适应复杂的环境和需求。\n根据维基百科的定义，人类的记忆系统可细分成：\n工作记忆：支持我们处理日常任务和进行决策。\n工具记忆：帮助我们形成习惯和自动化反应。\n语义记忆：允许我们累积知识和理解世界。\n情节记忆：使我们能够回忆过去的经历和事件。\n动作记忆：通过练习改善我们的物理技能和运动。\n遗忘的机制和认知卸载。记忆的形成经历过滤、压缩、遗忘和重组等环节。在这个过程中，大脑会删除大部分接收到的信息，只保留对当前活动或未来决策最关键的部分。\n建议阅读：\nA Survey on the Memory Mechanism of Large Language Model based Agents\nAgent的工具调用 我们更希望能帮助我们干活的agent 而不是一个只会说话的LLMs。换句话说人实际上并不需要一个聊天的对象，而是帮助解决问题的助手\n如何让LLMs具备工具调用的能力。 根据当前的主要工作， 主要是以openAI 的tool-using 的工作为主（也就是之前的function calling）。\n一句话解释：function calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\n具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\n更详细的tool-using内容，可以查看之前的博文：Agent学习笔记：OpenAI Function Calling完全指南\n除了tool-using意外， 比较重要的工具还有：code interpreter， 在代码开发的场景下， code interpreter充当了RL中的Env角色，让agent能够根据环境的反馈做出对应决策，而不只是生成代码。 除了openAI所提供的code interpreter外， 还有一些开源的code interpreter，如Open Interpreter 可以使用。\n从单Agent到多agent协作 来自斯坦福的虚拟小镇，因其独特的话题性而被广泛讨论。\nGenerative Agents: Interactive Simulacra of Human Behavior\nMetaGPT 在MetaGPT看来，可以将智能体想象成环境中的数字人，其中\n智能体 = 大语言模型（LLM） + 观察 + 思考 + 行动 + 记忆\n多智能体系统可以视为一个智能体社会，其中\n多智能体 = 智能体 + 环境 + 标准流程（SOP） + 通信 + 经济\nCode = SOP(Team) 是核心哲学。我们将SOP具象化，并且用于LLM构成的团队\n一些讨论 LLMs真的具备逻辑推理能力吗 何为推理（reasoning）：是对信息进行处理和推断，形成判断或结论的过程；**推理通常是一个包含多个推断步骤的过程。推理通常被认为需要抽象思维——也就是说，推理的能力不局限于具体的例子，而是更为普遍的。**LLMs的推理能力似乎是”涌现“的结果，而非专门针对性训练，所以LLMs是否真正具备推理能力？从一些结果表明，LLMs似乎是在背答案而非真正的推理。\n在 Can Large Language Models Reason? 这篇文章中作者认为CoT过程中LLM可能是在背答案：\nGPT-4 solved 10 out of 10 problems that had been published before 2021 (GPT-4’s pre-training cutoff date) and zero out of 10 problems that had been published after 2021. 不过，我还是持保留意见的， 作者所观察到的现象和人类很像，更擅长在自己熟悉的领域，因为存在大量的记忆，记忆间存在大量“近道”（shortcut）用于加速推导过程，甚至直接给出答案；所要做的让LLMs去执行“慢思考”而不是“快思考”。\n如果从transformer 的原理上讨论， 一个自回归的模型本身是和推理，逻辑甚至是智能无缘的。\n但人类的语言中， 天生具备了逻辑，或者人类的语言中融入的人类本身的逻辑。那么通过模拟语言的生成，一定程度也能恢复出语言背后的逻辑。\nAgent真的只是商业上的吹捧或者泡沫吗 泡沫的成份一定是有点。但也是未来十年值得做的东西。\n如果把LLM类比成CPU的话， 一个完整的计算机系统，除了CPU以外， 还需要内存，控制，输入输出等组件。不可能让LLM实现所有功能，这并不现实。但通过一定合理的架构，工作流的编排，能够大大提升LLM的准确性，进而加快落地实现。\n传统行业要如何在agent上发力 所有公司都有SOP，和大量存在在员工脑子中的“隐性知识”。 这些“知识”只有转换成agent的工作流（思维方式），或者是LLM的system prompt，才会变得更有价值和意义。现阶段有很多公司在做所谓的知识管理， 本质上是用公司的内部文本作为RAG系统而已，但一个问答系统很难量化出收益，并且对于使用者的要求挺高的。 实际上我认为，发力点应该是将内部人类可使用的工具逐渐API化，标准化以方便agent的工具调用，然后定义智能代理在各种业务场景中的具体任务和行动。这包括从简单的数据检索到复杂的决策制定\nAgent应该从事简单重复劳动还是具有创造性的探索工作 这题让GPT-4来回答吧：\n关于“Agent应该从事简单重复劳动还是具有创造性的探索工作”的问题，我们可以从多个角度进行分析：\n经济效益：从经济角度考虑，如果简单重复的劳动可以被自动化或由机器人执行，这将大幅度降低成本，并提高效率。企业通常会选择自动化这类工作以减少人力成本。然而，对于创造性的探索工作，由于需要不断地创新和解决新问题，这种类型的工作更可能需要人类智能的参与，因此，这种工作对于推动技术进步和经济增长具有重要价值。\n技术发展：随着AI和机器学习技术的发展，越来越多的简单劳动可以被智能系统所取代。例如，数据输入、监控等任务已被自动化系统广泛接管。然而，对于需要高度创造性和判断力的工作，如科学研究、艺术创作、复杂决策制定等，目前的技术还无法完全替代人类。\n人类福祉：从心理学的角度来看，长期从事简单重复的劳动可能导致工作满意度下降，影响心理健康。相反，创造性工作通常能带来更高的个人成就感和职业满意度。因此，鼓励Agent从事创造性探索工作可能更有助于提升整体福祉。\n社会需求和未来趋势：社会对创新和创造的需求日益增加，这种趋势推动了对创造性职业的需求。例如，新兴技术、可持续发展解决方案和个性化产品等领域，都需要大量的创新思维和创造力。因此，社会和市场需求可能更倾向于从事具有创造性的探索工作。\n综上所述，虽然从事简单重复的劳动在某些情况下具有其经济和效率上的优势，但从长远和全面的角度来看，鼓励Agent从事具有创造性的探索工作似乎更具前瞻性和可持续性。这不仅有助于经济和技术的发展，还能改善工作者的心理福祉和职业满意度。\nAgent在制造业良率分析上如何应用 以Yield分析和提升为例，在进行Yield提升和案例分析时，存在大量重复的分析和判断工作，这些可以通过AI进行加速和辅助，而不需人工逐一检查。\n工程师每天可能需要处理超过100个与良率相关的预警。其中约40%可能是由于测试问题、设备故障或是之前已经标注过的案例。这些都需要工程师逐一检查。由于人力有限，他们只能优先审查良率特别低的情况，可能会错过一些潜在的问题。通过使用特定规则和AI Agent，可以自动检查这些数据并作出判断。\n在处理实际案例时，工程师通常会根据案例特征在知识库中查询处理方法，这一过程也可以由Agent自动完成，并缩小后续检查的范围。如果需要进一步分析，工程师会调用不同的工具用于检查，这些分析也可以通过Agent来执行。如果仍无法找到问题，可以提醒人工介入。当Agent成功分析出结果时，还会将分析过程和结果整理成图文并茂的报告，供工程师检查和使用。一旦这些分析被人工审核并采用，它们就会成为高质量的知识，自动加入知识库中供下次使用。\nReading LLM Powered Autonomous Agents\nA busy person’s Intro to AI Agents\n基于大语言模型的AI Agents—Part 1\nChain of Thought\nTree of Thoughts\nGraph of Thoughts\nReAct\nReflexion\nMetaGPT: Meta Programming for Multi-Agent Collaborative Framework\nGenerative Agents: Interactive Simulacra of Human Behavior\nLLM Agents Papers\nAI Agent \u0026amp; 大模型经典论文推荐\nAI Agents are disrupting automation: Current approaches, market solutions and recommendations\nAGI-Edgerunners-LLM-Agents-Papers\nThe Rise and Potential of Large Language Model Based Agents: A Survey\nAwesome AI Agents\nLarge Language Model based Multi-Agents: A Survey of Progress and Challenges\nThe Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n","permalink":"https://niraya666.github.io/posts/%E5%9F%BA%E4%BA%8E%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84-agent%E7%A7%91%E6%99%AE%E5%90%91/","summary":"\u003ch2 id=\"写在开头\"\u003e写在开头\u003c/h2\u003e\n\u003cp\u003e本文是基于最近组内技术交流的文字稿整理。\u003c/p\u003e\n\u003ch2 id=\"what-is-agent\"\u003eWhat is Agent？\u003c/h2\u003e\n\u003cp\u003e在探讨复杂的人工智能技术之前，让我们先回顾一下生活中的一个简单例子：扫地机器人。这种智能设备在房间中自主导航，避开障碍物，寻找最有效的清洁路径。它就是一个现实生活中的Agent——一个可以自主决策和行动的实体。\u003c/p\u003e\n\u003cp\u003e在人工智能领域，Agent指的是任何可以感知其环境并根据感知结果做出决策的实体。这些决策旨在完成某些任务或达成特定的目标。Agent的行为可以简单如游戏里的机器人，也可以复杂如自动驾驶汽车。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%9f%ba%e4%ba%8e%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%20Agent%ef%bc%9a%e7%a7%91%e6%99%ae%e5%90%91-assets/Pasted%202024-05-10-14-58-28.webp\" alt=\"Pasted 2024-05-10-14-58-28.webp\"  /\u003e\n\u003c/p\u003e\n\u003ch3 id=\"开始于强化学习\"\u003e开始于强化学习\u003c/h3\u003e\n\u003cp\u003e在强化学习中， 我们往往能见到agent的概念。强化学习是一种机器学习方法，它教导Agent通过试错法找到最佳行动路径。就像训练小狗一样，我们通过奖励来引导Agent做出正确的决策。Agent的目标是在与环境的交互中寻找最优策略。理想情况下，如果Agent能够获取足够多的真实环境数据，它就能找到最佳解决方案。然而，由于真实环境的复杂性，完全模拟真实世界是不现实的。\u003c/p\u003e\n\u003cp\u003e目前，强化学习主要适用于环境简单、问题定义明确的场景，如围棋或视频游戏。这种方法在虚拟环境中通过大量试错来探索解决方案，这种方法虽然有效，但缺乏灵活性和高效性。与人类学习新技能的方式相比，强化学习的效率远低。人们通常通过少量的尝试就能迅速掌握新技能，而强化学习可能需要成千上万次的试错。\u003c/p\u003e\n\u003cp\u003e这时候，如果agent具有大脑就好了。\u003c/p\u003e\n\u003ch3 id=\"将llms作为大脑-赋能智能agent的关键技术\"\u003e将LLMs作为大脑: \u003cstrong\u003e赋能智能Agent的关键技术\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e相较于基于强化学习的Agent，人类的优势在于我们天生具备的记忆能力和逻辑判断能力，甚至包括反思和从经验中学习的能力。这些能力使得我们能够通过极少的试错迅速适应和掌握新技能。\u003c/p\u003e\n\u003cp\u003e语言模型（LLMs）为AI领域带来了革命性的变化。LLMs通过其深度学习的新范式，以及在思维链和自然语言理解方面的强大能力，预示着Agent将拥有更强大的学习和迁移能力。这种能力的提升将使得创建广泛应用且实用的Agent成为可能。\u003c/p\u003e\n\u003cp\u003e虽然LLM是否真正具备了推理能力仍然存疑，但LLM的出现无疑改变了很多。以COT（Chain of Thought）为例，这种模型通过打印解题的中间步骤，加强了在数学和逻辑推理方面的能力，减少了幻觉的出现。这一突破性的工作揭示了LLM在不同场景下的强大\u0026quot;in-context learning\u0026quot;能力，以及其在未经微调的情况下的泛化能力。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%9f%ba%e4%ba%8e%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%20Agent%ef%bc%9a%e7%a7%91%e6%99%ae%e5%90%91-assets/%e6%88%aa%e5%b1%8f2024-05-08%2015.37.21.png\" alt=\"截屏2024-05-08 15.37.21.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e将LLM视为AI Agent的大脑，为自动化系统提供了一种全新的构思方式。这种基于LLM的agent系统综合了规划、记忆、工具使用和行动的能力，通过API调用与外部世界互动，显示出了前所未有的灵活性和效率。\u003c/p\u003e\n\u003cp\u003e根据\u003ca href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\u003eLLM Powered Autonomous Agents\u003c/a\u003e一文对LLM-based agent系统的定义， agent需要具备的基本能力：规划+记忆+工具+行动。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e规划能力\u003c/strong\u003e：将复杂的任务分解成小任务， 并管理每一个子任务的目标， 并从过去的失败中反思，以吸取经验。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e记忆\u003c/strong\u003e：LLM的上下文长度有限， 通过额外的记忆系统以提升LLM作为大脑的能力。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e工具调用\u0026amp;行动\u003c/strong\u003e：LLM通过API调用的方式，执行任务， 与外界交互，而不是只是输出文字。\u003c/p\u003e\n\u003ch3 id=\"探索ai代理的独特能力人类与单一llm无法比拟\"\u003e\u003cstrong\u003e探索AI代理的独特能力：人类与单一LLM无法比拟\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eAI系统的主要优势在于它们的规模和效率。这些系统能够执行以下任务，超越人类能力：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e大规模数据处理\u003c/strong\u003e：AI能够高效地分析和处理超出人类理解范围的数据量。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e无需休息的持续操作\u003c/strong\u003e：AI系统可以不间断地运行，而无需像人类那样休息和恢复。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e超快速计算\u003c/strong\u003e：AI可以迅速执行复杂的计算，处理速度和效率远超人类。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAI代理与单一LLM的不同:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e根据Andrew Ng在讲座中分享的内容，使用相对“简单”模型的代理工作流程（例如GPT-3.5）在实际应用中往往能够超越使用“智能”模型（如GPT-4）的零次提示。这说明在特定场景下，选择适当的AI模型和策略可能比单一的高级模型更有效。\u003c/p\u003e\n\u003cp\u003eAI代理在决策制定中的应用也显示出其独特的优势。它们可以在没有情感偏见的情况下，基于大量数据做出快速且精确的决策。这种能力在需要快速响应和高精确度的领域尤为重要，如金融交易和紧急响应系统。\u003c/p\u003e\n\u003ch2 id=\"agent的规划和思维过程\"\u003e\u003cstrong\u003eAgent的规划和思维过程\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eAI Agent在处理复杂任务时，通过将大任务分解成小任务来提高效率。此外，自我反思能力允许Agent从过去的行动中学习，通过评估过去的决策来改善未来的表现。\u003c/p\u003e\n\u003cp\u003eCoT，Chain of Thought， \u003ca href=\"https://arxiv.org/abs/2201.11903\"\u003eWei et al. 2022\u003c/a\u003e。 即“思维链”，是一种使Agent逐步思考的方法。它通过要求模型展示解决问题的中间步骤来加强其逻辑推理能力，从而提高决策的质量和准确性。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/%e5%9f%ba%e4%ba%8e%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%20Agent%ef%bc%9a%e7%a7%91%e6%99%ae%e5%90%91-assets/%e6%88%aa%e5%b1%8f2024-05-08%2015.37.21.png\" alt=\"截屏2024-05-08 15.37.21.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTree of Thoughts， ToT\u003c/strong\u003e  (\u003ca href=\"https://arxiv.org/abs/2305.10601\"\u003eYao et al. 2023\u003c/a\u003e)\u003c/p\u003e","title":"基于大语言模型的 Agent：科普向"},{"content":"写在最开始 当我们在讨论基于大型语言模型（LLM-based）的智能代理（agent）时，我们究竟在谈论什么？根据Lilian W在其文章《LLM Powered Autonomous Agents》中的讨论，一个智能代理需要具备几个核心能力：规划（Planning）、记忆（Memory）、以及工具使用（Tool use）。特别地，工具使用方面的进展，得益于OpenAI在API中提供的function calling功能，为我们开启了新的可能性。\nOpenAI function calling，作为智能代理与外部工具交互的基本方式，对于每位从业者来说都是必备技能。随着技术的发展，我们期望的不只是能与我们对话的LLM，而是能够辅助我们使用各种工具、做出决策的智能伙伴。\n不过需要特别指出的是，最近OpenAI在Chat Completions API中已经废弃了“函数（function）”的使用，转而采用“工具（tool）”。这一变更旨在拓宽LLM集成的功能范围，为更复杂的交互模式铺平道路，如构建能够相互作用的多代理系统。\n尽管如此，由于语言习惯的原因，本文中仍然会使用function calling的术语来描述OpenAI的tool using功能，因为“function calling”的说法已经深入人心了。\n核心内容概览\nFunction Calling的定义：解释什么是function calling，以及它在智能代理工作中的作用。\nOpenAI Cookbook示例：提供实际的function calling示例，帮助读者理解其在实际应用中的用途。\n开源LLM的Tool Using：探索如何在开源大型语言模型中实现工具使用，以及LLM在tool using的时候经历了什么。\n评价与训练：讨论如何评价开源模型的工具使用能力，以及如何训练LLM进行有效的工具使用。\n鉴于整理笔记的速度远赶不上更新的速度， 会将第四部份作为单独的部分整理。\n何为function calling 一句话解释：function calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\n具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\nfunction calling的应用范围广泛，如\n创建智能助手：通过调用外部API回答问题。\n转换指令：将自然语言指令转换成API调用指令。\n数据提取：从文本中提取结构化数据。\nfunction calling的过程涵盖了从定义函数集、通过模型生成遵循自定义模式的JSON对象字符串，到在代码中解析这个字符串并调用相应函数的全过程。这一连串操作不仅自动化了交互过程，还确保了执行操作的安全性和准确性。\n一些常见的问题 JSON mode json mode 和tool-using 有什么关系？有了json mode 还需要用到tool-using吗？\n从json mode 的本质， 更多的是在system prompt 增加一句类似“请以json格式输出”之类的话，然后在LLM输出时增加json结果检查和格式转换。在使用时只需要在client.chat.completions.create 中 增加response_format={ \u0026quot;type\u0026quot;: \u0026quot;json_object\u0026quot; } 即可。\n那么json mode 什么时候会用到呢？一般在做文本提取，内容提取时可以使用；以RAG场景为例， 当我们希望LLM能够帮我们对用户的query进行改写时，我们肯定是希望模型能够返回干净的json格式改写结果，这样的结果可以直接使用，而不是在模型输出一些内容后，如：\n\u0026#34;\u0026#34;\u0026#34; 好的，以下是我的改写内容： ``` real-rewrite-query ``` \u0026#34;\u0026#34;\u0026#34; 其中包含了一些模型喜欢输出的客套话，此时我们需要通过正则匹配等方法将真正希望使用内容提取出来。而这时候json mode可以直接输出需要的内容， 而跳过了额外的提取步骤。在json mode 出现之前，这样的处理我们也尝试使用过tool-using 的模式，但有点大材小用了。\n很显然， tool-using的真正强大之处并不只是对输出格式进行处理， 而是能够让模型从提供的多个tools中选择需要使用的。\n如何使用OpenAI function calling 天气查询的简单示例 环境配置\n首先，我们需要安装一些必要的Python库。这些库将帮助我们与OpenAI的API进行交互，以及完成一些辅助功能。\n!pip install scipy --quiet !pip install tenacity --quiet !pip install tiktoken --quiet !pip install termcolor --quiet !pip install openai --quiet os.environ[\u0026#34;OPENAI_API_KEY\u0026#34;] = \u0026#34;...\u0026#34; from openai import OpenAI from tenacity import retry, wait_random_exponential, stop_after_attempt from termcolor import colored client = OpenAI() 工具函数定义\n接下来，我们定义一些实用函数，这些函数旨在方便我们向Chat Completions API发送请求，并管理与跟踪对话状态。\n@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3)) def chat_completion_request(messages, tools=None, tool_choice=None, model=\u0026#34;gpt-3.5-turbo\u0026#34;): try: response = client.chat.completions.create( model=model, messages=messages, tools=tools, tool_choice=tool_choice, ) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e def pretty_print_conversation(messages): role_to_color = { \u0026#34;system\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;function\u0026#34;: \u0026#34;magenta\u0026#34;, } for message in messages: if message[\u0026#34;role\u0026#34;] == \u0026#34;system\u0026#34;: print(colored(f\u0026#34;system: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;user\u0026#34;: print(colored(f\u0026#34;user: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;assistant\u0026#34; and message.get(\u0026#34;function_call\u0026#34;): print(colored(f\u0026#34;assistant: {message[\u0026#39;function_call\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;assistant\u0026#34; and not message.get(\u0026#34;function_call\u0026#34;): print(colored(f\u0026#34;assistant: {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) elif message[\u0026#34;role\u0026#34;] == \u0026#34;function\u0026#34;: print(colored(f\u0026#34;function ({message[\u0026#39;name\u0026#39;]}): {message[\u0026#39;content\u0026#39;]}\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]])) 函数规范定义\n我们还需要创建一些函数规范，以接口化与假设的天气API的交互。\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;The temperature unit to use. Infer this from the users location.\u0026#34;, }, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;format\u0026#34;], }, } }, { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_n_day_weather_forecast\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get an N-day weather forecast\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;The temperature unit to use. Infer this from the users location.\u0026#34;, }, \u0026#34;num_days\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The number of days to forecast\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;format\u0026#34;, \u0026#34;num_days\u0026#34;] }, } }, ] 对话示例\n当我们请求当前天气时， LLM会要求澄清问题，如地址等参数信息：\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like today\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message output:\nChatCompletionMessage(content=\u0026#39;Sure, may I know your current location?\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None) 当补充缺失的信息后， LLM将生成适当的函数参数\nmessages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;I\u0026#39;m in Shanghai, China.\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message output:\nChatCompletionMessage(content=None, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_VdqOOMp9pagf5ho39Y2HmYV4\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;San Francisco, CA\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;,\\n \u0026#34;num_days\u0026#34;: 4\\n}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)]) 完整流程如下：\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;what is the weather going to be like in Glasgow, Scotland over the next x days\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;in 5 days\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message messages.append(assistant_message) assistant_message 当然， 我们也可以强制要求使用特定的函数\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Give me a weather report for Toronto, Canada.\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools, tool_choice={\u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;get_n_day_weather_forecast\u0026#34;}} ) chat_response.choices[0].message output:\nChatCompletionMessage(content=None, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_0ldecDpV8Vdq8mGPoUewlue3\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Toronto, Canada\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;,\\n \u0026#34;num_days\u0026#34;: 1\\n}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)]) 并行函数调用\n对于一些特定模型，如**gpt-4-turbo-preview**, gpt-4-0125-preview, gpt-4-1106-preview, gpt-3.5-turbo-0125, and **gpt-3.5-turbo-1106**支持并行函数调用， 允许我们在单个回合中调用多个函数。\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Don\u0026#39;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;what is the weather going to be like in San Francisco and Glasgow over the next 4 days\u0026#34;}) chat_response = chat_completion_request( messages, tools=tools ) assistant_message = chat_response.choices[0].message.tool_calls assistant_message output:\n[ChatCompletionMessageToolCall(id=\u0026#39;call_tfl8eTCW64sHvHjiiatoYzku\u0026#39;, function=Function(arguments=\u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;San Francisco, CA\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;fahrenheit\u0026#34;, \u0026#34;num_days\u0026#34;: 4}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;), ChatCompletionMessageToolCall(id=\u0026#39;call_bAqj55RygP2Y1T85RHqgskku\u0026#39;, function=Function(arguments=\u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;Glasgow, UK\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;, \u0026#34;num_days\u0026#34;: 4}\u0026#39;, name=\u0026#39;get_n_day_weather_forecast\u0026#39;), type=\u0026#39;function\u0026#39;)] 实现本地函数调用\n首先我们需要构造两个用于演示的假function\nimport json def get_current_weather(location, format=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Simulates getting the current weather for a given location. The response is hardcoded for demonstration purposes. Args: location (str): The city and state, e.g., San Francisco, CA. format (str, optional): The temperature unit to use. Defaults to \u0026#34;fahrenheit\u0026#34;. Returns: str: JSON string with the current weather data. \u0026#34;\u0026#34;\u0026#34; if \u0026#34;tokyo\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Partly Cloudy\u0026#34;}) elif \u0026#34;san francisco\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;San Francisco\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;72\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Sunny\u0026#34;}) elif \u0026#34;paris\u0026#34; in location.lower(): return json.dumps({\u0026#34;location\u0026#34;: \u0026#34;Paris\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Rainy\u0026#34;}) else: return json.dumps({\u0026#34;location\u0026#34;: location, \u0026#34;temperature\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Data Unavailable\u0026#34;}) def get_n_day_weather_forecast(location, num_days, format=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Simulates getting an N-day weather forecast for a given location. The response is hardcoded for demonstration purposes. Args: location (str): The city and state, e.g., San Francisco, CA. num_days (int): The number of days to forecast. format (str, optional): The temperature unit to use. Defaults to \u0026#34;fahrenheit\u0026#34;. Returns: str: JSON string with the N-day weather forecast data. \u0026#34;\u0026#34;\u0026#34; # This example just returns a fixed response regardless of the input. # In a real scenario, the response would depend on the location, num_days, and format. forecast = [ {\u0026#34;day\u0026#34;: 1, \u0026#34;temperature\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Sunny\u0026#34;}, {\u0026#34;day\u0026#34;: 2, \u0026#34;temperature\u0026#34;: \u0026#34;18\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Cloudy\u0026#34;}, {\u0026#34;day\u0026#34;: 3, \u0026#34;temperature\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;format\u0026#34;: format, \u0026#34;description\u0026#34;: \u0026#34;Rainy\u0026#34;} ] # Return only the forecast for the requested number of days. return json.dumps(forecast[:num_days]) available_functions = { \u0026#34;get_current_weather\u0026#34;: get_current_weather, \u0026#34;get_n_day_weather_forecast\u0026#34;: get_n_day_weather_forecast, } 尝试请求天气\nmessages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like in Tokyo!\u0026#34;}) chat_response = chat_completion_request(messages, tools=tools, tool_choice=\u0026#34;auto\u0026#34;) assistant_message = chat_response.choices[0].message assistant_message = json.loads(assistant_message.model_dump_json()) assistant_message[\u0026#34;content\u0026#34;] = str(assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;]) #a temporary patch but this should be handled differently # remove \u0026#34;function_call\u0026#34; from assistant message del assistant_message[\u0026#34;function_call\u0026#34;] assistant_message \u0026#34;\u0026#34;\u0026#34; {\u0026#39;content\u0026#39;: \u0026#39;{\\\u0026#39;arguments\\\u0026#39;: \\\u0026#39;{\\\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\\\n}\\\u0026#39;, \\\u0026#39;name\\\u0026#39;: \\\u0026#39;get_current_weather\\\u0026#39;}\u0026#39;, \u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;tool_calls\u0026#39;: [{\u0026#39;id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;function\u0026#39;: {\u0026#39;arguments\u0026#39;: \u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\n}\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;}, \u0026#39;type\u0026#39;: \u0026#39;function\u0026#39;}]} \u0026#34;\u0026#34;\u0026#34; messages.append(assistant_message) 可以看到，LLM返回了json格式的参数信息（其实是str）以及需要调用的function名称， 拥有这些信息之后就可以调用之前定义的函数了\n# get the weather information to pass back to the model function_name_to_call = assistant_message[\u0026#39;tool_calls\u0026#39;][0][\u0026#39;function\u0026#39;][\u0026#39;name\u0026#39;] function_arguments = assistant_message[\u0026#39;tool_calls\u0026#39;][0][\u0026#39;function\u0026#39;][\u0026#39;arguments\u0026#39;] weather = available_functions[function_name_to_call](function_arguments) 将函数执行结果和对话历史返回给LLM\nmessages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;], \u0026#34;content\u0026#34;: weather}) messages \u0026#34;\u0026#34;\u0026#34; [{\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#34;What\u0026#39;s the weather like in Tokyo!\u0026#34;}, {\u0026#39;content\u0026#39;: \u0026#39;{\\\u0026#39;arguments\\\u0026#39;: \\\u0026#39;{\\\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\\\n}\\\u0026#39;, \\\u0026#39;name\\\u0026#39;: \\\u0026#39;get_current_weather\\\u0026#39;}\u0026#39;, \u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;tool_calls\u0026#39;: [{\u0026#39;id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;function\u0026#39;: {\u0026#39;arguments\u0026#39;: \u0026#39;{\\n \u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;,\\n \u0026#34;format\u0026#34;: \u0026#34;celsius\u0026#34;\\n}\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;}, \u0026#39;type\u0026#39;: \u0026#39;function\u0026#39;}]}, {\u0026#39;role\u0026#39;: \u0026#39;tool\u0026#39;, \u0026#39;tool_call_id\u0026#39;: \u0026#39;call_Tz8S1HgvnaBzf6CFZP1u4d1J\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;get_current_weather\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;{\u0026#34;location\u0026#34;: \u0026#34;Tokyo\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;fahrenheit\u0026#34;}\u0026#39;}] \u0026#34;\u0026#34;\u0026#34; 最终输出结果（for user）\nfinal_response = chat_completion_request(messages, tools=tools) final_response.choices[0].message.content output：\n\u0026#39;The current weather in Tokyo is partly cloudy with a temperature of 10°C (50°F).\u0026#39; 通过function calling 实现sql执行 下载演示所用的sqlite数据\n!wget https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip -O chinook.zip !unzip chinook.zip 连接数据库，和定义一些function\nimport sqlite3 conn = sqlite3.connect(\u0026#34;/content/chinook.db\u0026#34;) print(\u0026#34;Opened database successfully\u0026#34;) def get_table_names(conn): \u0026#34;\u0026#34;\u0026#34;Return a list of table names.\u0026#34;\u0026#34;\u0026#34; table_names = [] tables = conn.execute(\u0026#34;SELECT name FROM sqlite_master WHERE type=\u0026#39;table\u0026#39;;\u0026#34;) for table in tables.fetchall(): table_names.append(table[0]) return table_names def get_column_names(conn, table_name): \u0026#34;\u0026#34;\u0026#34;Return a list of column names.\u0026#34;\u0026#34;\u0026#34; column_names = [] columns = conn.execute(f\u0026#34;PRAGMA table_info(\u0026#39;{table_name}\u0026#39;);\u0026#34;).fetchall() for col in columns: column_names.append(col[1]) return column_names def get_database_info(conn): \u0026#34;\u0026#34;\u0026#34;Return a list of dicts containing the table name and columns for each table in the database.\u0026#34;\u0026#34;\u0026#34; table_dicts = [] for table_name in get_table_names(conn): columns_names = get_column_names(conn, table_name) table_dicts.append({\u0026#34;table_name\u0026#34;: table_name, \u0026#34;column_names\u0026#34;: columns_names}) return table_dicts 获取db 的schema\ndatabase_schema_dict = get_database_info(conn) database_schema_string = \u0026#34;\\n\u0026#34;.join( [ f\u0026#34;Table: {table[\u0026#39;table_name\u0026#39;]}\\nColumns: {\u0026#39;, \u0026#39;.join(table[\u0026#39;column_names\u0026#39;])}\u0026#34; for table in database_schema_dict ] ) 定义tools工具列表\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ask_database\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Use this function to answer user questions about music. Input should be a fully formed SQL query.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; SQL query extracting info to answer the user\u0026#39;s question. SQL should be written using this database schema: {database_schema_string} The query should be returned in plain text, not in JSON. \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, } } ] 定义用于执行sql的function\ndef ask_database(conn, query): \u0026#34;\u0026#34;\u0026#34;Function to query SQLite database with a provided SQL query.\u0026#34;\u0026#34;\u0026#34; try: results = str(conn.execute(query).fetchall()) except Exception as e: results = f\u0026#34;query failed with error: {e}\u0026#34; return results def execute_function_call(message): if message.tool_calls[0].function.name == \u0026#34;ask_database\u0026#34;: query = json.loads(message.tool_calls[0].function.arguments)[\u0026#34;query\u0026#34;] results = ask_database(conn, query) else: results = f\u0026#34;Error: function {message.tool_calls[0].function.name} does not exist\u0026#34; return results messages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Answer user questions by generating SQL queries against the Chinook Music Database.\u0026#34;}) messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hi, who are the top 5 artists by number of tracks?\u0026#34;}) chat_response = chat_completion_request(messages, tools) assistant_message = chat_response.choices[0].message messages.append(assistant_message) if assistant_message.tool_calls: results = execute_function_call(assistant_message) messages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message.tool_calls[0].id, \u0026#34;name\u0026#34;: assistant_message.tool_calls[0].function.name, \u0026#34;content\u0026#34;: results}) 此时的messages\n[{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Answer user questions by generating SQL queries against the Chinook Music Database.\u0026#39;}, {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Hi, who are the top 5 artists by number of tracks?\u0026#39;}, ChatCompletionMessage(content=\u0026#39;Function(arguments=\\\u0026#39;{\\\\n \u0026#34;query\u0026#34;: \u0026#34;SELECT artists.Name, COUNT(tracks.TrackId) AS num_tracks FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY artists.ArtistId ORDER BY num_tracks DESC LIMIT 5\u0026#34;\\\\n}\\\u0026#39;, name=\\\u0026#39;ask_database\\\u0026#39;)\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id=\u0026#39;call_vSLysEQncbGvMgXGhttIow6v\u0026#39;, function=Function(arguments=\u0026#39;{\\n \u0026#34;query\u0026#34;: \u0026#34;SELECT artists.Name, COUNT(tracks.TrackId) AS num_tracks FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId JOIN tracks ON albums.AlbumId = tracks.AlbumId GROUP BY artists.ArtistId ORDER BY num_tracks DESC LIMIT 5\u0026#34;\\n}\u0026#39;, name=\u0026#39;ask_database\u0026#39;), type=\u0026#39;function\u0026#39;)]), {\u0026#39;role\u0026#39;: \u0026#39;tool\u0026#39;, \u0026#39;tool_call_id\u0026#39;: \u0026#39;call_vSLysEQncbGvMgXGhttIow6v\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;ask_database\u0026#39;, \u0026#39;content\u0026#39;: \u0026#34;[(\u0026#39;Iron Maiden\u0026#39;, 213), (\u0026#39;U2\u0026#39;, 135), (\u0026#39;Led Zeppelin\u0026#39;, 114), (\u0026#39;Metallica\u0026#39;, 112), (\u0026#39;Deep Purple\u0026#39;, 92)]\u0026#34;}] 最终输出：\nfinal_response = chat_completion_request(messages, tools=tools) final_response.choices[0].message.content # output \u0026#34;\u0026#34;\u0026#34; The top 5 artists by number of tracks are: 1. Iron Maiden - 213 tracks 2. U2 - 135 tracks 3. Led Zeppelin - 114 tracks 4. Metallica - 112 tracks 5. Deep Purple - 92 tracks \u0026#34;\u0026#34;\u0026#34; 基于function calling构建智能代理：自动获取与深度解析arXiv学术文章 在这一节中将介绍如何构建一个能够从arXiv上查找论文， 下载分析并总结学术论文的Agent。这个Agent不仅可以帮助用户快速了解特定领域的最新研究动态，而且还能深入分析和总结选定文章的核心内容。\nAgent核心功能\n获取arXiv文章 get_articles 代理通过**arxiv**库搜索关于特定主题的文章，为用户提供简要的文章摘要和链接。\n阅读并总结文章 read_article_and_summarize 利用**PyPDF2**库读取选中文章的PDF文件，代理能够提炼出文章的主要论点、支撑证据和结论。\n环境配置\n!pip install scipy --quiet !pip install tenacity --quiet !pip install tiktoken==0.3.3 --quiet !pip install termcolor --quiet !pip install openai --quiet !pip install arxiv --quiet !pip install pandas --quiet !pip install PyPDF2 --quiet !pip install tqdm --quiet import os import arxiv import ast import concurrent import json import os import pandas as pd import tiktoken from csv import writer from IPython.display import display, Markdown, Latex from openai import OpenAI from PyPDF2 import PdfReader from scipy import spatial from tenacity import retry, wait_random_exponential, stop_after_attempt from tqdm import tqdm from termcolor import colored GPT_MODEL = \u0026#34;gpt-3.5-turbo-0613\u0026#34; EMBEDDING_MODEL = \u0026#34;text-embedding-ada-002\u0026#34; client = OpenAI() 所有下载的论文都被存储在本地**./data/papers目录中，并且每篇文章的详细信息（包括其嵌入向量）都记录在arxiv_library.csv**文件中。\ndirectory = \u0026#39;./data/papers\u0026#39; # Check if the directory already exists if not os.path.exists(directory): # If the directory doesn\u0026#39;t exist, create it and any necessary intermediate directories os.makedirs(directory) print(f\u0026#34;Directory \u0026#39;{directory}\u0026#39; created successfully.\u0026#34;) else: # If the directory already exists, print a message indicating it print(f\u0026#34;Directory \u0026#39;{directory}\u0026#39; already exists.\u0026#34;) # Set a directory to store downloaded papers data_dir = os.path.join(os.curdir, \u0026#34;data\u0026#34;, \u0026#34;papers\u0026#34;) paper_dir_filepath = \u0026#34;./data/arxiv_library.csv\u0026#34; # Generate a blank dataframe where we can store downloaded files df = pd.DataFrame(list()) df.to_csv(paper_dir_filepath) 我们将定义一些utils function用于：\n文章获取与存储：通过**get_articles**函数查询主题相关的文章，系统自动下载文章并记录重要信息及embedding向量。\n文章选择与内容提取：根据用户的查询，系统通过计算embedding向量的相似度来选择最相关的文章，并提取出文章的文本内容。\n内容分块与总结：长文本被分割成多个较小的块，每个块被独立总结。\n汇总总结：所有独立块的总结被汇总成一篇全面的总结，更好地回应用户的查询。\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def embedding_request(text): response = client.embeddings.create(input=text, model=EMBEDDING_MODEL) return response @retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def get_articles(query, library=paper_dir_filepath, top_k=5): \u0026#34;\u0026#34;\u0026#34;This function gets the top_k articles based on a user\u0026#39;s query, sorted by relevance. It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize. \u0026#34;\u0026#34;\u0026#34; client = arxiv.Client() search = arxiv.Search( query = query, max_results = top_k, sort_by = arxiv.SortCriterion.SubmittedDate ) result_list = [] for result in client.results(search): result_dict = {} result_dict.update({\u0026#34;title\u0026#34;: result.title}) result_dict.update({\u0026#34;summary\u0026#34;: result.summary}) # Taking the first url provided result_dict.update({\u0026#34;article_url\u0026#34;: [x.href for x in result.links][0]}) result_dict.update({\u0026#34;pdf_url\u0026#34;: [x.href for x in result.links][1]}) result_list.append(result_dict) # Store references in library file response = embedding_request(text=result.title) file_reference = [ result.title, result.download_pdf(data_dir), response.data[0].embedding, ] # Write to file with open(library, \u0026#34;a\u0026#34;) as f_object: writer_object = writer(f_object) writer_object.writerow(file_reference) f_object.close() return result_list def strings_ranked_by_relatedness( query: str, df: pd.DataFrame, relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), top_n: int = 100, ) -\u0026gt; list[str]: \u0026#34;\u0026#34;\u0026#34;Returns a list of strings and relatednesses, sorted from most related to least.\u0026#34;\u0026#34;\u0026#34; query_embedding_response = embedding_request(query) query_embedding = query_embedding_response.data[0].embedding strings_and_relatednesses = [ (row[\u0026#34;filepath\u0026#34;], relatedness_fn(query_embedding, row[\u0026#34;embedding\u0026#34;])) for i, row in df.iterrows() ] strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True) strings, relatednesses = zip(*strings_and_relatednesses) return strings[:top_n] def read_pdf(filepath): \u0026#34;\u0026#34;\u0026#34;Takes a filepath to a PDF and returns a string of the PDF\u0026#39;s contents\u0026#34;\u0026#34;\u0026#34; # creating a pdf reader object reader = PdfReader(filepath) pdf_text = \u0026#34;\u0026#34; page_number = 0 for page in reader.pages: page_number += 1 pdf_text += page.extract_text() + f\u0026#34;\\nPage Number: {page_number}\u0026#34; return pdf_text # Split a text into smaller chunks of size n, preferably ending at the end of a sentence def create_chunks(text, n, tokenizer): \u0026#34;\u0026#34;\u0026#34;Returns successive n-sized chunks from provided text.\u0026#34;\u0026#34;\u0026#34; tokens = tokenizer.encode(text) i = 0 while i \u0026lt; len(tokens): # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens j = min(i + int(1.5 * n), len(tokens)) while j \u0026gt; i + int(0.5 * n): # Decode the tokens and check for full stop or newline chunk = tokenizer.decode(tokens[i:j]) if chunk.endswith(\u0026#34;.\u0026#34;) or chunk.endswith(\u0026#34;\\n\u0026#34;): break j -= 1 # If no end of sentence found, use n tokens as the chunk size if j == i + int(0.5 * n): j = min(i + n, len(tokens)) yield tokens[i:j] i = j def extract_chunk(content, template_prompt): \u0026#34;\u0026#34;\u0026#34;This function applies a prompt to some input content. In this case it returns a summarized chunk of text\u0026#34;\u0026#34;\u0026#34; prompt = template_prompt + content response = client.chat.completions.create( model=GPT_MODEL, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt}], temperature=0 ) return response.choices[0].message.content def summarize_text(query): \u0026#34;\u0026#34;\u0026#34;This function does the following: - Reads in the arxiv_library.csv file in including the embeddings - Finds the closest file to the user\u0026#39;s query - Scrapes the text out of the file and chunks it - Summarizes each chunk in parallel - Does one final summary and returns this to the user\u0026#34;\u0026#34;\u0026#34; # A prompt to dictate how the recursive summarizations should approach the input paper summary_prompt = \u0026#34;\u0026#34;\u0026#34;Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\u0026#34;\u0026#34;\u0026#34; # If the library is empty (no searches have been performed yet), we perform one and download the results library_df = pd.read_csv(paper_dir_filepath).reset_index() if len(library_df) == 0: print(\u0026#34;No papers searched yet, downloading first.\u0026#34;) get_articles(query) print(\u0026#34;Papers downloaded, continuing\u0026#34;) library_df = pd.read_csv(paper_dir_filepath).reset_index() library_df.columns = [\u0026#34;title\u0026#34;, \u0026#34;filepath\u0026#34;, \u0026#34;embedding\u0026#34;] library_df[\u0026#34;embedding\u0026#34;] = library_df[\u0026#34;embedding\u0026#34;].apply(ast.literal_eval) strings = strings_ranked_by_relatedness(query, library_df, top_n=1) print(\u0026#34;Chunking text from paper\u0026#34;) pdf_text = read_pdf(strings[0]) # Initialise tokenizer tokenizer = tiktoken.get_encoding(\u0026#34;cl100k_base\u0026#34;) results = \u0026#34;\u0026#34; # Chunk up the document into 1500 token chunks chunks = create_chunks(pdf_text, 1500, tokenizer) text_chunks = [tokenizer.decode(chunk) for chunk in chunks] print(\u0026#34;Summarizing each chunk of text\u0026#34;) # Parallel process the summaries with concurrent.futures.ThreadPoolExecutor( max_workers=len(text_chunks) ) as executor: futures = [ executor.submit(extract_chunk, chunk, summary_prompt) for chunk in text_chunks ] with tqdm(total=len(text_chunks)) as pbar: for _ in concurrent.futures.as_completed(futures): pbar.update(1) for future in futures: data = future.result() results += data # Final summary print(\u0026#34;Summarizing into overall summary\u0026#34;) response = client.chat.completions.create( model=GPT_MODEL, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;\u0026#34;\u0026#34;Write a summary collated from this collection of key points extracted from an academic paper. The summary should highlight the core argument, conclusions and evidence, and answer the user\u0026#39;s query. User query: {query} The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions. Key points:\\n{results}\\nSummary:\\n\u0026#34;\u0026#34;\u0026#34;, } ], temperature=0, ) return response 实现一个**Conversation**类，用于支持与API进行多轮对话\n@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) def chat_completion_request(messages, functions=None, model=GPT_MODEL): try: response = client.chat.completions.create( model=model, messages=messages, functions=functions, ) return response except Exception as e: print(\u0026#34;Unable to generate ChatCompletion response\u0026#34;) print(f\u0026#34;Exception: {e}\u0026#34;) return e class Conversation: def __init__(self): self.conversation_history = [] def add_message(self, role, content): message = {\u0026#34;role\u0026#34;: role, \u0026#34;content\u0026#34;: content} self.conversation_history.append(message) def display_conversation(self, detailed=False): role_to_color = { \u0026#34;system\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;assistant\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;function\u0026#34;: \u0026#34;magenta\u0026#34;, } for message in self.conversation_history: print( colored( f\u0026#34;{message[\u0026#39;role\u0026#39;]}: {message[\u0026#39;content\u0026#39;]}\\n\\n\u0026#34;, role_to_color[message[\u0026#34;role\u0026#34;]], ) ) 完成以上基本工作， 接下来是agent的核心内容\n定义工具列表 tools\narxiv_functions = [ { \u0026#34;name\u0026#34;: \u0026#34;get_articles\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Use this function to get academic papers from arXiv to answer user questions.\u0026#34;\u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; User query in JSON. Responses should be summarized and should include the article URL reference \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, }, { \u0026#34;name\u0026#34;: \u0026#34;read_article_and_summarize\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Use this function to read whole papers and provide a summary for users. You should NEVER call this function before get_articles has been called in the conversation.\u0026#34;\u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;\u0026#34;\u0026#34; Description of the article in plain text based on the user\u0026#39;s query \u0026#34;\u0026#34;\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], }, } ] 定义function用于工具调用\ndef chat_completion_with_function_execution(messages, functions=[None]): \u0026#34;\u0026#34;\u0026#34;This function makes a ChatCompletion API call with the option of adding functions\u0026#34;\u0026#34;\u0026#34; response = chat_completion_request(messages, functions) full_message = response.choices[0] if full_message.finish_reason == \u0026#34;function_call\u0026#34;: print(f\u0026#34;Function generation requested, calling function\u0026#34;) return call_arxiv_function(messages, full_message) else: print(f\u0026#34;Function not required, responding to user\u0026#34;) return response def call_arxiv_function(messages, full_message): \u0026#34;\u0026#34;\u0026#34;Function calling function which executes function calls when the model believes it is necessary. Currently extended by adding clauses to this if statement.\u0026#34;\u0026#34;\u0026#34; if full_message.message.function_call.name == \u0026#34;get_articles\u0026#34;: try: parsed_output = json.loads( full_message.message.function_call.arguments ) print(\u0026#34;Getting search results\u0026#34;) results = get_articles(parsed_output[\u0026#34;query\u0026#34;]) except Exception as e: print(parsed_output) print(f\u0026#34;Function execution failed\u0026#34;) print(f\u0026#34;Error message: {e}\u0026#34;) messages.append( { \u0026#34;role\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;name\u0026#34;: full_message.message.function_call.name, \u0026#34;content\u0026#34;: str(results), } ) try: print(\u0026#34;Got search results, summarizing content\u0026#34;) response = chat_completion_request(messages) return response except Exception as e: print(type(e)) raise Exception(\u0026#34;Function chat request failed\u0026#34;) elif ( full_message.message.function_call.name == \u0026#34;read_article_and_summarize\u0026#34; ): parsed_output = json.loads( full_message.message.function_call.arguments ) print(\u0026#34;Finding and reading paper\u0026#34;) summary = summarize_text(parsed_output[\u0026#34;query\u0026#34;]) return summary else: raise Exception(\u0026#34;Function does not exist and cannot be called\u0026#34;) arXiv conversation, Start with a system message\npaper_system_message = \u0026#34;\u0026#34;\u0026#34;You are arXivGPT, a helpful assistant pulls academic papers to answer user questions. You summarize the papers clearly so the customer can decide which to read to answer their question. You always provide the article_url and title so the user can understand the name of the paper and click through to access it. Begin!\u0026#34;\u0026#34;\u0026#34; paper_conversation = Conversation() paper_conversation.add_message(\u0026#34;system\u0026#34;, paper_system_message) # Add a user message paper_conversation.add_message(\u0026#34;user\u0026#34;, \u0026#34;Hi, how does PPO reinforcement learning work?\u0026#34;) chat_response = chat_completion_with_function_execution( paper_conversation.conversation_history, functions=arxiv_functions ) assistant_message = chat_response.choices[0].message.content paper_conversation.add_message(\u0026#34;assistant\u0026#34;, assistant_message) display(Markdown(assistant_message)) output:\nFunction generation requested, calling function Getting search results Got search results, summarizing content I found several papers related to PPO reinforcement learning. Here are a few summaries: Title: \u0026#34;Bandit Profit-maximization for Targeted Marketing\u0026#34; Summary: This paper presents near-optimal algorithms for optimizing profit over multiple demand curves, which are dependent on different ancillary variables while maintaining the same price. It is relevant to PPO reinforcement learning as it tackles a sequential profit-maximization problem. Article URL: Link Title: \u0026#34;Inferring potential landscapes: A Schrödinger bridge approach to Maximum Caliber\u0026#34; Summary: This work extends Schrödinger bridges to account for integral constraints along paths, specifically in the context of Maximum Caliber, a Maximum Entropy principle applied in a dynamic context. While not directly related to PPO reinforcement learning, it can provide insights into stochastic dynamics and inference of time-varying potential landscapes. Article URL: Link Title: \u0026#34;a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification\u0026#34; Summary: This paper proposes an architecture-agnostic detection cost function (a-DCF) for evaluating spoofing-robust automatic speaker verification (ASV) systems. Although it does not focus on PPO reinforcement learning, it provides a metric for evaluating ASV systems in the presence of spoofing attacks. Article URL: Link These papers should provide insights into different aspects of reinforcement learning and related topics. Add another user message to induce our system to use the second tool\npaper_conversation.add_message( \u0026#34;user\u0026#34;, \u0026#34;Can you read the PPO sequence generation paper for me and give me a summary\u0026#34;, ) updated_response = chat_completion_with_function_execution( paper_conversation.conversation_history, functions=arxiv_functions ) display(Markdown(updated_response.choices[0].message.content)) output:\nFunction generation requested, calling function Finding and reading paper Chunking text from paper Summarizing each chunk of text 100%|██████████| 4/4 [00:04\u0026lt;00:00, 1.11s/it] Summarizing into overall summary Core Argument: The paper discusses the potential of using a general-purpose large language model (LLM) to learn the structural biophysics of DNA. The authors show that fine-tuning a LLM, specifically chatGPT 3.5-turbo, can enhance its ability to analyze and design DNA sequences and their structures. The study focuses on the formation of secondary structures in DNA, which are governed by base pairing and stacking bonds. The authors propose a method that involves chaining together models fine-tuned for subtasks and using a chain-of-thought approach to improve the model\u0026#39;s performance. Evidence: The authors use the NUPACK software suite to provide data for training and validation. The expert pipeline approach involves using models that have been fine-tuned for subtasks and feeding their outputs into each other. The models perform better when they explicitly consider the nearest neighbor window and the reverse complement of the sequences. The pipeline approach, where a separate model determines the reverse complement and feeds it to another model for secondary structure prediction, enhances the accuracy of the predictions. The performance of the models improves with larger training sets. Conclusions: The study demonstrates the potential of using LLMs to learn DNA structural biophysics. Integrating experimental data and machine learning is important in scientific research. The expert pipeline approach and breaking down the problem into smaller subtasks improve the performance of the models in DNA sequence analysis. The combination of chain-of-thought and model pipeline provides the best results in analysis tasks. The CoT approach, combined with the reverse complement transformation, yields the highest accuracy in design tasks. The addition of an error checking layer further improves accuracy in design tasks. Sequence design is more challenging than analysis, but error correction can compensate for the increased difficulty. Larger training sets benefit design tasks more. Future research directions include exploring chaining smaller models for performance improvement and using an LLM architecture involving both an encoder and decoder for direct sequence comparison. Function calling via open source LLMs 在考虑成本和隐私性的背景下，我们可能会倾向于在开源的大型语言模型（LLM）上实现函数调用功能。目前，有几个框架支持以类似OpenAI API的形式调用工具（tools call）：\nXinference\nText Generation Inference (TGI)\n而在开源大型语言模型（LLM）方面，支持工具调用的主要有：\nLlama-3\nMixtral-8x7B-Instruct-v0.1\nqwent\nchatGLM-6B\nNexusRaven-13B\ngorilla-openfunctions-v1\n等等\n以下以Xinference和chatGLM-6B为例，探索如何通过OpenAI API的形式调用开源模型的函数调用功能。\n环境\n%pip install -U -q xinference[transformers] openai langchain !pip install typing-extensions --upgrade ## Start Local Server !nohup xinference-local \u0026gt; xinference.log 2\u0026gt;\u0026amp;1 \u0026amp; 模型加载\n!xinference launch -u my-llm --model-name chatglm3 --size-in-billions 6 --model-format pytorch ## Interact with the running model import openai messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Who are you?\u0026#34; } ] client = openai.Client(api_key=\u0026#34;empty\u0026#34;, base_url=f\u0026#34;http://0.0.0.0:9997/v1\u0026#34;) client.chat.completions.create( model=\u0026#34;my-llm\u0026#34;, messages=messages, ) # ChatCompletion(id=\u0026#39;chatda6056ac-da01-11ee-b92e-0242ac1c000c\u0026#39;, choices=[Choice(finish_reason=\u0026#39;stop\u0026#39;, index=0, logprobs=None, message=ChatCompletionMessage(content=\u0026#34;I am an AI assistant named ChatGLM3-6B, which is developed based on the language model jointly trained by Tsinghua University KEG Lab and Zhipu AI Company in 2023. My job is to provide appropriate answers and support to users\u0026#39; questions and requests.\u0026#34;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=None))], created=1709541198, model=\u0026#39;my-llm\u0026#39;, object=\u0026#39;chat.completion\u0026#39;, system_fingerprint=None, usage=CompletionUsage(completion_tokens=-1, prompt_tokens=-1, total_tokens=-1)) without tool using\ncompletion = client.chat.completions.create( model=\u0026#34;my-llm\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the weather like in London?\u0026#34;}] ) # print(handle_response(completion)) print(completion.choices[0].message.content) \u0026#34;\u0026#34;\u0026#34; London has a temperate climate with warm summers and cool winters. The average temperature during the summer months (June to August) is around 18°C, while the winter months (December to February) are around 6°C. The city experiences heavy rainfall throughout the year, with an annual precipitation of around 350 mm. The average precipitation on the weekends is around 40 mm. London\u0026#39;s cloudy skies are common throughout the year, but they are especially prevalent in December and January. \u0026#34;\u0026#34;\u0026#34; tool using\ntools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;]}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, }, } ] def get_completion(messages, model=\u0026#34;my-llm\u0026#34;, temperature=0, max_tokens=500, tools=None, tool_choice=None): response = client.chat.completions.create( model=model, messages=messages, temperature=temperature, max_tokens=max_tokens, tools=tools, tool_choice=tool_choice ) return response.choices[0].message # Defines a dummy function to get the current weather def get_current_weather(location, unit=\u0026#34;fahrenheit\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Get the current weather in a given location\u0026#34;\u0026#34;\u0026#34; weather = { \u0026#34;location\u0026#34;: location, \u0026#34;temperature\u0026#34;: \u0026#34;50\u0026#34;, \u0026#34;unit\u0026#34;: unit, } return json.dumps(weather) messages = [] messages.append({\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What\u0026#39;s the weather like in Boston!\u0026#34;}) assistant_message = get_completion(messages, tools=tools, tool_choice=\u0026#34;auto\u0026#34;) assistant_message = json.loads(assistant_message.model_dump_json()) assistant_message[\u0026#34;content\u0026#34;] = str(assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;]) #a temporary patch but this should be handled differently # remove \u0026#34;function_call\u0026#34; from assistant message del assistant_message[\u0026#34;function_call\u0026#34;] messages.append(assistant_message) # get the weather information to pass back to the model weather = get_current_weather(messages[1][\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;arguments\u0026#34;]) messages.append({\u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: assistant_message[\u0026#34;tool_calls\u0026#34;][0][\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;], \u0026#34;content\u0026#34;: weather}) final_response = get_completion(messages, tools=tools) final_response \u0026#34;\u0026#34;\u0026#34; ChatCompletionMessage(content=\u0026#39;The current weather in Boston is 50 degrees Fahrenheit.\u0026#39;, role=\u0026#39;assistant\u0026#39;, function_call=None, tool_calls=[]) \u0026#34;\u0026#34;\u0026#34; LLM 在执行function calling时经历了什么 可惜我们并不能看到openAI的模型在服务器端发生了什么，但是根据开源的模型和推理框架，我们某种程度上，也能对LLM在执行function calling的背后逻辑一探究竟。\n这部分内容可以从推理框架和开源模型的源码中找到答案。\n根据xinference 的源码：https://github.com/xorbitsai/inference/blob/main/xinference/model/llm/utils.py#L42\n我们主要关注ChatGLM3 和Qwen\n当我们使用以下假设对话时：\nmessages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;今天北京的天气怎么样？\u0026#34; } ] tools = [ { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, } ] 根据GLM的官方文档，则最终给到ChatGLM3模型的prompt应该长这样：\n\u0026lt;|system|\u0026gt; Answer the following questions as best as you can. You have access to the following tools: [ { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given location\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, }, \u0026#34;unit\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;], }, } ] \u0026lt;|user|\u0026gt; 今天北京的天气怎么样？ \u0026lt;|assistant|\u0026gt; 好的，让我们来查看今天的天气 \u0026lt;|assistant|\u0026gt;get_current_weather ```python tool_call(location=\u0026#34;beijing\u0026#34;, unit=\u0026#34;celsius\u0026#34;) ``` \u0026lt;|observation|\u0026gt; {\u0026#34;temperature\u0026#34;: 22} \u0026lt;|assistant|\u0026gt; 根据查询结果，今天北京的气温为 22 摄氏度。 根据xinference 中有关qwen的代码\nelif prompt_style.style_name == \u0026#34;QWEN\u0026#34;: if tools: tool_desc = \u0026#34;\u0026#34;\u0026#34;{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters} Format the arguments as a JSON object.\u0026#34;\u0026#34;\u0026#34; react_instruction = \u0026#34;\u0026#34;\u0026#34;Answer the following questions as best you can. You have access to the following APIs: {tools_text} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tools_name_text}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated zero or more times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin!\u0026#34;\u0026#34;\u0026#34; tools_text = [] tools_name_text = [] for func_info in tools: parameters = [] required_parameters = func_info[\u0026#34;function\u0026#34;][\u0026#34;parameters\u0026#34;].get( \u0026#34;required\u0026#34;, [] ) for name, p in func_info[\u0026#34;function\u0026#34;][\u0026#34;parameters\u0026#34;][ \u0026#34;properties\u0026#34; ].items(): param = dict({\u0026#34;name\u0026#34;: name}, **p) if name in required_parameters: param[\u0026#34;required\u0026#34;] = True parameters.append(param) name = func_info[\u0026#34;function\u0026#34;][\u0026#34;name\u0026#34;] desc = func_info[\u0026#34;function\u0026#34;][\u0026#34;description\u0026#34;] tool_string = tool_desc.format( name_for_model=name, name_for_human=name, # Hint: You can add the following format requirements in description: # \u0026#34;Format the arguments as a JSON object.\u0026#34; # \u0026#34;Enclose the code within triple backticks (`) at the beginning and end of the code.\u0026#34; description_for_model=desc, parameters=json.dumps(parameters, ensure_ascii=False), ) tools_text.append(tool_string) tools_name_text.append(name) tools_text_string = \u0026#34;\\n\\n\u0026#34;.join(tools_text) tools_name_text_string = \u0026#34;, \u0026#34;.join(tools_name_text) tool_system = react_instruction.format( tools_text=tools_text_string, tools_name_text=tools_name_text_string, ) else: tool_system = \u0026#34;\u0026#34; ret = f\u0026#34;\u0026lt;|im_start|\u0026gt;system\\n{prompt_style.system_prompt}\u0026lt;|im_end|\u0026gt;\u0026#34; for message in chat_history: role = get_role(message[\u0026#34;role\u0026#34;]) content = message[\u0026#34;content\u0026#34;] ret += prompt_style.intra_message_sep if tools: if role == \u0026#34;user\u0026#34;: if tool_system: content = tool_system + f\u0026#34;\\n\\nQuestion: {content}\u0026#34; tool_system = \u0026#34;\u0026#34; else: content = f\u0026#34;Question: {content}\u0026#34; elif role == \u0026#34;assistant\u0026#34;: tool_calls = message.get(\u0026#34;tool_calls\u0026#34;) if tool_calls: func_call = tool_calls[0][\u0026#34;function\u0026#34;] f_name, f_args = ( func_call[\u0026#34;name\u0026#34;], func_call[\u0026#34;arguments\u0026#34;], ) content = f\u0026#34;Thought: I can use {f_name}.\\nAction: {f_name}\\nAction Input: {f_args}\u0026#34; elif content: content = f\u0026#34;Thought: I now know the final answer.\\nFinal answer: {content}\u0026#34; elif role == \u0026#34;tool\u0026#34;: role = \u0026#34;function\u0026#34; content = f\u0026#34;Observation: {content}\u0026#34; else: raise Exception(f\u0026#34;Unsupported message role: {role}\u0026#34;) if content: content = content.lstrip(\u0026#34;\\n\u0026#34;).rstrip() ret += f\u0026#34;\u0026lt;|im_start|\u0026gt;{role}\\n{content}\u0026lt;|im_end|\u0026gt;\u0026#34; else: ret += f\u0026#34;\u0026lt;|im_start|\u0026gt;{role}\\n\u0026#34; return ret 会稍微复杂一些， 利用了react的COT方式（代码中的react_instruction），要求模型以一系列的**Thought（思考）、Action（行动）、Action Input（行动输入）和Observation**（观察结果）步骤，最终给出问题的答案，以增加正确性。\n假设使用以下工具列表和对话历史：\n# 工具列表 tools = [ { \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;geo_lookup\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Retrieves geographical information.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;required\u0026#34;: [\u0026#34;query\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The query to lookup.\u0026#34; } } } } } ] chat_history = [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What is the population of Tokyo?\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;tool_calls\u0026#34;: [ { \u0026#34;function\u0026#34;: \u0026#34;geo_lookup\u0026#34;, \u0026#34;arguments\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;Tokyo\u0026#34; } } ], \u0026#34;content\u0026#34;: \u0026#34;The population of Tokyo is about 14 million.\u0026#34; } ] 可以推断出，最终输入Qwen的prompt应该长这样：\nsystem Answer the following questions as best you can. You have access to the following APIs: geo_lookup: Call this tool to interact with the geo_lookup API. What is the geo_lookup API useful for? Retrieves geographical information. Parameters: [{\u0026#34;name\u0026#34;: \u0026#34;query\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The query to lookup.\u0026#34;, \u0026#34;required\u0026#34;: true}] Format the arguments as a JSON object. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [geo_lookup] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated zero or more times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! user Question: What is the population of Tokyo? assistant Thought: I can use geo_lookup to find the information. Action: geo_lookup Action Input: {\u0026#34;query\u0026#34;: \u0026#34;Tokyo\u0026#34;} Observation: The population of Tokyo is about 14 million. Thought: I now know the final answer. Final answer: The population of Tokyo is about 14 million. 在这个例子中，用户询问东京的人口数量。助手利用**geo_lookup**工具进行查询，具体的行动步骤包括：\n思考：助手决定可以使用**geo_lookup**工具来查找信息。\n行动：实际调用**geo_lookup**工具。\n行动输入：向工具传递的参数，即查询**\u0026quot;Tokyo\u0026quot;**。\n观察：观察到的结果，这里是东京的人口大约为1400万。\n最终思考：基于观察结果，助手得出了最终答案。\n最终答案：向用户提供的答案，即东京的人口数量。\n更详细的内容，建议看这篇知乎文章：Qwen Function Calling 的对话模板及训练方法总结; 以及qwen的官方文档：ReAct Prompting 示例\n参考 openAI function calling\nHow to call functions with chat models\nHow_to_call_functions_for_knowledge_retrieval\nQwen Function Calling 的对话模板及训练方法总结\nqwen的官方文档\nGLM的官方文档\ntool-using via Groq API\nJson mode in Groq\nOpenAI JSON Mode \u0026amp; Seeding\nOpenAI API Guide: Using JSON Mode\n","permalink":"https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0openai-function-calling%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/","summary":"\u003ch2 id=\"写在最开始\"\u003e写在最开始\u003c/h2\u003e\n\u003cp\u003e当我们在讨论基于大型语言模型（LLM-based）的智能代理（agent）时，我们究竟在谈论什么？根据Lilian W在其文章《LLM Powered Autonomous Agents》中的讨论，一个智能代理需要具备几个核心能力：规划（Planning）、记忆（Memory）、以及工具使用（Tool use）。特别地，工具使用方面的进展，得益于OpenAI在API中提供的function calling功能，为我们开启了新的可能性。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png\" alt=\"AGI\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eOpenAI function calling，作为智能代理与外部工具交互的基本方式，对于每位从业者来说都是必备技能。随着技术的发展，我们期望的不只是能与我们对话的LLM，而是能够辅助我们使用各种工具、做出决策的智能伙伴。\u003c/p\u003e\n\u003cp\u003e不过需要特别指出的是，最近OpenAI在Chat Completions API中已经废弃了“函数（function）”的使用，转而采用“工具（tool）”。这一变更旨在拓宽LLM集成的功能范围，为更复杂的交互模式铺平道路，如构建能够相互作用的多代理系统。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/Agent%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/tool-using/%e6%88%aa%e5%b1%8f2024-03-28%2015.16.54.png\" alt=\"截屏2024-03-28 15.16.54.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/Agent%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/tool-using/%e6%88%aa%e5%b1%8f2024-03-28%2015.17.38.png\" alt=\"截屏2024-03-28 15.17.38.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e尽管如此，由于语言习惯的原因，本文中仍然会使用function calling的术语来描述OpenAI的tool using功能，因为“function calling”的说法已经深入人心了。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e核心内容概览\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFunction Calling的定义\u003c/strong\u003e：解释什么是function calling，以及它在智能代理工作中的作用。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOpenAI Cookbook示例\u003c/strong\u003e：提供实际的function calling示例，帮助读者理解其在实际应用中的用途。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e开源LLM的Tool Using\u003c/strong\u003e：探索如何在开源大型语言模型中实现工具使用，以及LLM在tool using的时候经历了什么。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cdel\u003e评价与训练\u003c/del\u003e\u003c/strong\u003e\u003cdel\u003e：讨论如何评价开源模型的工具使用能力，以及如何训练LLM进行有效的工具使用。\u003c/del\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e鉴于整理笔记的速度远赶不上更新的速度， 会将第四部份作为单独的部分整理。\u003c/p\u003e\n\u003ch2 id=\"何为function-calling\"\u003e何为function calling\u003c/h2\u003e\n\u003cp\u003e一句话解释：\u003cstrong\u003efunction calling从本质上并不是严格的工具调用， 而是作为工具调用的前奏，它通过更加结构化的方式指导LLM输出，为在本地执行具体函数提供了参数，铺平了道路。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e具体来说，function calling允许LLM在执行过程中通过指定的参数来调用并执行一个特定的函数。这种方式不仅实现了代码的重用和模块化处理，而且能够从模型中获取更可靠的结构化数据回应。在API调用过程中，开发者可以描述想要执行的功能，并让模型智能地选择输出包含所需参数的JSON对象。\u003cstrong\u003e这个过程中，Chat Completions API本身不直接执行任何函数调用，而是生成了可以在开发者代码中实现函数调用的JSON。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003efunction calling的应用范围广泛，如\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e创建智能助手：通过调用外部API回答问题。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e转换指令：将自然语言指令转换成API调用指令。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e数据提取：从文本中提取结构化数据。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efunction calling的过程涵盖了从定义函数集、通过模型生成遵循自定义模式的JSON对象字符串，到在代码中解析这个字符串并调用相应函数的全过程。这一连串操作不仅自动化了交互过程，还确保了执行操作的安全性和准确性。\u003c/p\u003e\n\u003ch2 id=\"一些常见的问题\"\u003e一些常见的问题\u003c/h2\u003e\n\u003ch3 id=\"json-mode\"\u003eJSON mode\u003c/h3\u003e\n\u003cp\u003ejson mode 和tool-using 有什么关系？有了json mode 还需要用到tool-using吗？\u003c/p\u003e\n\u003cp\u003e从json mode 的本质， 更多的是在system prompt 增加一句类似“请以json格式输出”之类的话，然后在LLM输出时增加json结果检查和格式转换。在使用时只需要在\u003ccode\u003eclient.chat.completions.create \u003c/code\u003e中 增加\u003ccode\u003eresponse_format={ \u0026quot;type\u0026quot;: \u0026quot;json_object\u0026quot; }\u003c/code\u003e 即可。\u003c/p\u003e","title":"Agent学习笔记：OpenAI Function Calling完全指南"},{"content":"个人介绍 后现代人类 AI从业者与终身学习者 工作狂 世界探索者 INFJ 阅读 听歌 创作 欢迎来到我的个人博客！\n","permalink":"https://niraya666.github.io/about/","summary":"\u003ch3 id=\"个人介绍\"\u003e个人介绍\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e后现代人类\u003c/li\u003e\n\u003cli\u003eAI从业者与终身学习者\u003c/li\u003e\n\u003cli\u003e工作狂\u003c/li\u003e\n\u003cli\u003e世界探索者\u003c/li\u003e\n\u003cli\u003eINFJ\u003c/li\u003e\n\u003cli\u003e阅读 听歌 创作\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e欢迎来到我的个人博客！\u003c/p\u003e","title":"About"},{"content":" 引言 在信息化时代，数据和知识的快速提取变得尤为重要。特别是对于学术和技术文档，如何从格式丰富且结构复杂的PDF文件中准确地提取信息，是提高研究效率的关键。本文将探讨几种先进的技术方法，用于优化学术论文的PDF解析及其表格内容的处理。\n对于文档解析，langchain 和 llama_index 提供的 document loader 能够支持多种文件类型，覆盖绝大多数文档格式的需求。但对于pdf解析而言还存在着一些挑战。\n人类与机器的阅读差异 尽管PDF格式适合人类阅读，提供了优秀的视觉体验和格式保真，但它并不适合机器阅读。PDF文件通常包含复杂的布局和多样的内容元素，如多列布局、公式和表格，这些都给自动解析带来了挑战。\n常见的PDF解析问题 使用传统的PDF解析库可能遇到多种问题：\n多列布局导致的文本流读取错误。\n公式和表格的解析效果差，难以正确提取信息。\n解析过程中结构化信息（如标题和列表）的丢失。\n影印版PDF的文本无法被标准OCR工具识别。\n高级解析技术 根据unstractued提供的方案，文档解析可以大致分为两种方法：结构化解析和基于图像的解析。结构化解析侧重于从文档中提取文字和其它字符数据。而图像基解析则包括以下两种技术：\nOCR技术：首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字。\n基于Transformer的端到端解析：使用深度学习模型直接将图像转换为相应的文字内容。在这方面，Dount 和 Nougat 模型表现出色，尤其是 Nougat 能够将图像中的文章整理成Markdown或Latex格式，非常适合需要保留结构信息（如标题层级、公式和表格）的场景。\n只可惜Nougat只针对英文的学术论文做了训练，但遇到其他语言如中文论文的效果就不是那么的好了。\n必须强调一点的是，Markdown格式因其简洁性和易于解析的特点，被广泛认为是LLM（大型语言模型）友好的文档格式。Markdown通过明确的标记语法，帮助模型更好地理解文档结构和内容，从而提高信息提取的准确性和效率。特别在存在大量公式，表格的学术论文场景下，Markdown可能是更合适的格式选择。\n快速上手：使用Nougat将pdf解析成适合LLM读取的markdown 依赖按照\n!pip install -q pymupdf python-Levenshtein nltk !pip install -q git+https://github.com/huggingface/transformers.git Load model and processor\nfrom transformers import AutoProcessor, VisionEncoderDecoderModel import torch processor = AutoProcessor.from_pretrained(\u0026#34;facebook/nougat-base\u0026#34;) model = VisionEncoderDecoderModel.from_pretrained(\u0026#34;facebook/nougat-base\u0026#34;) device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; model.to(device) 将pdf转成图像\nfrom typing import Optional, List import io import fitz from pathlib import Path def rasterize_paper( pdf: Path, outpath: Optional[Path] = None, dpi: int = 96, return_pil=False, pages=None, ) -\u0026gt; Optional[List[io.BytesIO]]: \u0026#34;\u0026#34;\u0026#34; Rasterize a PDF file to PNG images. Args: pdf (Path): The path to the PDF file. outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None. dpi (int, optional): The output DPI. Defaults to 96. return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False. pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None. Returns: Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None. \u0026#34;\u0026#34;\u0026#34; pillow_images = [] if outpath is None: return_pil = True try: if isinstance(pdf, (str, Path)): pdf = fitz.open(pdf) if pages is None: pages = range(len(pdf)) for i in pages: page_bytes: bytes = pdf[i].get_pixmap(dpi=dpi).pil_tobytes(format=\u0026#34;PNG\u0026#34;) if return_pil: pillow_images.append(io.BytesIO(page_bytes)) else: with (outpath / (\u0026#34;%02d.png\u0026#34; % (i + 1))).open(\u0026#34;wb\u0026#34;) as f: f.write(page_bytes) except Exception: pass if return_pil: return pillow_images from transformers import StoppingCriteria, StoppingCriteriaList from collections import defaultdict class RunningVarTorch: def __init__(self, L=15, norm=False): self.values = None self.L = L self.norm = norm def push(self, x: torch.Tensor): assert x.dim() == 1 if self.values is None: self.values = x[:, None] elif self.values.shape[1] \u0026lt; self.L: self.values = torch.cat((self.values, x[:, None]), 1) else: self.values = torch.cat((self.values[:, 1:], x[:, None]), 1) def variance(self): if self.values is None: return if self.norm: return torch.var(self.values, 1) / self.values.shape[1] else: return torch.var(self.values, 1) class StoppingCriteriaScores(StoppingCriteria): def __init__(self, threshold: float = 0.015, window_size: int = 200): super().__init__() self.threshold = threshold self.vars = RunningVarTorch(norm=True) self.varvars = RunningVarTorch(L=window_size) self.stop_inds = defaultdict(int) self.stopped = defaultdict(bool) self.size = 0 self.window_size = window_size @torch.no_grad() def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor): last_scores = scores[-1] self.vars.push(last_scores.max(1)[0].float().cpu()) self.varvars.push(self.vars.variance()) self.size += 1 if self.size \u0026lt; self.window_size: return False varvar = self.varvars.variance() for b in range(len(last_scores)): if varvar[b] \u0026lt; self.threshold: if self.stop_inds[b] \u0026gt; 0 and not self.stopped[b]: self.stopped[b] = self.stop_inds[b] \u0026gt;= self.size else: self.stop_inds[b] = int( min(max(self.size, 1) * 1.15 + 150 + self.window_size, 4095) ) else: self.stop_inds[b] = 0 self.stopped[b] = False return all(self.stopped.values()) and len(self.stopped) \u0026gt; 0 将pdf转成markdown\nfrom tqdm import tqdm def trans_pdf_to_markdown(filepath): output = \u0026#34;\u0026#34; images = rasterize_paper(pdf=filepath, return_pil=True) for i in tqdm(range(len(images))): image = Image.open(images[i]) pixel_values = processor(images=image, return_tensors=\u0026#34;pt\u0026#34;).pixel_values # autoregressively generate tokens, with custom stopping criteria (as defined by the Nougat authors) outputs = model.generate(pixel_values.to(device), min_length=1, max_length=3584, bad_words_ids=[[processor.tokenizer.unk_token_id]], return_dict_in_generate=True, output_scores=True, stopping_criteria=StoppingCriteriaList([StoppingCriteriaScores()]), ) generated = processor.batch_decode(outputs[0], skip_special_tokens=True)[0] generated = processor.post_process_generation(generated, fix_markdown=True) output+=generated output+=\u0026#34;\\n\\n\u0026#34; return output filepath = \u0026#39;xxx.pdf\u0026#39; makrdown_ = trans_pdf_to_markdown(filepath) 具体参考Nougat的官方示例。\n表格类数据的RAG问题 表格类目前的解决方案主要有两种：\n对于内容较小的表格，一般采用LLM对表格进行summary， 以table summary构建查询的index，在召回后， 进行表格内容的复原或者根据问题后处理提供至LLM进行生成。\n对于内容较多的表格， 一般采取结构化处理， 也就是存储至数据库， 在查询阶段根据问题匹配对应的表schema， 有模型执行text-to-sql任务，生成sql并执行获得结果，最终根据结果回答内容。\n对于第一种方案，在Langchain 的cookbook中提到了Semi-structured RAG的方法值得借鉴；\n此外在HRoT这篇工作中， 也提到了类似的方法；\n在HRoT这篇工作中， 增加了将table基于问题进行重构的算法，\nType-Aware Table Reconstruction algorithm\n该算法的实现步骤如下：\n问题分类：首先，算法对问题进行分类，判断问题是算术问题还是跨列选择问题。这有助于确定需要从表格中检索的信息类型。\n获取表格和证据：对于算术问题，算法获取与问题相关的表格集合。然后，从这些表格中提取出作为证据的文本和表格片段。\n表格分区：对于每个表格，算法将其分区为多个子表格。这是通过获取表格的跨度列表（Lt）来完成的，该列表包含了表格中每个头部（行和列）的起始和结束位置。\n确定保留的行和列：算法根据问题类型和证据来确定哪些行和列是回答问题所必需的。这是通过分析每个证据的子标题（即表格中的行和列标题）来实现的。\n重建表格：在确定了需要保留的行和列之后，算法根据这些信息重建表格。重建的表格只包含对问题回答有帮助的信息，从而减少了无关信息的干扰。\n在TableQuery 这篇工作中， 利用一系列模块组件，实现了基于LLM对数据库的高效查询。\nTableQuery的架构设计包括以下几个主要模块：\nDatastore：Datastore是一个表格集合，可以通过自然语言查询。这些表格可以是数据库或电子表格目录。除了表格数据，Datastore还包含模式文件，这些文件包含每个表的元数据，如表关键词、列名、列类型、列关键词（用户为列名提供的关键词）等。模式文件可以手动为这些表创建，也可以通过应用各种启发式方法自动生成。\nTable Selector：表选择器根据输入查询从Datastore中选择合适的表。这是通过从输入查询中提取关键词，并找到与问题关键词（以及模式中的表关键词、列名、列关键词等）重叠系数最大的表来完成的。\nKnown Fields Extractor：已知字段提取器提取查询中已经给出值的列。对于这些列中的每一个，也提取了相应的值。这是通过一个预训练的深度学习模型来完成的，该模型用于在自由文本上执行问答。\nUnknown Fields Extractor：未知字段提取器提取需要从选定表中检索值的列。这是通过排除已知字段提取器已经从查询中提取的列，并找到其列关键词与问题关键词重叠系数最大的列来完成的。\nAggregate Function Classifier：一些查询可能需要对选定表中的未知字段的值执行进一步操作。这是通过在生成的SQL查询中包含SQL聚合函数（如COUNT、SUM、MIN、MAX、AVG等）来实现的。聚合函数分类器决定给定输入查询要使用的聚合函数。为了执行这项任务，作者训练了一个两层的神经网络，该网络以使用通用句子编码器编码的查询作为输入，并输出要使用的适当聚合函数（如果有的话）。\nSQL Generator：结合已知字段和值、未知字段和聚合函数来构建SQL查询，当该查询在选定的表上运行时，返回所需的结果。\n关于text-to-sql的更详尽内容， 可以参考这篇综述文章：Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey。 限于篇幅就不展开了。\n参考文献 langchain: Document loaders\nNougat: Neural Optical Understanding for Academic Documents\nlangchain: Semi-structured RAG\nHRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering\nTableQuery： Querying tabular data with natural language\nNatural Language Interfaces for Tabular Data Querying and Visualization: A Survey\n","permalink":"https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/post-cover/rag_toolkits_2.JPG\" alt=\"image\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"引言\"\u003e\u003cstrong\u003e引言\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e在信息化时代，数据和知识的快速提取变得尤为重要。特别是对于学术和技术文档，如何从格式丰富且结构复杂的PDF文件中准确地提取信息，是提高研究效率的关键。本文将探讨几种先进的技术方法，用于优化学术论文的PDF解析及其表格内容的处理。\u003c/p\u003e\n\u003cp\u003e对于文档解析，\u003cstrong\u003e\u003ccode\u003elangchain\u003c/code\u003e\u003c/strong\u003e 和 \u003cstrong\u003e\u003ccode\u003ellama_index\u003c/code\u003e\u003c/strong\u003e 提供的 \u003cstrong\u003e\u003ccode\u003edocument loader\u003c/code\u003e\u003c/strong\u003e 能够支持多种文件类型，覆盖绝大多数文档格式的需求。但对于pdf解析而言还存在着一些挑战。\u003c/p\u003e\n\u003ch3 id=\"人类与机器的阅读差异\"\u003e\u003cstrong\u003e人类与机器的阅读差异\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e尽管PDF格式适合人类阅读，提供了优秀的视觉体验和格式保真，但它并不适合机器阅读。PDF文件通常包含复杂的布局和多样的内容元素，如多列布局、公式和表格，这些都给自动解析带来了挑战。\u003c/p\u003e\n\u003ch3 id=\"常见的pdf解析问题\"\u003e\u003cstrong\u003e常见的PDF解析问题\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e使用传统的PDF解析库可能遇到多种问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e多列布局导致的文本流读取错误。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e公式和表格的解析效果差，难以正确提取信息。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e解析过程中结构化信息（如标题和列表）的丢失。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e影印版PDF的文本无法被标准OCR工具识别。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"高级解析技术\"\u003e\u003cstrong\u003e高级解析技术\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e根据\u003ca href=\"https://unstructured.io/\"\u003eunstractued\u003c/a\u003e提供的方案，文档解析可以大致分为两种方法：结构化解析和基于图像的解析。结构化解析侧重于从文档中提取文字和其它字符数据。而图像基解析则包括以下两种技术：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOCR技术\u003c/strong\u003e：首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e基于Transformer的端到端解析\u003c/strong\u003e：使用深度学习模型直接将图像转换为相应的文字内容。在这方面，\u003cstrong\u003e\u003ccode\u003eDount\u003c/code\u003e\u003c/strong\u003e 和 \u003cstrong\u003e\u003ccode\u003eNougat\u003c/code\u003e\u003c/strong\u003e 模型表现出色，尤其是 \u003cstrong\u003e\u003ccode\u003eNougat\u003c/code\u003e\u003c/strong\u003e 能够将图像中的文章整理成Markdown或Latex格式，非常适合需要保留结构信息（如标题层级、公式和表格）的场景。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e只可惜Nougat只针对英文的学术论文做了训练，但遇到其他语言如中文论文的效果就不是那么的好了。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://unstructured-io.github.io/unstructured/_images/strategy.png\" alt=\"\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e必须强调一点的是，Markdown格式因其简洁性和易于解析的特点，被广泛认为是LLM（大型语言模型）友好的文档格式。Markdown通过明确的标记语法，帮助模型更好地理解文档结构和内容，从而提高信息提取的准确性和效率。特别在存在大量公式，表格的学术论文场景下，Markdown可能是更合适的格式选择。\u003c/p\u003e\n\u003ch2 id=\"快速上手使用nougat将pdf解析成适合llm读取的markdown\"\u003e快速上手：使用Nougat将pdf解析成适合LLM读取的markdown\u003c/h2\u003e\n\u003cp\u003e依赖按照\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eq\u003c/span\u003e \u003cspan class=\"n\"\u003epymupdf\u003c/span\u003e \u003cspan class=\"n\"\u003epython\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eLevenshtein\u003c/span\u003e \u003cspan class=\"n\"\u003enltk\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"err\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003epip\u003c/span\u003e \u003cspan class=\"n\"\u003einstall\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eq\u003c/span\u003e \u003cspan class=\"n\"\u003egit\u003c/span\u003e\u003cspan class=\"o\"\u003e+\u003c/span\u003e\u003cspan class=\"n\"\u003ehttps\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"o\"\u003e//\u003c/span\u003e\u003cspan class=\"n\"\u003egithub\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecom\u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"n\"\u003ehuggingface\u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"n\"\u003etransformers\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egit\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eLoad model and processor\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etransformers\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoProcessor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eVisionEncoderDecoderModel\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etorch\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eprocessor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoProcessor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;facebook/nougat-base\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eVisionEncoderDecoderModel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;facebook/nougat-base\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edevice\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecuda\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eis_available\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;cpu\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edevice\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e将pdf转成图像\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etyping\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eOptional\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eList\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eio\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003efitz\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003epathlib\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003ePath\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003erasterize_paper\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ePath\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eoutpath\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eOptional\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ePath\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003edpi\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e96\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ereturn_pil\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003eOptional\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eList\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eio\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eBytesIO\u003c/span\u003e\u003cspan class=\"p\"\u003e]]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Rasterize a PDF file to PNG images.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Args:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        pdf (Path): The path to the PDF file.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        outpath (Optional[Path], optional): The output directory. If None, the PIL images will be returned instead. Defaults to None.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        dpi (int, optional): The output DPI. Defaults to 96.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        return_pil (bool, optional): Whether to return the PIL images instead of writing them to disk. Defaults to False.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        pages (Optional[List[int]], optional): The pages to rasterize. If None, all pages will be rasterized. Defaults to None.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    Returns:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e        Optional[List[io.BytesIO]]: The PIL images if `return_pil` is True, otherwise None.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003epillow_images\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eoutpath\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003ereturn_pil\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"nb\"\u003eisinstance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ePath\u003c/span\u003e\u003cspan class=\"p\"\u003e)):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003epdf\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003efitz\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003epages\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003epages\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003epages\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003epage_bytes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003ebytes\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epdf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_pixmap\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edpi\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edpi\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epil_tobytes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eformat\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;PNG\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ereturn_pil\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"n\"\u003epillow_images\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eio\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eBytesIO\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epage_bytes\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eoutpath\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"si\"\u003e%02d\u003c/span\u003e\u003cspan class=\"s2\"\u003e.png\u0026#34;\u003c/span\u003e \u003cspan class=\"o\"\u003e%\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;wb\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epage_bytes\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"ne\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ereturn_pil\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003epillow_images\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etransformers\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eStoppingCriteria\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eStoppingCriteriaList\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ecollections\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultdict\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eRunningVarTorch\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e15\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enorm\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eL\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enorm\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecat\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e]),\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecat\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e:],\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e]),\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003evariance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eStoppingCriteriaScores\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eStoppingCriteria\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.015\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e200\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nb\"\u003esuper\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evars\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eRunningVarTorch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enorm\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evarvars\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eRunningVarTorch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eL\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultdict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edefaultdict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003ebool\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nd\"\u003e@torch.no_grad\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__call__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003einput_ids\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLongTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003escores\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eFloatTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003elast_scores\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003escores\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evars\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elast_scores\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecpu\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evarvars\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epush\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evars\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evariance\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003evarvar\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evarvars\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evariance\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eb\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elast_scores\u003c/span\u003e\u003cspan class=\"p\"\u003e)):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003evarvar\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                        \u003cspan class=\"nb\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mf\"\u003e1.15\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e150\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewindow_size\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4095\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estop_inds\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nb\"\u003eall\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estopped\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e将pdf转成markdown\u003c/p\u003e","title":"RAG工具箱：文档解析与表格处理"},{"content":"游记：2024-春 昆明大理 昆明 时间似乎停止了 不知为何在昆明有一种回到兰州的感觉，一样的三线省会，似曾相识的破败老城区街道，赫鲁晓夫楼，砖瓦房，和五六十年代的家属大院，似乎时间就停止流动了。我不认为这是种贬义的表达，至少清晨行走在老街区，唤起了我上学时，清晨走出校园到火车站赶火车的尘封记忆。 不过需要提一嘴的是，似乎当前的实体经济，特别是在老城区，不论是这次旅行所看到的，还是之前在故乡和别的城市所看到的，可以说是很糟糕了。目光所及有一半的店面处于关门转让的状态，而开着的那一半，又有近乎三成处于清仓甩卖中。\n古镇PTSD 在全国绝大多数古镇逐渐趋同演化的当下，不知这是否是件悲哀的事。因为去云南省博物馆的途中会路过官渡古镇，遂决定顺便去看看。从古镇的东北边进入至正门而出，这次访问让我有幸同时见证了从破败而真实的古镇到商业化过度开发的古镇的转变。\n在历史长河中被剥夺了名字的人 在昆明这个城市名字的由来中，有一段被历史辗转淡忘的故事。昆明这一名称起源于昆明夷——西汉时期活跃在洱海周边的一个游牧民族。而在这片土地上生活的滇人，一个拥有先进青铜技术的农耕文明，不断地抵抗游牧民族的侵扰。尽管如此，在历史的长河中，正是这些滇人创造了辉煌的文明，却在历史的演进中失去了自己的名字。这不禁让人感受到一种悲剧的讽刺——在自己孕育辉煌的土地上，却被剥夺了命名的权力。这反映出历史的无情，以及文化与权力之间复杂的交织。\n故乡与迁徙 在省博的临时展区，有幸看到了一个关于迁徙和故乡的特别展。 正巧这次来云南的最初目的也是为了探寻人生的下一个迁徙地。\n今天，流动成为常态。我们祖辈所拥有的那种一生居于一地的安稳逐渐被打破。很多人离开熟悉的地方，到另一处，与来自天南地北的人一起，不知不觉将异乡生活成故乡。\n而提到故乡，你会想到什么？是那方伴你长大的土地，是老屋里围炉团聚的家人，是家乡菜的味道和浓浓的乡音，是家门口熟悉的街道、树林和田地，是小时候玩耍的院子，和那群如今已各奔东西的伙伴。无论走多远，想到那一草一木，一人一景，就安宁。\n这也许就是人们一直以来追寻的栖息地。在远方，在心里，它叫“香格里拉”。\n现代的我们因为各种原因，在故乡与异乡之间辗转，继续着“迁徙与流动”。展览策划过程中，我们以“故乡”“现居地”“理想地”为题，以“故事+展品”的方式开启活动征集，鼓励不同身份的人群通过不同角度的讲述，分享他们的感受与思考。在近一个月的时间里，我们得到了大家的积极回响。在大家的分享中，我们与不同的故事和记忆相连，也看到因为不同而更加多元、广阔的世界。\n这里展示的文字和物品，是记忆的承载，也是对“故的未来。\n乡”的“好久不见”，它记录着我们的成长，也指引着我们\n大理 这次来大理的初始动机是实地考察和调研“数字游民”这个群体，他们是否真正像想象中的自由，他们是如何工作和生活的， 他们背后的故事又是什么？ 以及他们为什么选择大理？\n感谢Dalihub， 让我有幸认识一群有趣的人。\n来自海边房子\nDalihub的秘密空间\n救火途中的直升机\n自由飞翔\n猜你喜欢是“坏”的吗 在大理的第二天，我便幸运地参与了一场与我的工作密切相关的线下沙龙活动，主题涉及推荐系统。我的工作列表中正好有一项是关于如何防止大型科技公司通过推荐系统作恶的问题。 活动中，主讲人“西雅图大黄蜂”提出了一个观点：所有的算法和技术本质上都是中性的，真正的“恶”是由使用它们的人带来的。作为技术从业者，我们当然不希望看到自己的发明像原子弹一样被用于恶劣的目的。但现实往往很残酷，技术的接受与否很大程度上是由资本决定的，而非我们。\n之前我考虑过一个想法：将推荐系统的召回和排序分开，召回过程保留在服务器端，而排序则转移到用户端，这样一来可以提高用户的隐私保护。技术上并不复杂，真正的挑战来自于资本或大公司缺乏推行此类改变的动力。除非有一天，大多数用户愿意为保护隐私牺牲一定的便利性，否则这种改变还遥遥无期，至少在当前的中国是这样的。\n再次感谢主理人Nian和主讲人魏峰，一次难忘的体验。\n真的躺平吗？不，只是喘口气 选择大理的理由： 成本，气候， 和有趣的人们\n在大理，你可以遇到形形色色的人物：那些选择在此旅居的设计师，决定在退休后移居大理的上海夫妇，因成本考虑而搬来的独立开发者，还有视大理为家的咨询师小姐姐。这些都是构成大理独特社群氛围的不同面貌。\n当你不确定下一步该做什么时，不妨去洱海边走走，那里的美景足以让你放慢脚步，深呼吸，重新找回自己的节奏。在大理，即便是“躺平”，也是一种享受生活、与自然和谐共处的态度。\n洱海边落日\n取舍，及时调整自己的欲望 在网上能看到很多对于大理的劝退文， 无外乎就是过度商业化网红化， 城市规划稀烂，交通不方便，宰客现象严重等等。 以上我都认同， 对于一个习惯了大城市便利的人而言， 初到大理的感受的确是如此。 和在地的小伙伴聊下来，发现及时调整自己的欲望还是挺重要的。\n但是，在早晨拉开窗帘看到洱海的一瞬间， 似乎这一切都是值得的。\n随处可见的丁达尔效应\n去跳海，去发疯！\n西南旅游小Tips 注意防晒， 保湿，加湿器和润唇膏很重要。 尽量避开春季，因为春季是风季，很有可能因为大风而错过苍山的缆车。 大理古城的主干道不值得驻足，真正有趣的东西隐藏在巷子里\n这一切就像是一场梦\n感谢在大理遇到的所有人事物\n下一次再见\n","permalink":"https://niraya666.github.io/travel/%E6%B8%B8%E8%AE%B02024-%E6%98%A5-%E6%98%86%E6%98%8E%E5%A4%A7%E7%90%86/","summary":"\u003ch1 id=\"游记2024-春-昆明大理\"\u003e游记：2024-春 昆明大理\u003c/h1\u003e\n\u003ch2 id=\"昆明\"\u003e昆明\u003c/h2\u003e\n\u003ch3 id=\"时间似乎停止了\"\u003e时间似乎停止了\u003c/h3\u003e\n\u003cp\u003e不知为何在昆明有一种回到兰州的感觉，一样的三线省会，似曾相识的破败老城区街道，赫鲁晓夫楼，砖瓦房，和五六十年代的家属大院，似乎时间就停止流动了。我不认为这是种贬义的表达，至少清晨行走在老街区，唤起了我上学时，清晨走出校园到火车站赶火车的尘封记忆。\n不过需要提一嘴的是，似乎当前的实体经济，特别是在老城区，不论是这次旅行所看到的，还是之前在故乡和别的城市所看到的，可以说是很糟糕了。目光所及有一半的店面处于关门转让的状态，而开着的那一半，又有近乎三成处于清仓甩卖中。\u003c/p\u003e\n\u003ch3 id=\"古镇ptsd\"\u003e古镇PTSD\u003c/h3\u003e\n\u003cp\u003e在全国绝大多数古镇逐渐趋同演化的当下，不知这是否是件悲哀的事。因为去云南省博物馆的途中会路过官渡古镇，遂决定顺便去看看。从古镇的东北边进入至正门而出，这次访问让我有幸同时见证了从破败而真实的古镇到商业化过度开发的古镇的转变。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53659997366_7bdf10738b_w.jpg\" alt=\"官渡古镇\"  /\u003e\n\u003c/p\u003e\n\u003ch3 id=\"在历史长河中被剥夺了名字的人\"\u003e在历史长河中被剥夺了名字的人\u003c/h3\u003e\n\u003cp\u003e在昆明这个城市名字的由来中，有一段被历史辗转淡忘的故事。昆明这一名称起源于昆明夷——西汉时期活跃在洱海周边的一个游牧民族。而在这片土地上生活的滇人，一个拥有先进青铜技术的农耕文明，不断地抵抗游牧民族的侵扰。尽管如此，在历史的长河中，正是这些滇人创造了辉煌的文明，却在历史的演进中失去了自己的名字。这不禁让人感受到一种悲剧的讽刺——在自己孕育辉煌的土地上，却被剥夺了命名的权力。这反映出历史的无情，以及文化与权力之间复杂的交织。\u003c/p\u003e\n\u003ch3 id=\"故乡与迁徙\"\u003e故乡与迁徙\u003c/h3\u003e\n\u003cp\u003e在省博的临时展区，有幸看到了一个关于迁徙和故乡的特别展。 正巧这次来云南的最初目的也是为了探寻人生的下一个迁徙地。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e今天，流动成为常态。我们祖辈所拥有的那种一生居于一地的安稳逐渐被打破。很多人离开熟悉的地方，到另一处，与来自天南地北的人一起，不知不觉将异乡生活成故乡。\u003c/p\u003e\n\u003cp\u003e而提到故乡，你会想到什么？是那方伴你长大的土地，是老屋里围炉团聚的家人，是家乡菜的味道和浓浓的乡音，是家门口熟悉的街道、树林和田地，是小时候玩耍的院子，和那群如今已各奔东西的伙伴。无论走多远，想到那一草一木，一人一景，就安宁。\u003c/p\u003e\n\u003cp\u003e这也许就是人们一直以来追寻的栖息地。在远方，在心里，它叫“香格里拉”。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e现代的我们因为各种原因，在故乡与异乡之间辗转，继续着“迁徙与流动”。展览策划过程中，我们以“故乡”“现居地”“理想地”为题，以“故事+展品”的方式开启活动征集，鼓励不同身份的人群通过不同角度的讲述，分享他们的感受与思考。在近一个月的时间里，我们得到了大家的积极回响。在大家的分享中，我们与不同的故事和记忆相连，也看到因为不同而更加多元、广阔的世界。\u003c/p\u003e\n\u003cp\u003e这里展示的文字和物品，是记忆的承载，也是对“故的未来。\u003c/p\u003e\n\u003cp\u003e乡”的“好久不见”，它记录着我们的成长，也指引着我们\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"大理\"\u003e大理\u003c/h2\u003e\n\u003cp\u003e这次来大理的初始动机是实地考察和调研“数字游民”这个群体，他们是否真正像想象中的自由，他们是如何工作和生活的， 他们背后的故事又是什么？ 以及他们为什么选择大理？\u003c/p\u003e\n\u003cp\u003e感谢Dalihub， 让我有幸认识一群有趣的人。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660356884_1676ae103e_z.jpg\" alt=\"E0322096-D284-4044-BFAD-C6655AA9EAB9_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e来自海边房子\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660223478_846693b3a1_w.jpg\" alt=\"259DFD62-6C94-4135-9D79-CD1A775DCFF1_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eDalihub的秘密空间\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660223473_2d7c23b817_w.jpg\" alt=\"5E8B65D3-9970-41AB-9E2A-133E8CB0952C_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e救火途中的直升机\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660223468_35ffcb583c_w.jpg\" alt=\"7F7303CF-6B12-4417-B71A-B0826682E0F2_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e自由飞翔\u003c/p\u003e\n\u003ch3 id=\"猜你喜欢是坏的吗\"\u003e猜你喜欢是“坏”的吗\u003c/h3\u003e\n\u003cp\u003e在大理的第二天，我便幸运地参与了一场与我的工作密切相关的线下沙龙活动，主题涉及推荐系统。我的工作列表中正好有一项是关于如何防止大型科技公司通过推荐系统作恶的问题。\n活动中，主讲人“西雅图大黄蜂”提出了一个观点：所有的算法和技术本质上都是中性的，真正的“恶”是由使用它们的人带来的。作为技术从业者，我们当然不希望看到自己的发明像原子弹一样被用于恶劣的目的。但现实往往很残酷，技术的接受与否很大程度上是由资本决定的，而非我们。\u003c/p\u003e\n\u003cp\u003e之前我考虑过一个想法：将推荐系统的召回和排序分开，召回过程保留在服务器端，而排序则转移到用户端，这样一来可以提高用户的隐私保护。技术上并不复杂，真正的挑战来自于资本或大公司缺乏推行此类改变的动力。除非有一天，大多数用户愿意为保护隐私牺牲一定的便利性，否则这种改变还遥遥无期，至少在当前的中国是这样的。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://mmbiz.qpic.cn/mmbiz_png/XVQWiaVrXNFVWAFWZMMYiczNYLvg9dPX3JKlYNQexPiceosvQXz4CHIic0ZN0Naw6BHDxQj7Rd1kZvFoBV9HBxKZ4A/640?wx_fmt=png\u0026amp;from=appmsg\u0026amp;tp=webp\u0026amp;wxfrom=5\u0026amp;wx_lazy=1\u0026amp;wx_co=1\" alt=\"的多面人生.JPG\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e再次感谢主理人Nian和主讲人魏峰，一次难忘的体验。\u003c/p\u003e\n\u003ch3 id=\"真的躺平吗不只是喘口气\"\u003e真的躺平吗？不，只是喘口气\u003c/h3\u003e\n\u003cp\u003e选择大理的理由： 成本，气候， 和有趣的人们\u003c/p\u003e\n\u003cp\u003e在大理，你可以遇到形形色色的人物：那些选择在此旅居的设计师，决定在退休后移居大理的上海夫妇，因成本考虑而搬来的独立开发者，还有视大理为家的咨询师小姐姐。这些都是构成大理独特社群氛围的不同面貌。\u003c/p\u003e\n\u003cp\u003e当你不确定下一步该做什么时，不妨去洱海边走走，那里的美景足以让你放慢脚步，深呼吸，重新找回自己的节奏。在大理，即便是“躺平”，也是一种享受生活、与自然和谐共处的态度。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53659998511_c358f49f0b_c.jpg\" alt=\"458FFF9D-2F2B-44BA-97C6-61BCE1D4637D_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e洱海边落日\u003c/p\u003e\n\u003ch3 id=\"取舍及时调整自己的欲望\"\u003e取舍，及时调整自己的欲望\u003c/h3\u003e\n\u003cp\u003e在网上能看到很多对于大理的劝退文， 无外乎就是过度商业化网红化， 城市规划稀烂，交通不方便，宰客现象严重等等。 以上我都认同， 对于一个习惯了大城市便利的人而言， 初到大理的感受的确是如此。 和在地的小伙伴聊下来，发现及时调整自己的欲望还是挺重要的。\u003c/p\u003e\n\u003cp\u003e但是，在早晨拉开窗帘看到洱海的一瞬间， 似乎这一切都是值得的。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53659999386_a95d8713d9_w.jpg\" alt=\"732C07E9-39CD-430A-87F5-2C0D20084DF2_1_105_c.jpeg\"  /\u003e\n\n随处可见的丁达尔效应\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e去跳海，去发疯！\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://live.staticflickr.com/65535/53660223128_1d2709b4df_w.jpg\" alt=\"63447450-9DD1-4BE2-8450-25E4E5F54DBA_1_105_c.jpeg\"  /\u003e\n\u003c/p\u003e","title":"游记：2024-春 昆明大理"},{"content":"写在最前面 在过去的几个月中，我主要致力于与RAG（检索增强生成模型）相关的研究和实验工作。通过深入地分析众多论文和项目代码，我发现虽然在RAG领域取得基本成就相对容易，但要实现出色的成果却异常困难。\n此系列文章旨在分享我在RAG研究中的心得和挑战。正如我们从OpenAI的开发者日活动中看到的，即使是在这个领域的领头羊也在不断试验和努力以寻求突破。\n在我们深入探索RAG系统提升的各种方法之前，有一个基本问题需要解决：我们如何准确地评估一个RAG系统的性能？这就像是在机器学习或深度学习任务中没有给定的测试集和评价标准，我们难以判断实验的成功与否。\n因此，本系列的第一篇文章将聚焦于介绍RAG系统的评价方法、相关指标以及测试框架，为我们接下来的探索设定明确的标准和目标。\n测试框架 以下是一些测试框架，为RAG系统评估提供了强大的支持。\nTruLens TruLens提供了一个独特的视角来评估和跟踪大型语言模型（LLM）实验，通过一系列创新的功能和方法，帮助开发者和研究人员更深入地了解模型性能和行为。\nTruLens的反馈功能（Feedback Functions）是其核心概念之一，提供了一种程序化的方法来评估应用的运行表现。这些函数从“可扩展性”和“有意义性”两个维度出发，考虑评估的范围，旨在为用户提供有价值的反馈，帮助他们理解和改进他们的LLM应用。\n在RAG应用中，提供准确的上下文信息至关重要，以避免生成不真实的“幻觉”答案。TruLens采用了创新性的RAG三元组评估方法，专门针对RAG架构的每个环节进行幻觉风险评估，确保模型的每个部分都能有效地工作，从而减少误导信息的产生。\n上下文相关性（Context Relevance） 上下文相关性是评估RAG应用的第一步，确保每一段检索到的上下文都与提出的查询紧密相关。TruLens利用序列化记录的结构来评估上下文的相关性，这是一个关键的步骤，确保模型在正确的信息上生成回答。\n真实性（Groundedness） 在检索到的上下文信息的基础上，大型语言模型将生成答案。TruLens强调了独立验证每个回答的重要性，以确保它们基于可靠信息，并且能够在检索到的上下文中找到支持的证据。这一步骤是确保模型回答的真实性和可靠性的关键。\n答案相关性（Answer Relevance） 最后，评估需要确保最终回答有效地解答了原始问题，这通过评估应用的最终回答与用户输入的相关性来实现。这一过程确保了模型的输出不仅是真实的，而且是对用户查询有用的。\nTruLens还提出了“诚实、无害和有帮助”的评估原则（Honest, Harmless, and Helpful Evaluations），这些原则旨在确保LLM应用在提供帮助的同时，也是安全和可信的。\nRagas Ragas框架专为评估检索增强生成（RAG）系统而设计，定义了四个核心评估指标：上下文相关性（context_relevancy）、上下文回溯（context_recall）、忠实度（faithfulness）和答案相关性（answer_relevancy）。这些指标共同构成了Ragas的评分体系，提供了一个全面的方法来评价RAG系统的性能。\n此外，Ragas巧妙地利用大语言模型（LLM）进行无参考评估，有效降低了评估成本。通过这种方法，Ragas能够提供一种既经济又有效的方式来评估RAG系统，尤其是在处理大量数据和复杂查询时。\n其他测试框架\nDeepEval\nDeepEval How to Evaluate RAG Applications in CI/CD Pipelines with DeepEval\nARES\ngithub: https://github.com/stanford-futuredata/ARES\nPaper: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\nLangChain Evals\nLlama Index Evals\nUpTrain\n数据 在评估数据集时，不必依赖人工标注的标准答案，而是通过底层的大语言模型 (LLM) 来进行评估。\n为了对 RAG 流程进行评估，需要以下几种信息：\nquestion：这是RAG流程的输入，即用户的查询问题。\nanswer：这是由RAG流程生成的答案，也就是输出结果。\ncontexts：这是为了解答question而从外部知识源检索到的相关上下文信息。\n指标 在深入研究检索增强生成（RAG）系统和其他相关技术时，了解和使用正确的评估指标至关重要。以下是几个关键指标，它们帮助我们量化和评估检索系统的效能：\n检索指标 MRR（平均倒数排名） MRR是衡量检索系统性能的一种方法，特别关注于检索结果中的首次正确命中的排名。MRR的高值表示系统能够更频繁地将相关结果排在前列。\n$$ MRR = \\frac{1}{查询数量}\\sum_{i=1}^{查询数量} \\frac{1}{首次正确命中的排名} $$\n这个指标特别有用，因为它直接关注于用户最有可能查看的第一个搜索结果的质量。\n召回率（Recall） 召回率是另一个重要指标，它衡量了系统检索到的相关文档数量与总的相关文档数量之间的比例。高召回率意味着系统能够检索到更多的相关文档。\n$$ Recall = \\frac{检索到的相关文档数量}{总的相关文档数量} $$\nNDCG（标准化折扣累积增益） NDCG（Normalized Discounted Cumulative Gain）是一个在信息检索、推荐系统和机器学习领域常用的评估指标，用于衡量一个系统或模型返回的结果列表的质量。NDCG特别关注于结果的排序质量，即最相关或最有价值的结果是否排在了列表的前面。与其他评估指标相比，NDCG的独特之处在于它考虑了结果的相关性（relevance）不仅是二元的（相关或不相关），而且可以是多级的（例如，从不相关到非常相关的多个级别）。\nEM（精确匹配） EM度量了系统输出的答案与标准答案完全一致的比例，是评估系统准确度的直接方式。在某些场景下，即使是非常小的差异也可能导致答案被视为不正确，这使得EM成为一个严格的评估标准。\n基于大语言模型评估的LLM生成指标 在RAG任务中，对LLM回答的问题主要关注了回答的可验证性（verifiability），即是否严格遵循检索到的上下文如实回答。可验证性由两部分组成：\n高引用召回率（high citation recall）：即所有生成的内容都有充分的引用（外部知识）支持。\n高引用精度（high citation precision）：每个引用是否真的支持生成的内容。\n回答相关性（Answer Relevance） 回答相关性关注的是系统生成的回答与用户提出的问题之间的相关性。理想情况下，回答应该直接且准确地对应于问题，没有偏离主题或提供不相关的信息。\nTruLens中回答相关性的计算方式： TruLens通过提供一种基于LLM的评估方法，允许开发者和研究人员通过编程方式获取对系统生成回答的相关性评估。这种方法利用链式推理（Chain of Thought, CoT）增强理解和推理过程，为评估提供透明度和可解释性。\nfrom trulens_eval.feedback.provider.openai import OpenAI openai_provider = OpenAI() qa_relevance = ( Feedback(openai_provider.relevance_with_cot_reasons, name=\u0026#34;Answer Relevance\u0026#34;) .on_input_output() ) 在relevance_with_cot_reasons方法中，使用聊天完成模型来评估回答对于提示的相关性，并揭示评分背后的推理过程。这种方法不仅考虑回答的内容和长度，而且还评估其是否全面回答了问题，并提供了与问题所有部分相关的上下文信息。\nRAGAS中回答相关性的计算逻辑： RAGAS通过利用LLM重新生成问题（QUESTION_GEN），然后计算这个重新生成的问题与原始问题之间的相似度来评估回答的相关性。这种方法特别关注于系统生成回答的准确性和与原始问题的对应关系。\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_answer_relevance.py QUESTION_GEN = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34; Generate question for the given answer. Answer:\\nThe PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India Question: When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from? Answer:{answer} Question: \u0026#34;\u0026#34;\u0026#34; ) 不过，由于用户提出的问题通常比较简略，使用RAGAS计算出的回答相关性通常较低。\n上下文相关性（Context Relevance） 上下文相关性专注于评估给定上下文（例如从数据库或文档中检索到的信息片段）与用户查询之间的相关性。高相关性的上下文信息为大型语言模型（LLM）提供了生成准确回答的基础。\n在TruLens中上下文相关性的计算方式： 在TruLens，上下文相关性的评估使用了与回答相关性相同的反馈函数，但是采用了不同的选择器来专注于输入（用户问题）和检索到的上下文信息之间的相关性。这通过对输入问题和来源节点中的文本应用.on_input().on(TruLlama.select_source_nodes().node.text)选择器并计算它们的平均相关性得分来实现。\nqs_relevance = ( Feedback(openai_provider.relevance_with_cot_reasons, name=\u0026#34;Context Relevance\u0026#34;) .on_input() .on(TruLlama.select_source_nodes().node.text) .aggregate(np.mean) ) 底层计算逻辑：此过程首先识别出与用户问题直接相关的上下文信息，评估这些信息的相关性，可能会应用链式推理（CoT）方法来提高评分的准确性和深度。最终生成的平均分表示了上下文信息的整体相关性，分值范围从0到1。\n在RAGAS中上下文相关性的计算逻辑： RAGAS采用了一种略有不同的方法来计算上下文相关性。它通过提取与问题相关的上下文句子（candidate sentences），并对这些句子进行自洽性检查。然后，使用提取的句子数量与检索到的上下文中的总句子数量的比率作为评分标准。\n$$ \\text{context relevancy} = \\frac{|S|}{|\\text{Total number of sentences in retrieved context}|} $$\n这个方法通过使用LLM根据问题和上下文，从上下文中提取出能够支持回答的句子，进而计算这些候选句子和上下文的长度占比（这里使用的是词元数量的比值）。\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_context_relevancy.py CONTEXT_RELEVANCE = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;\\ Please extract relevant sentences from the provided context that is absolutely required answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \u0026#34;Insufficient Information\u0026#34;. While extracting candidate sentences you\u0026#39;re not allowed to make any changes to sentences from given context. question:{question} context:\\n{context} candidate sentences:\\n\u0026#34;\u0026#34;\u0026#34; ) 真实性或忠实度（Groundedness or Faithfulness） 在TruLens和RAGAS中，真实性或忠实度的评估旨在衡量生成的声明或回答在事实上的准确性及其对源材料的依赖程度。这一评估标准对于确保生成内容的质量和可信度至关重要。\nTruLens中的Groundedness 在TruLens框架中，groundedness的评估通过检查声明中的每个句子是否在源材料中有支持来进行。这个过程利用大型语言模型（LLM）和链式推理（Chain of Thought, CoT）方法来增强评估的准确性和深度。具体来说，评估方法会将整个声明作为一个整体进行处理，并对声明中的每个句子赋予一个从0到10的评分，0代表没有任何信息重叠，而10代表信息完全重叠。\ngrounded = Groundedness(groundedness_provider=openai_provider) groundedness = ( Feedback(grounded.groundedness_measure_with_cot_reasons, name=\u0026#34;Groundedness\u0026#34;) .on(TruLlama.select_source_nodes().node.text) .on_output() .aggregate(grounded.grounded_statements_aggregator) ) RAGAS中的Faithfulness 而在RAGAS框架中，Faithfulness（忠实度）的概念与TruLens中的Groundedness（真实性）相似，旨在评估生成回答的事实一致性。忠实度得分通过比较生成回答中的声明与给定上下文的一致性来计算，特别是检查回答中的声明是否可以从给定的上下文中推断出来。\n$$ \\text{Faithfulness score} = \\frac{|\\text{Number of claims that can be inferred from given context}|}{|\\text{Total number of claims in the generated answer}|} $$\n# https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_faithfulness.py LONG_FORM_ANSWER_PROMPT = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34;\\ Given a question and answer, create one or more statements from each sentence in the given answer. question: Who was Albert Einstein and what is he best known for? answer: He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics. statements:\\nAlbert Einstein was born in Germany.\\nAlbert Einstein was best known for his theory of relativity. question: Cadmium Chloride is slightly soluble in this chemical, it is also called what? answer: alcohol statements:\\nCadmium Chloride is slightly soluble in alcohol. question: Were Shahul and Jithin of the same nationality? answer: They were from different countries. statements:\\nShahul and Jithin were from different countries. question:{question} answer: {answer} statements:\\n\u0026#34;\u0026#34;\u0026#34; # noqa: E501 ) NLI_STATEMENTS_MESSAGE = HumanMessagePromptTemplate.from_template( \u0026#34;\u0026#34;\u0026#34; Prompt: Natural language inference Consider the given context and following statements, then determine whether they are supported by the information present in the context.Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. Context:\\nJohn is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects. statements:\\n1. John is majoring in Biology.\\n2. John is taking a course on Artificial Intelligence.\\n3. John is a dedicated student.\\n4. John has a part-time job.\\n5. John is interested in computer programming.\\n Answer: 1. John is majoring in Biology. Explanation: John\u0026#39;s major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology. Verdict: No. 2. John is taking a course on Artificial Intelligence. Explanation: The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI. Verdict: No. 3. John is a dedicated student. Explanation: The prompt states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication. Verdict: Yes. 4. John has a part-time job. Explanation: There is no information given in the context about John having a part-time job. Therefore, it cannot be deduced that John has a part-time job. Verdict: No. 5. John is interested in computer programming. Explanation: The context states that John is pursuing a degree in Computer Science, which implies an interest in computer programming. Verdict: Yes. Final verdict for each statement in order: No. No. Yes. No. Yes. context:\\n{context} statements:\\n{statements} Answer: 微调模型评估 在评估RAG系统时，虽然GPT-4等大型语言模型（LLM）因其高性能而被广泛使用，但由于成本和本地部署的需要，选择一个有效的本地部署开源模型也成为了许多研究和应用场景的必需。然而，与GPT-4等高级模型相比，这些本地开源模型的能力通常存在差距，特别是在特定领域内的应用效果上。因此，针对特定领域微调一个LLM用于评估变得尤为重要。\n以下是使用Trulens测试框架，基于笔者所在领域的语料库，对同一个RAG系统使用不同的开源LLM进行评价的结果展示。特别关注的是不同验证模型的失败评分率，这里的失败评分率指的是模型无法按照提示给出有效评分的情况（例如，要求在0～1范围内打分，但模型未给出评分或评分超出范围）。\n失败评分率对比\neval model Groundedness-FailRatio Context-Relevance-FailRatio Answer-Relevance-FailRatio Records gpt-3.5-turbo 0.00 0.00 0.00 41 gpt-4-turbo 0.00 0.00 0.00 41 Qwen1.5-14B-chat 0.07 0.05 0.02 41 Qwen1.5-7B-chat 0.10 0.10 0.07 41 zephyr-7b-beta 0.27 0.22 0.22 41 Qwen1.5-4B-chat 0.32 0.15 0.17 41 chatGLM3-6b-32K 0.36 0.71 0.54 41 微调策略和建议 为了确保评价结果的稳定性和准确性，微调模型时使用的数据集应特别关注于测试框架中使用的提示格式。这样做旨在提高模型对特定提示格式的响应能力，从而提高评价的准确度和一致性。\n此外，从测试结果来看，建议至少使用13B以上的模型进行评价。较小的模型可能在遵循指令和评分准确性方面存在挑战，从而影响评价结果的可靠性。\n对于这个场景的微调，一般采用LoRA微调方法即可满足需求。LoRA（Low-Rank Adaptation）是一种有效的微调技术，可以在不大幅增加模型参数的前提下，通过更新模型的少数关键参数来实现性能的提升。这种方法特别适用于需要特定领域知识增强的场景。\n如果能够收集到足够多的人类反馈结果，采用RLHF（Reinforcement Learning from Human Feedback）专门训练一个评价模型也是一个可行的选择。RLHF通过从人类反馈中学习来优化模型的性能，这可以在成本可控的范围内提供更精确的评价结果。\n传统NLP评估 BLEU BLEU（Bilingual Evaluation Understudy）通过计算机器翻译输出与一个或多个人工翻译的参考译文之间的词汇精确度来评价翻译的质量。BLEU的主要目的是自动地评估文本翻译的好坏，尽量接近人类翻译质量评估的结果。\nROUGE ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是自然语言处理（NLP）任务中常用的一种评估指标，尤其在自动文摘（自动摘要）和机器翻译等领域中非常流行。它主要用于评估自动生成的文本与人工编写的参考文本之间的相似度。ROUGE指标通过计算生成文本与参考文本之间的重叠来量度生成文本的质量。\n常见的ROUGE度量方式有：基于n-gram的重叠度量方法(如ROUGE-1, ROUGE-2)。它计算生成文本与参考文本之间共有的n-grams的数量，并以此评估生成文本的质量。以及基于最长公共子序列（LCS） 的 ROUGE-L，它考虑了句子级别的结构相似性，不仅仅是简单的词汇重叠。通过计算最长公共子序列的长度，ROUGE-L能够捕捉到生成文本和参考文本之间的顺序依赖性，从而提供更全面的相似度评估。\n对于n-gram ROUGE有两个变体： 召回率（Recall）：参考文本中与生成文本共有的n-grams数量除以参考文本中的n-grams总数。精确率（Precision）：参考文本中与生成文本共有的n-grams数量除以生成文本中的n-grams总数。以及二者的调和平均 F1-score。\nMore Details：\npaper: ROUGE: A Package for Automatic Evaluation of Summaries\nblog: GenAI model evaluation metric — ROUGE\n例子：\n# !pip install rouge from rouge import Rouge rouge = Rouge() long = \u0026#39;It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us.\u0026#39; short = \u0026#39;It was an age of wisdom, foolishness, belief, Light, Darkness, hope, and despair, with both light and darkness.\u0026#39; scores = rouge.get_scores(short, long) print(scores) # output: \u0026#39;\u0026#39;\u0026#39; [{\u0026#39;rouge-1\u0026#39;: {\u0026#39;r\u0026#39;: 0.39285714285714285, \u0026#39;p\u0026#39;: 0.6470588235294118, \u0026#39;f\u0026#39;: 0.4888888841876543}, \u0026#39;rouge-2\u0026#39;: {\u0026#39;r\u0026#39;: 0.06976744186046512, \u0026#39;p\u0026#39;: 0.17647058823529413, \u0026#39;f\u0026#39;: 0.09999999593888906}, \u0026#39;rouge-l\u0026#39;: {\u0026#39;r\u0026#39;: 0.39285714285714285, \u0026#39;p\u0026#39;: 0.6470588235294118, \u0026#39;f\u0026#39;: 0.4888888841876543}}] \u0026#39;\u0026#39;\u0026#39; where r, p, and f representing for recall, precision, and f_score.\nROUGE指标的缺点：\n依赖于参考文本： 如果参考文本的质量不高或者数量不足，可能会导致评分不准确\n忽略语义信息： 即使两段文本表达相同的意思，但使用了不同的词汇或表达方式，ROUGE评分也可能较低\n无法评价文本的流畅性和一致性\n对长文本的评价能力有限\n无法全面评估信息的新颖性和重要性\n数据集 一些公开的RAG数据集\nHotpotQA (HQA) 数据集链接：HotpotQA at Hugging Face\n特点：基于维基百科的问答数据集，需要阅读多个支持文档来回答和推理问题。问题多样，不局限于任何预先存在的知识库。提供了句子级别的支持以强化LLM的推理需求。最后，提供了新类型的事实对比问题，测试LLMs提取和比较文本中各种实体属性的能力。\nQasper (QASP) 数据集链接：Qasper at Hugging Face\n特点：基于NLP论文的问答数据集，筛选自Semantic Scholar Open Research Corpus (S2ORC)。\nNarrativeQA (NQA) 数据集链接：NarrativeQA at Hugging Face\n特点：NarrativeQA是一个英语语言的故事和相应问题的数据集，旨在测试阅读理解能力，特别是对长文档的理解。\nQuALITY (QLTY) 数据集链接：QuALITY at Hugging Face\n特点：一个基于故事和文章的多项选择问答数据集，来源包括Project Gutenberg和Open American National Corpus等资源。\nPopQA 论文链接：PopQA Paper\n数据集链接：PopQA at Hugging Face\n特点：PopQA是一个大规模的开放领域问答（QA）数据集，包含14k个以实体为中心的QA对。每个问题都是通过使用模板将从Wikidata检索到的知识元组转换而来的。\nTriviaQA 数据集链接：TriviaQA at Hugging Face\n特点：TriviaqQA是一个阅读理解数据集，包含超过650K的问题-答案-证据三元组。TriviaqQA包括由琐事爱好者编写的95K个问题-答案对。\nASQA 数据集链接：ASQA at Hugging Face\n特点：ASQA是第一个专注于含糊事实问题的长形式问答数据集。与以往的长形式答案数据集不同，每个问题都标注了长形式答案和可由生成段落回答的提取式问答对。\nPUBHEALTH 数据集链接：PUBHEALTH at Hugging Face\n特点：一个包含11,832个用于事实检查的声明的数据集，这些声明涉及一系列健康话题，包括生物医学主题（如传染病、干细胞研究）、政府医疗政策（如堕胎、心理健康、妇女健康）以及其他与公共健康相关的故事。\n推荐阅读 RAGAS Paper\n用 RAGAs（检索增强生成评估）评估 RAG（检索增强型生成）应用 [译]\n如何构建高效的 RAG 系统\nEvaluating Verifiability in Generative Search Engines\nBuilding and Evaluating Advanced RAG\nSteps In Evaluating Retrieval Augmented Generation (RAG) Pipelines\nGenAI model evaluation metric — ROUGE\nRetrieval Augmented Generation (RAG) for LLMs\nRAG Evaluation\n","permalink":"https://niraya666.github.io/posts/rag_toolkit_eval/","summary":"\u003ch2 id=\"写在最前面\"\u003e写在最前面\u003c/h2\u003e\n\u003cp\u003e在过去的几个月中，我主要致力于与RAG（检索增强生成模型）相关的研究和实验工作。通过深入地分析众多论文和项目代码，我发现虽然在RAG领域取得基本成就相对容易，但要实现出色的成果却异常困难。\u003c/p\u003e\n\u003cp\u003e此系列文章旨在分享我在RAG研究中的心得和挑战。正如我们从OpenAI的开发者日活动中看到的，即使是在这个领域的领头羊也在不断试验和努力以寻求突破。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/rag_toolkits/1_1.png\" alt=\"from openAI devday\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e在我们深入探索RAG系统提升的各种方法之前，有一个基本问题需要解决：我们如何准确地评估一个RAG系统的性能？这就像是在机器学习或深度学习任务中没有给定的测试集和评价标准，我们难以判断实验的成功与否。\u003c/p\u003e\n\u003cp\u003e因此，本系列的第一篇文章将聚焦于介绍RAG系统的评价方法、相关指标以及测试框架，为我们接下来的探索设定明确的标准和目标。\u003c/p\u003e\n\u003ch2 id=\"测试框架\"\u003e测试框架\u003c/h2\u003e\n\u003cp\u003e以下是一些测试框架，为RAG系统评估提供了强大的支持。\u003c/p\u003e\n\u003ch3 id=\"trulens\"\u003e\u003cstrong\u003eTruLens\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eTruLens提供了一个独特的视角来评估和跟踪大型语言模型（LLM）实验，通过一系列创新的功能和方法，帮助开发者和研究人员更深入地了解模型性能和行为。\u003c/p\u003e\n\u003cp\u003eTruLens的反馈功能（Feedback Functions）是其核心概念之一，提供了一种程序化的方法来评估应用的运行表现。这些函数从“可扩展性”和“有意义性”两个维度出发，考虑评估的范围，旨在为用户提供有价值的反馈，帮助他们理解和改进他们的LLM应用。\u003c/p\u003e\n\u003cp\u003e在RAG应用中，提供准确的上下文信息至关重要，以避免生成不真实的“幻觉”答案。TruLens采用了创新性的RAG三元组评估方法，专门针对RAG架构的每个环节进行幻觉风险评估，确保模型的每个部分都能有效地工作，从而减少误导信息的产生。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://www.trulens.org/assets/images/RAG_Triad.jpg\" alt=\"TruLens RAG_Triad\"  /\u003e\n\u003c/p\u003e\n\u003ch4 id=\"上下文相关性context-relevance\"\u003e上下文相关性（Context Relevance）\u003c/h4\u003e\n\u003cp\u003e上下文相关性是评估RAG应用的第一步，确保每一段检索到的上下文都与提出的查询紧密相关。TruLens利用序列化记录的结构来评估上下文的相关性，这是一个关键的步骤，确保模型在正确的信息上生成回答。\u003c/p\u003e\n\u003ch4 id=\"真实性groundedness\"\u003e真实性（Groundedness）\u003c/h4\u003e\n\u003cp\u003e在检索到的上下文信息的基础上，大型语言模型将生成答案。TruLens强调了独立验证每个回答的重要性，以确保它们基于可靠信息，并且能够在检索到的上下文中找到支持的证据。这一步骤是确保模型回答的真实性和可靠性的关键。\u003c/p\u003e\n\u003ch4 id=\"答案相关性answer-relevance\"\u003e答案相关性（Answer Relevance）\u003c/h4\u003e\n\u003cp\u003e最后，评估需要确保最终回答有效地解答了原始问题，这通过评估应用的最终回答与用户输入的相关性来实现。这一过程确保了模型的输出不仅是真实的，而且是对用户查询有用的。\u003c/p\u003e\n\u003cp\u003eTruLens还提出了“诚实、无害和有帮助”的评估原则（Honest, Harmless, and Helpful Evaluations），这些原则旨在确保LLM应用在提供帮助的同时，也是安全和可信的。\u003c/p\u003e\n\u003ch3 id=\"ragas\"\u003eRagas\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://docs.ragas.io/en/stable/_static/imgs/component-wise-metrics.png\" alt=\"Ragas\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eRagas框架专为评估检索增强生成（RAG）系统而设计，定义了四个核心评估指标：上下文相关性（context_relevancy）、上下文回溯（context_recall）、忠实度（faithfulness）和答案相关性（answer_relevancy）。这些指标共同构成了Ragas的评分体系，提供了一个全面的方法来评价RAG系统的性能。\u003c/p\u003e\n\u003cp\u003e此外，Ragas巧妙地利用大语言模型（LLM）进行无参考评估，有效降低了评估成本。通过这种方法，Ragas能够提供一种既经济又有效的方式来评估RAG系统，尤其是在处理大量数据和复杂查询时。\u003c/p\u003e\n\u003cp\u003e其他测试框架\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDeepEval\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/confident-ai/deepeval\"\u003eDeepEval\u003c/a\u003e \u003ca href=\"https://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pipelines-with-deepeval\"\u003eHow to Evaluate RAG Applications in CI/CD Pipelines with DeepEval\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eARES\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003egithub: \u003ca href=\"https://github.com/stanford-futuredata/ARES\"\u003ehttps://github.com/stanford-futuredata/ARES\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePaper: ARES: \u003ca href=\"https://arxiv.org/abs/2311.09476\"\u003eAn Automated Evaluation Framework for Retrieval-Augmented Generation Systems\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://python.langchain.com/docs/guides/evaluation/\"\u003eLangChain Evals\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html\"\u003eLlama Index Evals\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/uptrain-ai/uptrain\"\u003eUpTrain\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"数据\"\u003e数据\u003c/h2\u003e\n\u003cp\u003e在评估数据集时，不必依赖人工标注的标准答案，而是通过底层的大语言模型 (LLM) 来进行评估。\u003c/p\u003e","title":"RAG工具箱：评估RAG系统的方法论"},{"content":"Hello-World! 欢迎来到我的博客\n在这里，我将深入探索生成式人工智能的奥秘，同时也会涉猎音乐、电影等领域，分享一些个人的思考和感悟。\n为什么我决定写博客 在生活的纷扰和无尽的日常中，我发现自己一直在与拖延症作斗争。直到今天，我终于下定决心，决定将心中的思绪和感悟记录下来，开启我的博客之旅。\n有几个原因驱使我做出了这个决定。\n首先，岁月不饶人，尤其是经历了新冠疫情之后，我明显感觉到我的记忆力不如以往。过去能够轻松驾驭多重任务的我，如今却常在走入客厅的半路上忘记初衷，或是在浏览器的搜索框前失去了寻找的目的。这种突如其来的迷茫，让我开始思索，我的思绪是否正如秋日里的落叶，悄然飘落。\n其次，在深夜的静思中，我时常回想起坂本龙一那句引人深思的话：“我还能看到几次满月？”这不仅是对时间流逝的感慨，更是一种对生命有限性的深刻体悟。在这有限的时光里，我究竟能留下什么？假如我的时间之沙仅剩下几颗，我的存在又有何意义？我不求答案，但愿通过这些文字，如同在时间的长河中种下一棵树，哪怕是最微小的存在，也能留下自己生命的痕迹。\n最后，我被“数据主义”（Dataism）这一概念深深吸引，它如同一面镜子，映照出在数字时代，我们的数据、思考和情感不仅仅是信息的载体，更是构成我们数字化身份的基石。随着AI的羽翼日渐丰满，我开始憧憬一个可能的未来，其中一个由我的数据、思想和经历塑造出的“我”，在某个未知的时间点复苏。这种思考，如同在深海中发现了一座灯塔，为我的存在指明了一条全新的路径。在这个时代，我选择不再是沉默的旁观者，而是通过我的文字，积极参与到这场未知的探索中。\n因此，这篇博客标志着我的新开始。虽然不确定未来的路会怎样，但至少，在这个过程中，我会找到自己的声音，并希望能够与你共鸣。\n","permalink":"https://niraya666.github.io/posts/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello-World!\u003c/h1\u003e\n\u003cp\u003e欢迎来到我的博客\u003c/p\u003e\n\u003cp\u003e在这里，我将深入探索生成式人工智能的奥秘，同时也会涉猎音乐、电影等领域，分享一些个人的思考和感悟。\u003c/p\u003e\n\u003ch2 id=\"为什么我决定写博客\"\u003e为什么我决定写博客\u003c/h2\u003e\n\u003cp\u003e在生活的纷扰和无尽的日常中，我发现自己一直在与拖延症作斗争。直到今天，我终于下定决心，决定将心中的思绪和感悟记录下来，开启我的博客之旅。\u003c/p\u003e\n\u003cp\u003e有几个原因驱使我做出了这个决定。\u003c/p\u003e\n\u003cp\u003e首先，岁月不饶人，尤其是经历了新冠疫情之后，我明显感觉到我的记忆力不如以往。过去能够轻松驾驭多重任务的我，如今却常在走入客厅的半路上忘记初衷，或是在浏览器的搜索框前失去了寻找的目的。这种突如其来的迷茫，让我开始思索，我的思绪是否正如秋日里的落叶，悄然飘落。\u003c/p\u003e\n\u003cp\u003e其次，在深夜的静思中，我时常回想起坂本龙一那句引人深思的话：“我还能看到几次满月？”这不仅是对时间流逝的感慨，更是一种对生命有限性的深刻体悟。在这有限的时光里，我究竟能留下什么？假如我的时间之沙仅剩下几颗，我的存在又有何意义？我不求答案，但愿通过这些文字，如同在时间的长河中种下一棵树，哪怕是最微小的存在，也能留下自己生命的痕迹。\u003c/p\u003e\n\u003cp\u003e最后，我被“数据主义”（Dataism）这一概念深深吸引，它如同一面镜子，映照出在数字时代，我们的数据、思考和情感不仅仅是信息的载体，更是构成我们数字化身份的基石。随着AI的羽翼日渐丰满，我开始憧憬一个可能的未来，其中一个由我的数据、思想和经历塑造出的“我”，在某个未知的时间点复苏。这种思考，如同在深海中发现了一座灯塔，为我的存在指明了一条全新的路径。在这个时代，我选择不再是沉默的旁观者，而是通过我的文字，积极参与到这场未知的探索中。\u003c/p\u003e\n\u003cp\u003e因此，这篇博客标志着我的新开始。虽然不确定未来的路会怎样，但至少，在这个过程中，我会找到自己的声音，并希望能够与你共鸣。\u003c/p\u003e\n\u003cscript src=\"https://utteranc.es/client.js\"\n        repo=\"https://github.com/Niraya666/niraya666.github.io\"\n        issue-term=\"pathname\"\n        theme=\"github-dark\"\n        crossorigin=\"anonymous\"\n        async\u003e\n\u003c/script\u003e","title":"My First Post: Hello-World!"}]