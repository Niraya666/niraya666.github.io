<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RAG工具箱：基于多模态大模型的文档解析方案（2025版） | LZY Blog</title>
<meta name="keywords" content="RAG, RAG-Toolkits">
<meta name="description" content="Updated on 2025-03-29: Add SmolDocling &amp; VLM Summary
技术迭代速度之快令人惊叹，前作 RAG工具箱：文档解析与表格处理 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。
本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。
过去的技术栈总结
在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决&quot;garbage in, garbage out&quot;的输入质量问题，后续处理环节将难以发挥应有价值。
传统文档解析技术长期受限于以下核心痛点：


结构化信息缺失：无法准确识别文档标题、副标题等层级结构


特殊内容处理薄弱：数学公式、专业符号解析准确率低下


复杂表格解析困境：跨页表格、合并单元格等场景支持不足


图像信息提取瓶颈：扫描文档、手写体识别效果欠佳


版式适应性问题：多栏布局、影印版本等文档格式兼容性差


从技术角度，过去文档解析的底层逻辑和框架：


纯文本解析: PyPDF, PyMuPDF只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力


OCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；


基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；


随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。
Benchmark
为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。
现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench


OCRBench、OCRBench-V2


OmniDocBench


CC-OCR


…and more


（关于benchmark的具体内容见附录）
这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选
当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。
不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。
根据benchmark和实际测试结果，目前几个值得关注的开源VLM：


Qwen2.5-VL


Phi-4-multimodal


Llama 3.2 Vision


olmocr


and more …


Qwen2.5-VL系列模型
cookbook
Blog
Technical Report
这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。
在Technical Report 中一些和document-parse有关的内容：

在第一阶段视觉预训练中（仅训练ViT），针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p&gt;）、表格（&lt;table&gt;）、图表（&lt;div class=&quot;chart&quot;&gt;）、公式（&lt;div class=&quot;formula&quot;&gt;）、图像标注（&lt;div class=&quot;image caption&quot;&gt;）、OCR文本（&lt;div class=&quot;image ocr&quot;&gt;）、乐谱（&lt;div class=&quot;music sheet&quot;&gt;）、化学式（&lt;div class=&quot;chemical formula&quot;&gt;）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://niraya666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://niraya666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://niraya666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://niraya666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://niraya666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="RAG工具箱：基于多模态大模型的文档解析方案（2025版）" />
<meta property="og:description" content="Updated on 2025-03-29: Add SmolDocling &amp; VLM Summary
技术迭代速度之快令人惊叹，前作 RAG工具箱：文档解析与表格处理 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。
本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。
过去的技术栈总结
在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决&quot;garbage in, garbage out&quot;的输入质量问题，后续处理环节将难以发挥应有价值。
传统文档解析技术长期受限于以下核心痛点：


结构化信息缺失：无法准确识别文档标题、副标题等层级结构


特殊内容处理薄弱：数学公式、专业符号解析准确率低下


复杂表格解析困境：跨页表格、合并单元格等场景支持不足


图像信息提取瓶颈：扫描文档、手写体识别效果欠佳


版式适应性问题：多栏布局、影印版本等文档格式兼容性差


从技术角度，过去文档解析的底层逻辑和框架：


纯文本解析: PyPDF, PyMuPDF只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力


OCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；


基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；


随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。
Benchmark
为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。
现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench


OCRBench、OCRBench-V2


OmniDocBench


CC-OCR


…and more


（关于benchmark的具体内容见附录）
这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选
当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。
不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。
根据benchmark和实际测试结果，目前几个值得关注的开源VLM：


Qwen2.5-VL


Phi-4-multimodal


Llama 3.2 Vision


olmocr


and more …


Qwen2.5-VL系列模型
cookbook
Blog
Technical Report
这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。
在Technical Report 中一些和document-parse有关的内容：

在第一阶段视觉预训练中（仅训练ViT），针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p&gt;）、表格（&lt;table&gt;）、图表（&lt;div class=&quot;chart&quot;&gt;）、公式（&lt;div class=&quot;formula&quot;&gt;）、图像标注（&lt;div class=&quot;image caption&quot;&gt;）、OCR文本（&lt;div class=&quot;image ocr&quot;&gt;）、乐谱（&lt;div class=&quot;music sheet&quot;&gt;）、化学式（&lt;div class=&quot;chemical formula&quot;&gt;）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/" />
<meta property="og:image" content="https://niraya666.github.io/images/papermod-cover.png" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-03-12T16:44:00+08:00" />
<meta property="article:modified_time" content="2025-03-12T16:44:00+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://niraya666.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="RAG工具箱：基于多模态大模型的文档解析方案（2025版）"/>
<meta name="twitter:description" content="Updated on 2025-03-29: Add SmolDocling &amp; VLM Summary
技术迭代速度之快令人惊叹，前作 RAG工具箱：文档解析与表格处理 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。
本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。
过去的技术栈总结
在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决&quot;garbage in, garbage out&quot;的输入质量问题，后续处理环节将难以发挥应有价值。
传统文档解析技术长期受限于以下核心痛点：


结构化信息缺失：无法准确识别文档标题、副标题等层级结构


特殊内容处理薄弱：数学公式、专业符号解析准确率低下


复杂表格解析困境：跨页表格、合并单元格等场景支持不足


图像信息提取瓶颈：扫描文档、手写体识别效果欠佳


版式适应性问题：多栏布局、影印版本等文档格式兼容性差


从技术角度，过去文档解析的底层逻辑和框架：


纯文本解析: PyPDF, PyMuPDF只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力


OCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；


基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；


随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。
Benchmark
为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。
现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench


OCRBench、OCRBench-V2


OmniDocBench


CC-OCR


…and more


（关于benchmark的具体内容见附录）
这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选
当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。
不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。
根据benchmark和实际测试结果，目前几个值得关注的开源VLM：


Qwen2.5-VL


Phi-4-multimodal


Llama 3.2 Vision


olmocr


and more …


Qwen2.5-VL系列模型
cookbook
Blog
Technical Report
这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。
在Technical Report 中一些和document-parse有关的内容：

在第一阶段视觉预训练中（仅训练ViT），针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p&gt;）、表格（&lt;table&gt;）、图表（&lt;div class=&quot;chart&quot;&gt;）、公式（&lt;div class=&quot;formula&quot;&gt;）、图像标注（&lt;div class=&quot;image caption&quot;&gt;）、OCR文本（&lt;div class=&quot;image ocr&quot;&gt;）、乐谱（&lt;div class=&quot;music sheet&quot;&gt;）、化学式（&lt;div class=&quot;chemical formula&quot;&gt;）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://niraya666.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RAG工具箱：基于多模态大模型的文档解析方案（2025版）",
      "item": "https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RAG工具箱：基于多模态大模型的文档解析方案（2025版）",
  "name": "RAG工具箱：基于多模态大模型的文档解析方案（2025版）",
  "description": "Updated on 2025-03-29: Add SmolDocling \u0026amp; VLM Summary\n技术迭代速度之快令人惊叹，前作 RAG工具箱：文档解析与表格处理 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。\n本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。\n过去的技术栈总结 在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决\u0026quot;garbage in, garbage out\u0026quot;的输入质量问题，后续处理环节将难以发挥应有价值。\n传统文档解析技术长期受限于以下核心痛点：\n结构化信息缺失：无法准确识别文档标题、副标题等层级结构\n特殊内容处理薄弱：数学公式、专业符号解析准确率低下\n复杂表格解析困境：跨页表格、合并单元格等场景支持不足\n图像信息提取瓶颈：扫描文档、手写体识别效果欠佳\n版式适应性问题：多栏布局、影印版本等文档格式兼容性差\n从技术角度，过去文档解析的底层逻辑和框架：\n纯文本解析: PyPDF, PyMuPDF只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力\nOCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；\n基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；\n随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。\nBenchmark 为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。\n现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench\nOCRBench、OCRBench-V2\nOmniDocBench\nCC-OCR\n…and more\n（关于benchmark的具体内容见附录）\n这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选\n当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。\n不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。\n根据benchmark和实际测试结果，目前几个值得关注的开源VLM：\nQwen2.5-VL\nPhi-4-multimodal\nLlama 3.2 Vision\nolmocr\nand more …\nQwen2.5-VL系列模型 cookbook\nBlog\nTechnical Report\n这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。\n在Technical Report 中一些和document-parse有关的内容：\n在第一阶段视觉预训练中（仅训练ViT），针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p\u0026gt;）、表格（\u0026lt;table\u0026gt;）、图表（\u0026lt;div class=\u0026quot;chart\u0026quot;\u0026gt;）、公式（\u0026lt;div class=\u0026quot;formula\u0026quot;\u0026gt;）、图像标注（\u0026lt;div class=\u0026quot;image caption\u0026quot;\u0026gt;）、OCR文本（\u0026lt;div class=\u0026quot;image ocr\u0026quot;\u0026gt;）、乐谱（\u0026lt;div class=\u0026quot;music sheet\u0026quot;\u0026gt;）、化学式（\u0026lt;div class=\u0026quot;chemical formula\u0026quot;\u0026gt;）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系\n",
  "keywords": [
    "RAG", "RAG-Toolkits"
  ],
  "articleBody": "Updated on 2025-03-29: Add SmolDocling \u0026 VLM Summary\n技术迭代速度之快令人惊叹，前作 RAG工具箱：文档解析与表格处理 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。\n本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。\n过去的技术栈总结 在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决\"garbage in, garbage out\"的输入质量问题，后续处理环节将难以发挥应有价值。\n传统文档解析技术长期受限于以下核心痛点：\n结构化信息缺失：无法准确识别文档标题、副标题等层级结构\n特殊内容处理薄弱：数学公式、专业符号解析准确率低下\n复杂表格解析困境：跨页表格、合并单元格等场景支持不足\n图像信息提取瓶颈：扫描文档、手写体识别效果欠佳\n版式适应性问题：多栏布局、影印版本等文档格式兼容性差\n从技术角度，过去文档解析的底层逻辑和框架：\n纯文本解析: PyPDF, PyMuPDF只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力\nOCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；\n基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；\n随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。\nBenchmark 为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。\n现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench\nOCRBench、OCRBench-V2\nOmniDocBench\nCC-OCR\n…and more\n（关于benchmark的具体内容见附录）\n这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选\n当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。\n不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。\n根据benchmark和实际测试结果，目前几个值得关注的开源VLM：\nQwen2.5-VL\nPhi-4-multimodal\nLlama 3.2 Vision\nolmocr\nand more …\nQwen2.5-VL系列模型 cookbook\nBlog\nTechnical Report\n这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。\n在Technical Report 中一些和document-parse有关的内容：\n在第一阶段视觉预训练中（仅训练ViT），针对Document Parsing，设计了一套标准化的HTML标签体系，包含：段落（p\u003e）、表格（）、图表（）、公式（）、图像标注（）、OCR文本（）、乐谱（）、化学式（）等模块。每个模块均通过 data-bbox 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系\n针对OCR 数据， 利用高质量的合成图像和现实世界的自然场景图像, 整合了一个大规模的多语言OCR数据集,支持多种语言; 针对图表类型数据，使用python可视化库合成了100万个样本； 对于表格数据，利用表格识别模型处理了600万个真实样本用于训练\n在post-training，设计了一个两阶段的数据过滤pipeline，用于数据清洗和低质量数据过滤，针对的对于不同领域提高数据质量；\n在第三阶段，长上下文预训练： 序列长度扩展至 32768，专注于长视频、长代理任务和长文档，训练长上下文能力\n可见， 该模型已针对Document Parsing 做了针对化的训练；至少单纯从视觉问答和 OCR benchmark的跑分结果看，72B 模型与 GPT-4o 和 Claude 3.5 Sonnet 相当\n【但仅仅把这个模型用于document parsing有点浪费了】\n同时Qwen官方也提供了详细的cookbook，包括了具体实现，这里就不展开了，不过记得将base-url改成\n\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\" -\u003e \"https://dashscope.aliyuncs.com/compatible-mode/v1\" 试了下效果：\n结果：\n```html \u003chtml\u003e\u003cbody\u003e \u003cp\u003eDuring planning, the Q-planning algorithm randomly samples only from state-action pairs that have previously been experienced (in Step 1), so the model is never queried with a pair about which it has no information.\u003c/p\u003e \u003cp\u003eThe overall architecture of Dyna agents, of which the Dyna-Q algorithm is one example, is shown in Figure 8.1. The central column represents the basic interaction between agent and environment, giving rise to a trajectory of real experience. The arrow on the left of the figure represents direct reinforcement learning operating on real experience to improve the value function and the policy. On the right are model-based processes. The model is learned from real experience and gives rise to simulated experience. We use the term search control to refer to the process that selects the starting states and actions for the simulated experiences generated by the model. Finally, planning is achieved by applying reinforcement learning methods to the simulated experiences just as if they had really happened. Typically, as in Dyna-Q, the same reinforcement learning method is used both for learning from real experience and for planning from simulated experience. The reinforcement learning method is thus the “final common path” for both learning and planning. Learning and planning are deeply integrated in the sense that they share almost all the same machinery, differing only in the source of their experience.\u003c/p\u003e \u003cdiv class=\"image\"\u003e\u003cimg/\u003e\u003c/div\u003e \u003cp\u003eFigure 8.1: The general Dyna Architecture. Real experience, passing back and forth between the environment and the policy, affects policy and value functions in much the same way as does simulated experience generated by the model of the environment.\u003c/p\u003e \u003cp\u003eConceptually, planning, acting, model-learning, and direct RL occur simultaneously and in parallel in Dyna agents. For concreteness and implementation on a serial computer, however, we fully specify the order in which they occur within a time step. In Dyna-Q, the acting, model-learning, and direct RL processes require little computation, and we assume they consume just a fraction of the time. The remaining time in each step can be devoted to the planning process, which is inherently computation-intensive. Let us assume that there is time in each step, after acting, model-learning, and direct RL, to complete\u003c/p\u003e \u003c/body\u003e\u003c/html\u003e ``` 【实测遇到的问题】：\n图表类image的bbox经常不包含图表label\n瑕不掩瑜，可以说效果是非常惊艳了。\nMistral OCR https://mistral.ai/en/news/mistral-ocr\nhttps://docs.mistral.ai/capabilities/document/\nMistral AI 推出的OCR API，专注于复杂文档的多模态理解,擅长处理科学论文等含图表混合内容的文档\n输入格式：支持 PNG、JPEG 图像及 PDF\n输出格式：生成有序的 Markdown 或 JSON 结构化数据\n基础版价格为 1000 页/美元\n体验感受\n快速解析完一篇论文的pdf（5～10 sec）\n结构化json输出\n能够截取pdf中的图像，并提供对应的位置信息和Base64编码\n基本上是目前首选了\n猜测，大概率是使用的是mistral自家的mistralai/Pixtral-12B-2409 模型\nhow to use\npip install mistralai from mistralai import Mistral api_key = \"API_KEY\" client = Mistral(api_key=api_key) ocr_model = \"mistral-ocr-latest\" system = \"\"\"You are an AI Assistant with document understanding via URLs. You will be provided with URLs, and you must answer any questions related to those documents. # OPEN URLS INSTRUCTIONS You can open URLs by using the `open_urls` tool. It will open webpages and apply OCR to them, retrieving the contents. Use those contents to answer the user. Only URLs pointing to PDFs and images are supported; you may encounter an error if they are not; provide that information to the user if required.\"\"\" def _perform_ocr(url: str) -\u003e str: try: # Apply OCR to the PDF URL response = client.ocr.process( model=ocr_model, document={ \"type\": \"document_url\", \"document_url\": url } ) except Exception: try: # IF PDF OCR fails, try Image OCR response = client.ocr.process( model=ocr_model, document={ \"type\": \"image_url\", \"image_url\": url } ) except Exception as e: return e # Return the error to the model if it fails, otherwise return the contents return \"\\n\\n\".join([f\"### Page {i+1}\\n{response.pages[i].markdown}\" for i in range(len(response.pages))]) pdf_response = _perform_ocr(\"https://arxiv.org/pdf/2201.04234\") 输出格式：\n{ \"pages\": [ { \"index\": 1, \"markdown\": \"markdown-content\", \"images\": [ { \"id\": \"img-0.jpeg\", \"top_left_x\": 292, \"top_left_y\": 217, \"bottom_right_x\": 1405, \"bottom_right_y\": 649, \"image_base64\": \"...\" } ], \"dimensions\": { \"dpi\": 200, \"height\": 2200, \"width\": 1700 } }, ... } 甚至包括了图像的坐标信息\n通过以下代码，能够或者最终完整的markdown\nfrom mistralai.models import OCRResponse from IPython.display import Markdown, display def replace_images_in_markdown(markdown_str: str, images_dict: dict) -\u003e str: for img_name, base64_str in images_dict.items(): markdown_str = markdown_str.replace(f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\") return markdown_str def get_combined_markdown(ocr_response: OCRResponse) -\u003e str: markdowns: list[str] = [] for page in pdf_response.pages: image_data = {} for img in page.images: image_data[img.id] = img.image_base64 markdowns.append(replace_images_in_markdown(page.markdown, image_data)) return \"\\n\\n\".join(markdowns) display(Markdown(get_combined_markdown(pdf_response))) OlmOCR github：https://github.com/allenai/olmocr\ndemo：https://olmocr.allenai.org/\n其核心VLM—allenai/olmOCR-7B-0225-preview， 是采用Qwen2-VL-7B 在250,000 页的多样化 PDF 数据集上进行微调而得的\n不仅是开源了模型， 同时还开源了一整套的解析工作管道，及其微调数据集、训练和推理代码。\n环境：\napt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools git clone https://github.com/allenai/olmocr.git cd olmocr pip install -e . pip install sgl-kernel==0.0.3.post1 --force-reinstall --no-deps pip install \"sglang[all]\u003e=0.4.3.post2\" --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/ local usage\npython -m olmocr.pipeline /content/localworkspace --pdfs /content/pdf/*.pdf 解析成功后，会在本地生成一个jsonl文件，包括每一页的解析结果\n(样例)\n{ \"id\":\"id\", \"text\": \"markdown-content\", \"source\": \"olmocr\", \"added\": \"time\", \"created\": \"time\", \"metadata\": {\"Source-File\": \"file-path\", \"olmocr-version\": \"0.1.58\", \"pdf-total-pages\": 17, \"total-input-tokens\": 67608, \"total-output-tokens\": 14821, \"total-fallback-pages\": 0}, \"attributes\": {\"pdf_page_numbers\": [[0, 3519, 1], [3519, 8803, 2], [8803, 12566, 3], [12566, 15747, 4], [15747, 19147, 5], [19147, 22975, 6], [22975, 25485, 7], [25485, 31108, 8], [31108, 35902, 9], [35902, 38215, 10], [38215, 39584, 11], [39584, 42844, 12], [42844, 45389, 13], [45389, 48985, 14], [48985, 49297, 15], [49297, 49579, 16], [49579, 51463, 17]] } 实际测试共72页pdf（arxiv论文），使用Nvidia-L4 GPU\ninput_tokens 151.32 tokens/sec\noutput_tokens 47.16 tokens/sec\n总耗时40min。\nSmolDocling Huggingface\npaper\n由 IBM Research 和 Hugging Face 开发的VLM，专为文档转换设计，可以处理整个页面，生成一种称为 DocTags 的自定义标记格式，这种格式能详细记录文档元素的文本内容、布局结构和位置信息。模型参数仅为 256 M。\n模型通过端到端方式处理整个文档页面，生成 DocTags 序列。DocTags 是一种受 OTSL 启发的标记格式，采用 XML 风格标签（如 、、、）与位置标签（如 ）结合，基于 0-500 网格映射页面尺寸，确保捕获元素的空间位置和上下文关联。支持多种文档元素的重现，包括标题、图表、表单、代码、公式、表格、脚注、列表、页眉/页脚和章节标题，并提供 OCR（光学字符识别）、阅读顺序和层次链接（如标题与图表/表格的对应）。\nTo the Future from Document Parsing to Document Intelligence 当前基于VLM的方案依旧存在一些问题\n在不同领域和场景下的文档处理中，需要调整解析的prompt或增加相应的LLM优化内容。这些prompt或工作流通常是定制化的，缺乏良好的泛化能力。如果能够开发出一个智能代理应用，针对不同场景的文档进行解析和优化调整，那将是非常理想的。然而，目前市场上尚未出现这样的解决方案。\n文档解析是RAG任务的第一步，与之密切相关的下一步是文本分块（chunking），这与解析过程息息相关，甚至可以结合在一起或者合并在一起。在VLM解析中，我们已经能够获得文档的某种结构化信息（或语义层级关系）。这种信息对于智能代理进行文本分块具有参考价值，而不仅仅是基于长度的简单分割。\n解析成markdown/json的执念或许是过时的想法了 为何不直接做多模态的RAG呢？\n与其设计复杂的系统：从文档解析成markdown/json-分块-建立索引-检索，不如直接将vision-encoder 隐藏层输出用于索引的构建，这也是ColPali这篇工作的想法。\n甚至是类似CAG（Cache-Augmented Generation）中的想法， 直接利用VLM的KV-Cache 也是完全有可能的。\n附录1: VLM Benchmark OCRBench 和 OCRBench v2 OCRBench 和其升级版 OCRBench v2 是用于评估大型多模态模型（LMMs）在光学字符识别（OCR）任务中的性能的基准。OCRBench 包括文本识别、场景文本相关的视觉问答（VQA）、文档导向的VQA、关键信息提取和手写数学表达式识别等任务。OCRBench v2 扩展了任务范围，覆盖更多场景（如街道场景、收据、公式、图表等），包含10,000个经过人工验证的问题-答案对。\nOCRBench v2 paper\nOCRBench paper\nOmniDocBench v2 OmniDocBench v2 是一个专门为PDF文档解析设计的基准\n它包括981页PDF，涵盖9种文档类型（如学术论文、教科书、幻灯片等）、4种布局类型和3种语言类型\n丰富的注释信息，包括块级（如文本段落、标题、表格）和跨度级（如文本行、公式）元素\nGitHub 仓库\npaper\nCC-OCR 包括四个轨道：多场景文本阅读、多语言文本阅读、文档解析和关键信息提取\n覆盖10种语言（如英语50.3%、中文28.2%）\npaper\n附录2: 比较热门的几个开源文档解析项目 marker https://github.com/vikparuchuri/marker\n使用 Surya 模型检测文本块的布局和阅读顺序，从 PDF 中提取文本，尤其是图像或非文本元素\n可选 LLM 处理，（支持 Gemini ，Claude，ollama）来优化输出，处理复杂元素\n如何使用：\npip install marker-pdf[full] # 命令行 marker_single /path/to/file.pdf from marker.converters.pdf import PdfConverter from marker.models import create_model_dict from marker.output import text_from_rendered converter = PdfConverter( artifact_dict=create_model_dict(), ) rendered = converter(\"FILEPATH\") text, _, images = text_from_rendered(rendered) 核心部分PdfConverter\nMarker 和 Surya OCR 模型的权重受 cc-by-nc-sa-4.0 许可限制，但对于年收入低于 500 万美元且终身 VC/天使融资低于 500 万美元的组织，该限制可豁免\nMinerU https://github.com/opendatalab/MinerU/tree/master\n文档：https://mineru.readthedocs.io/zh-cn/latest/\nMinerU 主要通过 PDF-Extract-Kit 使用以下模型：\n布局检测：DocLayout-YOLO_ft, YOLO-v10_ft, LayoutLMv3_ft\n公式检测：YOLOv8_ft\n公式识别：UniMERNet\nOCR：PaddleOCR\n表格识别：PaddleOCR+TableMaster, StructEqTable, StructTable-InternVL2-1B\nmarkitdown https://github.com/microsoft/markitdown\n如何使用：\npip install markitdown openai 普通的pdf解析\nfrom markitdown import MarkItDown md = MarkItDown() result = md.convert(\"you-pdf-file.pdf\") print(result.text_content) 其本质是采用pdfminer，做PDF 文本提取\n支持llm做image caption：\nfrom markitdown import MarkItDown from openai import OpenAI client = OpenAI() md = MarkItDown(llm_client=client, llm_model=\"gpt-4o\") result = md.convert(\"example.jpg\") print(result.text_content) (RAGFlow)DeepDoc github\n支持多种文档格式如PDF、DOCX、EXCEL、PPT\n两个组成部分：视觉处理和解析器\n通过OCR技术识别文本，并进行版面分析以识别不同类型的区域，如表格、标题、段落\n附录3: VLM 汇总表格 模型名称 参数量 OCRBench OCRBenchV2 ChartQA Phi-4-Multimodal 5.6B 84.4 - 81.4 Qwen2.5-VL-3B 3B 82.2 54.3 80.0 Qwen2.5-VL-7B 7B 87.7 56.3 85.0 Qwen2.5-VL-72B 72B - 61.5 - Gemma3-4B 4B - - 79.8 Gemma3-12B 12B - - 83.5 Gemma3-27B 27B - - 83.4 SmolDocling 256M - - - GPT-4o-mini - 77.1 - 54.5 参考和数据来源：\nPhi-4-Mini Technical Report\nGemma3Report\nQwen2.5-VL Technical Report\nSmolDocling\n",
  "wordCount" : "1189",
  "inLanguage": "en",
  "image": "https://niraya666.github.io/images/papermod-cover.png","datePublished": "2025-03-12T16:44:00+08:00",
  "dateModified": "2025-03-12T16:44:00+08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LZY Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://niraya666.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niraya666.github.io/" accesskey="h" title="LZY Blog (Alt + H)">LZY Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niraya666.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/posts/" title="AI">
                    <span>AI</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/musik/" title="Musik!">
                    <span>Musik!</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/monthly/" title="月刊">
                    <span>月刊</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/essay/" title="杂文">
                    <span>杂文</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/travel/" title="游记">
                    <span>游记</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://niraya666.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://niraya666.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      RAG工具箱：基于多模态大模型的文档解析方案（2025版）
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-03-12 16:44:00 +0800 CST'>March 12, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Theme PaperMod

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e8%bf%87%e5%8e%bb%e7%9a%84%e6%8a%80%e6%9c%af%e6%a0%88%e6%80%bb%e7%bb%93" aria-label="过去的技术栈总结">过去的技术栈总结</a></li>
                <li>
                    <a href="#benchmark" aria-label="Benchmark">Benchmark</a></li>
                <li>
                    <a href="#qwen25-vl%e7%b3%bb%e5%88%97%e6%a8%a1%e5%9e%8b" aria-label="Qwen2.5-VL系列模型">Qwen2.5-VL系列模型</a></li>
                <li>
                    <a href="#mistral-ocr" aria-label="Mistral OCR">Mistral OCR</a></li>
                <li>
                    <a href="#olmocr" aria-label="OlmOCR">OlmOCR</a></li>
                <li>
                    <a href="#smoldocling" aria-label="SmolDocling">SmolDocling</a></li>
                <li>
                    <a href="#to-the-future" aria-label="To the Future">To the Future</a><ul>
                        
                <li>
                    <a href="#from-document-parsing-to-document-intelligence" aria-label="from Document Parsing to Document Intelligence">from Document Parsing to Document Intelligence</a></li>
                <li>
                    <a href="#%e8%a7%a3%e6%9e%90%e6%88%90markdownjson%e7%9a%84%e6%89%a7%e5%bf%b5%e6%88%96%e8%ae%b8%e6%98%af%e8%bf%87%e6%97%b6%e7%9a%84%e6%83%b3%e6%b3%95%e4%ba%86" aria-label="解析成markdown/json的执念或许是过时的想法了">解析成markdown/json的执念或许是过时的想法了</a></li></ul>
                </li>
                <li>
                    <a href="#%e9%99%84%e5%bd%951-vlm-benchmark" aria-label="附录1: VLM Benchmark">附录1: VLM Benchmark</a><ul>
                        
                <li>
                    <a href="#ocrbench-%e5%92%8c-ocrbench-v2" aria-label="OCRBench 和 OCRBench v2">OCRBench 和 OCRBench v2</a></li>
                <li>
                    <a href="#omnidocbench-v2" aria-label="OmniDocBench v2">OmniDocBench v2</a></li>
                <li>
                    <a href="#cc-ocr" aria-label="CC-OCR">CC-OCR</a></li></ul>
                </li>
                <li>
                    <a href="#%e9%99%84%e5%bd%952-%e6%af%94%e8%be%83%e7%83%ad%e9%97%a8%e7%9a%84%e5%87%a0%e4%b8%aa%e5%bc%80%e6%ba%90%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e9%a1%b9%e7%9b%ae" aria-label="附录2: 比较热门的几个开源文档解析项目">附录2: 比较热门的几个开源文档解析项目</a><ul>
                        
                <li>
                    <a href="#marker" aria-label="marker">marker</a></li>
                <li>
                    <a href="#mineru" aria-label="MinerU">MinerU</a></li>
                <li>
                    <a href="#markitdown" aria-label="markitdown">markitdown</a></li>
                <li>
                    <a href="#ragflowdeepdoc" aria-label="(RAGFlow)DeepDoc">(RAGFlow)DeepDoc</a></li></ul>
                </li>
                <li>
                    <a href="#%e9%99%84%e5%bd%953-vlm-%e6%b1%87%e6%80%bb%e8%a1%a8%e6%a0%bc" aria-label="附录3: VLM 汇总表格">附录3: VLM 汇总表格</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><a href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E6%96%B9%E6%A1%882025%E7%89%88/#smoldocling">Updated on 2025-03-29: Add SmolDocling &amp; VLM Summary</a></p>
<p>技术迭代速度之快令人惊叹，前作 <a href="https://niraya666.github.io/posts/rag%E5%B7%A5%E5%85%B7%E7%AE%B1%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86/">RAG工具箱：文档解析与表格处理</a> 在短短数月内已显现出代际差距，尽管前作也仅仅只是抛砖引玉式地讨论了pdf的解析方案，不过在新技术的面前，既有的复杂解析架构逐渐失去存在价值，也被端到端范式所取代。在笔者看来，基于多模态大模型的端到端文档解析方案，将成为最优解。</p>
<p>本文将探讨文档解析的终极形态——基于多模态大模型（VLM）的解析技术，包括Mistral-OCR、OlmOCR等前沿工具的实现与实践，并展望该领域的技术发展趋势，和对于RAG的影响。</p>
<h2 id="过去的技术栈总结">过去的技术栈总结<a hidden class="anchor" aria-hidden="true" href="#过去的技术栈总结">#</a></h2>
<p>在RAG系统中，文档解析质量直接决定系统上限。不同场景下的文档形态差异显著，若不能有效解决&quot;garbage in, garbage out&quot;的输入质量问题，后续处理环节将难以发挥应有价值。</p>
<p>传统文档解析技术长期受限于以下核心痛点：</p>
<ol>
<li>
<p>结构化信息缺失：无法准确识别文档标题、副标题等层级结构</p>
</li>
<li>
<p>特殊内容处理薄弱：数学公式、专业符号解析准确率低下</p>
</li>
<li>
<p>复杂表格解析困境：跨页表格、合并单元格等场景支持不足</p>
</li>
<li>
<p>图像信息提取瓶颈：扫描文档、手写体识别效果欠佳</p>
</li>
<li>
<p>版式适应性问题：多栏布局、影印版本等文档格式兼容性差</p>
</li>
</ol>
<p>从技术角度，过去文档解析的底层逻辑和框架：</p>
<ul>
<li>
<p>纯文本解析: <code>PyPDF</code>, <code>PyMuPDF</code>只能解析pdf中的文字,对于公式表格和复杂排版解析无能,对于扫描版低质量的pdf无能为力</p>
</li>
<li>
<p>OCR方案（PaddleOCR等）: 首先使用目标检测模型对文档布局进行分析，识别出标题、表格等关键元素的位置，然后在这些位置上使用OCR技术提取文字；由于需要调用多个模型，整套系统非常复杂；</p>
</li>
<li>
<p>基于transformer 的解析方案（代表: Dount, Nougat）：专门针对英文的学术文章做的训练, 能够将pdf文章整理成Markdown或Latex格式；但对于其他语言和其他类型的文档泛化效果很差；</p>
</li>
</ul>
<p>随着模型能力提升，采用VLM做解析是非常自然的想法，尽管GPT-4o的发布使该技术获得广泛关注，但其高昂的API成本制约了实际应用。值得庆幸的是，开源社区的技术突破正在改变这一局面：不论是LLM基座模型多模态理解能力的增强，还是视觉编码器的提升，至少在当下，开源VLM已具备实用级文档解析能力，而无需针对下游任务的微调，同时成本上已经在可接受范围了。</p>
<h2 id="benchmark">Benchmark<a hidden class="anchor" aria-hidden="true" href="#benchmark">#</a></h2>
<p>为了判断一个模型是否适合Document Parsing，需要benchmark测试分数，作为模型挑选的标准。</p>
<p>现阶段，针对LLMs在OCR、文档信息提取场景下主要采用以下几个常见的bench</p>
<ul>
<li>
<p><a href="https://huggingface.co/spaces/echo840/ocrbench-leaderboard">OCRBench</a>、<a href="https://huggingface.co/spaces/ling99/OCRBench-v2-leaderboard">OCRBench-V2</a></p>
</li>
<li>
<p><a href="https://github.com/opendatalab/OmniDocBench">OmniDocBench</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2412.02210">CC-OCR</a></p>
</li>
<li>
<p>…and more</p>
</li>
</ul>
<p>（关于benchmark的具体内容见附录）</p>
<p>这些bench都基本上包含了通用场景下的OCR能力， 多语言的文档解析能力的测试，能够一定程度上作为模型筛选的关注首选</p>
<p>当然，除了模型能力以外，还需要关注模型的参数量，因为与其成本和latency息息相关。</p>
<p>不过，对于每一个具体场景，还是需要构建自己的测试集用于判断模型是否能够胜任任务， 因为benchmark所包含的测试场景数据，分布语言等等和具体的场景不见得完全一样。</p>
<p>根据benchmark和实际测试结果，目前几个值得关注的开源VLM：</p>
<ul>
<li>
<p><strong><a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5">Qwen2.5-VL</a></strong></p>
</li>
<li>
<p><a href="https://huggingface.co/microsoft/Phi-4-multimodal-instruct">Phi-4-multimodal</a></p>
</li>
<li>
<p><a href="https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct">Llama 3.2 Vision</a></p>
</li>
<li>
<p><a href="https://huggingface.co/allenai/olmOCR-7B-0225-preview">olmocr</a></p>
</li>
<li>
<p>and more …</p>
</li>
</ul>
<h2 id="qwen25-vl系列模型">Qwen2.5-VL系列模型<a hidden class="anchor" aria-hidden="true" href="#qwen25-vl系列模型">#</a></h2>
<p><a href="https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks">cookbook</a></p>
<p><a href="https://qwenlm.github.io/zh/blog/qwen2.5-vl/">Blog</a></p>
<p><a href="https://arxiv.org/abs/2502.13923">Technical Report</a></p>
<p>这应该是开源的模型中，效果排前列的多模态模型（截止至今），同时还具备了多种参数量（3B，7B，72B）可选择。</p>
<p>在Technical Report 中一些和document-parse有关的内容：</p>
<blockquote>
<p>在第一阶段视觉预训练中（仅训练ViT），针对<strong>Document Parsing</strong>，设计了<strong>一套标准化的HTML标签体系</strong>，包含：段落（<code>p&gt;</code>）、表格（<code>&lt;table&gt;</code>）、图表（<code>&lt;div class=&quot;chart&quot;&gt;</code>）、公式（<code>&lt;div class=&quot;formula&quot;&gt;</code>）、图像标注（<code>&lt;div class=&quot;image caption&quot;&gt;</code>）、OCR文本（<code>&lt;div class=&quot;image ocr&quot;&gt;</code>）、乐谱（<code>&lt;div class=&quot;music sheet&quot;&gt;</code>）、化学式（<code>&lt;div class=&quot;chemical formula&quot;&gt;</code>）等模块。每个模块均通过 <code>data-bbox</code> 属性标注其原始坐标位置，保留空间布局信息; 同时所有文档元素的布局信息（如位置、尺寸）通过原生分辨率下的绝对坐标直接编码到HTML标签中，使模型能同时学习内容语义和空间关系</p>
</blockquote>
<blockquote>
<p>针对<strong>OCR 数据</strong>， 利用高质量的<strong>合成图像</strong>和现实世界的自然场景图像, 整合了一个大规模的<strong>多语言OCR数据集</strong>,支持多种语言; 针对<strong>图表类型数据</strong>，使用python可视化库合成了100万个样本； 对于<strong>表格数据</strong>，利用表格识别模型处理了600万个真实样本用于训练</p>
</blockquote>
<blockquote>
<p>在post-training，设计了一个两阶段的数据过滤pipeline，用于数据清洗和低质量数据过滤，针对的对于不同领域提高数据质量；</p>
</blockquote>
<blockquote>
<p>在第三阶段，长上下文预训练： <strong>序列长度扩展至 32768</strong>，专注于长视频、长代理任务和长文档，训练长上下文能力</p>
</blockquote>
<p>可见， 该模型已针对Document Parsing 做了针对化的训练；<strong>至少单纯从视觉问答和 OCR benchmark的跑分结果看，72B 模型与 GPT-4o 和 Claude 3.5 Sonnet 相当</strong></p>
<p>【但仅仅把这个模型用于document parsing有点浪费了】</p>
<p>同时Qwen官方也提供了详细的cookbook，包括了具体实现，这里就不展开了，不过记得将base-url改成</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="s2">&#34;https://dashscope-intl.aliyuncs.com/compatible-mode/v1&#34;</span> <span class="o">-&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="s2">&#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34;</span>
</span></span></code></pre></div><p>试了下效果：</p>
<p><img loading="lazy" src="/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89-assets/1.png" alt="1.png"  />
</p>
<p>结果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">```</span><span class="n">html</span>
</span></span><span class="line"><span class="cl"><span class="o">&lt;</span><span class="n">html</span><span class="o">&gt;&lt;</span><span class="n">body</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">During</span> <span class="n">planning</span><span class="p">,</span> <span class="n">the</span> <span class="n">Q</span><span class="o">-</span><span class="n">planning</span> <span class="n">algorithm</span> <span class="n">randomly</span> <span class="n">samples</span> <span class="n">only</span> <span class="kn">from</span> <span class="nn">state</span><span class="o">-</span><span class="n">action</span> <span class="n">pairs</span> <span class="n">that</span> <span class="n">have</span> <span class="n">previously</span> <span class="n">been</span> <span class="n">experienced</span> <span class="p">(</span><span class="ow">in</span> <span class="n">Step</span> <span class="mi">1</span><span class="p">),</span> <span class="n">so</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">never</span> <span class="n">queried</span> <span class="k">with</span> <span class="n">a</span> <span class="n">pair</span> <span class="n">about</span> <span class="n">which</span> <span class="n">it</span> <span class="n">has</span> <span class="n">no</span> <span class="n">information</span><span class="o">.&lt;/</span><span class="n">p</span><span class="o">&gt;</span> 
</span></span><span class="line"><span class="cl"> <span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">The</span> <span class="n">overall</span> <span class="n">architecture</span> <span class="n">of</span> <span class="n">Dyna</span> <span class="n">agents</span><span class="p">,</span> <span class="n">of</span> <span class="n">which</span> <span class="n">the</span> <span class="n">Dyna</span><span class="o">-</span><span class="n">Q</span> <span class="n">algorithm</span> <span class="ow">is</span> <span class="n">one</span> <span class="n">example</span><span class="p">,</span> <span class="ow">is</span> <span class="n">shown</span> <span class="ow">in</span> <span class="n">Figure</span> <span class="mf">8.1</span><span class="o">.</span> <span class="n">The</span> <span class="n">central</span> <span class="n">column</span> <span class="n">represents</span> <span class="n">the</span> <span class="n">basic</span> <span class="n">interaction</span> <span class="n">between</span> <span class="n">agent</span> <span class="ow">and</span> <span class="n">environment</span><span class="p">,</span> <span class="n">giving</span> <span class="n">rise</span> <span class="n">to</span> <span class="n">a</span> <span class="n">trajectory</span> <span class="n">of</span> <span class="n">real</span> <span class="n">experience</span><span class="o">.</span> <span class="n">The</span> <span class="n">arrow</span> <span class="n">on</span> <span class="n">the</span> <span class="n">left</span> <span class="n">of</span> <span class="n">the</span> <span class="n">figure</span> <span class="n">represents</span> <span class="n">direct</span> <span class="n">reinforcement</span> <span class="n">learning</span> <span class="n">operating</span> <span class="n">on</span> <span class="n">real</span> <span class="n">experience</span> <span class="n">to</span> <span class="n">improve</span> <span class="n">the</span> <span class="n">value</span> <span class="n">function</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">policy</span><span class="o">.</span> <span class="n">On</span> <span class="n">the</span> <span class="n">right</span> <span class="n">are</span> <span class="n">model</span><span class="o">-</span><span class="n">based</span> <span class="n">processes</span><span class="o">.</span> <span class="n">The</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">learned</span> <span class="kn">from</span> <span class="nn">real</span> <span class="n">experience</span> <span class="ow">and</span> <span class="n">gives</span> <span class="n">rise</span> <span class="n">to</span> <span class="n">simulated</span> <span class="n">experience</span><span class="o">.</span> <span class="n">We</span> <span class="n">use</span> <span class="n">the</span> <span class="n">term</span> <span class="n">search</span> <span class="n">control</span> <span class="n">to</span> <span class="n">refer</span> <span class="n">to</span> <span class="n">the</span> <span class="n">process</span> <span class="n">that</span> <span class="n">selects</span> <span class="n">the</span> <span class="n">starting</span> <span class="n">states</span> <span class="ow">and</span> <span class="n">actions</span> <span class="k">for</span> <span class="n">the</span> <span class="n">simulated</span> <span class="n">experiences</span> <span class="n">generated</span> <span class="n">by</span> <span class="n">the</span> <span class="n">model</span><span class="o">.</span> <span class="n">Finally</span><span class="p">,</span> <span class="n">planning</span> <span class="ow">is</span> <span class="n">achieved</span> <span class="n">by</span> <span class="n">applying</span> <span class="n">reinforcement</span> <span class="n">learning</span> <span class="n">methods</span> <span class="n">to</span> <span class="n">the</span> <span class="n">simulated</span> <span class="n">experiences</span> <span class="n">just</span> <span class="k">as</span> <span class="k">if</span> <span class="n">they</span> <span class="n">had</span> <span class="n">really</span> <span class="n">happened</span><span class="o">.</span> <span class="n">Typically</span><span class="p">,</span> <span class="k">as</span> <span class="ow">in</span> <span class="n">Dyna</span><span class="o">-</span><span class="n">Q</span><span class="p">,</span> <span class="n">the</span> <span class="n">same</span> <span class="n">reinforcement</span> <span class="n">learning</span> <span class="n">method</span> <span class="ow">is</span> <span class="n">used</span> <span class="n">both</span> <span class="k">for</span> <span class="n">learning</span> <span class="kn">from</span> <span class="nn">real</span> <span class="n">experience</span> <span class="ow">and</span> <span class="k">for</span> <span class="n">planning</span> <span class="kn">from</span> <span class="nn">simulated</span> <span class="n">experience</span><span class="o">.</span> <span class="n">The</span> <span class="n">reinforcement</span> <span class="n">learning</span> <span class="n">method</span> <span class="ow">is</span> <span class="n">thus</span> <span class="n">the</span> <span class="err">“</span><span class="n">final</span> <span class="n">common</span> <span class="n">path</span><span class="err">”</span> <span class="k">for</span> <span class="n">both</span> <span class="n">learning</span> <span class="ow">and</span> <span class="n">planning</span><span class="o">.</span> <span class="n">Learning</span> <span class="ow">and</span> <span class="n">planning</span> <span class="n">are</span> <span class="n">deeply</span> <span class="n">integrated</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">sense</span> <span class="n">that</span> <span class="n">they</span> <span class="n">share</span> <span class="n">almost</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">same</span> <span class="n">machinery</span><span class="p">,</span> <span class="n">differing</span> <span class="n">only</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">source</span> <span class="n">of</span> <span class="n">their</span> <span class="n">experience</span><span class="o">.&lt;/</span><span class="n">p</span><span class="o">&gt;</span> 
</span></span><span class="line"><span class="cl"> <span class="o">&lt;</span><span class="n">div</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;image&#34;</span><span class="o">&gt;&lt;</span><span class="n">img</span><span class="o">/&gt;&lt;/</span><span class="n">div</span><span class="o">&gt;</span> 
</span></span><span class="line"><span class="cl"> <span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Figure</span> <span class="mf">8.1</span><span class="p">:</span> <span class="n">The</span> <span class="n">general</span> <span class="n">Dyna</span> <span class="n">Architecture</span><span class="o">.</span> <span class="n">Real</span> <span class="n">experience</span><span class="p">,</span> <span class="n">passing</span> <span class="n">back</span> <span class="ow">and</span> <span class="n">forth</span> <span class="n">between</span> <span class="n">the</span> <span class="n">environment</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">policy</span><span class="p">,</span> <span class="n">affects</span> <span class="n">policy</span> <span class="ow">and</span> <span class="n">value</span> <span class="n">functions</span> <span class="ow">in</span> <span class="n">much</span> <span class="n">the</span> <span class="n">same</span> <span class="n">way</span> <span class="k">as</span> <span class="n">does</span> <span class="n">simulated</span> <span class="n">experience</span> <span class="n">generated</span> <span class="n">by</span> <span class="n">the</span> <span class="n">model</span> <span class="n">of</span> <span class="n">the</span> <span class="n">environment</span><span class="o">.&lt;/</span><span class="n">p</span><span class="o">&gt;</span> 
</span></span><span class="line"><span class="cl"> <span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Conceptually</span><span class="p">,</span> <span class="n">planning</span><span class="p">,</span> <span class="n">acting</span><span class="p">,</span> <span class="n">model</span><span class="o">-</span><span class="n">learning</span><span class="p">,</span> <span class="ow">and</span> <span class="n">direct</span> <span class="n">RL</span> <span class="n">occur</span> <span class="n">simultaneously</span> <span class="ow">and</span> <span class="ow">in</span> <span class="n">parallel</span> <span class="ow">in</span> <span class="n">Dyna</span> <span class="n">agents</span><span class="o">.</span> <span class="n">For</span> <span class="n">concreteness</span> <span class="ow">and</span> <span class="n">implementation</span> <span class="n">on</span> <span class="n">a</span> <span class="n">serial</span> <span class="n">computer</span><span class="p">,</span> <span class="n">however</span><span class="p">,</span> <span class="n">we</span> <span class="n">fully</span> <span class="n">specify</span> <span class="n">the</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">which</span> <span class="n">they</span> <span class="n">occur</span> <span class="n">within</span> <span class="n">a</span> <span class="n">time</span> <span class="n">step</span><span class="o">.</span> <span class="n">In</span> <span class="n">Dyna</span><span class="o">-</span><span class="n">Q</span><span class="p">,</span> <span class="n">the</span> <span class="n">acting</span><span class="p">,</span> <span class="n">model</span><span class="o">-</span><span class="n">learning</span><span class="p">,</span> <span class="ow">and</span> <span class="n">direct</span> <span class="n">RL</span> <span class="n">processes</span> <span class="n">require</span> <span class="n">little</span> <span class="n">computation</span><span class="p">,</span> <span class="ow">and</span> <span class="n">we</span> <span class="n">assume</span> <span class="n">they</span> <span class="n">consume</span> <span class="n">just</span> <span class="n">a</span> <span class="n">fraction</span> <span class="n">of</span> <span class="n">the</span> <span class="n">time</span><span class="o">.</span> <span class="n">The</span> <span class="n">remaining</span> <span class="n">time</span> <span class="ow">in</span> <span class="n">each</span> <span class="n">step</span> <span class="n">can</span> <span class="n">be</span> <span class="n">devoted</span> <span class="n">to</span> <span class="n">the</span> <span class="n">planning</span> <span class="n">process</span><span class="p">,</span> <span class="n">which</span> <span class="ow">is</span> <span class="n">inherently</span> <span class="n">computation</span><span class="o">-</span><span class="n">intensive</span><span class="o">.</span> <span class="n">Let</span> <span class="n">us</span> <span class="n">assume</span> <span class="n">that</span> <span class="n">there</span> <span class="ow">is</span> <span class="n">time</span> <span class="ow">in</span> <span class="n">each</span> <span class="n">step</span><span class="p">,</span> <span class="n">after</span> <span class="n">acting</span><span class="p">,</span> <span class="n">model</span><span class="o">-</span><span class="n">learning</span><span class="p">,</span> <span class="ow">and</span> <span class="n">direct</span> <span class="n">RL</span><span class="p">,</span> <span class="n">to</span> <span class="n">complete</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span> 
</span></span><span class="line"><span class="cl"><span class="o">&lt;/</span><span class="n">body</span><span class="o">&gt;&lt;/</span><span class="n">html</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="err">```</span>
</span></span></code></pre></div><p>【实测遇到的问题】：</p>
<ul>
<li>
<p>图表类image的bbox经常不包含图表label</p>
<p><img loading="lazy" src="/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89-assets/%e6%88%aa%e5%b1%8f2025-03-11%20%e4%b8%8b%e5%8d%881.58.48.png" alt="截屏2025-03-11 下午1.58.48.png"  />
</p>
</li>
</ul>
<p>瑕不掩瑜，可以说效果是非常惊艳了。</p>
<h2 id="mistral-ocr">Mistral OCR<a hidden class="anchor" aria-hidden="true" href="#mistral-ocr">#</a></h2>
<p><a href="https://mistral.ai/en/news/mistral-ocr">https://mistral.ai/en/news/mistral-ocr</a></p>
<p><a href="https://docs.mistral.ai/capabilities/document/">https://docs.mistral.ai/capabilities/document/</a></p>
<p>Mistral AI 推出的OCR API，专注于复杂文档的多模态理解,擅长处理科学论文等含图表混合内容的文档</p>
<p>输入格式：支持 PNG、JPEG 图像及 PDF</p>
<p>输出格式：生成有序的 Markdown 或 JSON 结构化数据</p>
<p>基础版价格为 <strong>1000 页/美元</strong></p>
<p>体验感受</p>
<ul>
<li>
<p>快速解析完一篇论文的pdf（5～10 sec）</p>
</li>
<li>
<p>结构化json输出</p>
</li>
<li>
<p>能够截取pdf中的图像，并提供对应的位置信息和Base64编码</p>
</li>
<li>
<p>基本上是目前首选了</p>
</li>
</ul>
<p>猜测，大概率是使用的是mistral自家的mistralai/Pixtral-12B-2409 模型</p>
<p><strong>how to use</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install mistralai
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">mistralai</span> <span class="kn">import</span> <span class="n">Mistral</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&#34;API_KEY&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">Mistral</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ocr_model</span> <span class="o">=</span> <span class="s2">&#34;mistral-ocr-latest&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">system</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;You are an AI Assistant with document understanding via URLs. You will be provided with URLs, and you must answer any questions related to those documents.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2"># OPEN URLS INSTRUCTIONS
</span></span></span><span class="line"><span class="cl"><span class="s2">You can open URLs by using the `open_urls` tool. It will open webpages and apply OCR to them, retrieving the contents. Use those contents to answer the user.
</span></span></span><span class="line"><span class="cl"><span class="s2">Only URLs pointing to PDFs and images are supported; you may encounter an error if they are not; provide that information to the user if required.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_perform_ocr</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>   <span class="c1"># Apply OCR to the PDF URL</span>
</span></span><span class="line"><span class="cl">        <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">ocr</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="o">=</span><span class="n">ocr_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">document</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;document_url&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;document_url&#34;</span><span class="p">:</span> <span class="n">url</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>  <span class="c1"># IF PDF OCR fails, try Image OCR</span>
</span></span><span class="line"><span class="cl">            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">ocr</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">model</span><span class="o">=</span><span class="n">ocr_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">document</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;image_url&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;image_url&#34;</span><span class="p">:</span> <span class="n">url</span>
</span></span><span class="line"><span class="cl">                    <span class="p">}</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">e</span>  <span class="c1"># Return the error to the model if it fails, otherwise return the contents</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="s2">&#34;</span><span class="se">\n\n</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&#34;### Page </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">response</span><span class="o">.</span><span class="n">pages</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">markdown</span><span class="si">}</span><span class="s2">&#34;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">pages</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">pdf_response</span> <span class="o">=</span> <span class="n">_perform_ocr</span><span class="p">(</span><span class="s2">&#34;https://arxiv.org/pdf/2201.04234&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>输出格式：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;pages&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;index&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;markdown&#34;</span><span class="p">:</span> <span class="s2">&#34;markdown-content&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;images&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;img-0.jpeg&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;top_left_x&#34;</span><span class="p">:</span> <span class="mi">292</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;top_left_y&#34;</span><span class="p">:</span> <span class="mi">217</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;bottom_right_x&#34;</span><span class="p">:</span> <span class="mi">1405</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;bottom_right_y&#34;</span><span class="p">:</span> <span class="mi">649</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;image_base64&#34;</span><span class="p">:</span> <span class="s2">&#34;...&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;dimensions&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;dpi&#34;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;height&#34;</span><span class="p">:</span> <span class="mi">2200</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;width&#34;</span><span class="p">:</span> <span class="mi">1700</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">      <span class="o">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>甚至包括了图像的坐标信息</p>
<p>通过以下代码，能够或者最终完整的markdown</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">mistralai.models</span> <span class="kn">import</span> <span class="n">OCRResponse</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span><span class="p">,</span> <span class="n">display</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">replace_images_in_markdown</span><span class="p">(</span><span class="n">markdown_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">images_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">img_name</span><span class="p">,</span> <span class="n">base64_str</span> <span class="ow">in</span> <span class="n">images_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">markdown_str</span> <span class="o">=</span> <span class="n">markdown_str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;![</span><span class="si">{</span><span class="n">img_name</span><span class="si">}</span><span class="s2">](</span><span class="si">{</span><span class="n">img_name</span><span class="si">}</span><span class="s2">)&#34;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;![</span><span class="si">{</span><span class="n">img_name</span><span class="si">}</span><span class="s2">](</span><span class="si">{</span><span class="n">base64_str</span><span class="si">}</span><span class="s2">)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">markdown_str</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_combined_markdown</span><span class="p">(</span><span class="n">ocr_response</span><span class="p">:</span> <span class="n">OCRResponse</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">markdowns</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">pdf_response</span><span class="o">.</span><span class="n">pages</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">image_data</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">page</span><span class="o">.</span><span class="n">images</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">image_data</span><span class="p">[</span><span class="n">img</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">image_base64</span>
</span></span><span class="line"><span class="cl">    <span class="n">markdowns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">replace_images_in_markdown</span><span class="p">(</span><span class="n">page</span><span class="o">.</span><span class="n">markdown</span><span class="p">,</span> <span class="n">image_data</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="s2">&#34;</span><span class="se">\n\n</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">markdowns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">get_combined_markdown</span><span class="p">(</span><span class="n">pdf_response</span><span class="p">)))</span>
</span></span></code></pre></div><h2 id="olmocr">OlmOCR<a hidden class="anchor" aria-hidden="true" href="#olmocr">#</a></h2>
<p>github：<a href="https://github.com/allenai/olmocr">https://github.com/allenai/olmocr</a></p>
<p>demo：<a href="https://olmocr.allenai.org/">https://olmocr.allenai.org/</a></p>
<p>其核心VLM—<a href="https://huggingface.co/allenai/olmOCR-7B-0225-preview">allenai/olmOCR-7B-0225-preview</a>， 是采用Qwen2-VL-7B 在250,000 页的多样化 PDF 数据集上进行微调而得的</p>
<p>不仅是开源了模型， 同时还开源了一整套的解析工作管道，及其微调数据集、训练和推理代码。</p>
<p>环境：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
</span></span><span class="line"><span class="cl">git clone https://github.com/allenai/olmocr.git
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> olmocr
</span></span><span class="line"><span class="cl">pip install -e .
</span></span><span class="line"><span class="cl">pip install sgl-kernel<span class="o">==</span>0.0.3.post1 --force-reinstall --no-deps
</span></span><span class="line"><span class="cl">pip install <span class="s2">&#34;sglang[all]&gt;=0.4.3.post2&#34;</span> --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/
</span></span></code></pre></div><p>local usage</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python -m olmocr.pipeline /content/localworkspace --pdfs /content/pdf/*.pdf
</span></span></code></pre></div><p>解析成功后，会在本地生成一个jsonl文件，包括每一页的解析结果</p>
<p>(样例)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;id&#34;</span><span class="p">:</span><span class="s2">&#34;id&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;text&#34;</span><span class="p">:</span> <span class="s2">&#34;markdown-content&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;source&#34;</span><span class="p">:</span> <span class="s2">&#34;olmocr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;added&#34;</span><span class="p">:</span> <span class="s2">&#34;time&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;created&#34;</span><span class="p">:</span> <span class="s2">&#34;time&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;metadata&#34;</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="nt">&#34;Source-File&#34;</span><span class="p">:</span> <span class="s2">&#34;file-path&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;olmocr-version&#34;</span><span class="p">:</span> <span class="s2">&#34;0.1.58&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;pdf-total-pages&#34;</span><span class="p">:</span> <span class="mi">17</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;total-input-tokens&#34;</span><span class="p">:</span> <span class="mi">67608</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;total-output-tokens&#34;</span><span class="p">:</span> <span class="mi">14821</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;total-fallback-pages&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span> 
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;attributes&#34;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&#34;pdf_page_numbers&#34;</span><span class="p">:</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3519</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3519</span><span class="p">,</span> <span class="mi">8803</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8803</span><span class="p">,</span> <span class="mi">12566</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">12566</span><span class="p">,</span> <span class="mi">15747</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">15747</span><span class="p">,</span> <span class="mi">19147</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">19147</span><span class="p">,</span> <span class="mi">22975</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">22975</span><span class="p">,</span> <span class="mi">25485</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">25485</span><span class="p">,</span> <span class="mi">31108</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">31108</span><span class="p">,</span> <span class="mi">35902</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">35902</span><span class="p">,</span> <span class="mi">38215</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">38215</span><span class="p">,</span> <span class="mi">39584</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span> <span class="p">[</span><span class="mi">39584</span><span class="p">,</span> <span class="mi">42844</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span> <span class="p">[</span><span class="mi">42844</span><span class="p">,</span> <span class="mi">45389</span><span class="p">,</span> <span class="mi">13</span><span class="p">],</span> <span class="p">[</span><span class="mi">45389</span><span class="p">,</span> <span class="mi">48985</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">48985</span><span class="p">,</span> <span class="mi">49297</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> <span class="p">[</span><span class="mi">49297</span><span class="p">,</span> <span class="mi">49579</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="p">[</span><span class="mi">49579</span><span class="p">,</span> <span class="mi">51463</span><span class="p">,</span> <span class="mi">17</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>实际测试共72页pdf（arxiv论文），使用Nvidia-L4 GPU</p>
<p>input_tokens 151.32 tokens/sec</p>
<p>output_tokens 47.16 tokens/sec</p>
<p>总耗时40min。</p>
<h2 id="smoldocling">SmolDocling<a hidden class="anchor" aria-hidden="true" href="#smoldocling">#</a></h2>
<p><a href="https://huggingface.co/ds4sd/SmolDocling-256M-preview">Huggingface</a></p>
<p><a href="https://arxiv.org/abs/2503.11576">paper</a></p>
<p>由 IBM Research 和 Hugging Face 开发的VLM，专为文档转换设计，可以处理整个页面，生成一种称为 DocTags 的自定义标记格式，这种格式能详细记录文档元素的文本内容、布局结构和位置信息。模型参数仅为 256 M。</p>
<p><img loading="lazy" src="/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89-assets/%e6%88%aa%e5%b1%8f2025-03-29%20%e4%b8%8a%e5%8d%8811.05.49.png" alt="截屏2025-03-29 上午11.05.49.png"  />
</p>
<p>模型通过端到端方式处理整个文档页面，生成 DocTags 序列。<code>DocTags</code> 是一种受 OTSL 启发的标记格式，采用 XML 风格标签（如 <code>&lt;text&gt;</code>、<code>&lt;table&gt;</code>、<code>&lt;code&gt;</code>、<code>&lt;formula&gt;</code>）与位置标签（如 <code>&lt;loc_x1&gt;&lt;loc_y1&gt;&lt;loc_x2&gt;&lt;loc_y2&gt;</code>）结合，基于 0-500 网格映射页面尺寸，确保捕获元素的空间位置和上下文关联。支持多种文档元素的重现，包括标题、图表、表单、代码、公式、表格、脚注、列表、页眉/页脚和章节标题，并提供 OCR（光学字符识别）、阅读顺序和层次链接（如标题与图表/表格的对应）。</p>
<p><img loading="lazy" src="/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89-assets/%e6%88%aa%e5%b1%8f2025-03-29%20%e4%b8%8a%e5%8d%8811.06.09.png" alt="截屏2025-03-29 上午11.06.09.png"  />
</p>
<h2 id="to-the-future">To the Future<a hidden class="anchor" aria-hidden="true" href="#to-the-future">#</a></h2>
<h3 id="from-document-parsing-to-document-intelligence">from Document Parsing to Document Intelligence<a hidden class="anchor" aria-hidden="true" href="#from-document-parsing-to-document-intelligence">#</a></h3>
<p>当前基于VLM的方案依旧存在一些问题</p>
<p>在不同领域和场景下的文档处理中，需要调整解析的prompt或增加相应的LLM优化内容。这些prompt或工作流通常是定制化的，缺乏良好的泛化能力。如果能够开发出一个智能代理应用，针对不同场景的文档进行解析和优化调整，那将是非常理想的。然而，目前市场上尚未出现这样的解决方案。</p>
<p>文档解析是RAG任务的第一步，与之密切相关的下一步是文本分块（chunking），这与解析过程息息相关，甚至可以结合在一起或者合并在一起。在VLM解析中，我们已经能够获得文档的某种结构化信息（或语义层级关系）。这种信息对于智能代理进行文本分块具有参考价值，而不仅仅是基于长度的简单分割。</p>
<h3 id="解析成markdownjson的执念或许是过时的想法了">解析成markdown/json的执念或许是过时的想法了<a hidden class="anchor" aria-hidden="true" href="#解析成markdownjson的执念或许是过时的想法了">#</a></h3>
<p>为何不直接做多模态的RAG呢？</p>
<p>与其设计复杂的系统：从文档解析成markdown/json-分块-建立索引-检索，不如直接将vision-encoder 隐藏层输出用于索引的构建，这也是<a href="https://arxiv.org/abs/2407.01449">ColPali</a>这篇工作的想法。</p>
<p>甚至是类似<a href="https://arxiv.org/abs/2412.15605">CAG</a>（Cache-Augmented Generation）中的想法， 直接利用VLM的KV-Cache 也是完全有可能的。</p>
<hr>
<h2 id="附录1-vlm-benchmark">附录1: VLM Benchmark<a hidden class="anchor" aria-hidden="true" href="#附录1-vlm-benchmark">#</a></h2>
<h3 id="ocrbench-和-ocrbench-v2">OCRBench 和 OCRBench v2<a hidden class="anchor" aria-hidden="true" href="#ocrbench-和-ocrbench-v2">#</a></h3>
<p>OCRBench 和其升级版 OCRBench v2 是用于评估大型多模态模型（LMMs）在光学字符识别（OCR）任务中的性能的基准。OCRBench 包括文本识别、场景文本相关的视觉问答（VQA）、文档导向的VQA、关键信息提取和手写数学表达式识别等任务。OCRBench v2 扩展了任务范围，覆盖更多场景（如街道场景、收据、公式、图表等），包含10,000个经过人工验证的问题-答案对。</p>
<p><a href="https://arxiv.org/abs/2501.00321">OCRBench v2 paper</a></p>
<p><a href="https://arxiv.org/abs/2305.07895">OCRBench paper</a></p>
<h3 id="omnidocbench-v2">OmniDocBench v2<a hidden class="anchor" aria-hidden="true" href="#omnidocbench-v2">#</a></h3>
<p>OmniDocBench v2 是一个专门为PDF文档解析设计的基准</p>
<p>它包括981页PDF，涵盖9种文档类型（如学术论文、教科书、幻灯片等）、4种布局类型和3种语言类型</p>
<p>丰富的注释信息，包括块级（如文本段落、标题、表格）和跨度级（如文本行、公式）元素</p>
<p><a href="https://github.com/opendatalab/OmniDocBench">GitHub 仓库</a></p>
<p><a href="https://arxiv.org/abs/2412.07626">paper</a></p>
<h3 id="cc-ocr">CC-OCR<a hidden class="anchor" aria-hidden="true" href="#cc-ocr">#</a></h3>
<p>包括四个轨道：多场景文本阅读、多语言文本阅读、文档解析和关键信息提取</p>
<p>覆盖10种语言（如英语50.3%、中文28.2%）</p>
<p><a href="https://arxiv.org/abs/2412.02210">paper</a></p>
<h2 id="附录2-比较热门的几个开源文档解析项目">附录2: 比较热门的几个开源文档解析项目<a hidden class="anchor" aria-hidden="true" href="#附录2-比较热门的几个开源文档解析项目">#</a></h2>
<h3 id="marker">marker<a hidden class="anchor" aria-hidden="true" href="#marker">#</a></h3>
<p><a href="https://github.com/vikparuchuri/marker">https://github.com/vikparuchuri/marker</a></p>
<p><img loading="lazy" src="/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89-assets/image.png" alt="image.png"  />
</p>
<p>使用 Surya 模型检测文本块的布局和阅读顺序，从 PDF 中提取文本，尤其是图像或非文本元素</p>
<p>可选 LLM 处理，（支持 Gemini ，Claude，ollama）来优化输出，处理复杂元素</p>
<p>如何使用：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install marker-pdf<span class="o">[</span>full<span class="o">]</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 命令行</span>
</span></span><span class="line"><span class="cl">marker_single /path/to/file.pdf 
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">marker.converters.pdf</span> <span class="kn">import</span> <span class="n">PdfConverter</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">marker.models</span> <span class="kn">import</span> <span class="n">create_model_dict</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">marker.output</span> <span class="kn">import</span> <span class="n">text_from_rendered</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">converter</span> <span class="o">=</span> <span class="n">PdfConverter</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">artifact_dict</span><span class="o">=</span><span class="n">create_model_dict</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rendered</span> <span class="o">=</span> <span class="n">converter</span><span class="p">(</span><span class="s2">&#34;FILEPATH&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">text</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">images</span> <span class="o">=</span> <span class="n">text_from_rendered</span><span class="p">(</span><span class="n">rendered</span><span class="p">)</span>
</span></span></code></pre></div><p>核心部分<a href="https://github.com/VikParuchuri/marker/blob/master/marker/converters/pdf.py">PdfConverter</a></p>
<p><img loading="lazy" src="/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89-assets/image%201.png" alt="image 1.png"  />
</p>
<p>Marker 和 Surya OCR 模型的权重受 cc-by-nc-sa-4.0 许可限制，但对于年收入低于 500 万美元且终身 VC/天使融资低于 500 万美元的组织，该限制可豁免</p>
<h3 id="mineru">MinerU<a hidden class="anchor" aria-hidden="true" href="#mineru">#</a></h3>
<p><a href="https://github.com/opendatalab/MinerU/tree/master">https://github.com/opendatalab/MinerU/tree/master</a></p>
<p>文档：<a href="https://mineru.readthedocs.io/zh-cn/latest/">https://mineru.readthedocs.io/zh-cn/latest/</a></p>
<p>MinerU 主要通过 <a href="https://github.com/opendatalab/PDF-Extract-Kit">PDF-Extract-Kit</a> 使用以下模型：</p>
<ul>
<li>
<p>布局检测：DocLayout-YOLO_ft, YOLO-v10_ft, LayoutLMv3_ft</p>
</li>
<li>
<p>公式检测：YOLOv8_ft</p>
</li>
<li>
<p>公式识别：UniMERNet</p>
</li>
<li>
<p>OCR：PaddleOCR</p>
</li>
<li>
<p>表格识别：PaddleOCR+TableMaster, StructEqTable, StructTable-InternVL2-1B</p>
</li>
</ul>
<p><img loading="lazy" src="/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89-assets/image%202.png" alt="image 2.png"  />
</p>
<h3 id="markitdown">markitdown<a hidden class="anchor" aria-hidden="true" href="#markitdown">#</a></h3>
<p><a href="https://github.com/microsoft/markitdown">https://github.com/microsoft/markitdown</a></p>
<p>如何使用：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install markitdown openai
</span></span></code></pre></div><p>普通的pdf解析</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">markitdown</span> <span class="kn">import</span> <span class="n">MarkItDown</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">md</span> <span class="o">=</span> <span class="n">MarkItDown</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">md</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;you-pdf-file.pdf&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">text_content</span><span class="p">)</span>
</span></span></code></pre></div><p>其本质是采用<code>pdfminer</code>，做PDF 文本提取</p>
<p>支持llm做image caption：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">markitdown</span> <span class="kn">import</span> <span class="n">MarkItDown</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">md</span> <span class="o">=</span> <span class="n">MarkItDown</span><span class="p">(</span><span class="n">llm_client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span> <span class="n">llm_model</span><span class="o">=</span><span class="s2">&#34;gpt-4o&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">md</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;example.jpg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">text_content</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="ragflowdeepdoc">(RAGFlow)DeepDoc<a hidden class="anchor" aria-hidden="true" href="#ragflowdeepdoc">#</a></h3>
<p><a href="https://github.com/infiniflow/ragflow/tree/main/deepdoc">github</a></p>
<p>支持多种文档格式如PDF、DOCX、EXCEL、PPT</p>
<p>两个组成部分：视觉处理和解析器</p>
<p>通过OCR技术识别文本，并进行版面分析以识别不同类型的区域，如表格、标题、段落</p>
<p><img loading="lazy" src="/img/RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89-assets/image%203.png" alt="image 3.png"  />
</p>
<h2 id="附录3-vlm-汇总表格">附录3: VLM 汇总表格<a hidden class="anchor" aria-hidden="true" href="#附录3-vlm-汇总表格">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">模型名称</th>
          <th style="text-align: left">参数量</th>
          <th style="text-align: left">OCRBench</th>
          <th style="text-align: left">OCRBenchV2</th>
          <th style="text-align: left">ChartQA</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Phi-4-Multimodal</td>
          <td style="text-align: left">5.6B</td>
          <td style="text-align: left">84.4</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">81.4</td>
      </tr>
      <tr>
          <td style="text-align: left">Qwen2.5-VL-3B</td>
          <td style="text-align: left">3B</td>
          <td style="text-align: left">82.2</td>
          <td style="text-align: left">54.3</td>
          <td style="text-align: left">80.0</td>
      </tr>
      <tr>
          <td style="text-align: left">Qwen2.5-VL-7B</td>
          <td style="text-align: left">7B</td>
          <td style="text-align: left">87.7</td>
          <td style="text-align: left">56.3</td>
          <td style="text-align: left">85.0</td>
      </tr>
      <tr>
          <td style="text-align: left">Qwen2.5-VL-72B</td>
          <td style="text-align: left">72B</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">61.5</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left">Gemma3-4B</td>
          <td style="text-align: left">4B</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">79.8</td>
      </tr>
      <tr>
          <td style="text-align: left">Gemma3-12B</td>
          <td style="text-align: left">12B</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">83.5</td>
      </tr>
      <tr>
          <td style="text-align: left">Gemma3-27B</td>
          <td style="text-align: left">27B</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">83.4</td>
      </tr>
      <tr>
          <td style="text-align: left">SmolDocling</td>
          <td style="text-align: left">256M</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left">GPT-4o-mini</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">77.1</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">54.5</td>
      </tr>
  </tbody>
</table>
<p>参考和数据来源：</p>
<p><a href="https://arxiv.org/abs/2503.01743">Phi-4-Mini Technical Report</a></p>
<p><a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf">Gemma3Report</a></p>
<p><a href="https://arxiv.org/abs/2502.13923">Qwen2.5-VL Technical Report</a></p>
<p><a href="https://arxiv.org/abs/2503.11576">SmolDocling</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://niraya666.github.io/tags/rag/">RAG</a></li>
      <li><a href="https://niraya666.github.io/tags/rag-toolkits/">RAG-Toolkits</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://niraya666.github.io/posts/langmem-%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
    <span class="title">« Prev</span>
    <br>
    <span>LangMem: 一些学习笔记</span>
  </a>
  <a class="next" href="https://niraya666.github.io/posts/rag-tutorial-for-beginner/">
    <span class="title">Next »</span>
    <br>
    <span>RAG工具箱：RAG Tutorial for Beginner</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：基于多模态大模型的文档解析方案（2025版） on x"
            href="https://x.com/intent/tweet/?text=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E5%25B7%25A5%25E5%2585%25B7%25E7%25AE%25B1%25E5%259F%25BA%25E4%25BA%258E%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E6%2596%2587%25E6%25A1%25A3%25E8%25A7%25A3%25E6%259E%2590%25E6%2596%25B9%25E6%25A1%25882025%25E7%2589%2588%2f&amp;hashtags=RAG%2cRAG-Toolkits">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：基于多模态大模型的文档解析方案（2025版） on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E5%25B7%25A5%25E5%2585%25B7%25E7%25AE%25B1%25E5%259F%25BA%25E4%25BA%258E%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E6%2596%2587%25E6%25A1%25A3%25E8%25A7%25A3%25E6%259E%2590%25E6%2596%25B9%25E6%25A1%25882025%25E7%2589%2588%2f&amp;title=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89&amp;summary=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89&amp;source=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E5%25B7%25A5%25E5%2585%25B7%25E7%25AE%25B1%25E5%259F%25BA%25E4%25BA%258E%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E6%2596%2587%25E6%25A1%25A3%25E8%25A7%25A3%25E6%259E%2590%25E6%2596%25B9%25E6%25A1%25882025%25E7%2589%2588%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：基于多模态大模型的文档解析方案（2025版） on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E5%25B7%25A5%25E5%2585%25B7%25E7%25AE%25B1%25E5%259F%25BA%25E4%25BA%258E%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E6%2596%2587%25E6%25A1%25A3%25E8%25A7%25A3%25E6%259E%2590%25E6%2596%25B9%25E6%25A1%25882025%25E7%2589%2588%2f&title=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：基于多模态大模型的文档解析方案（2025版） on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E5%25B7%25A5%25E5%2585%25B7%25E7%25AE%25B1%25E5%259F%25BA%25E4%25BA%258E%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E6%2596%2587%25E6%25A1%25A3%25E8%25A7%25A3%25E6%259E%2590%25E6%2596%25B9%25E6%25A1%25882025%25E7%2589%2588%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：基于多模态大模型的文档解析方案（2025版） on whatsapp"
            href="https://api.whatsapp.com/send?text=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89%20-%20https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E5%25B7%25A5%25E5%2585%25B7%25E7%25AE%25B1%25E5%259F%25BA%25E4%25BA%258E%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E6%2596%2587%25E6%25A1%25A3%25E8%25A7%25A3%25E6%259E%2590%25E6%2596%25B9%25E6%25A1%25882025%25E7%2589%2588%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：基于多模态大模型的文档解析方案（2025版） on telegram"
            href="https://telegram.me/share/url?text=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E5%25B7%25A5%25E5%2585%25B7%25E7%25AE%25B1%25E5%259F%25BA%25E4%25BA%258E%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E6%2596%2587%25E6%25A1%25A3%25E8%25A7%25A3%25E6%259E%2590%25E6%2596%25B9%25E6%25A1%25882025%25E7%2589%2588%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：基于多模态大模型的文档解析方案（2025版） on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e5%9f%ba%e4%ba%8e%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%96%87%e6%a1%a3%e8%a7%a3%e6%9e%90%e6%96%b9%e6%a1%88%ef%bc%882025%e7%89%88%ef%bc%89&u=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E5%25B7%25A5%25E5%2585%25B7%25E7%25AE%25B1%25E5%259F%25BA%25E4%25BA%258E%25E5%25A4%259A%25E6%25A8%25A1%25E6%2580%2581%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E6%2596%2587%25E6%25A1%25A3%25E8%25A7%25A3%25E6%259E%2590%25E6%2596%25B9%25E6%25A1%25882025%25E7%2589%2588%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="utterances">
  <script src="https://utteranc.es/client.js"
        repo="https://github.com/Niraya666/niraya666.github.io.git"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>


<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: {'[+]': ['ams']}
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://niraya666.github.io/">LZY Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
