<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RLHF 之路：强化学习复习之上篇 | LZY Blog</title>
<meta name="keywords" content="RL, alignment">
<meta name="description" content="写在前面
决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。
学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。
于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）
主要的教材来自：


Reinforcement Learning: An Introduction


B站UP主 shuhuai008 的系列推导视频


本篇笔记将包含以下的内容：


MDP


DP


Monte Carlo Methods


TD方法


马尔可夫决策过程(Markov Decision Process，MDP)

MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.

相关概念
随机变量（Random Variance）： ( $X, \ y, \ x \perp y$)，随机变量之间存在的独立性。
随机过程（Stochastic Process）： ${S_t}_{t=1}^{\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。
Markov链/过程（Markov Chain/Process）：强调了Markov性质（Markov Property），即未来的状态仅依赖于当前状态而与过去无关，形式化地表示为：
$$
P(S_{t&#43;1}|S_t, S_{t-1}, &hellip;,S_1) = P(S_{t&#43;1}|S_{t})
$$
状态空间模型（State Space Model）： Markov Chain &#43; Observation； 如 HMM， Kalman Filter，particle Filter。">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://niraya666.github.io/posts/rlhf-%E4%B9%8B%E8%B7%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E4%B9%8B%E4%B8%8A%E7%AF%87/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://niraya666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://niraya666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://niraya666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://niraya666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://niraya666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://niraya666.github.io/posts/rlhf-%E4%B9%8B%E8%B7%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E4%B9%8B%E4%B8%8A%E7%AF%87/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="RLHF 之路：强化学习复习之上篇" />
<meta property="og:description" content="写在前面
决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。
学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。
于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）
主要的教材来自：


Reinforcement Learning: An Introduction


B站UP主 shuhuai008 的系列推导视频


本篇笔记将包含以下的内容：


MDP


DP


Monte Carlo Methods


TD方法


马尔可夫决策过程(Markov Decision Process，MDP)

MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.

相关概念
随机变量（Random Variance）： ( $X, \ y, \ x \perp y$)，随机变量之间存在的独立性。
随机过程（Stochastic Process）： ${S_t}_{t=1}^{\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。
Markov链/过程（Markov Chain/Process）：强调了Markov性质（Markov Property），即未来的状态仅依赖于当前状态而与过去无关，形式化地表示为：
$$
P(S_{t&#43;1}|S_t, S_{t-1}, &hellip;,S_1) = P(S_{t&#43;1}|S_{t})
$$
状态空间模型（State Space Model）： Markov Chain &#43; Observation； 如 HMM， Kalman Filter，particle Filter。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://niraya666.github.io/posts/rlhf-%E4%B9%8B%E8%B7%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E4%B9%8B%E4%B8%8A%E7%AF%87/" />
<meta property="og:image" content="https://niraya666.github.io/images/papermod-cover.png" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-23T15:21:00+08:00" />
<meta property="article:modified_time" content="2025-01-23T15:21:00+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://niraya666.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="RLHF 之路：强化学习复习之上篇"/>
<meta name="twitter:description" content="写在前面
决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。
学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。
于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）
主要的教材来自：


Reinforcement Learning: An Introduction


B站UP主 shuhuai008 的系列推导视频


本篇笔记将包含以下的内容：


MDP


DP


Monte Carlo Methods


TD方法


马尔可夫决策过程(Markov Decision Process，MDP)

MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.

相关概念
随机变量（Random Variance）： ( $X, \ y, \ x \perp y$)，随机变量之间存在的独立性。
随机过程（Stochastic Process）： ${S_t}_{t=1}^{\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。
Markov链/过程（Markov Chain/Process）：强调了Markov性质（Markov Property），即未来的状态仅依赖于当前状态而与过去无关，形式化地表示为：
$$
P(S_{t&#43;1}|S_t, S_{t-1}, &hellip;,S_1) = P(S_{t&#43;1}|S_{t})
$$
状态空间模型（State Space Model）： Markov Chain &#43; Observation； 如 HMM， Kalman Filter，particle Filter。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://niraya666.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RLHF 之路：强化学习复习之上篇",
      "item": "https://niraya666.github.io/posts/rlhf-%E4%B9%8B%E8%B7%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E4%B9%8B%E4%B8%8A%E7%AF%87/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RLHF 之路：强化学习复习之上篇",
  "name": "RLHF 之路：强化学习复习之上篇",
  "description": "写在前面 决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。\n学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。 于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）\n主要的教材来自：\nReinforcement Learning: An Introduction\nB站UP主 shuhuai008 的系列推导视频\n本篇笔记将包含以下的内容：\nMDP\nDP\nMonte Carlo Methods\nTD方法\n马尔可夫决策过程(Markov Decision Process，MDP) MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\n相关概念 随机变量（Random Variance）： ( $X, \\ y, \\ x \\perp y$)，随机变量之间存在的独立性。\n随机过程（Stochastic Process）： ${S_t}_{t=1}^{\\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。\nMarkov链/过程（Markov Chain/Process）：强调了Markov性质（Markov Property），即未来的状态仅依赖于当前状态而与过去无关，形式化地表示为：\n$$ P(S_{t+1}|S_t, S_{t-1}, \u0026hellip;,S_1) = P(S_{t+1}|S_{t}) $$\n状态空间模型（State Space Model）： Markov Chain + Observation； 如 HMM， Kalman Filter，particle Filter。\n",
  "keywords": [
    "RL", "alignment"
  ],
  "articleBody": "写在前面 决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。\n学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。 于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）\n主要的教材来自：\nReinforcement Learning: An Introduction\nB站UP主 shuhuai008 的系列推导视频\n本篇笔记将包含以下的内容：\nMDP\nDP\nMonte Carlo Methods\nTD方法\n马尔可夫决策过程(Markov Decision Process，MDP) MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\n相关概念 随机变量（Random Variance）： ( $X, \\ y, \\ x \\perp y$)，随机变量之间存在的独立性。\n随机过程（Stochastic Process）： ${S_t}_{t=1}^{\\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。\nMarkov链/过程（Markov Chain/Process）：强调了Markov性质（Markov Property），即未来的状态仅依赖于当前状态而与过去无关，形式化地表示为：\n$$ P(S_{t+1}|S_t, S_{t-1}, …,S_1) = P(S_{t+1}|S_{t}) $$\n状态空间模型（State Space Model）： Markov Chain + Observation； 如 HMM， Kalman Filter，particle Filter。\nMarkov奖励过程（Markov Reward Process, MRP）：Markov chain + Reward；在Markov链的基础上加入奖励函数，以描述智能体在每个状态下获得的即时奖励。\nMarkov决策过程（Markov Decision Process, MDP）： Markov Chain + Reward + Action；MRP的进一步扩展，在Markov链和奖励基础上再加入动作选择。\n$$ S: \\rm{State \\ set} \\to s_t $$\n$$ A: \\rm{action \\ set}, \\forall s\\in S, \\ A(s) \\to A_t $$\n$$ R: \\rm{reward \\ set} \\to R_t, R_{t+1} $$\n动态特性 the state-transition probabilities\n在MDP模型中，如何根据状态、动作和奖励来计算系统的转移概率\n状态转移概率 $P: \\ p(s’, r| s,a) \\triangleq P_r{S_{t+1}=s’, R_{t=1}=r|S_t=s, A_t=a}$ 状态转移函数：\n$$ p(s’|s,a) = \\sum_{r\\in R}p(s’, r|s,a) $$\n状态转移概率 $p(s’, r | s, a)$ 定义了在某个状态执行某个动作时转移到下一状态并获得特定奖励的概率。\n状态转移函数 $p(s’ | s, a)$ 则是对奖励进行求和，得到仅与状态和动作相关的转移概率。\n价值函数 That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\nMDP目标：找到最优策略\npolicy：$\\pi$\n确定性策略: $a \\triangleq \\pi(s)$\n随机性策略： $\\pi(a|s) \\triangleq pr(A_t=a|S_t=s)$\n策略（Policy）是智能体在给定状态下选择动作的规则。策略决定了智能体在每个状态下应该采取什么行动。\n强化学习的主要任务之一就是学习一个最优策略，使得智能体在长期内能够获得最高的累计奖励。\n什么样的策略是好策略，如何找到好策略 → 回报\nReward: 奖励是智能体在特定状态下执行某个动作后立即收到的反馈信号，用来衡量动作的好坏。\n奖励通常用函数 $R(s, a)$表示，表示在状态 $s$ 下执行动作 $a$ 所得到的即时奖励。\nReturn： 累积回报；通过对未来的奖励进行累积并引入折扣因子 $\\gamma$ 来计算，从而衡量在一个状态或时间步开始后，智能体在长期内的收益。目的是让较近的奖励比未来的奖励更有权重，即智能体更关注短期内的回报。\n$G_t = R_{t+1} + \\gamma R_{t+2}+\\gamma ^2R_{t+3}…+\\gamma ^{T-1}R_T = \\sum_{i=0}^{\\infty}\\gamma^iR_{t+i+1} \\ (T\\to \\infty)$\n$\\gamma \\in [0,1]$\n累积回报 $G_t$ 是强化学习中最优化的目标，即智能体通过学习找到一个策略，使得每个时间步的累积回报 $G_t$ 最大化。这个值衡量了从时间步 $t$开始，智能体在长期内可以期望获得的总奖励。\nBellman Expectation Equation It expresses the value of a decision problem at a certain point in time in terms of the payoff from some initial choices and the value of the remaining decision problem that results from those initial choices.\n状态值函数（State-Value Function） $v_{\\pi}(s)$ ：\n定义：状态值函数 $v_{\\pi}(s)$ 表示在策略 $\\pi$ 下，从状态 $s$ 开始时期望的回报，即： $$ V_{\\pi}(s) = \\mathbb{E}_{\\pi} [G_t | S_t = s] $$\n含义：该函数衡量了在状态 $s$ 下遵循策略 $\\pi$ 时，智能体能够获得的期望总回报。 动作值函数（Action-Value Function） $q_{\\pi}(s, a)$ ：\n定义：动作值函数 $q_{\\pi}(s, a)$ 表示在策略 $\\pi$ 下，从状态 $s$ 开始、采取动作 $a$ 后期望的回报，即： $$ q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} [G_t | S_t = s, A_t = a] $$\n含义：该函数衡量了在状态 $s$ 下执行动作 $a$ 后，并继续遵循策略 $\\pi$ 时，智能体能够获得的期望总回报。 状态值函数 $V_{\\pi}(s)$ 可以用动作值函数 $q_{\\pi}(s, a)$ 表示为：\n$$ V_{\\pi}(s) = \\sum_{a \\in A(s)} \\pi(a|s) q_{\\pi}(s, a) $$\n等于在该状态下执行不同动作的期望回报之和（加权平均 ），权重是执行每个动作的概率 $\\pi(a | s)$。\n同样，动作值函数 $q_{\\pi}(s, a)$ 可以通过立即奖励和后续状态的折扣值计算：\n$$ q_{\\pi}(s, a) = \\sum_{r, s^{\\prime}} p(s^{\\prime}, r | s, a) \\left( r + \\gamma V_{\\pi}(s^{\\prime}) \\right) $$\n其中， $p(s^{\\prime}, r | s, a)$ 是状态 $s$ 下执行动作 $a$ 后转移到状态 $s^{\\prime}$ 并获得奖励 $r$ 的概率。\nBellman期望方程的状态值函数形式：\n$$ V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s^{\\prime}, r} p(s^{\\prime}, r | s, a) [r + \\gamma v_{\\pi}(s^{\\prime})] $$\n当前状态值 $V_{\\pi}(s)$ 和后续状态值之间的关系\nBellman期望方程的动作值函数形式：\n$$ q_{\\pi}(s, a) = \\sum_{s^{\\prime}, r} p(s^{\\prime}, r | s, a) \\left[ r + \\gamma \\sum_{a{\\prime}} \\pi(a^{\\prime} | s^{\\prime}) q_{\\pi}(s^{\\prime}, a^{\\prime}) \\right] $$\nBellman期望方程将当前状态（或状态-动作对）的值递归地表示为与后续状态值的关系\n通过递归地计算这些期望值用于在策略评估\nTo recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the sum of immediate reward + the discounted value of the state that follows.\nBellman Optimality Equation the optimal state-value function\n$V_{*}(s) \\triangleq \\max_{\\pi} V_{\\pi}(s)$ the optimal action-value function\n$q_*(s,a)\\triangleq \\max_{\\pi}q_{\\pi}(s,a)$\nFor the state-action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. to write $q_$ in terms of $v_$ as follows: $q_(s,a) = E[R_{t+1}+\\gamma v_(S_{t+1})|S_t = s, A_t = a]$\n…\n$V_{\\pi *}(s)=\\max q_{\\pi *}(s,a)$\n…\nThe Bellman optimality equation：\n$$ \\begin{align*} v_(s) \u0026= \\max_{a \\in \\mathcal{A}(s)} q_(s, a) \\ \u0026= \\max_a \\mathbb{E}{\\pi} \\left[ G_t \\mid S_t = s, A_t = a \\right] \\ \u0026= \\max_a \\mathbb{E}{\\pi} \\left[ R_{t+1} + \\gamma G_{t+1} \\mid S_t = s, A_t = a \\right] \\ \u0026= \\max_a \\mathbb{E} \\left[ R_{t+1} + \\gamma v_(S_{t+1}) \\mid S_t = s, A_t = a \\right] \\ \u0026= \\max_a \\sum_{s’, r} p(s’, r \\mid s, a) \\left[ r + \\gamma v_(s’) \\right]. \\end{align*} $$\n$$ \\begin{align*} q_(s, a) \u0026= \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a’} q_(S_{t+1}, a’) \\mid S_t = s, A_t = a \\right] \\ \u0026= \\sum_{s’, r} p(s’, r \\mid s, a) \\left[ r + \\gamma \\max_{a’} q_(s’, a’) \\right]. \\end{align} $$\n在理论上，我们定义了最优值函数和最优策略，并且可以通过贝尔曼最优性方程来解决。\n但：\n计算成本\n过于复杂而无最优的解析解\n直接计算最优策略或值函数既不现实，也没有必要。有时只能通过逼近来寻找次优解，而非真正的最优解。\n动态规划 如何求解Bellman Optimality Equation\n采用动态规划DP；虽然动态规划要求环境模型（如 $p(s^{\\prime}, r \\mid s, a)$）是已知的，因此在强化学习中直接应用可能受到限制，但它仍是理论基础。强化学习中的许多方法都可以被视为在没有模型的情况下逼近动态规划的效果。\n动态规划：通过将问题分解为更小的子问题，逐步解决并保存每个子问题的结果，以避免重复计算，从而高效地求解问题。\n策略迭代（Policy Iteration） = 策略评估（Evaluation）+策略改进\n价值迭代 （Value Iteration）：直接通过Bellman最优方程迭代更新状态值函数 $V(s)$，直到收敛\n策略评估公式推导\n已知MDP $p(s’,r|s,a)$ , 给定$\\pi$ 求 $v_{\\pi} \\ (\\forall s \\in S)$ 记\n$$ V_{\\pi} = \\begin{pmatrix} v_{\\pi}(s_1) \\ v_{\\pi}(s_2) \\ . \\ . \\ .\\ v_{\\pi}(s_{|s|})\n\\end{pmatrix}_{|s|\\times 1} $$\n通过递归公式：$v_{\\pi}(s) = \\mathbb{E}{\\pi}[G_t \\mid S_t = s]$\n代入回报的定义 $G_t = R_{t+1} + \\gamma G_{t+1}$ 得到：\n$$ V_{\\pi}(S)=E_{\\pi}[G_t|S_t=s] \\ =E_{\\pi}[R_{t+1}+\\gamma V_{\\pi}(s_{t+1})]\\=\\sum_a\\pi(a|s)\\sum_{s’,r}p(s’,r|s,a)[r+\\gamma V_{\\pi}(s’)] $$\n有两个部分：\n奖励部分（即期望即时奖励）\n$$ \\sum_a \\pi(a \\mid s) \\sum_{r} r \\cdot p(r \\mid s, a) = \\sum_a \\pi(a \\mid s) E_{\\pi}[R_{t+1}|S_{t}=s,A_t=a]= \\ \\sum_a \\pi(a \\mid s) r_{\\pi}(s,a) = r_{\\pi}(s). $$\n这里 $r_{\\pi}(s)$ 是策略 $\\pi$ 下的状态 $s$ 对应的期望收益。\n未来价值部分（即折扣后的期望价值）\n$$ \\gamma \\sum_{s^{\\prime}} p_{\\pi}(s, s^{\\prime}) \\cdot v_{\\pi}(s^{\\prime}), $$\n其中 $p_{\\pi}(s, s^{\\prime}) = \\sum_a \\pi(a \\mid s) p(s^{\\prime} \\mid s, a)$ 是策略 $\\pi$ 引导下的状态转移概率。\n综合这两个部分，价值函数满足递归关系：\n$$ v_{\\pi}(s) = r_{\\pi}(s) + \\gamma \\sum_{s^{\\prime}} p_{\\pi}(s, s^{\\prime}) v_{\\pi}(s^{\\prime}). $$\n策略评估（Policy Evaluation）——解析解 用矩阵 $V_{\\pi}$、 $r_{\\pi}$ 、和 $P_{\\pi}$ 表示上述关系：\n$V_{\\pi}$ : 状态价值向量（$|S|×1$）\n$r_{\\pi}$ : 状态的期望奖励向量（$|S|×1$）\n$P_{\\pi}$ : 策略引导的转移概率矩阵（$|S|×|S|$），第 $(i, j)$ 项为从状态 $i$ 到 $j$ 的转移概率\n则递归关系可以写为：\n$$ V_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} V_{\\pi}. $$\n化简为线性方程：\n$$ (I - \\gamma P_{\\pi}) V_{\\pi} = r_{\\pi}. $$\n求解：\n$$ V_{\\pi} = (I - \\gamma P_{\\pi})^{-1} r_{\\pi}. $$\n矩阵求逆 ($I - \\gamma P_{\\pi})^{-1}$ 的时间复杂度是 $O(|S|^3)$，其中 $|S|$ 是状态的数量。这对于大规模状态空间是不可行的。\n迭代方法（动态规划思想）：避免直接求解矩阵逆，使用迭代更新 $V_{\\pi}$ 的方式：\n$$ V_{\\pi}^{(k+1)} = r_{\\pi} + \\gamma P_{\\pi} V_{\\pi}^{(k)}. $$\n通过 $O(|S|^2)$ 的复杂度更高效地计算近似解。\n策略评估（Policy Evaluation）—— 迭代解 迭代方法使用 Bellman 方程作为递归关系，通过不断更新 $V_k(s)$ 的近似值，逐步收敛到真实的 $V_{\\pi}(s)$ 。\n更新公式为：\n$$ V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s^{\\prime},r} p(s^{\\prime},r|s,a) \\left[ r + \\gamma V_k(s^{\\prime}) \\right]. $$\n具体实现\n随机初始化 $V_0(s)$\n对每个状态 $s$，使用 Bellman 方程更新价值函数：\n$$ V_{k+1}(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s^{\\prime},r} p(s^{\\prime},r|s,a) \\left[ r + \\gamma V_k(s^{\\prime}) \\right]. $$\n当两次迭代的最大差异（例如$\\max_s |V_{k+1}(s) - V_k(s)|$ ）小于某个阈值 $\\theta$ 时停止。\n伪代码\nPolicy Improvement 策略改进是 策略迭代算法（Policy Iteration） 的一个关键步骤，其目标是利用当前策略 $\\pi$ 的价值函数 $V_\\pi$ 或 $Q_\\pi$ 推导出一个更优的策略 $\\pi^{\\prime}$ 。\n基于当前策略的价值函数，找到一个更优的策略 $\\pi^{\\prime}$ 。\n核心是保证新策略的价值 $V_{\\pi^{\\prime}}$ 至少不低于当前策略 $V_\\pi$。\n策略改进定理（Policy Improvement Theorem） 给定两个策略 $\\pi$ 和 $\\pi^{\\prime}$ ，若：\n$$ q_\\pi(s, \\pi^{\\prime}(s)) \\geq V_\\pi(s), \\quad \\forall s \\in S, $$\n则有：\n$$ V_{\\pi^{\\prime}}(s) \\geq V_\\pi(s), \\quad \\forall s \\in S. $$\n若新的策略 $\\pi^{\\prime}$ 的价值函数 $V_{\\pi^{\\prime}}(s) \\geq V_\\pi(s)$ 对所有状态 $s$ 都成立，那么策略 $\\pi^{\\prime}$ 至少与 $\\pi$ 一样好。\n若在某些状态 $s$ 上严格成立 $V_{\\pi^{\\prime}}(s) \u003e V_\\pi(s)$ ，则 $\\pi^{\\prime}$ 是更优的策略。\n也就是说，无需再计算$V_{\\pi^{\\prime}}(s)$\n证明(略)\nGreedy 策略 如何找到一个比当前策略更优的策略？ — 贪心策略， 在当前状态 $s$ 选择短期收益和长期期望回报的最优动作。\n具体而言 $\\pi^{\\prime}(s)$ 是在每个状态 $s$ 下选择一个动作 $a$ ，使得当前的动作价值函数 $Q_\\pi(s, a)$ 最大化：\n$$ \\pi^{\\prime}(s) = \\arg\\max_a Q_\\pi(s, a). $$\nPolicy Iteration 通过 策略评估 和 策略改进 两个交替步骤，逐步优化策略，最终收敛到最优策略 $\\pi^*$ 。\nPolicy Evaluation： 计算当前策略 $\\pi$ 的价值函数 $V_\\pi(s)$ 或动作价值函数 $Q_\\pi(s, a)$，使用贝尔曼方程，得到策略下每个状态的长期回报\nPolicy Improvement： 基于当前策略的价值函数，找到一个更优的策略 $\\pi^{\\prime}$ , 核心是保证新策略的价值 $V_{\\pi^{\\prime}}$ 至少不低于当前策略 $V_\\pi$\n重复交替执行 策略评估 和 策略改进，直到策略不再变化（收敛）\nValue Iteration Policy Iteration 的简化形式，通过截断策略评估的过程，直接利用Bellman方程递推更新值函数，逐步逼近最优值函数 $V^$ 和最优策略 $\\pi^$ 。\n价值迭代的目标是直接计算最优值函数 $V^{(s)}$ ，然后通过值函数导出最优策略 $\\pi^{(s)}$ 。\n策略迭代 vs. 价值迭代\nPolicy Iteration: Policy Evaluation+Policy Improvement, Policy Evaluation 需要通过多次迭代对当前策略的值函数 $V_\\pi$ 进行精确求解\nValue Iteration： 对策略迭代的优化，截断了策略评估过程，不需要完全求解当前策略 $\\pi$ 的值函数 $V_\\pi$ ，而是直接使用近似值 $V_k$ 来更新，结合策略改进一步完成\n递推公式：\n$$ V_{k+1}(s) = \\max_a \\sum_{s^{\\prime}, r} P(s^{\\prime}, r \\mid s, a) \\left[ r + \\gamma V_k(s^{\\prime}) \\right]. $$\n价值迭代是极端情况下的策略迭代\n策略迭代 –\u003e 价值迭代 –\u003e 就地策略迭代（异步策略迭代）\nMonte Carlo Methods 回顾RL：\n两个主体： Agent, Environment 一个框架： MDP 五大元素：$S$, $A$, $R$, $\\pi$, $p(s,‘r|s,a)$ 核心问题： 如何找到最优策略\n引入价值函数的概念： $v_{\\pi}$, $q_{\\pi}$\n策略迭代： 策略评估，策略改进\n对于动态规划（DP）方法，策略评估和改进会通过递归迭代逐渐逼近：${v_k} \\to V_{\\pi}$\n蒙特卡洛方法的引入：与动态规划不同，蒙特卡洛方法无需明确知道环境的动态特性（即$p(s^{\\prime}, r \\mid s, a)$ 的分布）。它基于实际或模拟的经验，通过采样得到值函数和最优策略。即model-free。\n**关键思想：**平均多个 episode 的回报，来估计每个状态或动作的价值；通过不断采样和学习，实现状态值函数和策略的逼近。\n非平稳性问题： 由于策略和环境的动态性，导致奖励或状态分布随着时间发生变化。具体而言， 在强化学习中，智能体会随着经验的积累不断调整其策略，策略的变化会导致智能体访问的状态分布和奖励的期望值随时间变化，这些变化导致采样数据的分布在整个学习过程中不是固定的，这与传统监督学习中的**独立同分布（i.i.d.）**假设冲突。\n蒙特卡洛方法不依赖某一时刻的单次采样，而是基于整个 episode 的累计回报 $G_t$ 进行估计，对于某个状态 $s$ ，通过采样多个 episode 的平均值来估计其值函数：\n$$ v(s) = \\text{average}(G_t \\mid s_t = s) $$\n基于多个完整 episode 的平均能够有效减少单次采样波动。\n蒙特卡洛方法不需要知道 $p(s^{\\prime}, r \\mid s, a)$的显式形式，不关心状态转移和奖励分布的具体形式，只需要能通过采样得到状态-动作-奖励序列。\n特性 动态规划（DP） 蒙特卡洛方法 环境模型需求 需要明确的动态模型 不需要动态模型（基于采样） 数据需求 全部状态-动作-奖励的概率分布 采样得到的状态-动作-奖励序列 适用范围 对环境已知、问题明确的场景 适合实际应用和无法明确建模的环境 更新方式 基于Bellman方程的递归更新 基于完整episode的采样平均 当无法获得环境的动态模型（即状态转移概率 $p(s^{\\prime}, r \\mid s, a)$），无法通过动态规划直接计算价值函数和最优策略时，可以使用蒙特卡洛方法。\n蒙特卡洛方法通过采样数据（而非模型）来估计价值函数和改进策略，适合于与环境交互而无需明确模型的场景。\nMonte Carlo Prediction \u0026 Estimation 在策略评估中，我们希望估计给定策略 $\\pi$的状态价值函数 $V_\\pi(s)$或者动作价值函数 $Q_\\pi(s, a)$\n由于我们无法直接求解 Bellman 方程：\n$$ V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p(s^{\\prime}, r \\mid s, a) \\big[r + \\gamma V_\\pi(s^{\\prime})\\big] $$\n所以蒙特卡洛方法通过对每次访问的回报 $G_t$ 进行平均来近似 $V_\\pi(s)$：\n$$ V_\\pi(s) \\approx \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)} $$\n其中 N 是轨迹数量，大数定律保证当 $N \\to \\infty$ 时，估计值会收敛到真实的值。\n存在两种策略评估方法：\n首次访问（First-visit MC）：仅使用轨迹中某状态第一次出现时的回报来更新该状态的估计值。\n每次访问（Every-visit MC）：轨迹中每次访问某状态时都使用其回报更新该状态的估计值。\n差异点 First-visit MC Every-visit MC 回报的使用 只使用状态在一个 episode 中的第一次访问 使用状态在一个 episode 中的所有访问 更新次数 每个 episode 中最多更新一次 每次访问状态时都更新 数据独立性 更高：使用的回报较少，数据相关性低 较低：回报相关性高 收敛速度 较慢（舍弃部分数据） 较快（充分利用所有回报） 计算开销 较低：更新次数少 较高：更新次数多 理论分析难度 较容易 较复杂 蒙特卡洛方法需要保证所有状态都能被充分访问，否则某些状态可能永远不会在采样中出现，导致对应的值函数无法被估计。为解决这个问题，假设：**Exploring Starts（试探性出发）**即每个状态都有被访问的可能性。\n但这种假设在真实环境中很难实现，实际中可能需要使用随机策略保证探索性，或者采用其他方法（如$\\epsilon$-贪婪策略）来实现状态空间的充分覆盖。\n基于试探性出发假设的MC控制 在策略评估中使用了两个假设： 无限幕，和试探性出发假设；\n无限幕可通过GPI（广义策略迭代）方式解决\n在实际中，满足试探性出发假设往往不现实，因此需要替代方法（如软策略或离轨方法）\n基于 ES 的 MC 控制\n同轨和离轨 由于试探性出发假设难以满足，可以通过其他方法保证所有状态-动作对都被探索。\n如何避免试探性出发假设？\n用来生成样本的策略，简称行动策略 $b$ 必须是软策略：$\\forall s \\in S, \\ a\\in A(s),$有 $b(a|s)\u003e0$\n待评估，待改进的策略，简称 目标策略 $\\pi$\n⇒\n同轨策略方法： $\\pi =b$, 且为软性策略\n离轨策略方法：$\\pi \\neq b$\n方法 1：同轨策略方法（On-policy Methods）\n在同轨 MC 控制中, 采样和评估使用的是同一个策略 $\\pi = b$， 每次迭代后，基于 $\\epsilon$-greedy策略更新。\n$\\epsilon$-greedy策略是一种常用的软性策略（Soft Policy，即保证所有动作都有非零概率被选择），以概率 $1 - \\epsilon$ 选择当前最优动作（贪婪选择），以概率 $\\epsilon$ 随机选择任意动作。更新方式：\n$$ \\pi(a|s) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}, \u0026 a = \\arg\\max_a Q(s, a) \\ \\frac{\\epsilon}{|A(s)|}, \u0026 a \\neq \\arg\\max_a Q(s, a) \\end{cases} $$\n方法 2：离轨策略方法（Off-policy Methods）\n使用两个策略，\n行动策略 $b$：随机性更高，保证探索性。\n目标策略 $\\pi$：逐渐优化到最优。\n行动策略 $b$ 和 目标策略 $\\pi$ 不同， 行为策略 $b$ 用于生成数据，必须是软性策略，目标策略 $\\pi$ 是被评估和改进的策略，通常是贪婪或接近贪婪的策略。\n这种方法与同轨（On-policy）方法的关键区别在于，数据并非直接来源于目标策略，而是来源于行为策略。这种差异导致以下两个问题：\n数据分布不同：行动策略和目标策略不一致。 在 Monte Carlo 中，我们需要根据目标策略 $\\pi$来估计其对应的值函数 $v_\\pi(s)$ 或 $q_\\pi(s, a)$，但是，在 Off-policy 方法中，采样数据是由行动策略 $b$ 生成的， $\\mathbb{E}b[G_t \\mid S_t = s] \\neq \\mathbb{E}\\pi[G_t \\mid S_t = s]$，因此，我们无法直接使用行为策略生成的回报 $G_t$ 来估计目标策略的值。\n**如何校正数据分布。**为了让行为策略生成的数据 $G_t$ 能用于估计目标策略的值，需要校正数据的分布，使其符合目标策略 $\\pi$。重要性采样通过对回报进行加权，能够将行动策略的数据分布调整为目标策略的数据分布。\nimportance-sampling ratio：\n$$ \\rho_t = \\frac{\\pi(A_t \\mid S_t)}{b(A_t \\mid S_t)}. $$\n通过将回报 $G_t$ 按照 $\\rho_t$ 加权，即可校正行为动策略的回报，使其在数学期望上等价于目标策略的回报。\n$$ \\begin{aligned} V_{\\pi}(s) = E_{\\pi}[G_t|S_t=s] \\\\ =\\sum G_t \\cdot Pr{A_t,R_{t+1},S_{t+1},…S_t|S_t = s,A_{t:T-1} \\sim \\pi} \\\\ = \\sum G_t \\cdot \\pi(A_t|S_t) \\cdot p(R_{t+1},S_{t+1}|S_t,A_t) …p(R_{t},S_{t}|S_{t-1},A_{t-1}) \\\\ =\\sum G_t \\cdot \\prod_{k=t}^{T-1} [\\pi(A_k|S_k)p(R_{k+1},S_{k+1}|S_k,A_k)] \\\\ = \\sum G_t \\prod_{k=t}^{T-1}[\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}b(A_k|S_k)\\cdot p(R_{k+1},S_{k+1}|S_k, A_k)] \\\\ = \\sum \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)} G_t \\cdot \\prod_{k=t}^{T-1} [b(A_k|s_k)\\cdot p(R_{k+1},S_{k+1}|S_k, A_k)] \\\\ = \\sum \\rho_{t:T-1} \\cdot G_t \\cdot Pr{A_t,R_{t+1},S_{t+1},…S_t | S_t=s,A_{t:T-1}\\sim b} \\ = E_b[\\rho_{t:T-1}|S_t=s] \\\\ \\approx \\frac{1}{N} \\sum_N^{i=1} \\rho_{t:T-1} G_t^{(i)} \\end{aligned} $$\n(普通重要性采样)\n增量更新\n方法 特点 优缺点 基于试探性出发（ES） 所有状态-动作对都有非零概率被访问。 假设不现实，难以在实际中满足。 同轨控制（On-policy） 使用 (\\epsilon)-贪婪策略，策略生成和评估一致。 简单易实现，但可能需要更长时间才能达到最优策略。 离轨控制（Off-policy） 行为策略和目标策略不同，通过重要性采样校正回报。 灵活且更强大，但重要性采样可能导致高方差。 Temporal-Difference Learning 策略评估 TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\nUnlike MC methods, temporal-difference (TD) does not require the episodes are complete. TD methods can learn from incomplete episodes by bootstrapping.\n即它根据部分观察到的信息（当前的奖励和下一步状态的估计值）来更新值函数，而不是等待整个序列的结束。\nTD 核心思想是（结合了MC和DP）\nMC-like：TD 利用实际经验（state-transition 和 reward）直接学习\nDP-like：使用对状态值的估计（bootstrap）来更新\nTD 的核心任务：Prediction 问题。 即给定一个策略 $\\pi$，估计状态值函数 $v_\\pi(s)$，即策略 $\\pi$ 在状态 $s$ 上的期望回报\n在DP中， 使用环境模型计算期望值 $\\mathbb{E}[G_t | S_t = s]$ 来更新值函数；在MC中，完全依赖实际回报 $G_t$ 来更新值函数。需要等待整个 episode 结束，计算从 $t$ 时刻开始的完整回报。\n但考虑到，环境的状态转移概率分布是未知的，无法直接应用 DP 方法，且计算开销较大，同时MC 方法需要等待一个 episode 结束才能计算累积回报，很难进行在线更新；\n于是乎，TD 方法 仅使用当前的奖励 $R_{t+1}$ 和对下一状态的估计值 $V(S_{t+1})$来更新值函数，从而避开以上两种方法的弊端；\n以 TD(0) 为例，TD(0) 是最简单的一步 TD 方法（直走一步，之后的步骤就不走了）：\n$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right] $$\nTD 方法的target是 $R_{t+1} + \\gamma V(S_{t+1})$\n$R_{t+1}$ 采样获得；$V(S_{t+1})$ 自举得到。\nSARSA (On-Policy) State (S), Action (A), Reward (R), next State (S’), and next Action (A’)\n同轨策略下的TD控制\n基于当前执行的策略（policy）\n更新公式来更新动作值函数 $Q(s, a)$\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right] $$\n更新值函数时基于当前执行的策略\n遵循当前策略来选择下一步的动作 $A_{t+1}$\non-policy，更倾向于避免过于激进的探索\nQ-Learning （Off-Policy） 离轨策略下的TD控制\n核心更新公式：\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right] $$\n$\\max_a Q(S_{t+1}, a)$：下一状态 $S_{t+1}$ 所有可能动作的最大动作值\n直接学习最优策略，而不依赖于当前使用的策略 （$\\epsilon$-greedy 策略），能够更快找到最优解 特点 Q-learning (Off-policy) SARSA (On-policy) 策略类型 Off-policy：学习的是最优策略 On-policy：基于当前策略更新 更新目标 $R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a)$ $R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})$ 探索行为的影响 更新与探索行为无关，只考虑最优动作 更新考虑探索行为，基于当前策略 学习的激进程度 更激进，直接学习最优策略 更保守，考虑探索对学习过程的影响 适用场景 适合需要快速找到最优策略的场景 适合对探索行为敏感，需更稳定学习的场景 Expected SARSA SARSA+Q-learning的结合\n与 SARSA 和 Q-learning 相比，Expected Sarsa 在更新动作值函数时使用了期望回报，而不是基于单个动作的采样回报\n更新公式\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma E_{a \\sim \\pi} [Q(S_{t+1}, A_{t+1})| S_{t+1} ] - Q(S_t, A_t) \\right] $$\n其中 $E_{a \\sim \\pi} [Q(S_{t+1}, A_{t+1})|S_{t+1}]$：在下一状态 $S_{t+1}$ 下，按照策略 $\\pi$ 选择动作时的动作值函数的期望，进一步展开\n$E_{a \\sim \\pi} [Q(S_{t+1}, A_{t+1})|S_{t+1}] = \\sum_{a} \\pi(a | S_{t+1}) Q(S_{t+1}, a)$\n即对下一状态中所有可能的动作值 $Q(S_{t+1}, a)$ 加权求和，权重为当前策略 $\\pi$ 对动作 $a$ 的选择概率\nSARSA 的更新基于具体采样的下一动作 $A_{t+1}$, 而Expected Sarsa 则基于所有可能动作的期望回报更新，消除了由于采样引起的随机性\n由于期望计算减少了采样的不确定性，Expected Sarsa 学习过程中的方差比 SARSA 和 Q-learning 更小，学习更加稳定\n相比 SARSA：Expected Sarsa 更稳定，减小了由于单一动作采样造成的方差。\n相比 Q-learning：Expected Sarsa 在高探索策略下更稳定，避免了完全依赖最优动作选择可能带来的不稳定。\nQ-learning 是 Expected SARSA 的一种特例\n当策略 $\\pi$ 是greedy policy时，策略 $\\pi$ 的定义如下：\n$$ \\pi(a \\mid S_{t+1}) = \\begin{cases} 1 \u0026 \\text{if } a = \\arg \\max_a Q(S_{t+1}, a), \\\\ 0 \u0026 \\text{else}. \\end{cases} $$\n将将贪婪策略代回 Expected SARSA的迭代公式，即可回到Q-learning的形式：\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right] $$\nDeep Q-Learning from Q-learning to Deep Q-learning\nQ-Learning 是一种表格方法，适用于状态和动作空间较小的情景\n很直接的想法： 通过一个参数化的 Q 函数 $Q_{\\theta}(s, a)$ 来近似 Q 值，而非使用 Q 表，也就是采用神经网络的方式。\n损失函数：比较 Q 值预测和 Q 目标，使用梯度下降来更新 DQN 的权重\nDQN 的稳定性问题：\nExperience Replay：创建一个回放缓冲区存储经验元组，随机采样小批量元组进行训练；防止遗忘之前的经验，同时降低经验之间的相关性\nFixed Q-Target：TD 目标和 Q 值的估计都使用相同的网络参数，导致训练中 Q 值和目标值同步移动，产生震荡；可以使用一个单独的目标网络计算 TD 目标，每隔 C 步将 DQN 的参数复制到目标网络中\nDouble DQN：训练初期，选择下一状态最佳动作时可能高估 Q 值，导致学习复杂化；使用两个网络解耦动作选择和 Q 值目标计算，即使用 DQN 网络选择下一状态的最佳动作，使用 目标网络计算选择动作的目标 Q 值。\n",
  "wordCount" : "1973",
  "inLanguage": "en",
  "image": "https://niraya666.github.io/images/papermod-cover.png","datePublished": "2025-01-23T15:21:00+08:00",
  "dateModified": "2025-01-23T15:21:00+08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://niraya666.github.io/posts/rlhf-%E4%B9%8B%E8%B7%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E4%B9%8B%E4%B8%8A%E7%AF%87/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LZY Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://niraya666.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niraya666.github.io/" accesskey="h" title="LZY Blog (Alt + H)">LZY Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niraya666.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/musik/" title="musik!">
                    <span>musik!</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/monthly/" title="月刊">
                    <span>月刊</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/essay/" title="杂文">
                    <span>杂文</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/travel/" title="游记">
                    <span>游记</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://niraya666.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://niraya666.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      RLHF 之路：强化学习复习之上篇
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-01-23 15:21:00 +0800 CST'>January 23, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Theme PaperMod

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%86%99%e5%9c%a8%e5%89%8d%e9%9d%a2" aria-label="写在前面">写在前面</a></li>
                <li>
                    <a href="#%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8bmarkov-decision-processmdp" aria-label="马尔可夫决策过程(Markov Decision Process，MDP)">马尔可夫决策过程(Markov Decision Process，MDP)</a><ul>
                        
                <li>
                    <a href="#%e7%9b%b8%e5%85%b3%e6%a6%82%e5%bf%b5" aria-label="相关概念">相关概念</a></li>
                <li>
                    <a href="#%e5%8a%a8%e6%80%81%e7%89%b9%e6%80%a7" aria-label="动态特性">动态特性</a></li>
                <li>
                    <a href="#%e4%bb%b7%e5%80%bc%e5%87%bd%e6%95%b0" aria-label="价值函数">价值函数</a></li>
                <li>
                    <a href="#bellman-expectation-equation" aria-label="Bellman Expectation Equation">Bellman Expectation Equation</a></li>
                <li>
                    <a href="#bellman-optimality-equation" aria-label="Bellman Optimality Equation">Bellman Optimality Equation</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%8a%a8%e6%80%81%e8%a7%84%e5%88%92" aria-label="动态规划">动态规划</a><ul>
                        
                <li>
                    <a href="#%e7%ad%96%e7%95%a5%e8%af%84%e4%bc%b0policy-evaluation%e8%a7%a3%e6%9e%90%e8%a7%a3" aria-label="策略评估（Policy Evaluation）——解析解">策略评估（Policy Evaluation）——解析解</a></li>
                <li>
                    <a href="#%e7%ad%96%e7%95%a5%e8%af%84%e4%bc%b0policy-evaluation--%e8%bf%ad%e4%bb%a3%e8%a7%a3" aria-label="策略评估（Policy Evaluation）—— 迭代解">策略评估（Policy Evaluation）—— 迭代解</a></li>
                <li>
                    <a href="#policy-improvement" aria-label="Policy Improvement">Policy Improvement</a><ul>
                        
                <li>
                    <a href="#%e7%ad%96%e7%95%a5%e6%94%b9%e8%bf%9b%e5%ae%9a%e7%90%86policy-improvement-theorem" aria-label="策略改进定理（Policy Improvement Theorem）">策略改进定理（Policy Improvement Theorem）</a></li>
                <li>
                    <a href="#greedy-%e7%ad%96%e7%95%a5" aria-label="Greedy 策略">Greedy 策略</a></li></ul>
                </li>
                <li>
                    <a href="#policy-iteration" aria-label="Policy Iteration">Policy Iteration</a></li>
                <li>
                    <a href="#value-iteration" aria-label="Value Iteration">Value Iteration</a></li></ul>
                </li>
                <li>
                    <a href="#monte-carlo-methods" aria-label="Monte Carlo Methods">Monte Carlo Methods</a><ul>
                        
                <li>
                    <a href="#monte-carlo-prediction--estimation" aria-label="Monte Carlo Prediction &amp; Estimation">Monte Carlo Prediction &amp; Estimation</a></li>
                <li>
                    <a href="#%e5%9f%ba%e4%ba%8e%e8%af%95%e6%8e%a2%e6%80%a7%e5%87%ba%e5%8f%91%e5%81%87%e8%ae%be%e7%9a%84mc%e6%8e%a7%e5%88%b6" aria-label="基于试探性出发假设的MC控制">基于试探性出发假设的MC控制</a></li>
                <li>
                    <a href="#%e5%90%8c%e8%bd%a8%e5%92%8c%e7%a6%bb%e8%bd%a8" aria-label="同轨和离轨">同轨和离轨</a></li></ul>
                </li>
                <li>
                    <a href="#temporal-difference-learning" aria-label="Temporal-Difference Learning">Temporal-Difference Learning</a><ul>
                        
                <li>
                    <a href="#%e7%ad%96%e7%95%a5%e8%af%84%e4%bc%b0" aria-label="策略评估">策略评估</a></li>
                <li>
                    <a href="#sarsa-on-policy" aria-label="SARSA (On-Policy)">SARSA (On-Policy)</a></li>
                <li>
                    <a href="#q-learningoff-policy" aria-label="Q-Learning （Off-Policy）">Q-Learning （Off-Policy）</a></li>
                <li>
                    <a href="#expected-sarsa" aria-label="Expected SARSA">Expected SARSA</a></li></ul>
                </li>
                <li>
                    <a href="#deep-q-learning" aria-label="Deep Q-Learning">Deep Q-Learning</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="写在前面">写在前面<a hidden class="anchor" aria-hidden="true" href="#写在前面">#</a></h2>
<p>决定开启一个新系列，是时候系统性地学习一下 Alignment、RLHF 等相关内容了。</p>
<p>学习过程中，经常会在一些公式推导上卡住，可能是因为之前的基础不够扎实，加上学过的内容遗忘较多。
于是，希望通过这一系列的笔记，帮助自己系统地回顾RL、和学习RLHF等内容。（顺带把之前手写的笔记电子化）</p>
<p>主要的教材来自：</p>
<ul>
<li>
<p><em><strong><a href="http://incompleteideas.net/book/RLbook2020.pdf">Reinforcement Learning: An Introduction</a></strong></em></p>
</li>
<li>
<p><strong>B站UP主 shuhuai008 的系列推导视频</strong></p>
</li>
</ul>
<p>本篇笔记将包含以下的内容：</p>
<ul>
<li>
<p>MDP</p>
</li>
<li>
<p>DP</p>
</li>
<li>
<p>Monte Carlo Methods</p>
</li>
<li>
<p>TD方法</p>
</li>
</ul>
<h2 id="马尔可夫决策过程markov-decision-processmdp">马尔可夫决策过程(Markov Decision Process，MDP)<a hidden class="anchor" aria-hidden="true" href="#马尔可夫决策过程markov-decision-processmdp">#</a></h2>
<blockquote>
<p>MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.</p>
</blockquote>
<h3 id="相关概念">相关概念<a hidden class="anchor" aria-hidden="true" href="#相关概念">#</a></h3>
<p><strong>随机变量（Random Variance）</strong>： ( $X, \ y, \ x \perp y$)，随机变量之间存在的独立性。</p>
<p><strong>随机过程（Stochastic Process）</strong>： ${S_t}_{t=1}^{\infty}$ 一个时间序列的随机变量集合，通常用来描述随着时间变化的状态。</p>
<p><strong>Markov链/过程（Markov Chain/Process）</strong>：强调了<strong>Markov性质</strong>（Markov Property），即<strong>未来的状态仅依赖于当前状态而与过去无关</strong>，形式化地表示为：</p>
<p>$$
P(S_{t+1}|S_t, S_{t-1}, &hellip;,S_1) = P(S_{t+1}|S_{t})
$$</p>
<p><strong>状态空间模型（State Space Model）：</strong> Markov Chain + Observation； 如 HMM， Kalman Filter，particle Filter。</p>
<p><strong>Markov奖励过程（Markov Reward Process, MRP）</strong>：Markov chain + Reward；在Markov链的基础上加入奖励函数，以描述智能体在每个状态下获得的即时奖励。</p>
<p><strong>Markov决策过程（Markov Decision Process, MDP）</strong>： Markov Chain + Reward + Action；MRP的进一步扩展，在Markov链和奖励基础上再加入动作选择。</p>
<p>$$
S: \rm{State \ set} \to s_t
$$</p>
<p>$$
A: \rm{action  \ set}, \forall s\in S,  \ A(s) \to A_t
$$</p>
<p>$$
R: \rm{reward \ set} \to R_t, R_{t+1}
$$</p>
<h3 id="动态特性"><strong>动态特性</strong><a hidden class="anchor" aria-hidden="true" href="#动态特性">#</a></h3>
<blockquote>
<p><em>the state-transition probabilities</em></p>
</blockquote>
<p>在MDP模型中，如何根据状态、动作和奖励来计算系统的转移概率</p>
<p>状态转移概率 $P: \ p(s&rsquo;, r| s,a) \triangleq P_r{S_{t+1}=s&rsquo;, R_{t=1}=r|S_t=s, A_t=a}$
状态转移函数：</p>
<p>$$
p(s&rsquo;|s,a) = \sum_{r\in R}p(s&rsquo;, r|s,a)
$$</p>
<ul>
<li>
<p><strong>状态转移概率</strong> $p(s&rsquo;, r | s, a)$  定义了在某个状态执行某个动作时转移到下一状态并获得特定奖励的概率。</p>
</li>
<li>
<p><strong>状态转移函数</strong> $p(s&rsquo; | s, a)$ 则是对奖励进行求和，得到仅与状态和动作相关的转移概率。</p>
</li>
</ul>
<h3 id="价值函数">价值函数<a hidden class="anchor" aria-hidden="true" href="#价值函数">#</a></h3>
<blockquote>
<p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
</blockquote>
<p>MDP目标：找到最优策略</p>
<p><strong>policy</strong>：$\pi$</p>
<ul>
<li>
<p>确定性策略: $a \triangleq \pi(s)$</p>
</li>
<li>
<p>随机性策略： $\pi(a|s) \triangleq pr(A_t=a|S_t=s)$</p>
</li>
</ul>
<p>策略（Policy）是智能体在给定状态下选择动作的规则。策略决定了智能体在每个状态下应该采取什么行动。</p>
<p>强化学习的主要任务之一就是<strong>学习一个最优策略</strong>，使得智能体在长期内能够获得最高的累计奖励。</p>
<p>什么样的策略是好策略，如何找到好策略 → 回报</p>
<p><strong>Reward</strong>: 奖励是智能体在特定状态下执行某个动作后立即收到的反馈信号，用来衡量动作的好坏。</p>
<p>奖励通常用函数 $R(s, a)$表示，表示在状态  $s$ 下执行动作  $a$ 所得到的即时奖励。</p>
<p><strong>Return</strong>： 累积回报；通过对未来的奖励进行累积并引入折扣因子 $\gamma$ 来计算，从而衡量在一个状态或时间步开始后，智能体在长期内的收益。目的是让较近的奖励比未来的奖励更有权重，即智能体更关注短期内的回报。</p>
<p>$G_t = R_{t+1} + \gamma R_{t+2}+\gamma ^2R_{t+3}&hellip;+\gamma ^{T-1}R_T = \sum_{i=0}^{\infty}\gamma^iR_{t+i+1} \ (T\to \infty)$</p>
<p>$\gamma \in [0,1]$</p>
<p>累积回报  $G_t$  是强化学习中最优化的目标，即智能体通过学习找到一个策略，使得每个时间步的累积回报 $G_t$ 最大化。这个值衡量了从时间步 $t$开始，智能体在长期内可以期望获得的总奖励。</p>
<h3 id="bellman-expectation-equation">Bellman Expectation Equation<a hidden class="anchor" aria-hidden="true" href="#bellman-expectation-equation">#</a></h3>
<blockquote>
<p>It expresses the value of a decision problem at a certain point in time in terms of the payoff from some initial choices and the value of the remaining decision problem that results from those initial choices.</p>
</blockquote>
<p><strong>状态值函数（State-Value Function）</strong> $v_{\pi}(s)$ ：</p>
<ul>
<li>定义：状态值函数  $v_{\pi}(s)$ 表示在策略  $\pi$ 下，从状态  $s$  开始时期望的回报，即：</li>
</ul>
<p>$$
V_{\pi}(s) = \mathbb{E}_{\pi} [G_t | S_t = s]
$$</p>
<ul>
<li>含义：该函数衡量了在状态  $s$  下遵循策略  $\pi$  时，智能体能够获得的期望总回报。</li>
</ul>
<p><strong>动作值函数（Action-Value Function）</strong>  $q_{\pi}(s, a)$ ：</p>
<ul>
<li>定义：动作值函数  $q_{\pi}(s, a)$  表示在策略 $\pi$ 下，从状态  $s$  开始、采取动作  $a$  后期望的回报，即：</li>
</ul>
<p>$$
q_{\pi}(s, a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a]
$$</p>
<ul>
<li>含义：该函数衡量了在状态  $s$  下执行动作  $a$  后，并继续遵循策略  $\pi$  时，智能体能够获得的期望总回报。</li>
</ul>
<p>状态值函数  $V_{\pi}(s)$  可以用动作值函数  $q_{\pi}(s, a)$  表示为：</p>
<p>$$
V_{\pi}(s) = \sum_{a \in A(s)} \pi(a|s) q_{\pi}(s, a)
$$</p>
<p>等于在该状态下执行不同动作的期望回报之和（加权平均 ），权重是执行每个动作的概率  $\pi(a | s)$。</p>
<p>同样，动作值函数  $q_{\pi}(s, a)$ 可以通过<strong>立即奖励</strong>和<strong>后续状态的折扣值</strong>计算：</p>
<p>$$
q_{\pi}(s, a) = \sum_{r, s^{\prime}} p(s^{\prime}, r | s, a) \left( r + \gamma V_{\pi}(s^{\prime}) \right)
$$</p>
<p>其中， $p(s^{\prime}, r | s, a)$ 是状态  $s$  下执行动作  $a$  后转移到状态  $s^{\prime}$  并获得奖励  $r$  的概率。</p>
<p><strong>Bellman期望方程的状态值函数形式</strong>：</p>
<p>$$
V_{\pi}(s) = \sum_{a \in A} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{\pi}(s^{\prime})]
$$</p>
<p>当前状态值 $V_{\pi}(s)$ 和后续状态值之间的关系</p>
<p><strong>Bellman期望方程的动作值函数形式</strong>：</p>
<p>$$
q_{\pi}(s, a) = \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) \left[ r + \gamma \sum_{a{\prime}} \pi(a^{\prime} | s^{\prime}) q_{\pi}(s^{\prime}, a^{\prime}) \right]
$$</p>
<ul>
<li>
<p>Bellman期望方程将当前状态（或状态-动作对）的值递归地表示为与后续状态值的关系</p>
</li>
<li>
<p>通过递归地计算这些期望值用于在策略评估</p>
</li>
</ul>
<blockquote>
<p>To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, <strong>which is a long process</strong>, we calculate the value as <strong>the sum of immediate reward + the discounted value of the state that follows.</strong></p>
</blockquote>
<h3 id="bellman-optimality-equation">Bellman Optimality Equation<a hidden class="anchor" aria-hidden="true" href="#bellman-optimality-equation">#</a></h3>
<p>the optimal state-value function</p>
<p>$V_{*}(s) \triangleq \max_{\pi} V_{\pi}(s)$
the optimal action-value function</p>
<p>$q_*(s,a)\triangleq \max_{\pi}q_{\pi}(s,a)$</p>
<p>For the state-action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. to write $q_<em>$ in terms of $v_</em>$ as follows: $q_<em>(s,a) = E[R_{t+1}+\gamma v_</em>(S_{t+1})|S_t = s, A_t = a]$</p>
<p>…</p>
<p>$V_{\pi *}(s)=\max q_{\pi *}(s,a)$</p>
<p>…</p>
<p>The Bellman optimality equation：</p>
<p>$$
\begin{align*}
v_<em>(s) &amp;= \max_{a \in \mathcal{A}(s)} q_</em>(s, a) \
&amp;= \max_a \mathbb{E}<em>{\pi</em><em>} \left[ G_t \mid S_t = s, A_t = a \right] \
&amp;= \max_a \mathbb{E}<em>{\pi</em></em>} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a \right] \
&amp;= \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_<em>(S_{t+1}) \mid S_t = s, A_t = a \right] \
&amp;= \max_a \sum_{s&rsquo;, r} p(s&rsquo;, r \mid s, a) \left[ r + \gamma v_</em>(s&rsquo;) \right].
\end{align*}
$$</p>
<p>$$
\begin{align*}
q_<em>(s, a) &amp;= \mathbb{E} \left[ R_{t+1} + \gamma \max_{a&rsquo;} q_</em>(S_{t+1}, a&rsquo;) \mid S_t = s, A_t = a \right] \
&amp;= \sum_{s&rsquo;, r} p(s&rsquo;, r \mid s, a) \left[ r + \gamma \max_{a&rsquo;} q_<em>(s&rsquo;, a&rsquo;) \right].
\end{align</em>}
$$</p>
<p>在理论上，我们定义了最优值函数和最优策略，并且可以通过贝尔曼最优性方程来解决。</p>
<p>但：</p>
<ul>
<li>
<p>计算成本</p>
</li>
<li>
<p>过于复杂而无最优的解析解</p>
</li>
</ul>
<p>直接计算最优策略或值函数既不现实，也没有必要。有时只能通过逼近来寻找次优解，而非真正的最优解。</p>
<h2 id="动态规划">动态规划<a hidden class="anchor" aria-hidden="true" href="#动态规划">#</a></h2>
<p>如何求解Bellman Optimality Equation</p>
<p>采用动态规划DP；虽然动态规划要求环境模型（如 $p(s^{\prime}, r \mid s, a)$）是已知的，因此在强化学习中直接应用可能受到限制，但它仍是理论基础。强化学习中的许多方法都可以被视为在没有模型的情况下逼近动态规划的效果。</p>
<blockquote>
<p>动态规划：通过将问题分解为更小的子问题，逐步解决并保存每个子问题的结果，以避免重复计算，从而高效地求解问题。</p>
</blockquote>
<p>策略迭代（<strong>Policy Iteration</strong>） = 策略评估（Evaluation）+策略改进</p>
<p>价值迭代 <strong>（Value Iteration）：<strong>直接通过</strong>Bellman最优方程</strong>迭代更新状态值函数 $V(s)$，直到收敛</p>
<p><strong>策略评估公式推导</strong></p>
<p>已知MDP $p(s&rsquo;,r|s,a)$ , 给定$\pi$ 求 $v_{\pi}  \ (\forall s \in S)$
记</p>
<p>$$
V_{\pi} = \begin{pmatrix}
v_{\pi}(s_1) \
v_{\pi}(s_2)   \
. \
. \
.\
v_{\pi}(s_{|s|})<br>
\end{pmatrix}_{|s|\times 1}
$$</p>
<p>通过递归公式：$v_{\pi}(s) = \mathbb{E}{\pi}[G_t \mid S_t = s]$</p>
<p>代入回报的定义 $G_t = R_{t+1} + \gamma G_{t+1}$ 得到：</p>
<p>$$
V_{\pi}(S)=E_{\pi}[G_t|S_t=s] \ =E_{\pi}[R_{t+1}+\gamma V_{\pi}(s_{t+1})]\=\sum_a\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)[r+\gamma V_{\pi}(s&rsquo;)]
$$</p>
<p>有两个部分：</p>
<p><strong>奖励部分</strong>（即期望即时奖励）</p>
<p>$$
\sum_a \pi(a \mid s) \sum_{r} r \cdot p(r \mid s, a) = \sum_a \pi(a \mid s)  E_{\pi}[R_{t+1}|S_{t}=s,A_t=a]= \ \sum_a \pi(a \mid s) r_{\pi}(s,a) = r_{\pi}(s).
$$</p>
<p>这里  $r_{\pi}(s)$  是策略  $\pi$  下的状态  $s$  对应的期望收益。</p>
<p><strong>未来价值部分</strong>（即折扣后的期望价值）</p>
<p>$$
\gamma \sum_{s^{\prime}} p_{\pi}(s, s^{\prime}) \cdot v_{\pi}(s^{\prime}),
$$</p>
<p>其中  $p_{\pi}(s, s^{\prime}) = \sum_a \pi(a \mid s) p(s^{\prime} \mid s, a)$ 是策略  $\pi$ 引导下的状态转移概率。</p>
<p>综合这两个部分，价值函数满足递归关系：</p>
<p>$$
v_{\pi}(s) = r_{\pi}(s) + \gamma \sum_{s^{\prime}} p_{\pi}(s, s^{\prime}) v_{\pi}(s^{\prime}).
$$</p>
<h3 id="策略评估policy-evaluation解析解"><strong>策略评估（Policy Evaluation）——解析解</strong><a hidden class="anchor" aria-hidden="true" href="#策略评估policy-evaluation解析解">#</a></h3>
<p>用矩阵 $V_{\pi}$、 $r_{\pi}$ 、和  $P_{\pi}$  表示上述关系：</p>
<ul>
<li>
<p>$V_{\pi}$ : 状态价值向量（$|S|×1$）</p>
</li>
<li>
<p> $r_{\pi}$ : 状态的期望奖励向量（$|S|×1$）</p>
</li>
<li>
<p> $P_{\pi}$ : 策略引导的转移概率矩阵（$|S|×|S|$），第 $(i, j)$ 项为从状态  $i$  到  $j$  的转移概率</p>
</li>
</ul>
<p>则递归关系可以写为：</p>
<p>$$
V_{\pi} = r_{\pi} + \gamma P_{\pi} V_{\pi}.
$$</p>
<p>化简为线性方程：</p>
<p>$$
(I - \gamma P_{\pi}) V_{\pi} = r_{\pi}.
$$</p>
<p>求解：</p>
<p>$$
V_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi}.
$$</p>
<p>矩阵求逆  ($I - \gamma P_{\pi})^{-1}$  的时间复杂度是  $O(|S|^3)$，其中 $|S|$  是状态的数量。这对于大规模状态空间是不可行的。</p>
<p><strong>迭代方法</strong>（动态规划思想）：避免直接求解矩阵逆，使用迭代更新  $V_{\pi}$  的方式：</p>
<p>$$
V_{\pi}^{(k+1)} = r_{\pi} + \gamma P_{\pi} V_{\pi}^{(k)}.
$$</p>
<p>通过  $O(|S|^2)$ 的复杂度更高效地计算近似解。</p>
<h3 id="策略评估policy-evaluation--迭代解"><strong>策略评估（Policy Evaluation）——  迭代解</strong><a hidden class="anchor" aria-hidden="true" href="#策略评估policy-evaluation--迭代解">#</a></h3>
<p>迭代方法使用 Bellman 方程作为递归关系，通过不断更新 $V_k(s)$  的近似值，逐步收敛到真实的  $V_{\pi}(s)$ 。</p>
<p>更新公式为：</p>
<p>$$
V_{k+1}(s) = \sum_a \pi(a|s) \sum_{s^{\prime},r} p(s^{\prime},r|s,a) \left[ r + \gamma V_k(s^{\prime}) \right].
$$</p>
<p><strong>具体实现</strong></p>
<p>随机初始化  $V_0(s)$</p>
<p>对每个状态  $s$，使用 Bellman 方程更新价值函数：</p>
<p>$$
V_{k+1}(s) \leftarrow \sum_a \pi(a|s) \sum_{s^{\prime},r} p(s^{\prime},r|s,a) \left[ r + \gamma V_k(s^{\prime}) \right].
$$</p>
<p>当两次迭代的最大差异（例如$\max_s |V_{k+1}(s) - V_k(s)|$ ）小于某个阈值  $\theta$  时停止。</p>
<p><strong>伪代码</strong></p>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2024-11-19%20%e4%b8%8b%e5%8d%884.53.52.png" alt="截屏2024-11-19 下午4.53.52.png"  />
</p>
<h3 id="policy-improvement"><strong>Policy Improvement</strong><a hidden class="anchor" aria-hidden="true" href="#policy-improvement">#</a></h3>
<p>策略改进是 <strong>策略迭代算法（Policy Iteration）</strong> 的一个关键步骤，其目标是利用当前策略  $\pi$ 的价值函数  $V_\pi$ 或  $Q_\pi$  推导出一个更优的策略  $\pi^{\prime}$ 。</p>
<ul>
<li>
<p>基于当前策略的价值函数，找到一个更优的策略 $\pi^{\prime}$ 。</p>
</li>
<li>
<p>核心是保证新策略的价值  $V_{\pi^{\prime}}$  至少不低于当前策略  $V_\pi$。</p>
</li>
</ul>
<h4 id="策略改进定理policy-improvement-theorem"><strong>策略改进定理（Policy Improvement Theorem）</strong><a hidden class="anchor" aria-hidden="true" href="#策略改进定理policy-improvement-theorem">#</a></h4>
<p>给定两个策略  $\pi$ 和  $\pi^{\prime}$ ，若：</p>
<p>$$
q_\pi(s, \pi^{\prime}(s)) \geq V_\pi(s), \quad \forall s \in S,
$$</p>
<p>则有：</p>
<p>$$
V_{\pi^{\prime}}(s) \geq V_\pi(s), \quad \forall s \in S.
$$</p>
<ul>
<li>
<p>若新的策略 $\pi^{\prime}$  的价值函数 $V_{\pi^{\prime}}(s) \geq V_\pi(s)$  对所有状态  $s$  都成立，那么策略  $\pi^{\prime}$ 至少与  $\pi$ 一样好。</p>
</li>
<li>
<p>若在某些状态  $s$  上严格成立  $V_{\pi^{\prime}}(s) &gt; V_\pi(s)$ ，则 $\pi^{\prime}$  是更优的策略。</p>
</li>
<li>
<p>也就是说，无需再计算$V_{\pi^{\prime}}(s)$</p>
</li>
</ul>
<p><strong>证明(略)</strong></p>
<h4 id="greedy-策略"><strong>Greedy 策略</strong><a hidden class="anchor" aria-hidden="true" href="#greedy-策略">#</a></h4>
<p>如何找到一个比当前策略更优的策略？ — 贪心策略， 在当前状态  $s$ 选择短期收益和长期期望回报的最优动作。</p>
<p>具体而言 $\pi^{\prime}(s)$  是在每个状态  $s$  下选择一个动作  $a$ ，使得当前的动作价值函数  $Q_\pi(s, a)$  最大化：</p>
<p>$$
\pi^{\prime}(s) = \arg\max_a Q_\pi(s, a).
$$</p>
<h3 id="policy-iteration"><strong>Policy Iteration</strong><a hidden class="anchor" aria-hidden="true" href="#policy-iteration">#</a></h3>
<p>通过 <strong>策略评估</strong> 和 <strong>策略改进</strong> 两个交替步骤，逐步优化策略，最终收敛到最优策略 $\pi^*$ 。</p>
<ul>
<li>
<p><strong>Policy Evaluation：</strong> 计算当前策略  $\pi$ 的价值函数  $V_\pi(s)$  或动作价值函数  $Q_\pi(s, a)$，使用贝尔曼方程，得到策略下每个状态的长期回报</p>
</li>
<li>
<p><strong>Policy Improvement：</strong> 基于当前策略的价值函数，找到一个更优的策略  $\pi^{\prime}$ , 核心是保证新策略的价值  $V_{\pi^{\prime}}$ 至少不低于当前策略  $V_\pi$</p>
</li>
</ul>
<p>重复交替执行 <strong>策略评估</strong> 和 <strong>策略改进</strong>，直到策略不再变化（收敛）</p>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2024-11-20%20%e4%b8%8b%e5%8d%884.31.02.png" alt="截屏2024-11-20 下午4.31.02.png"  />
</p>
<h3 id="value-iteration"><strong>Value Iteration</strong><a hidden class="anchor" aria-hidden="true" href="#value-iteration">#</a></h3>
<p><strong>Policy Iteration</strong> 的简化形式，通过截断策略评估的过程，直接利用Bellman方程递推更新值函数，逐步逼近最优值函数  $V^<em>$  和最优策略  $\pi^</em>$ 。</p>
<p><strong>价值迭代的目标是直接计算最优值函数  $V^{(s)}$ <em>，然后通过值函数导出最优策略</em>  $\pi^{(s)}$ 。</strong></p>
<p><strong>策略迭代 vs. 价值迭代</strong></p>
<p><strong>Policy Iteration: Policy Evaluation+Policy Improvement,</strong> Policy Evaluation 需要通过多次迭代对当前策略的值函数  $V_\pi$  进行精确求解</p>
<p><strong>Value Iteration：</strong> 对策略迭代的优化，截断了策略评估过程，不需要完全求解当前策略  $\pi$  的值函数  $V_\pi$ ，而是直接使用近似值  $V_k$  来更新，结合策略改进一步完成</p>
<p><strong>递推公式：</strong></p>
<p>$$
V_{k+1}(s) = \max_a \sum_{s^{\prime}, r} P(s^{\prime}, r \mid s, a) \left[ r + \gamma V_k(s^{\prime}) \right].
$$</p>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2024-11-21%20%e4%b8%8b%e5%8d%884.06.45.png" alt="截屏2024-11-21 下午4.06.45.png"  />
</p>
<p>价值迭代是极端情况下的策略迭代</p>
<p>策略迭代 &ndash;&gt; 价值迭代 &ndash;&gt; 就地策略迭代（异步策略迭代）</p>
<h2 id="monte-carlo-methods">Monte Carlo Methods<a hidden class="anchor" aria-hidden="true" href="#monte-carlo-methods">#</a></h2>
<p>回顾RL：</p>
<p>两个主体： Agent, Environment
一个框架： MDP
五大元素：$S$, $A$, $R$, $\pi$, $p(s,&lsquo;r|s,a)$
核心问题： 如何找到最优策略</p>
<ol>
<li>
<p>引入价值函数的概念： $v_{\pi}$, $q_{\pi}$</p>
</li>
<li>
<p>策略迭代： 策略评估，策略改进</p>
</li>
</ol>
<p>对于动态规划（DP）方法，策略评估和改进会通过递归迭代逐渐逼近：${v_k} \to V_{\pi}$</p>
<p><strong>蒙特卡洛方法的引入：<strong>与动态规划不同，<strong>蒙特卡洛方法无需明确知道环境的动态特性</strong>（即$p(s^{\prime}, r \mid s, a)$ 的分布）。它基于实际或模拟的经验，通过采样得到值函数和最优策略。即</strong>model-free。</strong></p>
<p>**关键思想：**平均多个 episode 的回报，来估计每个状态或动作的价值；通过不断采样和学习，实现状态值函数和策略的逼近。</p>
<p><strong>非平稳性问题：</strong> 由于策略和环境的动态性，导致奖励或状态分布随着时间发生变化。具体而言， 在强化学习中，智能体会随着经验的积累不断调整其策略，策略的变化会导致智能体访问的状态分布和奖励的期望值随时间变化，这些变化导致采样数据的分布在整个学习过程中不是固定的，这与传统监督学习中的**独立同分布（i.i.d.）**假设冲突。</p>
<p>蒙特卡洛方法不依赖某一时刻的单次采样，而是基于整个 episode 的累计回报 $G_t$ 进行估计，对于某个状态 $s$ ，通过采样多个 episode 的平均值来估计其值函数：</p>
<p>$$
v(s) = \text{average}(G_t \mid s_t = s)
$$</p>
<p>基于多个完整 episode 的平均能够有效减少单次采样波动。</p>
<p>蒙特卡洛方法不需要知道  $p(s^{\prime}, r \mid s, a)$的显式形式，不关心状态转移和奖励分布的具体形式，只需要能通过采样得到状态-动作-奖励序列。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>特性</strong></th>
          <th style="text-align: left"><strong>动态规划（DP）</strong></th>
          <th style="text-align: left"><strong>蒙特卡洛方法</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>环境模型需求</strong></td>
          <td style="text-align: left">需要明确的动态模型</td>
          <td style="text-align: left">不需要动态模型（基于采样）</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>数据需求</strong></td>
          <td style="text-align: left">全部状态-动作-奖励的概率分布</td>
          <td style="text-align: left">采样得到的状态-动作-奖励序列</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>适用范围</strong></td>
          <td style="text-align: left">对环境已知、问题明确的场景</td>
          <td style="text-align: left">适合实际应用和无法明确建模的环境</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>更新方式</strong></td>
          <td style="text-align: left">基于Bellman方程的递归更新</td>
          <td style="text-align: left">基于完整episode的采样平均</td>
      </tr>
  </tbody>
</table>
<ul>
<li>
<p>当无法获得环境的动态模型（即状态转移概率  $p(s^{\prime}, r \mid s, a)$），无法通过动态规划直接计算价值函数和最优策略时，可以使用蒙特卡洛方法。</p>
</li>
<li>
<p>蒙特卡洛方法通过采样数据（而非模型）来估计价值函数和改进策略，适合于与环境交互而无需明确模型的场景。</p>
</li>
</ul>
<h3 id="monte-carlo-prediction--estimation">Monte Carlo Prediction &amp; Estimation<a hidden class="anchor" aria-hidden="true" href="#monte-carlo-prediction--estimation">#</a></h3>
<p>在策略评估中，我们希望估计给定策略 $\pi$的状态价值函数 $V_\pi(s)$或者动作价值函数 $Q_\pi(s, a)$</p>
<p>由于我们无法直接求解 Bellman 方程：</p>
<p>$$
V_\pi(s) = \sum_a \pi(a \mid s) \sum_{s^{\prime}, r} p(s^{\prime}, r \mid s, a) \big[r + \gamma V_\pi(s^{\prime})\big]
$$</p>
<p>所以蒙特卡洛方法通过对每次访问的回报 $G_t$ 进行平均来近似 $V_\pi(s)$：</p>
<p>$$
V_\pi(s) \approx \frac{1}{N} \sum_{i=1}^N G_t^{(i)}
$$</p>
<p>其中 N 是轨迹数量，大数定律保证当 $N \to \infty$ 时，估计值会收敛到真实的值。</p>
<p>存在<strong>两种策略评估方法：</strong></p>
<ul>
<li>
<p><strong>首次访问（First-visit MC）</strong>：仅使用轨迹中某状态第一次出现时的回报来更新该状态的估计值。</p>
</li>
<li>
<p><strong>每次访问（Every-visit MC）</strong>：轨迹中每次访问某状态时都使用其回报更新该状态的估计值。</p>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>差异点</strong></th>
          <th style="text-align: left"><strong>First-visit MC</strong></th>
          <th style="text-align: left"><strong>Every-visit MC</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>回报的使用</strong></td>
          <td style="text-align: left">只使用状态在一个 episode 中的第一次访问</td>
          <td style="text-align: left">使用状态在一个 episode 中的所有访问</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>更新次数</strong></td>
          <td style="text-align: left">每个 episode 中最多更新一次</td>
          <td style="text-align: left">每次访问状态时都更新</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>数据独立性</strong></td>
          <td style="text-align: left">更高：使用的回报较少，数据相关性低</td>
          <td style="text-align: left">较低：回报相关性高</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>收敛速度</strong></td>
          <td style="text-align: left">较慢（舍弃部分数据）</td>
          <td style="text-align: left">较快（充分利用所有回报）</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>计算开销</strong></td>
          <td style="text-align: left">较低：更新次数少</td>
          <td style="text-align: left">较高：更新次数多</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>理论分析难度</strong></td>
          <td style="text-align: left">较容易</td>
          <td style="text-align: left">较复杂</td>
      </tr>
  </tbody>
</table>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2024-11-26%20%e4%b8%8b%e5%8d%883.35.02.png" alt="截屏2024-11-26 下午3.35.02.png"  />
</p>
<p>蒙特卡洛方法需要保证所有状态都能被充分访问，否则某些状态可能永远不会在采样中出现，导致对应的值函数无法被估计。为解决这个问题，假设：**Exploring Starts（试探性出发）**即每个状态都有被访问的可能性。</p>
<p>但这种假设在真实环境中很难实现，实际中可能需要使用<strong>随机策略</strong>保证探索性，或者采用其他方法（如$\epsilon$-贪婪策略）来实现状态空间的充分覆盖。</p>
<h3 id="基于试探性出发假设的mc控制">基于试探性出发假设的MC控制<a hidden class="anchor" aria-hidden="true" href="#基于试探性出发假设的mc控制">#</a></h3>
<p>在策略评估中使用了两个假设： 无限幕，和试探性出发假设；</p>
<p>无限幕可通过GPI（广义策略迭代）方式解决</p>
<p>在实际中，满足<strong>试探性出发假设</strong>往往不现实，因此需要替代方法（如软策略或离轨方法）</p>
<p><strong>基于 ES 的 MC 控制</strong></p>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2024-11-26%20%e4%b8%8b%e5%8d%884.02.16.png" alt="截屏2024-11-26 下午4.02.16.png"  />
</p>
<h3 id="同轨和离轨">同轨和离轨<a hidden class="anchor" aria-hidden="true" href="#同轨和离轨">#</a></h3>
<p>由于试探性出发假设难以满足，可以通过其他方法保证所有状态-动作对都被探索。</p>
<p><strong>如何避免试探性出发假设？</strong></p>
<ul>
<li>
<p>用来生成样本的策略，简称行动策略 $b$ 必须是软策略：$\forall s \in S, \ a\in A(s),$有 $b(a|s)&gt;0$</p>
</li>
<li>
<p>待评估，待改进的策略，简称 目标策略 $\pi$</p>
</li>
</ul>
<p>⇒</p>
<p><strong>同轨策略方法</strong>： $\pi =b$, 且为软性策略</p>
<p><strong>离轨策略方法</strong>：$\pi \neq b$</p>
<p><strong>方法 1：同轨策略方法（On-policy Methods）</strong></p>
<p>在同轨 MC 控制中, 采样和评估使用的是同一个策略 $\pi = b$， 每次迭代后，基于 $\epsilon$-greedy策略更新。</p>
<p>$\epsilon$-greedy策略是一种常用的软性策略（Soft Policy，即保证所有动作都有非零概率被选择），以概率 $1 - \epsilon$ 选择当前最优动作（贪婪选择），以概率 $\epsilon$ 随机选择任意动作。更新方式：</p>
<p>$$
\pi(a|s) =
\begin{cases}
1 - \epsilon + \frac{\epsilon}{|A(s)|}, &amp;  a = \arg\max_a Q(s, a) \
\frac{\epsilon}{|A(s)|}, &amp; a \neq \arg\max_a Q(s, a)
\end{cases}
$$</p>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2024-11-27%20%e4%b8%8b%e5%8d%884.15.43.png" alt="截屏2024-11-27 下午4.15.43.png"  />
</p>
<p><strong>方法 2：离轨策略方法（Off-policy Methods）</strong></p>
<p>使用两个策略，</p>
<ul>
<li>
<p><strong>行动策略</strong> $b$：随机性更高，保证探索性。</p>
</li>
<li>
<p><strong>目标策略</strong> $\pi$：逐渐优化到最优。</p>
</li>
</ul>
<p><strong>行动策略</strong> $b$ 和 <strong>目标策略</strong> $\pi$ 不同， <strong>行为策略</strong> $b$ 用于生成数据，必须是软性策略，<strong>目标策略</strong> $\pi$ 是被评估和改进的策略，通常是贪婪或接近贪婪的策略。</p>
<p>这种方法与同轨（On-policy）方法的关键区别在于，数据并非直接来源于目标策略，而是来源于行为策略。这种差异导致以下两个问题：</p>
<p><strong>数据分布不同：行动策略和目标策略不一致。</strong> 在 Monte Carlo 中，我们需要根据目标策略 $\pi$来估计其对应的值函数 $v_\pi(s)$ 或 $q_\pi(s, a)$，但是，在 Off-policy 方法中，采样数据是由行动策略 $b$ 生成的， $\mathbb{E}b[G_t \mid S_t = s] \neq \mathbb{E}\pi[G_t \mid S_t = s]$，因此，我们无法直接使用行为策略生成的回报 $G_t$ 来估计目标策略的值。</p>
<p>**如何校正数据分布。**为了让行为策略生成的数据 $G_t$ 能用于估计目标策略的值，需要校正数据的分布，使其符合目标策略 $\pi$。<strong>重要性采样</strong>通过对回报进行加权，能够将行动策略的数据分布调整为目标策略的数据分布。</p>
<p>importance-sampling ratio：</p>
<p>$$
\rho_t = \frac{\pi(A_t \mid S_t)}{b(A_t \mid S_t)}.
$$</p>
<p>通过将回报 $G_t$ 按照 $\rho_t$ 加权，即可校正行为动策略的回报，使其在数学期望上等价于目标策略的回报。</p>
<p>$$
\begin{aligned}
V_{\pi}(s) = E_{\pi}[G_t|S_t=s] \\ =\sum G_t \cdot Pr{A_t,R_{t+1},S_{t+1},&hellip;S_t|S_t = s,A_{t:T-1} \sim \pi} \\ = \sum G_t \cdot \pi(A_t|S_t) \cdot p(R_{t+1},S_{t+1}|S_t,A_t) &hellip;p(R_{t},S_{t}|S_{t-1},A_{t-1}) \\ =\sum G_t \cdot \prod_{k=t}^{T-1} [\pi(A_k|S_k)p(R_{k+1},S_{k+1}|S_k,A_k)] \\ = \sum G_t \prod_{k=t}^{T-1}[\frac{\pi(A_k|S_k)}{b(A_k|S_k)}b(A_k|S_k)\cdot p(R_{k+1},S_{k+1}|S_k, A_k)] \\ = \sum \prod_{k=t}^{T-1} \frac{\pi(A_k|S_k)}{b(A_k|S_k)} G_t \cdot \prod_{k=t}^{T-1} [b(A_k|s_k)\cdot p(R_{k+1},S_{k+1}|S_k, A_k)] \\ = \sum \rho_{t:T-1} \cdot G_t \cdot Pr{A_t,R_{t+1},S_{t+1},&hellip;S_t | S_t=s,A_{t:T-1}\sim b} \ = E_b[\rho_{t:T-1}|S_t=s] \\ \approx \frac{1}{N} \sum_N^{i=1} \rho_{t:T-1} G_t^{(i)}
\end{aligned}
$$</p>
<p>(普通重要性采样)</p>
<p>增量更新</p>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2024-11-27%20%e4%b8%8b%e5%8d%885.03.13.png" alt="截屏2024-11-27 下午5.03.13.png"  />
</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>方法</strong></th>
          <th style="text-align: left"><strong>特点</strong></th>
          <th style="text-align: left"><strong>优缺点</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>基于试探性出发（ES）</strong></td>
          <td style="text-align: left">所有状态-动作对都有非零概率被访问。</td>
          <td style="text-align: left">假设不现实，难以在实际中满足。</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>同轨控制（On-policy）</strong></td>
          <td style="text-align: left">使用 (\epsilon)-贪婪策略，策略生成和评估一致。</td>
          <td style="text-align: left">简单易实现，但可能需要更长时间才能达到最优策略。</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>离轨控制（Off-policy）</strong></td>
          <td style="text-align: left">行为策略和目标策略不同，通过重要性采样校正回报。</td>
          <td style="text-align: left">灵活且更强大，但重要性采样可能导致高方差。</td>
      </tr>
  </tbody>
</table>
<h2 id="temporal-difference-learning">Temporal-Difference Learning<a hidden class="anchor" aria-hidden="true" href="#temporal-difference-learning">#</a></h2>
<h3 id="策略评估">策略评估<a hidden class="anchor" aria-hidden="true" href="#策略评估">#</a></h3>
<blockquote>
<p>TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.</p>
</blockquote>
<p>Unlike MC methods, temporal-difference (TD) does not require the episodes are complete. TD methods can learn from incomplete episodes by bootstrapping.</p>
<p>即它根据部分观察到的信息（当前的奖励和下一步状态的估计值）来更新值函数，而不是等待整个序列的结束。</p>
<p>TD 核心思想是（结合了MC和DP）</p>
<ul>
<li>
<p>MC-like：TD 利用实际经验（state-transition 和 reward）直接学习</p>
</li>
<li>
<p>DP-like：使用对状态值的估计（bootstrap）来更新</p>
</li>
</ul>
<p>TD 的核心任务：<strong>Prediction 问题</strong>。 即给定一个策略 $\pi$，估计状态值函数 $v_\pi(s)$，即策略 $\pi$ 在状态 $s$ 上的期望回报</p>
<p>在DP中， 使用环境模型计算期望值 $\mathbb{E}[G_t | S_t = s]$ 来更新值函数；在MC中，完全依赖实际回报 $G_t$ 来更新值函数。需要等待整个 episode 结束，计算从 $t$ 时刻开始的完整回报。</p>
<p>但考虑到，环境的状态转移概率分布是未知的，无法直接应用 DP 方法，且计算开销较大，同时MC 方法需要等待一个 episode 结束才能计算累积回报，很难进行在线更新；</p>
<p>于是乎，<strong>TD 方法</strong> 仅使用当前的奖励 $R_{t+1}$ 和对下一状态的估计值 $V(S_{t+1})$来更新值函数，从而避开以上两种方法的弊端；</p>
<p>以 TD(0) 为例，TD(0) 是最简单的一步 TD 方法（直走一步，之后的步骤就不走了）：</p>
<p>$$
V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]
$$</p>
<p>TD 方法的target是 $R_{t+1} + \gamma V(S_{t+1})$</p>
<p>$R_{t+1}$ 采样获得；$V(S_{t+1})$ 自举得到。</p>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2025-01-08%20%e4%b8%8b%e5%8d%888.28.00.png" alt="截屏2025-01-08 下午8.28.00.png"  />
</p>
<h3 id="sarsa-on-policy">SARSA (On-Policy)<a hidden class="anchor" aria-hidden="true" href="#sarsa-on-policy">#</a></h3>
<p><strong>State (S), Action (A), Reward (R), next State (S’), and next Action (A’)</strong></p>
<p>同轨策略下的TD控制</p>
<p>基于当前执行的策略（policy）</p>
<p>更新公式来更新动作值函数 $Q(s, a)$</p>
<p>$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]
$$</p>
<ul>
<li>
<p>更新值函数时基于当前执行的策略</p>
</li>
<li>
<p>遵循当前策略来选择下一步的动作 $A_{t+1}$</p>
</li>
<li>
<p>on-policy，更倾向于避免过于激进的探索</p>
</li>
</ul>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2025-01-08%20%e4%b8%8b%e5%8d%888.29.08.png" alt="截屏2025-01-08 下午8.29.08.png"  />
</p>
<h3 id="q-learningoff-policy">Q-Learning （Off-Policy）<a hidden class="anchor" aria-hidden="true" href="#q-learningoff-policy">#</a></h3>
<p>离轨策略下的TD控制</p>
<p>核心更新公式：</p>
<p>$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right]
$$</p>
<p>$\max_a Q(S_{t+1}, a)$：下一状态 $S_{t+1}$ 所有可能动作的最大动作值</p>
<ul>
<li>直接学习最优策略，而不依赖于当前使用的策略 （$\epsilon$-greedy 策略），能够更快找到最优解</li>
</ul>
<table>
  <thead>
      <tr>
          <th style="text-align: left">特点</th>
          <th style="text-align: left">Q-learning (Off-policy)</th>
          <th style="text-align: left">SARSA (On-policy)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>策略类型</strong></td>
          <td style="text-align: left">Off-policy：学习的是最优策略</td>
          <td style="text-align: left">On-policy：基于当前策略更新</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>更新目标</strong></td>
          <td style="text-align: left">$R_{t+1} + \gamma \max_a Q(S_{t+1}, a)$</td>
          <td style="text-align: left">$R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>探索行为的影响</strong></td>
          <td style="text-align: left">更新与探索行为无关，只考虑最优动作</td>
          <td style="text-align: left">更新考虑探索行为，基于当前策略</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>学习的激进程度</strong></td>
          <td style="text-align: left">更激进，直接学习最优策略</td>
          <td style="text-align: left">更保守，考虑探索对学习过程的影响</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>适用场景</strong></td>
          <td style="text-align: left">适合需要快速找到最优策略的场景</td>
          <td style="text-align: left">适合对探索行为敏感，需更稳定学习的场景</td>
      </tr>
  </tbody>
</table>
<p><img loading="lazy" src="/img/path_to_RLHF/RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87-assets/%e6%88%aa%e5%b1%8f2025-01-09%20%e4%b8%8b%e5%8d%881.39.11.png" alt="截屏2025-01-09 下午1.39.11.png"  />
</p>
<h3 id="expected-sarsa">Expected SARSA<a hidden class="anchor" aria-hidden="true" href="#expected-sarsa">#</a></h3>
<p>SARSA+Q-learning的结合</p>
<p>与 SARSA 和 Q-learning 相比，Expected Sarsa 在更新动作值函数时使用了期望回报，而不是基于单个动作的采样回报</p>
<p>更新公式</p>
<p>$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma E_{a \sim \pi} [Q(S_{t+1}, A_{t+1})| S_{t+1} ] - Q(S_t, A_t) \right]
$$</p>
<p>其中 $E_{a \sim \pi} [Q(S_{t+1}, A_{t+1})|S_{t+1}]$：在下一状态  $S_{t+1}$  下，按照策略  $\pi$  选择动作时的动作值函数的期望，进一步展开</p>
<p>$E_{a \sim \pi} [Q(S_{t+1}, A_{t+1})|S_{t+1}] = \sum_{a} \pi(a | S_{t+1}) Q(S_{t+1}, a)$</p>
<p>即对下一状态中所有可能的动作值  $Q(S_{t+1}, a)$  加权求和，权重为当前策略  $\pi$  对动作  $a$ 的选择概率</p>
<ul>
<li>
<p>SARSA 的更新基于具体采样的下一动作  $A_{t+1}$, 而Expected Sarsa 则基于所有可能动作的期望回报更新，消除了由于采样引起的随机性</p>
</li>
<li>
<p>由于期望计算减少了采样的不确定性，Expected Sarsa 学习过程中的方差比 SARSA 和 Q-learning 更小，学习更加稳定</p>
</li>
</ul>
<p><strong>相比 SARSA</strong>：Expected Sarsa 更稳定，减小了由于单一动作采样造成的方差。</p>
<p><strong>相比 Q-learning</strong>：Expected Sarsa 在高探索策略下更稳定，避免了完全依赖最优动作选择可能带来的不稳定。</p>
<p>Q-learning 是 Expected SARSA 的一种特例</p>
<p>当策略  $\pi$ 是greedy policy时，策略 $\pi$ 的定义如下：</p>
<p>$$
\pi(a \mid S_{t+1}) =
\begin{cases}
1 &amp; \text{if } a = \arg \max_a Q(S_{t+1}, a), \\
0 &amp; \text{else}.
\end{cases}
$$</p>
<p>将将贪婪策略代回 Expected SARSA的迭代公式，即可回到Q-learning的形式：</p>
<p>$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right]
$$</p>
<h2 id="deep-q-learning">Deep Q-Learning<a hidden class="anchor" aria-hidden="true" href="#deep-q-learning">#</a></h2>
<p>from Q-learning to Deep Q-learning</p>
<p>Q-Learning 是一种表格方法，适用于状态和动作空间较小的情景</p>
<p>很直接的想法： 通过一个参数化的 Q 函数 $Q_{\theta}(s, a)$  来近似 Q 值，而非使用 Q 表，也就是采用神经网络的方式。</p>
<p><strong>损失函数</strong>：比较 Q 值预测和 Q 目标，使用梯度下降来更新 DQN 的权重</p>
<p><strong>DQN 的稳定性问题</strong>：</p>
<ul>
<li>
<p><strong>Experience Replay</strong>：创建一个回放缓冲区存储经验元组，随机采样小批量元组进行训练；防止遗忘之前的经验，同时降低经验之间的相关性</p>
</li>
<li>
<p><strong>Fixed Q-Target</strong>：TD 目标和 Q 值的估计都使用相同的网络参数，导致训练中 Q 值和目标值同步移动，产生震荡；可以使用一个<strong>单独的目标网络</strong>计算 TD 目标，每隔  C  步将 DQN 的参数复制到目标网络中</p>
</li>
<li>
<p><strong>Double DQN</strong>：训练初期，选择下一状态最佳动作时可能高估 Q 值，导致学习复杂化；使用两个网络解耦动作选择和 Q 值目标计算，即使用 <strong>DQN 网络</strong>选择下一状态的最佳动作，使用 <strong>目标网络</strong>计算选择动作的目标 Q 值。</p>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://niraya666.github.io/tags/rl/">RL</a></li>
      <li><a href="https://niraya666.github.io/tags/alignment/">Alignment</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://niraya666.github.io/posts/milvus-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%92%8C%E5%A4%87%E5%BF%98%E5%BD%95%E4%BB%8E2.0-%E5%88%B02.5%E7%89%88%E6%9C%AC%E7%9A%84%E4%BB%8E%E5%A4%B4%E5%AD%A6%E4%B9%A0/">
    <span class="title">Next »</span>
    <br>
    <span>Milvus-2.5版本：学习笔记和备忘录</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RLHF 之路：强化学习复习之上篇 on x"
            href="https://x.com/intent/tweet/?text=RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frlhf-%25E4%25B9%258B%25E8%25B7%25AF%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25A4%258D%25E4%25B9%25A0%25E4%25B9%258B%25E4%25B8%258A%25E7%25AF%2587%2f&amp;hashtags=RL%2calignment">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RLHF 之路：强化学习复习之上篇 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frlhf-%25E4%25B9%258B%25E8%25B7%25AF%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25A4%258D%25E4%25B9%25A0%25E4%25B9%258B%25E4%25B8%258A%25E7%25AF%2587%2f&amp;title=RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87&amp;summary=RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87&amp;source=https%3a%2f%2fniraya666.github.io%2fposts%2frlhf-%25E4%25B9%258B%25E8%25B7%25AF%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25A4%258D%25E4%25B9%25A0%25E4%25B9%258B%25E4%25B8%258A%25E7%25AF%2587%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RLHF 之路：强化学习复习之上篇 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fniraya666.github.io%2fposts%2frlhf-%25E4%25B9%258B%25E8%25B7%25AF%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25A4%258D%25E4%25B9%25A0%25E4%25B9%258B%25E4%25B8%258A%25E7%25AF%2587%2f&title=RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RLHF 之路：强化学习复习之上篇 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniraya666.github.io%2fposts%2frlhf-%25E4%25B9%258B%25E8%25B7%25AF%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25A4%258D%25E4%25B9%25A0%25E4%25B9%258B%25E4%25B8%258A%25E7%25AF%2587%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RLHF 之路：强化学习复习之上篇 on whatsapp"
            href="https://api.whatsapp.com/send?text=RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87%20-%20https%3a%2f%2fniraya666.github.io%2fposts%2frlhf-%25E4%25B9%258B%25E8%25B7%25AF%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25A4%258D%25E4%25B9%25A0%25E4%25B9%258B%25E4%25B8%258A%25E7%25AF%2587%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RLHF 之路：强化学习复习之上篇 on telegram"
            href="https://telegram.me/share/url?text=RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frlhf-%25E4%25B9%258B%25E8%25B7%25AF%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25A4%258D%25E4%25B9%25A0%25E4%25B9%258B%25E4%25B8%258A%25E7%25AF%2587%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RLHF 之路：强化学习复习之上篇 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=RLHF%20%e4%b9%8b%e8%b7%af%ef%bc%9a%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%a4%8d%e4%b9%a0%e4%b9%8b%e4%b8%8a%e7%af%87&u=https%3a%2f%2fniraya666.github.io%2fposts%2frlhf-%25E4%25B9%258B%25E8%25B7%25AF%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%25E5%25A4%258D%25E4%25B9%25A0%25E4%25B9%258B%25E4%25B8%258A%25E7%25AF%2587%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="utterances">
  <script src="https://utteranc.es/client.js"
        repo="https://github.com/Niraya666/niraya666.github.io.git"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>


<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: {'[+]': ['ams']}
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://niraya666.github.io/">LZY Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
