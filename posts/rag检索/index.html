<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RAG工具箱：检索 | LZY Blog</title>
<meta name="keywords" content="RAG, RAG-Toolkits">
<meta name="description" content="If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.
— from Build a search engine, not a vector DB">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://niraya666.github.io/posts/rag%E6%A3%80%E7%B4%A2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://niraya666.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://niraya666.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://niraya666.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://niraya666.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://niraya666.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://niraya666.github.io/posts/rag%E6%A3%80%E7%B4%A2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="RAG工具箱：检索" />
<meta property="og:description" content="If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.
— from Build a search engine, not a vector DB" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://niraya666.github.io/posts/rag%E6%A3%80%E7%B4%A2/" />
<meta property="og:image" content="https://niraya666.github.io/images/papermod-cover.png" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-25T17:08:00+08:00" />
<meta property="article:modified_time" content="2024-07-25T17:08:00+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://niraya666.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="RAG工具箱：检索"/>
<meta name="twitter:description" content="If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.
— from Build a search engine, not a vector DB"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://niraya666.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "RAG工具箱：检索",
      "item": "https://niraya666.github.io/posts/rag%E6%A3%80%E7%B4%A2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RAG工具箱：检索",
  "name": "RAG工具箱：检索",
  "description": "If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.\n— from Build a search engine, not a vector DB",
  "keywords": [
    "RAG", "RAG-Toolkits"
  ],
  "articleBody": " If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.\n— from Build a search engine, not a vector DB\n一个优秀的RAG背后， 一定有一个优秀的搜索引擎；\n很多时候，在针对通用场景下， 如果能够调用常见的搜索引擎，如google 或者bing， RAG的效果一般不会太差；\n但，当考虑到一些垂直场景， 我们需要构造属于自己的“搜索引擎”，而往往根据特定的场景，在索引构建上会有特殊处理；可以参考devv，在针对代码场景下， 对RAG的检索部份也做了不少的工作和设计；\n很显然， 检索不是仅仅使用向量匹配和向量库就能搞定的事情；\n考虑到前LLM时代的NLP检索（甚至是图像检索），或者是推荐系统， 其实会发现， 这些东西并没有什么太大的变化，思路和技术依旧是可以复用的。\nvector-search is not all you need 纯粹基于向量匹配实现起来很简单，但不是万能的\n其问题来源有：\nembedding模型一般在通用预料上做训练， 在特定领域下， 其效果并不会太好，很多时候需要在特定领域语料上再做微调；\n语义匹配不见得是万能的，对于一些特定场景，其局限性尤为明显。例如，当用户询问一个具体的名词时，就会发现单纯使用向量匹配只能匹配到大致的相似度，对于具体的关键词是无能为力的。 这种情况下，语义匹配往往难以捕捉到用户提问中的精确细节。例如，用户可能询问特定型号的苹果电脑（如“MacBook Pro 2021”），但语义匹配算法可能会返回类似类型或年份的苹果电脑，而不是确切的型号。此时语义匹配的效果并不会太好。\nembedding模型计算的是输入的两个文本（A和B）的相似度。然而，在实际应用中，输入的是一个问句（query），需要匹配的是一段文字。在这种情况下，我们关注的重点是文本之间的相关性而非简单的相似性。因此，通常在使用embedding模型后，还需要一个reranking模型，根据相关性对结果进行排序，以确保返回最相关的答案。\n对于组合的问题无效；如果query是一些复杂的需要一些推理的问题（如多跳），使用vector- search显然不是好办法；这类问题通常涉及多个子问题，每个子问题的答案都是解答下一个问题的基础， 举例来说，回答“谁是发明电话的人的孙子在2020年的职业是什么？”需要知道发明电话的人是谁，接着找出他的孙子，然后查询这个孙子的职业。面对需要多步推理的复杂问题时，向量搜索的能力有限，原因在于它不可能进行复杂的逻辑推理和多步信息整合。在这样的场景下，需要的是Agent将query进行分解，然后分别调用工具（搜索引擎）；\n其实我们可能过于迷信向量搜索。事实上，对于一些简单的场景，基于倒排索引的关键词匹配效果更好、更加简单且成本更低。倒排索引是一种高效的文本检索方法，通过建立关键词到文档的映射，可以快速找到包含特定关键词的文档。对于不需要复杂推理的简单查询，倒排索引可以提供快速且准确的搜索结果。\n不止只有向量检索 稀疏向量（Sparse）和稠密向量（Dense）是通过不同的算法计算的。稀疏向量主要由零值组成，只有少数几个非零值，而稠密向量则主要包含非零值。稀疏嵌入由算法如BM25和SPLADE生成，而稠密嵌入则由机器学习模型如GloVe和Transformers生成。\nTF-IDF(Term-Frequency Inverse-Document Frequency)是一种用于评估文本中一个词对一个文档的重要程度的统计方法。其原理基于两个指标：词频（TF）和逆文档频率（IDF）。\n词频（TF）：表示某个词在文档中出现的频率，计算公式为：\n$$ \\text{TF}(t,d) = \\frac{\\text{词t在文档d中出现的次数}}{\\text{文档d中的总词数}} $$\n逆文档频率（IDF）：衡量某词在整个文档集合中重要性的指标。公式为：\n$$ \\text{IDF}(t) = \\log\\left(\\frac{\\text{总文档数}}{\\text{包含词t的文档数} + 1}\\right) $$\nTF-IDF值通过将词频和逆文档频率相乘计算得到：\n$$ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $$\nBM25（Best Matching 25）是一种基于概率模型的文本检索算法，它在TF-IDF（词频-逆文档频率）的基础上进行改进。BM25通过引入二元独立模型，并添加归一化惩罚来计算文档长度相对于数据库中所有文档平均长度的权重。\nBM25的核心公式如下：\n$$ \\text{BM25}(q, D) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})} $$\n其中：\nq 代表查询\nD 代表文档\n$q_i$ 是查询中的第i个词\n$f(q_i, D)$ 是词 ($q_i$) 在文档 (D) 中的词频\n|D| 是文档 (D) 的长度（以词计）\n$\\text{avgdl}$ 是集合中所有文档的平均长度\n$k_1$ 和 b 是调节参数，通常 $k_1$ 在1.2到2之间，b 取0.75\n$\\text{IDF}(q_i)$是词 $q_i$ 的逆文档频率\nDense vector 使用稠密向量表示存储在数据库中的信息，包括文本、图像和其他类型的数据，。这些嵌入由机器学习模型生成，将数据转换为向量。也就是最近两年大家常说的向量检索。\n如何提升检索效果：\n自然而然的想法，不再使用单一的向量检索， 而是采用多种检索方式混合的方式，不论是混合不同的embedding模型的检索还是Sparse-vector和Dense-vector混合使用以获得二者的优势。通常来说，稠密向量擅长理解查询的上下文，而稀疏向量则擅长关键词匹配。\n那么需要将二者合并，but how？\nReciprocal Rank Fusion (RRF) 其核心思想是通过计算每个文档在不同排名列表中的倒数排名之和来确定最终排名。这样，排名较高的文档会得到更高的分数，而排名较低的文档会受到惩罚。\nRRF的计算公式如下：\n$$ \\text{RRF}(d) = \\sum{i=1}^{N} \\frac{1}{k + r_{i}(d)} $$\n其中：\n(d) 代表文档\n(N) 是排名列表的数量\n(k) 是一个常数，用于平滑处理，通常取值为60\n($r_{i}(d)$) 是文档 (d) 在第 (i) 个排名列表中的排名\n通过这个公式，每个文档的最终RRF分数是其在所有排名列表中的倒数排名之和，分数越高，排名越靠前。\n在langchain **ensemble-retriever**的源码中可以看到其具体的思想逻辑：\ndef weighted_reciprocal_rank( self, doc_lists: List[List[Document]] ) -\u003e List[Document]: \"\"\" Perform weighted Reciprocal Rank Fusion on multiple rank lists. You can find more details about RRF here: https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf Args: doc_lists: A list of rank lists, where each rank list contains unique items. Returns: list: The final aggregated list of items sorted by their weighted RRF scores in descending order. \"\"\" if len(doc_lists) != len(self.weights): raise ValueError( \"Number of rank lists must be equal to the number of weights.\" ) # Associate each doc's content with its RRF score for later sorting by it # Duplicated contents across retrievers are collapsed \u0026 scored cumulatively rrf_score: Dict[str, float] = defaultdict(float) for doc_list, weight in zip(doc_lists, self.weights): for rank, doc in enumerate(doc_list, start=1): rrf_score[ doc.page_content if self.id_key is None else doc.metadata[self.id_key] ] += weight / (rank + self.c) # Docs are deduplicated by their contents then sorted by their scores all_docs = chain.from_iterable(doc_lists) sorted_docs = sorted( unique_by_key( all_docs, lambda doc: doc.page_content if self.id_key is None else doc.metadata[self.id_key], ), reverse=True, key=lambda doc: rrf_score[ doc.page_content if self.id_key is None else doc.metadata[self.id_key] ], ) return sorted_docs 排序模型 在信息检索和问答系统中，处理用户查询通常分为两个步骤：召回（retrieval）和重排序（reranking）。\n召回的目标是用非常短的时间在海量数据中找到一定范围的备选项。这一过程的精度可能不会特别高，但可以将候选项从千万量级压缩到百量级。（这一过程涉及ANN也就是Approximate Nearest Neighbor， 在本文的后段会提及）\n重排序阶段，需要从召回阶段的备选项中选择符合需要的选项。由于候选项数量已经大幅减少，可以采用精度较高但相对计算时间较长的算法对这些备选项进行重新排序，以提高最终结果的准确性。\n重排序阶段场景的模型如`BAAI/bge 系列，从其huggingface仓库中的config.json可以发现：\n\"_name_or_path\": \"BAAI/bge-m3\", \"architectures\": [ \"XLMRobertaForSequenceClassification\" ], 其本质是基于**XLM-RoBERTa**的序列分类模型。在下游任务中，对于序列分类任务，XLMRobertaForSequenceClassification 在 XLM-RoBERTa 的基础上添加了一个分类层（一个全连接层）来进行分类；简单来说，对于信息检索或问答系统中，输入用户问题（query），和匹配到的doc，通过合并query和doc形成一个sequence（[CLS]query[SEP]doc[SEP]），通过XLMRobertaForSequenceClassification 可以计算出二者的相关性分数，遍历候选集中的doc（一般在百数量级左右），即可得到query同候选集相关性分数。\n一些新的工作 Matryoshka Representation Learning（MRL） 来源于OpenAI 的embedding 技术：New embedding models and API updates\n和NeurIPS 2022发表的论文：Matryoshka Representation Learning\nMotivation：现有的深度学习模型通常学习固定维度的表示，无法灵活适应任务需求；表示维度的难以确定；表示维度缺乏粗到细（coarse-to-fine）的粒度；\nMRL旨在学习具有不同粒度的表示，允许单个embedding适应下游任务的计算约束。其设计思想是为了在不同的embedding维度上使用损失函数，从而训练一个模型，使其在不同的embedding维度上都能保持良好的性能\n训练过程：MRL 在多个嵌套的维度上训练模型，使得每个低维度的表示都能作为数据点的有用表示。这样，模型在不同的任务和计算约束下都能表现良好。\n推理过程：在推理过程中，MRL 可以根据需要使用不同维度的嵌入。对于分类任务，可以从低维度开始，逐步使用更高维度的表示。对于检索任务，可以先使用低维度表示进行初步筛选，然后使用高维度表示进行精细排序。\n（具体可以参考论文和sentence_transformers中所实现的MatryoshkaLoss）\nbge-m3 paper\ngithub\nm3—即Multi-Linguality 支持多语言， Multi-Functionality 支持多种检索方式（Dense-retrieval，Sparse-Retrieval， Multi-Vec Retrieval），以及Multi-Granularity 支持多粒度的检索（sentence-level，Passage-level，和Doc-level）\npip install -U FlagEmbedding Dense Embedding from FlagEmbedding import BGEM3FlagModel model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"] model.encode(sentences_1) print(model.encode(sentences_1)['dense_vecs'].shape) # (2, 1024) Sparse Embedding (Lexical Weight) output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False) output_1 \"\"\" {'dense_vecs': array([[-0.03412 , -0.04706 , -0.0009317, ..., 0.0483 , 0.007576 , -0.02959 ], [-0.01026 , -0.0449 , -0.02432 , ..., -0.00828 , 0.01502 , 0.011086 ]], dtype=float16), 'lexical_weights': [defaultdict(int, {'4865': 0.0836, '83': 0.0814, '335': 0.1295, '11679': 0.252, '276': 0.17, '363': 0.2695, '32': 0.04083}), defaultdict(int, {'262': 0.04996, '5983': 0.1367, '2320': 0.04483, '111': 0.06335, '90017': 0.2517, '2588': 0.3335})], 'colbert_vecs': None} \"\"\" # you can see the weight for each token: print(model.convert_id_to_token(output_1['lexical_weights'])) # [{'What': 0.08356, 'is': 0.0814, 'B': 0.1296, 'GE': 0.252, 'M': 0.1702, '3': 0.2695, '?': 0.04092}, # {'De': 0.05005, 'fin': 0.1368, 'ation': 0.04498, 'of': 0.0633, 'BM': 0.2515, '25': 0.3335}] # compute the scores via lexical mathcing print(model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_1['lexical_weights'][1])) # 0 Multi-Vector (ColBERT) sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"] output_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False) print(output_1['colbert_vecs'][0].shape) # (8, 1024) print(model.colbert_score(output_1['colbert_vecs'][0], output_1['colbert_vecs'][1])) # tensor(0.4768) ColBERT score： 在计算完查询和文档之间的相似度矩阵后，对每个查询token选择与文档中token的最大相似度，将所有最大相似度累加，得到最终的colbert_score。\n基于LLM实现embedding 几个代表模型：\nAlibaba-NLP/gte-Qwen2-7B-instruct\nintfloat/e5-mistral-7b-instruct\ne5-mistral-7b-instruct论文：Improving Text Embeddings with Large Language Models\nconfig.json:\n{ \"_name_or_path\": \"mistralai/Mistral-7B-v0.1\", \"architectures\": [ \"MistralModel\" ], \"bos_token_id\": 1, \"eos_token_id\": 2, \"hidden_act\": \"silu\", \"hidden_size\": 4096, \"initializer_range\": 0.02, \"intermediate_size\": 14336, \"max_position_embeddings\": 32768, ... 通过LLM生成数据（合成数据）， 采用对比学习微调Mistral-7B；\nGiven a pretrained LLM, we append an [EOS] token to the end of the query and document, and then feed them into the LLM to obtain the query and document embeddings $(h_{q_{inst}^+}, h_d^+)$, by taking the last layer [EOS] vector.\n附录 如何找到合适的embedding模型和reranker模型 首先推荐从 huggingface MTEB Leaderboard上选择排名靠前的模型\n其次，实验！根据自己的场景构造测试集验证，才能选择出适合自己场景的embedding模型和reranker模型；\nembedding模型和reranker模型的注意事项 上下文长度限制。 这些模型大多本质是Bert-based 模型， 而Bert的上下文长度限制一般是512，具体可以在huggingface中的config.json中查看；以BAAI/bge-base-en-v1.5 为例\n{ \"_name_or_path\": \"/root/.cache/torch/sentence_transformers/BAAI_bge-base-en/\", \"architectures\": [ \"BertModel\" ], \"attention_probs_dropout_prob\": 0.1, \"classifier_dropout\": null, \"gradient_checkpointing\": false, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 768, \"id2label\": { \"0\": \"LABEL_0\" }, \"initializer_range\": 0.02, \"intermediate_size\": 3072, \"label2id\": { \"LABEL_0\": 0 }, \"layer_norm_eps\": 1e-12, \"max_position_embeddings\": 512, \"model_type\": \"bert\", \"num_attention_heads\": 12, \"num_hidden_layers\": 12, \"pad_token_id\": 0, \"position_embedding_type\": \"absolute\", \"torch_dtype\": \"float32\", \"transformers_version\": \"4.30.0\", \"type_vocab_size\": 2, \"use_cache\": true, \"vocab_size\": 30522 } 其中`\"max_position_embeddings\": 512, 也就是它的上下文长度，但输入sequence长度超过512个token，则模型只看到了最前面的512个token；\n向量维度\n微调\n如何微调embedding 模型 采用FlagEmbedding，参考：How to fine-tune bge embedding model?\n微调Sentence Transformer，参考：Training Overview\n还有哪些类型的reranker模型 根据ColBert文章\n作者将query-document matching 任务（reranking）分类成：\nRepresentation-based Similarity, 双塔，独立对查询和文档进行编码，然后通过计算它们的向量相似度进行匹配， 但query和doc间无更精细的交互。\nQuery-Document interaction： 在查询和文档之间直接进行交互，通常使用CNN来捕捉查询和文档之间的交互特征， 但交互计算复杂度较高，尤其是当查询和文档长度较长时。\nAll-to-All Interaction，基于Bert 的Cross-encoder结构， 查询和文档的每个词与对方的每个词进行全连接交互。这种方式可以捕捉细粒度的交互信息，通常需要较高的计算资源。\nLate Interaction， 迟交互型，在对查询和文档的每个词进行独立编码后，再进行交互计算。通常使用最大池化（MaxSim）来选择最相关的词进行匹配，从而结合了高效性和细粒度交互的优点。\n此外， 当然也有基于LLMs的reranker，参考：llm_reranker\n以 BAAI/bge-reranker-v2-gemma 为例， 只是做这样一件事\nprompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\" 以输出“Yes” 的logit 作为相关分数\n对比学习 在之前不断提到了embedding，当然， 和transformer中的embedding层不是一个东西；\n这里所指的embedding是通过对比学习的方式，将语义相近的文本映射到高维参数空间中的相近位置，使它们的距离尽可能短。相反，语义差别大的文本则被映射到参数空间中远离的位置。类似地，这种方法也可以应用于图像（相似图像检索）以及文本和图像的多模态语义匹配（如CLIP）。那么，对比学习有三个核心：loss function，网络架构和采样。\n对比学习有三个核心要素：网络架构、损失函数（loss function）和采样策略。\n常见的损失函数有contrastive loss、triplet loss，以及N-pair loss。\n网络架构方面,对比学习通常采用双塔(siamese network)或者三塔(triplet)结构。双塔结构包含两个相同或相似的编码器,分别用于处理正样本对;三塔结构则增加了一个用于处理负样本的编码器。这些编码器可以是各种深度学习模型,如CNN、RNN或Transformer等,具体选择取决于任务域和数据类型。这一步将原始较为复杂的数据类型，通过深度学习的方法降到相对较低的维度便于之后的计算。\n对于双塔结构， 一般采用Contrastive Loss， 其用于度量两个样本之间的相似度。其公式为：\n$$ L = \\frac{1}{2N} \\sum_{i=1}^N (y_i d_i^2 + (1 - y_i) \\max(0, m - d_i)^2) $$\n其中：\n$d_i$是样本对$(x_i, x_j)$ 的欧几里得距离。\n$y_i$ 是样本对的标签，1 表示正样本对，0 表示负样本对。\nm 是一个超参数，表示负样本对之间的最小距离，也称margin。\nTriplet Loss则使用于triplet-network结构, 其通过构造三元组 ( anchor, positive, negative) 来训练模型，使得正样本（positive）与锚点（anchor）的距离小于负样本（negative）与锚点的距离。其公式为：\n$$ L = \\sum_{i=1}^N \\left[ |f(x_i^a) - f(x_i^p)|_2^2 - |f(x_i^a) - f(x_i^n)|_2^2 + \\alpha \\right] $$\n其中：\n$f(x)$ 表示样本 x 的特征向量。\n$\\alpha$是一个超参数，表示正负样本对之间的最小距离差。\nN-pair Loss 是 Triplet Loss 的推广，通过引入多个负样本来提升训练效果。\n$$ L = \\sum_{i=1}^N \\log \\left( 1 + \\sum_{j \\neq i} \\exp \\left( f(x_i)^\\top f(x_j^n) - f(x_i)^\\top f(x_i^p) \\right) \\right) $$\n在训练过程中， 采样策略则很关键；但运气不好， 选择的样本都是Easy Negative mining，也就是$d(a,p)+margin",
  "wordCount" : "1076",
  "inLanguage": "en",
  "image": "https://niraya666.github.io/images/papermod-cover.png","datePublished": "2024-07-25T17:08:00+08:00",
  "dateModified": "2024-07-25T17:08:00+08:00",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://niraya666.github.io/posts/rag%E6%A3%80%E7%B4%A2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LZY Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://niraya666.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niraya666.github.io/" accesskey="h" title="LZY Blog (Alt + H)">LZY Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niraya666.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/musik/" title="musik!">
                    <span>musik!</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/monthly/" title="月刊">
                    <span>月刊</span>
                </a>
            </li>
            <li>
                <a href="https://niraya666.github.io/travel/" title="游记">
                    <span>游记</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://niraya666.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://niraya666.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      RAG工具箱：检索
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2024-07-25 17:08:00 +0800 CST'>July 25, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Theme PaperMod

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#vector-search-is-not-all-you-need" aria-label="vector-search is not all you need">vector-search is not all you need</a><ul>
                        
                <li>
                    <a href="#%e4%b8%8d%e6%ad%a2%e5%8f%aa%e6%9c%89%e5%90%91%e9%87%8f%e6%a3%80%e7%b4%a2" aria-label="不止只有向量检索">不止只有向量检索</a></li>
                <li>
                    <a href="#%e6%8e%92%e5%ba%8f%e6%a8%a1%e5%9e%8b" aria-label="排序模型">排序模型</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%b8%80%e4%ba%9b%e6%96%b0%e7%9a%84%e5%b7%a5%e4%bd%9c" aria-label="一些新的工作">一些新的工作</a><ul>
                        
                <li>
                    <a href="#matryoshka-representation-learningmrl" aria-label="Matryoshka Representation Learning（MRL）">Matryoshka Representation Learning（MRL）</a></li>
                <li>
                    <a href="#bge-m3" aria-label="bge-m3">bge-m3</a></li>
                <li>
                    <a href="#%e5%9f%ba%e4%ba%8ellm%e5%ae%9e%e7%8e%b0embedding" aria-label="基于LLM实现embedding">基于LLM实现embedding</a></li></ul>
                </li>
                <li>
                    <a href="#%e9%99%84%e5%bd%95" aria-label="附录">附录</a><ul>
                        
                <li>
                    <a href="#%e5%a6%82%e4%bd%95%e6%89%be%e5%88%b0%e5%90%88%e9%80%82%e7%9a%84embedding%e6%a8%a1%e5%9e%8b%e5%92%8creranker%e6%a8%a1%e5%9e%8b" aria-label="如何找到合适的embedding模型和reranker模型">如何找到合适的embedding模型和reranker模型</a></li>
                <li>
                    <a href="#embedding%e6%a8%a1%e5%9e%8b%e5%92%8creranker%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9" aria-label="embedding模型和reranker模型的注意事项">embedding模型和reranker模型的注意事项</a></li>
                <li>
                    <a href="#%e5%a6%82%e4%bd%95%e5%be%ae%e8%b0%83embedding-%e6%a8%a1%e5%9e%8b" aria-label="如何微调embedding 模型">如何微调embedding 模型</a></li>
                <li>
                    <a href="#%e8%bf%98%e6%9c%89%e5%93%aa%e4%ba%9b%e7%b1%bb%e5%9e%8b%e7%9a%84reranker%e6%a8%a1%e5%9e%8b" aria-label="还有哪些类型的reranker模型">还有哪些类型的reranker模型</a></li>
                <li>
                    <a href="#%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0" aria-label="对比学习">对比学习</a></li>
                <li>
                    <a href="#%e5%90%91%e9%87%8f%e5%ba%93%e6%98%af%e4%b8%aa%e4%bb%80%e4%b9%88%e4%b8%9c%e8%a5%bf" aria-label="向量库是个什么东西">向量库是个什么东西</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%8f%82%e8%80%83" aria-label="参考">参考</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves. This is likely something your organization has considered before, and if it doesn’t exist it’s because building a good search engine has traditionally been a significant undertaking.</p>
</blockquote>
<p>— from <a href="https://blog.elicit.com/search-vs-vector-db/">Build a search engine, not a vector DB</a></p>
<p>一个优秀的RAG背后， 一定有一个优秀的搜索引擎；</p>
<p>很多时候，在针对通用场景下，  如果能够调用常见的搜索引擎，如google 或者bing， RAG的效果一般不会太差；</p>
<p>但，当考虑到一些垂直场景， 我们需要构造属于自己的“搜索引擎”，而往往根据特定的场景，在索引构建上会有特殊处理；可以参考devv，在针对代码场景下， 对RAG的检索部份也做了不少的工作和设计；</p>
<p>很显然， 检索不是仅仅使用向量匹配和向量库就能搞定的事情；</p>
<p>考虑到前LLM时代的NLP检索（甚至是图像检索），或者是推荐系统， 其实会发现， 这些东西并没有什么太大的变化，思路和技术依旧是可以复用的。</p>
<h2 id="vector-search-is-not-all-you-need">vector-search is not all you need<a hidden class="anchor" aria-hidden="true" href="#vector-search-is-not-all-you-need">#</a></h2>
<p>纯粹基于向量匹配实现起来很简单，但不是万能的</p>
<p>其问题来源有：</p>
<p>embedding模型一般在通用预料上做训练， 在特定领域下， 其效果并不会太好，很多时候需要在特定领域语料上再做微调；</p>
<p>语义匹配不见得是万能的，对于一些特定场景，其局限性尤为明显。例如，当用户询问一个具体的名词时，就会发现单纯使用向量匹配只能匹配到大致的相似度，对于具体的关键词是无能为力的。 这种情况下，语义匹配往往难以捕捉到用户提问中的精确细节。例如，用户可能询问特定型号的苹果电脑（如“MacBook Pro 2021”），但语义匹配算法可能会返回类似类型或年份的苹果电脑，而不是确切的型号。此时语义匹配的效果并不会太好。</p>
<p>embedding模型计算的是输入的两个文本（A和B）的相似度。然而，在实际应用中，输入的是一个问句（query），需要匹配的是一段文字。在这种情况下，我们关注的重点是文本之间的相关性而非简单的相似性。因此，通常在使用embedding模型后，还需要一个reranking模型，根据相关性对结果进行排序，以确保返回最相关的答案。</p>
<p>对于组合的问题无效；如果query是一些复杂的需要一些推理的问题（如多跳），使用vector- search显然不是好办法；这类问题通常涉及多个子问题，每个子问题的答案都是解答下一个问题的基础， 举例来说，回答“谁是发明电话的人的孙子在2020年的职业是什么？”需要知道发明电话的人是谁，接着找出他的孙子，然后查询这个孙子的职业。面对需要多步推理的复杂问题时，向量搜索的能力有限，原因在于它不可能进行复杂的逻辑推理和多步信息整合。在这样的场景下，需要的是Agent将query进行分解，然后分别调用工具（搜索引擎）；</p>
<p>其实我们可能过于迷信向量搜索。事实上，对于一些简单的场景，基于倒排索引的关键词匹配效果更好、更加简单且成本更低。倒排索引是一种高效的文本检索方法，通过建立关键词到文档的映射，可以快速找到包含特定关键词的文档。对于不需要复杂推理的简单查询，倒排索引可以提供快速且准确的搜索结果。</p>
<h3 id="不止只有向量检索">不止只有向量检索<a hidden class="anchor" aria-hidden="true" href="#不止只有向量检索">#</a></h3>
<p>稀疏向量（Sparse）和稠密向量（Dense）是通过不同的算法计算的。稀疏向量主要由零值组成，只有少数几个非零值，而稠密向量则主要包含非零值。稀疏嵌入由算法如<strong>BM25</strong>和SPLADE生成，而稠密嵌入则由机器学习模型如GloVe和<strong>Transformers</strong>生成。</p>
<p><strong>TF-IDF</strong>(Term-Frequency Inverse-Document Frequency)是一种用于评估文本中一个词对一个文档的重要程度的统计方法。其原理基于两个指标：词频（TF）和逆文档频率（IDF）。</p>
<p>词频（TF）：表示某个词在文档中出现的频率，计算公式为：</p>
<p>$$
\text{TF}(t,d) = \frac{\text{词t在文档d中出现的次数}}{\text{文档d中的总词数}}
$$</p>
<p>逆文档频率（IDF）：衡量某词在整个文档集合中重要性的指标。公式为：</p>
<p>$$
\text{IDF}(t) = \log\left(\frac{\text{总文档数}}{\text{包含词t的文档数} + 1}\right)
$$</p>
<p>TF-IDF值通过将词频和逆文档频率相乘计算得到：</p>
<p>$$
\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)
$$</p>
<p><strong>BM25</strong>（Best Matching 25）是一种基于概率模型的文本检索算法，它在TF-IDF（词频-逆文档频率）的基础上进行改进。BM25通过引入二元独立模型，并添加归一化惩罚来计算文档长度相对于数据库中所有文档平均长度的权重。</p>
<p>BM25的核心公式如下：</p>
<p>$$
\text{BM25}(q, D) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
$$</p>
<p>其中：</p>
<ul>
<li>
<p>q 代表查询</p>
</li>
<li>
<p>D 代表文档</p>
</li>
<li>
<p>$q_i$ 是查询中的第i个词</p>
</li>
<li>
<p>$f(q_i, D)$ 是词 ($q_i$) 在文档 (D) 中的词频</p>
</li>
<li>
<p>|D| 是文档 (D) 的长度（以词计）</p>
</li>
<li>
<p>$\text{avgdl}$ 是集合中所有文档的平均长度</p>
</li>
<li>
<p>$k_1$ 和 b 是调节参数，通常 $k_1$ 在1.2到2之间，b 取0.75</p>
</li>
<li>
<p>$\text{IDF}(q_i)$是词 $q_i$ 的逆文档频率</p>
</li>
</ul>
<p><strong>Dense vector</strong> 使用稠密向量表示存储在数据库中的信息，包括文本、图像和其他类型的数据，。这些嵌入由机器学习模型生成，将数据转换为向量。也就是最近两年大家常说的向量检索。</p>
<p>如何提升检索效果：</p>
<p>自然而然的想法，不再使用单一的向量检索， 而是采用多种检索方式混合的方式，不论是混合不同的embedding模型的检索还是Sparse-vector和Dense-vector混合使用以获得二者的优势。通常来说，稠密向量擅长理解查询的上下文，而稀疏向量则擅长关键词匹配。</p>
<p>那么需要将二者合并，but how？</p>
<p><strong>Reciprocal Rank Fusion (RRF)</strong> 其核心思想是通过计算每个文档在不同排名列表中的倒数排名之和来确定最终排名。这样，排名较高的文档会得到更高的分数，而排名较低的文档会受到惩罚。</p>
<p>RRF的计算公式如下：</p>
<p>$$
\text{RRF}(d) = \sum{i=1}^{N} \frac{1}{k + r_{i}(d)}
$$</p>
<p>其中：</p>
<ul>
<li>
<p>(d) 代表文档</p>
</li>
<li>
<p>(N) 是排名列表的数量</p>
</li>
<li>
<p>(k) 是一个常数，用于平滑处理，通常取值为60</p>
</li>
<li>
<p>($r_{i}(d)$) 是文档 (d) 在第 (i) 个排名列表中的排名</p>
</li>
</ul>
<p>通过这个公式，每个文档的最终RRF分数是其在所有排名列表中的倒数排名之和，分数越高，排名越靠前。</p>
<p>在langchain **<a href="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/retrievers/ensemble.py#L287">ensemble-retriever</a>**的源码中可以看到其具体的思想逻辑：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">weighted_reciprocal_rank</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span> <span class="n">doc_lists</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Perform weighted Reciprocal Rank Fusion on multiple rank lists.
</span></span></span><span class="line"><span class="cl"><span class="s2">        You can find more details about RRF here:
</span></span></span><span class="line"><span class="cl"><span class="s2">        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">            doc_lists: A list of rank lists, where each rank list contains unique items.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">            list: The final aggregated list of items sorted by their weighted RRF
</span></span></span><span class="line"><span class="cl"><span class="s2">                    scores in descending order.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc_lists</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;Number of rank lists must be equal to the number of weights.&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Associate each doc&#39;s content with its RRF score for later sorting by it</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Duplicated contents across retrievers are collapsed &amp; scored cumulatively</span>
</span></span><span class="line"><span class="cl">        <span class="n">rrf_score</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">doc_list</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">doc_lists</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">doc_list</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">rrf_score</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">                    <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span>
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_key</span> <span class="ow">is</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">                    <span class="k">else</span> <span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">id_key</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="p">]</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Docs are deduplicated by their contents then sorted by their scores</span>
</span></span><span class="line"><span class="cl">        <span class="n">all_docs</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">doc_lists</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">sorted_docs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">unique_by_key</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">all_docs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="k">lambda</span> <span class="n">doc</span><span class="p">:</span> <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_key</span> <span class="ow">is</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span> <span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">id_key</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">doc</span><span class="p">:</span> <span class="n">rrf_score</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_key</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">id_key</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">sorted_docs</span>
</span></span></code></pre></div><h3 id="排序模型">排序模型<a hidden class="anchor" aria-hidden="true" href="#排序模型">#</a></h3>
<p><img loading="lazy" src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/InformationRetrieval.png" alt=""  />
</p>
<p>在信息检索和问答系统中，处理用户查询通常分为两个步骤：召回（retrieval）和重排序（reranking）。</p>
<p>召回的目标是用非常短的时间在海量数据中找到一定范围的备选项。这一过程的精度可能不会特别高，但可以将候选项从千万量级压缩到百量级。（这一过程涉及ANN也就是Approximate Nearest Neighbor， 在本文的后段会提及）</p>
<p>重排序阶段，需要从召回阶段的备选项中选择符合需要的选项。由于候选项数量已经大幅减少，可以采用精度较高但相对计算时间较长的算法对这些备选项进行重新排序，以提高最终结果的准确性。</p>
<p>重排序阶段场景的模型如`<code>BAAI/bge</code> 系列，从其huggingface仓库中的<code>config.json</code>可以发现：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;_name_or_path&#34;</span><span class="p">:</span> <span class="s2">&#34;BAAI/bge-m3&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;architectures&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;XLMRobertaForSequenceClassification&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="p">],</span>
</span></span></code></pre></div><p>其本质是基于**<a href="https://huggingface.co/docs/transformers/model_doc/xlm-roberta">XLM-RoBERTa</a>**的序列分类模型。在下游任务中，对于序列分类任务，<code>XLMRobertaForSequenceClassification</code> 在 XLM-RoBERTa 的基础上添加了一个分类层（一个全连接层）来进行分类；简单来说，对于信息检索或问答系统中，输入用户问题（query），和匹配到的doc，通过合并query和doc形成一个sequence（[CLS]query[SEP]doc[SEP]），通过<code>XLMRobertaForSequenceClassification</code>
可以计算出二者的相关性分数，遍历候选集中的doc（一般在百数量级左右），即可得到query同候选集相关性分数。</p>
<h2 id="一些新的工作">一些新的工作<a hidden class="anchor" aria-hidden="true" href="#一些新的工作">#</a></h2>
<h3 id="matryoshka-representation-learningmrl"><strong>Matryoshka</strong> <strong>Representation Learning（MRL）</strong><a hidden class="anchor" aria-hidden="true" href="#matryoshka-representation-learningmrl">#</a></h3>
<p>来源于OpenAI 的embedding 技术：<a href="https://openai.com/index/new-embedding-models-and-api-updates/">New embedding models and API updates</a></p>
<p>和NeurIPS 2022发表的论文：<strong><a href="https://arxiv.org/abs/2205.13147">Matryoshka Representation Learning</a></strong></p>
<p><strong>Motivation</strong>：现有的深度学习模型通常学习固定维度的表示，无法灵活适应任务需求；表示维度的难以确定；表示维度缺乏粗到细（coarse-to-fine）的粒度；</p>
<p>MRL旨在学习具有不同粒度的表示，允许单个embedding适应下游任务的计算约束。其设计思想是为了在不同的embedding维度上使用损失函数，从而训练一个模型，使其在不同的embedding维度上都能保持良好的性能</p>
<p><strong>训练过程</strong>：MRL 在多个嵌套的维度上训练模型，使得每个低维度的表示都能作为数据点的有用表示。这样，模型在不同的任务和计算约束下都能表现良好。</p>
<p><strong>推理过程</strong>：在推理过程中，MRL 可以根据需要使用不同维度的嵌入。对于分类任务，可以从低维度开始，逐步使用更高维度的表示。对于检索任务，可以先使用低维度表示进行初步筛选，然后使用高维度表示进行精细排序。</p>
<p>（具体可以参考论文和sentence_transformers中所实现的<a href="https://github.com/UKPLab/sentence-transformers/blob/737353354fbdf1a419eee864f998ffe9fdf3b682/sentence_transformers/losses/MatryoshkaLoss.py">MatryoshkaLoss</a>）</p>
<h3 id="bge-m3">bge-m3<a hidden class="anchor" aria-hidden="true" href="#bge-m3">#</a></h3>
<p><a href="https://arxiv.org/pdf/2402.03216">paper</a></p>
<p><a href="https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3">github</a></p>
<p>m3—即<strong>M</strong>ulti-Linguality 支持多语言， <strong>M</strong>ulti-Functionality 支持多种检索方式（Dense-retrieval，Sparse-Retrieval， Multi-Vec Retrieval），以及<strong>M</strong>ulti-Granularity 支持多粒度的检索（sentence-level，Passage-level，和Doc-level）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">FlagEmbedding</span>
</span></span></code></pre></div><ul>
<li>Dense Embedding</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">FlagEmbedding</span> <span class="kn">import</span> <span class="n">BGEM3FlagModel</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">BGEM3FlagModel</span><span class="p">(</span><span class="s1">&#39;BAAI/bge-m3&#39;</span><span class="p">,</span>  
</span></span><span class="line"><span class="cl">                       <span class="n">use_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Setting use_fp16 to True speeds up computation with a slight performance degradation</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sentences_1</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;What is BGE M3?&#34;</span><span class="p">,</span> <span class="s2">&#34;Defination of BM25&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentences_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentences_1</span><span class="p">)[</span><span class="s1">&#39;dense_vecs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># (2, 1024)</span>
</span></span></code></pre></div><ul>
<li>Sparse Embedding (Lexical Weight)</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">output_1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentences_1</span><span class="p">,</span> <span class="n">return_dense</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_colbert_vecs</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">output_1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">{&#39;dense_vecs&#39;: array([[-0.03412  , -0.04706  , -0.0009317, ...,  0.0483   ,  0.007576 ,
</span></span></span><span class="line"><span class="cl"><span class="s2">         -0.02959  ],
</span></span></span><span class="line"><span class="cl"><span class="s2">        [-0.01026  , -0.0449   , -0.02432  , ..., -0.00828  ,  0.01502  ,
</span></span></span><span class="line"><span class="cl"><span class="s2">          0.011086 ]], dtype=float16),
</span></span></span><span class="line"><span class="cl"><span class="s2"> &#39;lexical_weights&#39;: [defaultdict(int,
</span></span></span><span class="line"><span class="cl"><span class="s2">              {&#39;4865&#39;: 0.0836,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;83&#39;: 0.0814,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;335&#39;: 0.1295,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;11679&#39;: 0.252,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;276&#39;: 0.17,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;363&#39;: 0.2695,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;32&#39;: 0.04083}),
</span></span></span><span class="line"><span class="cl"><span class="s2">  defaultdict(int,
</span></span></span><span class="line"><span class="cl"><span class="s2">              {&#39;262&#39;: 0.04996,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;5983&#39;: 0.1367,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;2320&#39;: 0.04483,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;111&#39;: 0.06335,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;90017&#39;: 0.2517,
</span></span></span><span class="line"><span class="cl"><span class="s2">               &#39;2588&#39;: 0.3335})],
</span></span></span><span class="line"><span class="cl"><span class="s2"> &#39;colbert_vecs&#39;: None}
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># you can see the weight for each token:</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">convert_id_to_token</span><span class="p">(</span><span class="n">output_1</span><span class="p">[</span><span class="s1">&#39;lexical_weights&#39;</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># [{&#39;What&#39;: 0.08356, &#39;is&#39;: 0.0814, &#39;B&#39;: 0.1296, &#39;GE&#39;: 0.252, &#39;M&#39;: 0.1702, &#39;3&#39;: 0.2695, &#39;?&#39;: 0.04092}, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#  {&#39;De&#39;: 0.05005, &#39;fin&#39;: 0.1368, &#39;ation&#39;: 0.04498, &#39;of&#39;: 0.0633, &#39;BM&#39;: 0.2515, &#39;25&#39;: 0.3335}]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># compute the scores via lexical mathcing</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">compute_lexical_matching_score</span><span class="p">(</span><span class="n">output_1</span><span class="p">[</span><span class="s1">&#39;lexical_weights&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">output_1</span><span class="p">[</span><span class="s1">&#39;lexical_weights&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0</span>
</span></span></code></pre></div><ul>
<li>Multi-Vector (ColBERT)</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sentences_1</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;What is BGE M3?&#34;</span><span class="p">,</span> <span class="s2">&#34;Defination of BM25&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">output_1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentences_1</span><span class="p">,</span> <span class="n">return_dense</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_colbert_vecs</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output_1</span><span class="p">[</span><span class="s1">&#39;colbert_vecs&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (8, 1024)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">colbert_score</span><span class="p">(</span><span class="n">output_1</span><span class="p">[</span><span class="s1">&#39;colbert_vecs&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">output_1</span><span class="p">[</span><span class="s1">&#39;colbert_vecs&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(0.4768)</span>
</span></span></code></pre></div><p>ColBERT score： 在计算完查询和文档之间的相似度矩阵后，对每个查询token选择与文档中token的最大相似度，将所有最大相似度累加，得到最终的colbert_score。</p>
<h3 id="基于llm实现embedding">基于LLM实现embedding<a hidden class="anchor" aria-hidden="true" href="#基于llm实现embedding">#</a></h3>
<p>几个代表模型：</p>
<p><a href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct">Alibaba-NLP/gte-Qwen2-7B-instruct</a></p>
<p><a href="https://huggingface.co/intfloat/e5-mistral-7b-instruct">intfloat/e5-mistral-7b-instruct</a></p>
<p>e5-mistral-7b-instruct论文：<strong><a href="https://arxiv.org/abs/2401.00368">Improving Text Embeddings with Large Language Models</a></strong></p>
<p>config.json:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;_name_or_path&#34;</span><span class="p">:</span> <span class="s2">&#34;mistralai/Mistral-7B-v0.1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;architectures&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;MistralModel&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;bos_token_id&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;eos_token_id&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;hidden_act&#34;</span><span class="p">:</span> <span class="s2">&#34;silu&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;hidden_size&#34;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;initializer_range&#34;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;intermediate_size&#34;</span><span class="p">:</span> <span class="mi">14336</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;max_position_embeddings&#34;</span><span class="p">:</span> <span class="mi">32768</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="o">...</span>
</span></span></code></pre></div><p>通过LLM生成数据（合成数据）， 采用对比学习微调Mistral-7B；</p>
<blockquote>
<p>Given a pretrained LLM, we append an [EOS] token to the end of the query and document, and then feed them into the LLM to obtain the query and document embeddings $(h_{q_{inst}^+}, h_d^+)$, by taking the last layer [EOS] vector.</p>
</blockquote>
<h2 id="附录">附录<a hidden class="anchor" aria-hidden="true" href="#附录">#</a></h2>
<h3 id="如何找到合适的embedding模型和reranker模型">如何找到合适的embedding模型和reranker模型<a hidden class="anchor" aria-hidden="true" href="#如何找到合适的embedding模型和reranker模型">#</a></h3>
<p>首先推荐从 <a href="https://huggingface.co/spaces/mteb/leaderboard">huggingface MTEB Leaderboard</a>上选择排名靠前的模型</p>
<p>其次，实验！根据自己的场景构造测试集验证，才能选择出适合自己场景的embedding模型和reranker模型；</p>
<h3 id="embedding模型和reranker模型的注意事项">embedding模型和reranker模型的注意事项<a hidden class="anchor" aria-hidden="true" href="#embedding模型和reranker模型的注意事项">#</a></h3>
<ol>
<li>
<p>上下文长度限制。 这些模型大多本质是Bert-based 模型， 而Bert的上下文长度限制一般是512，具体可以在huggingface中的<code>config.json</code>中查看；以<a href="https://huggingface.co/BAAI/bge-base-en-v1.5/blob/main/config.json">BAAI/bge-base-en-v1.5</a> 为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;_name_or_path&#34;</span><span class="p">:</span> <span class="s2">&#34;/root/.cache/torch/sentence_transformers/BAAI_bge-base-en/&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;architectures&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;BertModel&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;attention_probs_dropout_prob&#34;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;classifier_dropout&#34;</span><span class="p">:</span> <span class="n">null</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;gradient_checkpointing&#34;</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;hidden_act&#34;</span><span class="p">:</span> <span class="s2">&#34;gelu&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;hidden_dropout_prob&#34;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;hidden_size&#34;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;id2label&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;0&#34;</span><span class="p">:</span> <span class="s2">&#34;LABEL_0&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;initializer_range&#34;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;intermediate_size&#34;</span><span class="p">:</span> <span class="mi">3072</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;label2id&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;LABEL_0&#34;</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;layer_norm_eps&#34;</span><span class="p">:</span> <span class="mf">1e-12</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;max_position_embeddings&#34;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;model_type&#34;</span><span class="p">:</span> <span class="s2">&#34;bert&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;num_attention_heads&#34;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;num_hidden_layers&#34;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;pad_token_id&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;position_embedding_type&#34;</span><span class="p">:</span> <span class="s2">&#34;absolute&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;torch_dtype&#34;</span><span class="p">:</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;transformers_version&#34;</span><span class="p">:</span> <span class="s2">&#34;4.30.0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;type_vocab_size&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;use_cache&#34;</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;vocab_size&#34;</span><span class="p">:</span> <span class="mi">30522</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>其中`<code>&quot;max_position_embeddings&quot;: 512,</code> 也就是它的上下文长度，但输入sequence长度超过512个token，则模型只看到了最前面的512个token；</p>
</li>
<li>
<p>向量维度</p>
</li>
<li>
<p>微调</p>
</li>
</ol>
<h3 id="如何微调embedding-模型">如何微调embedding 模型<a hidden class="anchor" aria-hidden="true" href="#如何微调embedding-模型">#</a></h3>
<p><strong>采用FlagEmbedding，<strong>参考：</strong><a href="https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md">How to fine-tune bge embedding model?</a></strong></p>
<p>微调Sentence Transformer，参考：<strong><a href="https://sbert.net/docs/sentence_transformer/training_overview.html#why-finetune">Training Overview</a></strong></p>
<h3 id="还有哪些类型的reranker模型">还有哪些类型的reranker模型<a hidden class="anchor" aria-hidden="true" href="#还有哪些类型的reranker模型">#</a></h3>
<p><img loading="lazy" src="/img/rag_toolkits/rag_retrieval_colbert.png" alt="截屏2024-07-23 15.57.36.png"  />
</p>
<p>根据ColBert文章</p>
<p>作者将query-document matching 任务（reranking）分类成：</p>
<ol>
<li>
<p>Representation-based Similarity, 双塔，独立对查询和文档进行编码，然后通过计算它们的向量相似度进行匹配， 但query和doc间无更精细的交互。</p>
</li>
<li>
<p>Query-Document interaction： 在查询和文档之间直接进行交互，通常使用CNN来捕捉查询和文档之间的交互特征， 但交互计算复杂度较高，尤其是当查询和文档长度较长时。</p>
</li>
<li>
<p>All-to-All Interaction，基于Bert 的Cross-encoder结构， 查询和文档的每个词与对方的每个词进行全连接交互。这种方式可以捕捉细粒度的交互信息，通常需要较高的计算资源。</p>
</li>
<li>
<p>Late Interaction， 迟交互型，在对查询和文档的每个词进行独立编码后，再进行交互计算。通常使用最大池化（MaxSim）来选择最相关的词进行匹配，从而结合了高效性和细粒度交互的优点。</p>
</li>
</ol>
<p>此外， 当然也有基于LLMs的reranker，参考：<strong><a href="https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker">llm_reranker</a></strong></p>
<p>以 BAAI/bge-reranker-v2-gemma 为例， 只是做这样一件事</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&#34;Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either &#39;Yes&#39; or &#39;No&#39;.&#34;</span>
</span></span></code></pre></div><p>以输出“Yes” 的logit 作为相关分数</p>
<h3 id="对比学习">对比学习<a hidden class="anchor" aria-hidden="true" href="#对比学习">#</a></h3>
<p>在之前不断提到了embedding，当然， 和transformer中的embedding层不是一个东西；</p>
<p>这里所指的embedding是通过对比学习的方式，将语义相近的文本映射到高维参数空间中的相近位置，使它们的距离尽可能短。相反，语义差别大的文本则被映射到参数空间中远离的位置。类似地，这种方法也可以应用于图像（相似图像检索）以及文本和图像的多模态语义匹配（如CLIP）。那么，对比学习有三个核心：loss function，网络架构和采样。</p>
<p>对比学习有三个核心要素：<strong>网络架构、损失函数</strong>（loss function）和<strong>采样策略</strong>。</p>
<p>常见的损失函数有contrastive loss、triplet loss，以及N-pair loss。</p>
<p>网络架构方面,对比学习通常采用双塔(siamese network)或者三塔(triplet)结构。双塔结构包含两个相同或相似的编码器,分别用于处理正样本对;三塔结构则增加了一个用于处理负样本的编码器。这些编码器可以是各种深度学习模型,如CNN、RNN或Transformer等,具体选择取决于任务域和数据类型。这一步将原始较为复杂的数据类型，通过深度学习的方法降到相对较低的维度便于之后的计算。</p>
<p>对于双塔结构， 一般采用Contrastive Loss， 其用于度量两个样本之间的相似度。其公式为：</p>
<p>$$
L = \frac{1}{2N} \sum_{i=1}^N (y_i d_i^2 + (1 - y_i) \max(0, m - d_i)^2)
$$</p>
<p>其中：</p>
<ul>
<li>
<p>$d_i$是样本对$(x_i, x_j)$ 的欧几里得距离。</p>
</li>
<li>
<p>$y_i$ 是样本对的标签，1 表示正样本对，0 表示负样本对。</p>
</li>
<li>
<p>m 是一个超参数，表示负样本对之间的最小距离，也称margin。</p>
</li>
</ul>
<p>Triplet Loss则使用于triplet-network结构, 其通过构造三元组 ( anchor, positive, negative) 来训练模型，使得正样本（positive）与锚点（anchor）的距离小于负样本（negative）与锚点的距离。其公式为：</p>
<p>$$
L = \sum_{i=1}^N \left[ |f(x_i^a) - f(x_i^p)|_2^2 - |f(x_i^a) - f(x_i^n)|_2^2 + \alpha \right]
$$</p>
<p>其中：</p>
<ul>
<li>
<p>$f(x)$ 表示样本 x  的特征向量。</p>
</li>
<li>
<p>$\alpha$是一个超参数，表示正负样本对之间的最小距离差。</p>
</li>
</ul>
<p>N-pair Loss 是 Triplet Loss 的推广，通过引入多个负样本来提升训练效果。</p>
<p>$$
L = \sum_{i=1}^N \log \left( 1 + \sum_{j \neq i} \exp \left( f(x_i)^\top f(x_j^n) - f(x_i)^\top f(x_i^p) \right) \right)
$$</p>
<p>在训练过程中， 采样策略则很关键；但运气不好， 选择的样本都是Easy Negative mining，也就是$d(a,p)+margin&lt;d(a,n)$，则模型能够很快收敛，但什么也没有学会，相反在另一个极端中， 采用的多是hard negative mining，也就是$d(a,n)&lt;d(a,p)$的情况，此时模型训练将很难收敛；实际情况中， 往往采用semi-hard 或者是distance-weighted sampling 来寻找难度适中的负样本,以避免模型过于关注极端情况。而由于在高维空间下， 因为高维空间中大部分体积都集中在高维球壳上，所以随机采样得到的点之间的距离往往非常接近，这也是不采用随机采样的策略的原因（<a href="https://ar5iv.labs.arxiv.org/html/1706.07567">Sampling Matters in Deep Embedding Learning</a>）。</p>
<h3 id="向量库是个什么东西">向量库是个什么东西<a hidden class="anchor" aria-hidden="true" href="#向量库是个什么东西">#</a></h3>
<p>向量库不是“库”，但也是“库”；</p>
<p>考虑到传统关系型数据库， 数据以表格形式存储， 但在AI时代， 大量非结构化数据的存储和查询，以表格形式存在这不太合理了；因为数据维度高， 需要一些特殊的查询方式；</p>
<p>不同于传统数据库的精确匹配查询,向量库主要用于相似性搜索。这种搜索基于向量间的距离,通常使用欧几里得距离或余弦相似度。以暴力遍历的最近邻查询为例(Nearest neighbor search),假设我们有n个d维向量,查询复杂度为$O(nd)$。当n和d都很大时,这种方法变得非常耗时。</p>
<p>而近似最近邻搜索(Approximate Nearest Neighbor search, ANN)则能将时间复杂度降低到亚线性,通常为$O(log n)$或更优。</p>
<p>常见的ANN算法：</p>
<ol>
<li>
<p>基于树的：k-D树(k-D tree):将空间递归地划分为子空间,适用于低维数据。</p>
</li>
<li>
<p>基于哈希的：局部敏感哈希(LSH, Locality-Sensitive Hashing):将相似的向量映射到相同的&quot;桶&quot;中。</p>
</li>
<li>
<p>乘积量化(Product Quantization):将高维向量分解为低维子向量,并对每个子向量进行量化。</p>
</li>
<li>
<p>基于Graph的：分层可导航小世界图(HNSW, Hierarchical Navigable Small World):构建多层图结构,在图上进行快速导航搜索。</p>
</li>
</ol>
<p>而以上算法的思路都可以归介于—将相似的样本放在一起（减少搜索空间）， 和构建高效的搜索结构。</p>
<hr>
<h2 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h2>
<p><a href="https://blog.elicit.com/search-vs-vector-db/">Build a search engine, not a vector DB</a></p>
<p><a href="https://weaviate.io/blog/hybrid-search-explained">Hybrid Search Explained</a></p>
<p><a href="https://mp.weixin.qq.com/s/NFjn8pUsQaSx85nhBphORA">再谈大模型RAG问答中的三个现实问题：兼看RAG-Fusion多query融合策略、回答引文生成策略及相关数据集概述</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/659934962">LLM - RAG文档应用处理与召回经验之谈</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/680592204">[RAG] BGE M3-Embedding | 什么？我的RAG底座模型又要换了？</a></p>
<p><a href="https://medium.com/@varun030403/colbert-a-complete-guide-1552468335ae">ColBERT: A complete guide</a></p>
<p><a href="https://github.com/wangshusen/SearchEngine/tree/main">搜索引擎技术</a></p>
<p><a href="https://ar5iv.labs.arxiv.org/html/2403.00784v1">Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges</a></p>
<p><a href="https://arxiv.org/abs/2004.12832">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</a></p>
<p><a href="https://github.com/RUC-NLPIR/LLM4IR-Survey">LLM4IR-Survey</a></p>
<p><a href="https://arxiv.org/abs/2310.08319">Fine-Tuning LLaMA for Multi-Stage Text Retrieval</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://niraya666.github.io/tags/rag/">RAG</a></li>
      <li><a href="https://niraya666.github.io/tags/rag-toolkits/">RAG-Toolkits</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://niraya666.github.io/posts/agent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%A6%82%E4%BD%95%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B%E7%9A%84tool-using%E8%83%BD%E5%8A%9B/">
    <span class="title">Next »</span>
    <br>
    <span>Agent学习笔记： 如何验证模型的tool-using能力</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：检索 on x"
            href="https://x.com/intent/tweet/?text=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e6%a3%80%e7%b4%a2&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E6%25A3%2580%25E7%25B4%25A2%2f&amp;hashtags=RAG%2cRAG-Toolkits">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：检索 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E6%25A3%2580%25E7%25B4%25A2%2f&amp;title=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e6%a3%80%e7%b4%a2&amp;summary=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e6%a3%80%e7%b4%a2&amp;source=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E6%25A3%2580%25E7%25B4%25A2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：检索 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E6%25A3%2580%25E7%25B4%25A2%2f&title=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e6%a3%80%e7%b4%a2">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：检索 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E6%25A3%2580%25E7%25B4%25A2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：检索 on whatsapp"
            href="https://api.whatsapp.com/send?text=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e6%a3%80%e7%b4%a2%20-%20https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E6%25A3%2580%25E7%25B4%25A2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：检索 on telegram"
            href="https://telegram.me/share/url?text=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e6%a3%80%e7%b4%a2&amp;url=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E6%25A3%2580%25E7%25B4%25A2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share RAG工具箱：检索 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=RAG%e5%b7%a5%e5%85%b7%e7%ae%b1%ef%bc%9a%e6%a3%80%e7%b4%a2&u=https%3a%2f%2fniraya666.github.io%2fposts%2frag%25E6%25A3%2580%25E7%25B4%25A2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="utterances">
  <script src="https://utteranc.es/client.js"
        repo="https://github.com/Niraya666/niraya666.github.io.git"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://niraya666.github.io/">LZY Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
